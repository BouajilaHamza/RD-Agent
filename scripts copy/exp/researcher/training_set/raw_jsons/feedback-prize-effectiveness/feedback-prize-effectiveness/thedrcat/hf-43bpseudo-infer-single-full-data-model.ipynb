{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"cfg = {\n    \"num_proc\": 2,\n    \"k_folds\": 1,\n    \"max_length\": 2048,\n    \"padding\": False,\n    \"stride\": 0,\n    \"data_dir\": \"../input/feedback-prize-effectiveness\",\n    \"load_from_disk\": None,\n    \"pad_multiple\": 8,\n    \"model_name_or_path\": \"../input/deberta-v3-large/deberta-v3-large\",\n    \"dropout\": 0.0,\n    \"trainingargs\": {\n        \"output_dir\": f\"../output\",\n        \"do_train\": True,\n        \"do_eval\": True,\n        \"per_device_train_batch_size\": 8,\n        \"per_device_eval_batch_size\": 1,\n        \"learning_rate\": 9e-6,\n        \"weight_decay\": 0.01,\n        \"num_train_epochs\": 3,\n        \"warmup_ratio\": 0.1,\n        \"optim\": 'adamw_torch',\n        \"logging_steps\": 50,\n        \"save_strategy\": \"epoch\",\n        \"evaluation_strategy\": \"epoch\",\n        \"report_to\": \"wandb\",\n        \"group_by_length\": True,\n        \"save_total_limit\": 1,\n        \"metric_for_best_model\": \"loss\",\n        \"greater_is_better\": False,\n        \"seed\": 42,\n        \"fp16\": True,\n        \"gradient_checkpointing\": True,\n        \"gradient_accumulation_steps\": 1,\n    }\n}","metadata":{"execution":{"iopub.status.busy":"2022-07-27T23:41:38.032955Z","iopub.execute_input":"2022-07-27T23:41:38.033491Z","iopub.status.idle":"2022-07-27T23:41:38.046592Z","shell.execute_reply.started":"2022-07-27T23:41:38.033432Z","shell.execute_reply":"2022-07-27T23:41:38.045584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport pickle\nimport codecs\nimport warnings\nimport logging\nfrom functools import partial\nfrom pathlib import Path\nfrom itertools import chain\nfrom text_unidecode import unidecode\nfrom typing import Any, Optional, Tuple\n\nimport pandas as pd\nfrom sklearn.model_selection import KFold\nfrom transformers import AutoTokenizer, set_seed\n\nfrom datasets import Dataset, load_from_disk\n\ndef replace_encoding_with_utf8(error: UnicodeError) -> Tuple[bytes, int]:\n    return error.object[error.start : error.end].encode(\"utf-8\"), error.end\n\ndef replace_decoding_with_cp1252(error: UnicodeError) -> Tuple[str, int]:\n    return error.object[error.start : error.end].decode(\"cp1252\"), error.end\n\ncodecs.register_error(\"replace_encoding_with_utf8\", replace_encoding_with_utf8)\ncodecs.register_error(\"replace_decoding_with_cp1252\", replace_decoding_with_cp1252)\n\ndef resolve_encodings_and_normalize(text: str) -> str:\n    text = (\n        text.encode(\"raw_unicode_escape\")\n        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n        .encode(\"cp1252\", errors=\"replace_encoding_with_utf8\")\n        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n    )\n    text = unidecode(text)\n    return text\n\ndef read_text_files(example, data_dir):\n    \n    id_ = example[\"essay_id\"]\n    \n    with open(data_dir / \"test\" / f\"{id_}.txt\", \"r\") as fp:\n        example[\"text\"] = resolve_encodings_and_normalize(fp.read())\n    \n    return example\n\nset_seed(cfg[\"trainingargs\"][\"seed\"])\n\nwarnings.simplefilter('ignore')\nlogging.disable(logging.WARNING)","metadata":{"execution":{"iopub.status.busy":"2022-07-27T23:41:38.799228Z","iopub.execute_input":"2022-07-27T23:41:38.799821Z","iopub.status.idle":"2022-07-27T23:41:43.339418Z","shell.execute_reply.started":"2022-07-27T23:41:38.799783Z","shell.execute_reply":"2022-07-27T23:41:43.338419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_dir = Path(cfg[\"data_dir\"])\n\nif cfg[\"load_from_disk\"]:\n    if not cfg[\"load_from_disk\"].endswith(\".dataset\"):\n        cfg[\"load_from_disk\"] += \".dataset\"\n    ds = load_from_disk(cfg[\"load_from_disk\"])\n    \n    pkl_file = f\"{cfg['load_from_disk'][:-len('.dataset')]}_pkl\"\n    with open(pkl_file, \"rb\") as fp: \n        grouped = pickle.load(fp)\n        \n    print(\"loading from saved files\")\nelse:\n    train_df = pd.read_csv(data_dir / \"test.csv\")\n            \n    text_ds = Dataset.from_dict({\"essay_id\": train_df.essay_id.unique()})\n    \n    text_ds = text_ds.map(\n        partial(read_text_files, data_dir=data_dir),\n        num_proc=cfg[\"num_proc\"],\n        batched=False,\n        desc=\"Loading text files\",\n    )\n    \n    text_df = text_ds.to_pandas()\n    \n    train_df[\"discourse_text\"] = [\n        resolve_encodings_and_normalize(x) for x in train_df[\"discourse_text\"]\n    ]\n    \n    train_df = train_df.merge(text_df, on=\"essay_id\", how=\"left\")\n    \ndisc_types = [\n    \"Claim\",\n    \"Concluding Statement\",\n    \"Counterclaim\",\n    \"Evidence\",\n    \"Lead\",\n    \"Position\",\n    \"Rebuttal\",\n]\n\ncls_tokens_map = {label: f\"[CLS_{label.upper()}]\" for label in disc_types}\nend_tokens_map = {label: f\"[END_{label.upper()}]\" for label in disc_types}\n\nlabel2id = {\n    \"Adequate\": 0,\n    \"Effective\": 1,\n    \"Ineffective\": 2,\n}\n\ntokenizer = AutoTokenizer.from_pretrained(cfg[\"model_name_or_path\"])\ntokenizer.add_special_tokens(\n    {\"additional_special_tokens\": list(cls_tokens_map.values())+list(end_tokens_map.values())}\n)\n\ncls_id_map = {\n    label: tokenizer.encode(tkn)[1]\n    for label, tkn in cls_tokens_map.items()\n}","metadata":{"execution":{"iopub.status.busy":"2022-07-27T23:41:49.251835Z","iopub.execute_input":"2022-07-27T23:41:49.252108Z","iopub.status.idle":"2022-07-27T23:41:50.515728Z","shell.execute_reply.started":"2022-07-27T23:41:49.252074Z","shell.execute_reply":"2022-07-27T23:41:50.514447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def find_positions(example):\n\n    text = example[\"text\"][0]\n    \n    # keeps track of what has already\n    # been located\n    min_idx = 0\n    \n    # stores start and end indexes of discourse_texts\n    idxs = []\n    \n    for dt in example[\"discourse_text\"]:\n        # calling strip is essential\n        matches = list(re.finditer(re.escape(dt.strip()), text))\n        \n        # If there are multiple matches, take the first one\n        # that is past the previous discourse texts.\n        if len(matches) > 1:\n            for m in matches:\n                if m.start() >= min_idx:\n                    break\n        # If no matches are found\n        elif len(matches) == 0:\n            idxs.append([-1]) # will filter out later\n            continue  \n        # If one match is found\n        else:\n            m = matches[0]\n            \n        idxs.append([m.start(), m.end()])\n\n        min_idx = m.start()\n\n    return idxs\n\ndef tokenize(example):\n    example[\"idxs\"] = find_positions(example)\n\n    text = example[\"text\"][0]\n    text = text.replace('\\n', '|')\n    chunks = []\n    labels = []\n    prev = 0\n\n    zipped = zip(\n        example[\"idxs\"],\n        example[\"discourse_type\"],\n#         example[\"discourse_effectiveness\"],\n    )\n    for idxs, disc_type in zipped:\n        \n        disc_effect = 'Effective'\n        # when the discourse_text wasn't found\n        if idxs == [-1]:\n            continue\n\n        s, e = idxs\n\n        # if the start of the current discourse_text is not \n        # at the end of the previous one.\n        # (text in between discourse_texts)\n        if s != prev:\n            chunks.append(text[prev:s])\n            prev = s\n\n        # if the start of the current discourse_text is \n        # the same as the end of the previous discourse_text\n        if s == prev:\n            chunks.append(cls_tokens_map[disc_type])\n            chunks.append(text[s:e])\n            chunks.append(end_tokens_map[disc_type])\n        \n        prev = e\n\n        labels.append(label2id[disc_effect])\n\n    tokenized = tokenizer(\n        \" \".join(chunks),\n        padding=False,\n        truncation=True,\n        max_length=cfg[\"max_length\"],\n        add_special_tokens=True,\n    )\n    \n    # at this point, labels is not the same shape as input_ids.\n    # The following loop will add -100 so that the loss function\n    # ignores all tokens except CLS tokens\n\n    # idx for labels list\n    idx = 0\n    final_labels = []\n    for id_ in tokenized[\"input_ids\"]:\n        # if this id belongs to a CLS token\n        if id_ in cls_id_map.values():\n            final_labels.append(labels[idx])\n            idx += 1\n        else:\n            # -100 will be ignored by loss function\n            final_labels.append(-100)\n    \n    tokenized[\"labels\"] = final_labels\n\n    return tokenized","metadata":{"execution":{"iopub.status.busy":"2022-07-27T23:41:54.594051Z","iopub.execute_input":"2022-07-27T23:41:54.594611Z","iopub.status.idle":"2022-07-27T23:41:54.609947Z","shell.execute_reply.started":"2022-07-27T23:41:54.594569Z","shell.execute_reply":"2022-07-27T23:41:54.608751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# I frequently restart my notebook, so to reduce time\n# you can set this to just load the tokenized dataset from disk.\n# It gets loaded in the 3rd code cell, but a check is done here\n# to skip tokenizing\nif cfg[\"load_from_disk\"] is None:\n\n    # make lists of discourse_text, discourse_effectiveness\n    # for each essay\n    grouped = train_df.groupby([\"essay_id\"]).agg(list)\n\n    ds = Dataset.from_pandas(grouped)\n\n    ds = ds.map(\n        tokenize,\n        batched=False,\n        num_proc=cfg[\"num_proc\"],\n        desc=\"Tokenizing\",\n    )\n\n    save_dir = f\"{cfg['trainingargs']['output_dir']}\"\n    ds.save_to_disk(f\"{save_dir}.dataset\")\n    with open(f\"{save_dir}_pkl\", \"wb\") as fp:\n        pickle.dump(grouped, fp)\n    print(\"Saving dataset to disk:\", cfg['trainingargs']['output_dir'])","metadata":{"execution":{"iopub.status.busy":"2022-07-27T23:41:56.851708Z","iopub.execute_input":"2022-07-27T23:41:56.852222Z","iopub.status.idle":"2022-07-27T23:41:56.934128Z","shell.execute_reply.started":"2022-07-27T23:41:56.852163Z","shell.execute_reply":"2022-07-27T23:41:56.933244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"disc_ids = [x for z in ds['discourse_id'] for x in z]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bad_matches = []\ncls_ids = set(list(cls_id_map.values()))\nfor id_, l, ids, dt in zip(ds[\"essay_id\"], ds[\"labels\"], ds[\"input_ids\"], grouped.discourse_text):\n    \n    # count number of labels (ignoring -100)\n    num_cls_label = sum([x!=-100 for x in l])\n    # count number of cls ids\n    num_cls_id = sum([x in cls_ids for x in ids])\n    # true number of discourse_texts\n    num_dt = len(dt)\n    \n    if num_cls_label != num_dt or num_cls_id != num_dt:\n        bad_matches.append((id_, l, ids, dt))\n        \nprint(\"Num bad matches\", len(bad_matches))\n# temp = train_df[train_df[\"essay_id\"]==bad_matches[0][0]]\n# temp_txt = temp.text.values[0]\n# print(temp_txt)\n# print(\"*\"*100)\n# print([x for x in temp.discourse_text if x.strip() not in temp_txt])","metadata":{"execution":{"iopub.status.busy":"2022-07-27T23:42:02.303566Z","iopub.execute_input":"2022-07-27T23:42:02.303839Z","iopub.status.idle":"2022-07-27T23:42:02.316782Z","shell.execute_reply.started":"2022-07-27T23:42:02.303808Z","shell.execute_reply":"2022-07-27T23:42:02.315575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"assert len(bad_matches) == 0","metadata":{"execution":{"iopub.status.busy":"2022-07-27T23:42:13.288806Z","iopub.execute_input":"2022-07-27T23:42:13.289262Z","iopub.status.idle":"2022-07-27T23:42:13.294675Z","shell.execute_reply.started":"2022-07-27T23:42:13.289224Z","shell.execute_reply":"2022-07-27T23:42:13.293756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\nimport torch\nfrom transformers import Trainer, TrainingArguments, AutoConfig, AutoModelForTokenClassification, DataCollatorForTokenClassification\nfrom torch.utils.checkpoint import checkpoint\nimport wandb\n\nargs = TrainingArguments(**cfg[\"trainingargs\"])\n\n# if using longformer pad to multiple of 512\n# for others pad to multiple of 8\n\ncollator = DataCollatorForTokenClassification(\n    tokenizer=tokenizer, pad_to_multiple_of=cfg[\"pad_multiple\"], padding=True\n)\n\noutput = args.output_dir\n\nfold_preds = []\n\nfor fold in range(cfg[\"k_folds\"]):\n    \n    args.output_dir = f\"{output}-fold{fold}\"\n    \n    model_config = AutoConfig.from_pretrained(\n        cfg[\"model_name_or_path\"],\n    )\n    model_config.update(\n        {\n            \"num_labels\": 3,\n            \"cls_tokens\": list(cls_id_map.values()),\n            \"label2id\": label2id,\n            \"id2label\": {v:k for k, v in label2id.items()},\n        }\n    )\n    \n    model = AutoModelForTokenClassification.from_pretrained(cfg[\"model_name_or_path\"], config=model_config)\n    \n    # need to resize embeddings because of added tokens\n    model.resize_token_embeddings(len(tokenizer))\n    \n    PATH = f'../input/hf-43b-pseudo-download/pytorch_model_1.bin'\n    \n    model.load_state_dict(torch.load(PATH))\n    \n    # split dataset to train and eval\n    keep_cols = {\"input_ids\", \"attention_mask\", \"labels\"}\n    test_dataset = ds.remove_columns([c for c in ds.column_names if c not in keep_cols])\n    \n    trainer = Trainer(\n        model=model,\n        args=args,\n        tokenizer=tokenizer,\n        data_collator=collator,\n    )\n    \n    preds = trainer.predict(test_dataset)\n    fold_preds.append(preds.predictions)\n    \n    del model\n    gc.collect()\n    torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2022-07-27T23:45:33.729052Z","iopub.execute_input":"2022-07-27T23:45:33.730021Z","iopub.status.idle":"2022-07-27T23:47:06.459453Z","shell.execute_reply.started":"2022-07-27T23:45:33.729977Z","shell.execute_reply":"2022-07-27T23:47:06.458526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np","metadata":{"execution":{"iopub.status.busy":"2022-07-27T23:43:08.744114Z","iopub.execute_input":"2022-07-27T23:43:08.744414Z","iopub.status.idle":"2022-07-27T23:43:08.749212Z","shell.execute_reply.started":"2022-07-27T23:43:08.744377Z","shell.execute_reply":"2022-07-27T23:43:08.748153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(fold_preds), fold_preds[0].shape","metadata":{"execution":{"iopub.status.busy":"2022-07-27T23:47:29.320248Z","iopub.execute_input":"2022-07-27T23:47:29.320568Z","iopub.status.idle":"2022-07-27T23:47:29.333386Z","shell.execute_reply.started":"2022-07-27T23:47:29.320531Z","shell.execute_reply":"2022-07-27T23:47:29.33241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = np.stack(fold_preds).mean(axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-07-27T23:48:58.613209Z","iopub.execute_input":"2022-07-27T23:48:58.613495Z","iopub.status.idle":"2022-07-27T23:48:58.619308Z","shell.execute_reply.started":"2022-07-27T23:48:58.613463Z","shell.execute_reply":"2022-07-27T23:48:58.618246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_torch = torch.tensor(preds, dtype=torch.float32)\npreds_torch.shape","metadata":{"execution":{"iopub.status.busy":"2022-07-27T23:49:13.789276Z","iopub.execute_input":"2022-07-27T23:49:13.789738Z","iopub.status.idle":"2022-07-27T23:49:13.804612Z","shell.execute_reply.started":"2022-07-27T23:49:13.789695Z","shell.execute_reply":"2022-07-27T23:49:13.803483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tokenizer = AutoTokenizer.from_pretrained('../input/hffbcktokenizer')","metadata":{"execution":{"iopub.status.busy":"2022-07-27T23:44:03.927274Z","iopub.execute_input":"2022-07-27T23:44:03.927767Z","iopub.status.idle":"2022-07-27T23:44:03.932133Z","shell.execute_reply.started":"2022-07-27T23:44:03.927728Z","shell.execute_reply":"2022-07-27T23:44:03.931205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_preds = []\n\nfor i in range(len(test_dataset)):\n    indices = np.array(test_dataset[i]['labels']) == 1\n    mypreds = preds_torch[i][:len(indices),:][indices]\n    mypreds = torch.nn.functional.softmax(mypreds, dim=-1)\n    all_preds.append(mypreds)\n    \nall_preds = torch.cat(all_preds, dim=0).numpy()\nall_preds.shape","metadata":{"execution":{"iopub.status.busy":"2022-07-27T23:49:19.402915Z","iopub.execute_input":"2022-07-27T23:49:19.403272Z","iopub.status.idle":"2022-07-27T23:49:19.425728Z","shell.execute_reply.started":"2022-07-27T23:49:19.403223Z","shell.execute_reply":"2022-07-27T23:49:19.424738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.read_csv('../input/feedback-prize-effectiveness/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-07-27T23:49:20.602834Z","iopub.execute_input":"2022-07-27T23:49:20.603301Z","iopub.status.idle":"2022-07-27T23:49:20.612449Z","shell.execute_reply.started":"2022-07-27T23:49:20.603248Z","shell.execute_reply":"2022-07-27T23:49:20.611301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub[sub.columns[-3:]] = all_preds[:,[2,0,1]]","metadata":{"execution":{"iopub.status.busy":"2022-07-27T23:49:22.389525Z","iopub.execute_input":"2022-07-27T23:49:22.389864Z","iopub.status.idle":"2022-07-27T23:49:22.396706Z","shell.execute_reply.started":"2022-07-27T23:49:22.389818Z","shell.execute_reply":"2022-07-27T23:49:22.395662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub['discourse_id'] = disc_ids","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-07-27T23:49:22.398714Z","iopub.execute_input":"2022-07-27T23:49:22.399093Z","iopub.status.idle":"2022-07-27T23:49:22.408225Z","shell.execute_reply.started":"2022-07-27T23:49:22.399048Z","shell.execute_reply":"2022-07-27T23:49:22.407368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.head()","metadata":{"execution":{"iopub.status.busy":"2022-07-27T23:49:25.981175Z","iopub.execute_input":"2022-07-27T23:49:25.982048Z","iopub.status.idle":"2022-07-27T23:49:25.997962Z","shell.execute_reply.started":"2022-07-27T23:49:25.982006Z","shell.execute_reply":"2022-07-27T23:49:25.997019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}