{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"* v01 : 999_v1_02 : 99_v1_02_22(oof=0.5814, LB=0.575), use discourse_type as feature, catboost, oof=0.5805, LB=0.573  \n  \n  --- [BUG] order of 1st level predictions are not correct ... ---  \n  --- This is corrected below ---  \n  \n* v02 : 999_v1_02 : 99_v1_02_22(oof=0.5814, LB=0.575), use discourse_type as feature, catboost, oof=0.5805, LB=0.572  \n* v03 : 999_v1_05 : 99_v1_02_27(oof=0.5803, LB=0.575), use discourse_type as feature, catboost, oof=0.5796, LB=0.572  \n* v04 : 999_v1_06 : 99_v1_02_28(oof=0.5805, LB=?), use discourse_type as feature, catboost, oof=0.5786, LB=0.573  \n* v06 : 999_v1_15 : 99_v1_04_01(oof=0.5797, LB=), use discourse_type as feature, catboost, oof=0.5788, LB=0.571  \n* v07 : 999_v1_15+all data : 99_v1_04_01(oof=0.5797, LB=), use discourse_type as feature, catboost, oof=0.5788 | replace 18_v2_01 with 24_v2_01, LB=0.570  \n* v08 : 999_v1_16+all data : 99_v1_04_02(oof=0.5798, LB=), use discourse_type as feature, catboost, oof=0.5789 | replace 18_v2_01 with 24_v2_01(deberta-large), LB=0.570  \n  \n  \n  --- legit pl ---  \n  \n* [BUG] v10 : 999_v1_22 : 29_v2_01(oof=0.5870, LB=), use Shujun's features, catboost, oof=0.5840, LB=0.570  \n* [BUG] v11 : 999_v1_22 : 29_v2_01(oof=0.5870, LB=), use Shujun's features, catboost, oof=0.5840, LB=0.570  \n  \n  \n  --- bug corrected (take average for prob_seq) ---  \n* v13 : 999_v1_22 : 29_v2_01(oof=0.5870, LB=), use Shujun's features, catboost, oof=0.5840, LB=  \n* v14 : 999_v1_22 with avg(5x5) : 29_v2_01(oof=0.5870, LB=), use Shujun's features, catboost, oof=0.5840, LB=0.568  \n* v15 : 99_v1_06_02 with avg(5x5) : 999_v1_06_01(single stacking, deberta-large, seed100) + 999_v1_06_02(single stacking, deberta-v3-large, seed100), oof=0.5777, LB=0.564  \n* v16 : 99_v1_06_06 : 999_v1_22(single stacking, deberta-large, seed100) + 999_v1_23(single stacking, deberta-v3-large, seed100) + 999_v1_25(single stacking, deberta-xlarge, seed100), oof=0.5756, avg(5x5) LB=  \n* v17 : 99_v1_06_07 : 999_v1_22(single stacking, deberta-large, seed100) cat&lgb + 999_v1_23(single stacking, deberta-v3-large, seed100) cat&lgb + 999_v1_25(single stacking, deberta-xlarge, seed100) cat&lgb, oof=0.5751, avg(5x5xnum_models) LB=0.564  \n  \n  \n  --- 2nd round pl ---  \n* v18 : 99_v1_06_10 : 999_v1_26(single stacking, deberta-large, seed100) + 999_v1_27(single stacking, deberta-v3-large, seed100), oof=0.5787, avg(5x5xnum_models) LB=0.565  \n  \n  \n  --- all data ---  \n* v19 : 999_v1_22(replace by all_data 32_v2_01(deberta-large), SWA(epoch1,2,3)) with avg(5x5) : 29_v2_01(oof=0.5870, LB=), use Shujun's features, catboost, LB=0.566  \n* v21 : 999_v1_22(replace by all_data 32_v2_01(deberta-large), SWA(epoch1,2)) with avg(5x5) : 29_v2_01(oof=0.5870, LB=), use Shujun's features, catboost, LB=0.571  \n* v22 : 99_v1_06_02(replaced by all_data 32_v2_01(deberta-large) + 32_vl_01(deberta-v3-large)) : 999_v1_22(single stacking, deberta-large, seed100) + 999_v1_23(single stacking, deberta-v3-large, seed100), avg(5x5xnum_models) LB=0.563  \n* v23 : 99_v1_06_06(replaced by all_data 32_v2_01(deberta-large) + 32_vl_01(deberta-v3-large) + 32_v2_02(deberta-xlarge)) : 999_v1_22(single stacking, deberta-large, seed100) + 999_v1_23(single stacking, deberta-v3-large, seed100) + 999_v1_25(single stacking, deberta-xlarge, seed100), avg(5x5xnum_models) LB=0.566  \n  \n  \n  --- rnn w/ 2nd round pl ---  \n* v25 : 999_v1_29 : 34_v2_02(CV=0.5865, LB=), use Shujun's features, catboost, CV=0.5836, avg(5x5xnum_models) LB=0.568  \n* v26 : 99_v1_06_17 : 999_v1_22(single stacking, deberta-large, seed100) + 999_v1_23(single stacking, deberta-v3-large, seed100) + 999_v1_29(single stacking, deberta-large, seed100, lstm, 2nd pl) + 999_v1_30(single stacking, deberta-v3-large, seed100, lstm, 2nd pl), oof=0.5753, avg(5x5xnum_models) LB=0.563  \n  \n  \n  --- sw(768,512,128) at inference + weight tuning ---  \n* v27 : 99_v1_07_01 : 999_v1_31(single stacking, deberta-large, seed100, sw(768,512,128)) + 999_v1_32(single stacking, deberta-v3-large, seed100, sw(768,512,128)), w=(0.43203959 0.56796041), oof=0.5748, avg(5x5xnum_models) LB=0.563  \n* v28 : 999_v1_34 : 34_vl_01(sw768-512-128, CV=0.5769, LB=), use Shujun's features, lgb, oof=0.5751, avg(5x5) LB=  \n  \n  \n  --- best weight tuning ---  \n* v29 : 34_v2_02(deberta-large, seed100, sw(768,512,128)) + 34_vl_01(deberta-v3-large, seed100, sw(768,512,128)) + 999_v1_22(single stacking, deberta-large, seed100) + 999_v1_23(single stacking, deberta-v3-large, seed100), w=(0.23678063 0.42236033 0.14750601 0.19335303), oof=0.5702, LB=  ","metadata":{}},{"cell_type":"markdown","source":"# Scripts","metadata":{}},{"cell_type":"code","source":"# https://www.kaggle.com/datasets/nbroad/deberta-v2-3-fast-tokenizer\n# The following is necessary if you want to use the fast tokenizer for deberta v2 or v3\n# This must be done before importing transformers\nimport shutil\nfrom pathlib import Path\n\ntransformers_path = Path(\"/opt/conda/lib/python3.7/site-packages/transformers\")\n\ninput_dir = Path(\"../input/deberta-v2-3-fast-tokenizer\")\n\nconvert_file = input_dir / \"convert_slow_tokenizer.py\"\nconversion_path = transformers_path/convert_file.name\n\nif conversion_path.exists():\n    conversion_path.unlink()\n\nshutil.copy(convert_file, transformers_path)\ndeberta_v2_path = transformers_path / \"models\" / \"deberta_v2\"\n\nfor filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py']:\n    filepath = deberta_v2_path/filename\n    if filepath.exists():\n        filepath.unlink()\n\n    shutil.copy(input_dir/filename, filepath)","metadata":{"execution":{"iopub.status.busy":"2022-08-22T02:48:34.247867Z","iopub.execute_input":"2022-08-22T02:48:34.248392Z","iopub.status.idle":"2022-08-22T02:48:34.292856Z","shell.execute_reply.started":"2022-08-22T02:48:34.248297Z","shell.execute_reply":"2022-08-22T02:48:34.291914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import transformers\nprint(transformers.__name__, transformers.__version__)","metadata":{"execution":{"iopub.status.busy":"2022-08-22T02:48:34.29541Z","iopub.execute_input":"2022-08-22T02:48:34.296146Z","iopub.status.idle":"2022-08-22T02:48:40.950896Z","shell.execute_reply.started":"2022-08-22T02:48:34.29611Z","shell.execute_reply":"2022-08-22T02:48:40.949433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile models.py\n\nimport torch\n\ndef to_gpu(data):\n    '''\n    https://www.kaggle.com/code/tascj0/a-text-span-detector\n    '''\n    if isinstance(data, dict):\n        return {k: to_gpu(v) for k, v in data.items()}\n    elif isinstance(data, list):\n        return [to_gpu(v) for v in data]\n    elif isinstance(data, torch.Tensor):\n        return data.cuda()\n    else:\n        return data\n\n\ndef to_np(t):\n    '''\n    https://www.kaggle.com/code/tascj0/a-text-span-detector\n    '''\n    if isinstance(t, torch.Tensor):\n        return t.data.cpu().numpy()\n    else:\n        return t\n\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom transformers import (AutoConfig, AutoModel, AutoTokenizer, AutoModelForTokenClassification)\nfrom sklearn.metrics import log_loss\n\nclass ResidualLSTM(nn.Module):\n    '''Based on Shujun's code'''\n    def __init__(self, d_model, rnn='GRU'):\n        super(ResidualLSTM, self).__init__()\n        self.downsample=nn.Linear(d_model,d_model//2)\n        if rnn=='GRU':\n            self.LSTM=nn.GRU(d_model//2, d_model//2, num_layers=2, bidirectional=False, dropout=0.2)\n        else:\n            self.LSTM=nn.LSTM(d_model//2, d_model//2, num_layers=2, bidirectional=False, dropout=0.2)\n        self.linear=nn.Linear(d_model//2, d_model)\n        self.norm= nn.LayerNorm(d_model)\n    def forward(self, x):\n        res=x\n        x=self.downsample(x)\n        x, _ = self.LSTM(x)\n        x = self.linear(x)\n        x=res+x\n        return self.norm(x)\n    \nclass LSTMBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, num_layers=1, p_drop=0):\n        super().__init__()\n        self.lstm = nn.LSTM(input_size=in_channels,\n                             hidden_size=out_channels,\n                             num_layers=num_layers,\n                             dropout=p_drop,\n                             batch_first=True, \n                             bidirectional=True)\n    def forward(self, x): #(bs,num_tokens,hidden_size)\n        x,_ = self.lstm(x)\n        return x\n    \nclass GRUBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, num_layers=1, p_drop=0):\n        super().__init__()\n        self.lstm = nn.GRU(input_size=in_channels,\n                           hidden_size=out_channels,\n                           num_layers=num_layers,\n                           dropout=p_drop,\n                           batch_first=True, \n                           bidirectional=True)\n    def forward(self, x): #(bs,num_tokens,hidden_size)\n        x,_ = self.lstm(x)\n        return x\n    \n    \nclass TransformerBlock(nn.Module):\n    def __init__(self, in_channels, num_layers=1, nhead=8):\n        super().__init__()\n        self.transformer = nn.TransformerEncoder(encoder_layer=nn.TransformerEncoderLayer(d_model=in_channels,nhead=nhead),\n                                                 num_layers=num_layers)\n    def forward(self, x):\n        x = self.transformer(x)\n        return x\n    \n    \nclass Model(nn.Module):\n    def __init__(self, \n                 model_name, \n                 tokenizer,\n                 num_labels, \n                 num_labels_2,\n                 rnn='none',\n                 loss='mse',\n                 head='simple',\n                 multi_layers=1,\n                 p_drop=0,\n                 l2norm='false',\n                 s=30,\n                 freeze_layers='false',\n                 mt='false',\n                 window_size=-100,\n                 inner_len=-100,\n                 edge_len=-100,\n                 model_pretraining=None,\n                 **kwargs,\n                ):\n        super().__init__()\n        self.num_labels = num_labels\n        self.num_labels_2 = num_labels_2\n        self.tokenizer = tokenizer\n        self.model_name = model_name\n        self.loss = loss\n        self.multi_layers = multi_layers\n        self.l2norm = l2norm\n        self.s = s\n        self.mt = mt\n        \n        self.window_size = window_size\n        self.inner_len = inner_len\n        self.edge_len = edge_len\n        \n        self.config = AutoConfig.from_pretrained(model_name)\n        self.config.update(\n            {\n                \"output_hidden_states\": True,\n                \"add_pooling_layer\": False,\n                \"num_labels\": self.num_labels,\n            }\n        )\n        if model_pretraining is not None and 'longformer' in model_name:\n            self.transformer = model_pretraining.model.longformer\n        else:\n            self.transformer = AutoModel.from_pretrained(model_name, config=self.config)\n            \n        # resize\n        self.transformer.resize_token_embeddings(len(tokenizer))\n            \n        if rnn=='none':\n            self.rnn = nn.Identity()\n        elif rnn=='lstm':\n            self.rnn = ResidualLSTM(self.config.hidden_size*self.multi_layers, rnn='LSTM')\n        elif rnn=='gru':\n            self.rnn = ResidualLSTM(self.config.hidden_size*self.multi_layers, rnn='GRU')\n        else:\n            raise Exception()\n    \n        if head=='simple':\n            self.head = nn.Sequential(\n                nn.Dropout(p_drop),\n                nn.Linear(self.config.hidden_size*self.multi_layers, self.num_labels)\n            )\n        elif head=='norm':\n            self.head = nn.Sequential(\n                nn.LayerNorm(self.config.hidden_size*self.multi_layers),\n                nn.Linear(self.config.hidden_size*self.multi_layers, 512),\n                nn.GELU(),\n                nn.LayerNorm(512),\n                nn.Linear(512, self.num_labels),\n            )\n        else:\n            raise Exception()\n        \n        # for multi-task\n        if self.mt=='true':\n            self.head2 = nn.Sequential(\n                nn.Dropout(p_drop),\n                nn.Linear(self.config.hidden_size*self.multi_layers, self.num_labels_2)\n            )\n        \n        if loss=='xentropy':\n            self.loss_fn = nn.CrossEntropyLoss(reduction='none')\n        else:\n            raise Exception()\n            \n    def forward_logits(self, input_ids, attention_mask, span_list, save_prob_seq=False):\n        assert self.multi_layers==1\n        \n        # sliding window approach to deal with longer tokens than max_length\n        # https://www.kaggle.com/competitions/feedback-prize-2021/discussion/313235\n        L = input_ids.size(1)\n        if self.window_size==-100 or L<=self.window_size:\n            x = self.transformer(input_ids=input_ids,\n                                 attention_mask=attention_mask).last_hidden_state\n        else:\n            assert len(input_ids)==1\n            segments = (L - self.window_size) // self.inner_len\n            if (L - self.window_size) % self.inner_len > self.edge_len:\n                segments += 1\n            elif segments == 0:\n                segments += 1\n            x = self.transformer(input_ids=input_ids[:,:self.window_size],\n                                 attention_mask=attention_mask[:,:self.window_size]).last_hidden_state\n            for i in range(1,segments+1):\n                start = self.window_size - self.edge_len + (i-1)*self.inner_len\n                end   = self.window_size - self.edge_len + (i-1)*self.inner_len + self.window_size\n                end = min(end, L)\n                x_next = self.transformer(input_ids=input_ids[:,start:end],\n                                          attention_mask=attention_mask[:,start:end]).last_hidden_state\n                if i==segments:\n                    x_next = x_next[:,self.edge_len:]\n                else:\n                    x_next = x_next[:,self.edge_len:self.edge_len+self.inner_len]\n                x = torch.cat([x,x_next], dim=1)\n            \n        #hidden_states = self.rnn(x) # (bs=1,num_tokens,hidden_size*multi_layers)\n        hidden_states = x\n        hidden_states = hidden_states.squeeze(0) # (num_tokens,hidden_size*multi_layers)\n            \n        span_list = span_list[0]\n        span_list_next = span_list[1:]+[-1]\n\n        logits_list = []\n        for i_token, i_token_next in zip(span_list, span_list_next):\n            tmp_logits = hidden_states[i_token:i_token_next,:].mean(dim=0) # (hidden_size*multi_layers)\n            logits_list.append(tmp_logits)\n        logits = torch.stack(logits_list) # (bs=num_discourse,hidden_size*multi_layers)\n        \n        # apply rnn\n        logits = logits.unsqueeze(0) # (bs=1,num_discourse,hidden_size*multi_layers)\n        logits = self.rnn(logits)\n        logits = logits.squeeze(0) # (bs=num_discourse,hidden_size*multi_layers)\n        \n        logits = self.head(logits) # (bs=num_discourse,num_labels)\n        \n        if save_prob_seq:\n            prob_seq = []\n            for i_token, i_token_next in zip(span_list, span_list_next):\n                tmp_logits = hidden_states[i_token:i_token_next,:] # (num_tokens,hidden_size*multi_layers)\n                tmp_prob_seq = self.head(tmp_logits).softmax(-1).detach().cpu().numpy() #(num_tokens,num_labels)\n                prob_seq.append(tmp_prob_seq)\n            return logits, prob_seq\n        else:\n            return logits\n        \n    def test_step(self, batch):\n        data = to_gpu(batch)\n        input_data = {\n            'input_ids':data['input_ids'],\n            'attention_mask':data['attention_mask'],\n            'span_list':data['span_list'],\n            'save_prob_seq':True,\n        }\n        logits, prob_seq = self.forward_logits(**input_data)\n        if self.loss in ['xentropy']:\n            pred = logits.softmax(-1).detach().cpu().numpy().reshape(-1,self.num_labels)\n        else:\n            raise Exception()\n        return {\n            'pred':pred,\n            'discourse_ids':data['discourse_ids'],\n            'prob_seq':prob_seq,\n        }\n    \nimport numpy as np\nimport pandas as pd\nfrom torch.utils.data import Dataset\nimport re\n\n# be careful for the order below\ndiscourse_type_list = [\n    'Lead',\n    'Position',\n    'Claim',\n    'Counterclaim',\n    'Rebuttal',\n    'Evidence',\n    'Concluding Statement'\n]\n    \nclass DatasetTest(Dataset):\n    def __init__(self, df, tokenizer):\n        self.df = df\n        self.unique_ids = sorted(df['essay_id'].unique())\n        self.tokenizer = tokenizer\n        self.discourse_type_token_ids_dict = {\n            discourse_type : tokenizer.convert_tokens_to_ids(f'[{discourse_type.upper()}]')\n            for discourse_type in discourse_type_list\n        }\n        self.inv_discourse_type_token_ids_dict = {v:k for k,v in self.discourse_type_token_ids_dict.items()}\n        \n    def __len__(self):\n        return len(self.unique_ids)\n    \n    def __getitem__(self, idx):\n        essay_id = self.unique_ids[idx]\n        sample_df = self.df[self.df['essay_id']==essay_id].reset_index(drop=True)\n        discourse_ids = sample_df['discourse_id'].values\n\n        text = ''\n        discourse_types = []\n        for discourse_type, discourse_text in zip(\n            sample_df['discourse_type'].values, sample_df['discourse_text'].values\n        ):\n            text += f' [{discourse_type.upper()}] {discourse_text}'\n            discourse_types.append(discourse_type.upper())\n        \n        tokens = self.tokenizer.encode_plus(text, add_special_tokens=True)\n        input_ids = torch.LongTensor(tokens['input_ids'])\n        attention_mask = torch.ones(len(input_ids)).long()\n        \n        span_list = []\n        for i_token, input_id in enumerate(tokens['input_ids']):\n            if input_id in self.discourse_type_token_ids_dict.values():\n                span_list.append(i_token)\n        \n        return dict(\n            essay_id = essay_id,\n            discourse_ids = discourse_ids,\n            text = text,\n            input_ids = input_ids,\n            attention_mask = attention_mask,\n            discourse_types = discourse_types,\n            span_list = span_list,\n        )\n        \nclass CustomCollator(object):\n    def __init__(self, tokenizer):\n        self.tokenizer = tokenizer\n        \n    def __call__(self, samples):\n        output = dict()\n        \n        for k in samples[0].keys():\n            output[k] = [sample[k] for sample in samples]\n        \n        # calculate max token length of this batch\n        batch_max = max([len(ids) for ids in output[\"input_ids\"]])\n        \n        # add padding\n        if self.tokenizer.padding_side == \"right\":\n            output[\"input_ids\"] = [s.tolist() + (batch_max - len(s)) * [self.tokenizer.pad_token_id] for s in output[\"input_ids\"]]\n            output[\"attention_mask\"] = [s.tolist() + (batch_max - len(s)) * [0] for s in output[\"attention_mask\"]]\n        else:\n            output[\"input_ids\"] = [(batch_max - len(s)) * [self.tokenizer.pad_token_id] + s.tolist() for s in output[\"input_ids\"]]\n            output[\"attention_mask\"] = [(batch_max - len(s)) * [0] + s.tolist() for s in output[\"attention_mask\"]]\n            \n        output[\"input_ids\"] = torch.LongTensor(output[\"input_ids\"])\n        output[\"attention_mask\"] = torch.LongTensor(output[\"attention_mask\"])\n        \n        return output","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-08-22T02:48:40.953688Z","iopub.execute_input":"2022-08-22T02:48:40.954284Z","iopub.status.idle":"2022-08-22T02:48:40.969226Z","shell.execute_reply.started":"2022-08-22T02:48:40.954254Z","shell.execute_reply":"2022-08-22T02:48:40.968206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile models_detector.py\n\nfrom torch import nn\nfrom transformers import AutoModelForTokenClassification, AutoTokenizer\n\nclass TextSpanDetectorOriginal(nn.Module):\n    def __init__(self, arch, num_classes=7, local_files_only=True):\n        super().__init__()\n        self.model = AutoModelForTokenClassification.from_pretrained(\n            arch,\n            num_labels=1 + 2 + num_classes,\n            local_files_only=local_files_only\n        )\n        self.tokenizer = AutoTokenizer.from_pretrained(\n            arch, \n            local_files_only=local_files_only\n        )","metadata":{"execution":{"iopub.status.busy":"2022-08-22T02:48:40.972267Z","iopub.execute_input":"2022-08-22T02:48:40.972628Z","iopub.status.idle":"2022-08-22T02:48:40.981974Z","shell.execute_reply.started":"2022-08-22T02:48:40.972591Z","shell.execute_reply":"2022-08-22T02:48:40.981014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile inference.py\n\nimport torch\nprint(torch.__name__, torch.__version__)\n\nimport argparse\nimport os\nfrom os.path import join as opj\nimport random\nimport warnings\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nwarnings.filterwarnings(\"ignore\")\n\nimport transformers\ntransformers.logging.set_verbosity_error()\n\n\ndef seed_everything(seed: int):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--fold\", type=int, default=-1, required=False)\n    parser.add_argument(\"--seed\", type=int, default=-1, required=False)\n    parser.add_argument(\"--model\", type=str, required=True)\n    parser.add_argument(\"--model_name\", type=str, required=True)\n    parser.add_argument(\"--input_path\", type=str, default='../input/feedback-prize-effectiveness/', required=False)\n    \n    parser.add_argument(\"--test_batch_size\", type=int, default=1, required=False)\n    parser.add_argument(\"--pretrain_path\", type=str, default='none', required=False)\n    parser.add_argument(\"--rnn\", type=str, default='none', required=False)\n    parser.add_argument(\"--head\", type=str, default='simple', required=False)\n    parser.add_argument(\"--loss\", type=str, default='xentropy', required=False)\n    parser.add_argument(\"--multi_layers\", type=int, default=1, required=False)\n    parser.add_argument(\"--num_labels\", type=int, default=3, required=False)\n    parser.add_argument(\"--num_labels_2\", type=int, default=7, required=False)\n    parser.add_argument(\"--l2norm\", type=str, default='false', required=False)\n    parser.add_argument(\"--max_length\", type=int, default=1024, required=False)\n    parser.add_argument(\"--mt\", type=str, default='false', required=False)\n    \n    parser.add_argument(\"--window_size\", type=int, default=-100, required=False)\n    parser.add_argument(\"--inner_len\", type=int, default=-100, required=False)\n    parser.add_argument(\"--edge_len\", type=int, default=-100, required=False)\n    \n    return parser.parse_args()\n\n    \nfrom models import Model, DatasetTest, CustomCollator\n    \nif __name__=='__main__':\n    NUM_JOBS = 2\n    args = parse_args()\n        \n    #train_df = pd.read_csv(opj(args.input_path, 'train.csv'))\n    test_df = pd.read_csv(opj(args.input_path, 'test.csv'))\n    #test_df = pd.read_csv(opj(args.input_path, 'train.csv'))\n    sub_df = pd.read_csv(opj(args.input_path, 'sample_submission.csv'))\n    \n    #unique_ids = test_df['essay_id'].unique()[:100]\n    #test_df = test_df[test_df['essay_id'].isin(unique_ids)].reset_index(drop=True)\n\n    #print('train_df.shape = ', train_df.shape)\n    print('test_df.shape = ', test_df.shape)\n    print('sub_df.shape = ', sub_df.shape)\n\n    LABEL = 'discourse_effectiveness'\n    \n    os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n    \n    from models import discourse_type_list\n    if ('deberta-v2' in args.model) or ('deberta-v3' in args.model):\n        print('use DebertaV2TokenizerFast')\n        from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast\n        tokenizer = DebertaV2TokenizerFast.from_pretrained(args.model, trim_offsets=False)\n        special_tokens_dict = {'additional_special_tokens': ['\\n\\n'] + [f'[{s.upper()}]' for s in discourse_type_list]}\n        _ = tokenizer.add_special_tokens(special_tokens_dict)\n    else:\n        from transformers import AutoTokenizer\n        tokenizer = AutoTokenizer.from_pretrained(args.model, trim_offsets=False)\n        special_tokens_dict = {'additional_special_tokens': [f'[{s.upper()}]' for s in discourse_type_list]}\n        _ = tokenizer.add_special_tokens(special_tokens_dict)\n        \n    #print('tokenizer = ', tokenizer)\n        \n    test_dataset = DatasetTest(\n        test_df,\n        tokenizer,\n    )\n    from torch.utils.data import DataLoader\n    test_dataloader = DataLoader(\n            test_dataset,\n            batch_size=args.test_batch_size,\n            shuffle=False,\n            collate_fn=CustomCollator(tokenizer),\n            num_workers=NUM_JOBS,\n            pin_memory=True,\n            drop_last=False,\n        )\n    \n    #model\n    model_pretraining = None\n    if 'longformer' in args.model:\n        from models_detector import TextSpanDetectorOriginal\n        model_pretraining = TextSpanDetectorOriginal(args.model)\n    model = Model(args.model, \n                  tokenizer,\n                  num_labels=args.num_labels, \n                  num_labels_2=args.num_labels_2,\n                  rnn=args.rnn,\n                  loss=args.loss,\n                  head=args.head,\n                  multi_layers=args.multi_layers,\n                  l2norm=args.l2norm,\n                  max_length=args.max_length,\n                  mt=args.mt,\n                  window_size=args.window_size,\n                  inner_len=args.inner_len,\n                  edge_len=args.edge_len,\n                  model_pretraining=model_pretraining,\n                 )\n    if args.fold!=-1:\n        weight_path = opj(args.model, f'model_fold{args.fold}.pth')\n    elif args.seed!=-1:\n        weight_path = opj(args.model, f'model_seed{args.seed}.pth')\n    model.load_state_dict(torch.load(weight_path))\n    model = model.cuda()\n    model.eval()\n    \n    from tqdm import tqdm\n    outputs = []\n    for batch_idx, batch in tqdm(enumerate(test_dataloader), total=len(test_dataloader)):\n        with torch.no_grad():\n            output = model.test_step(batch)\n            outputs.append(output)\n    del model\n    torch.cuda.empty_cache()\n            \n    preds = []\n    discourse_ids = []\n    prob_seqs = []\n    for o in outputs:\n        preds.append(o['pred'])\n        discourse_ids.extend(o['discourse_ids'])\n        prob_seqs.extend(o['prob_seq'])\n    preds = np.vstack(preds)\n    discourse_ids = np.hstack(discourse_ids)\n    \n    print('discourse_ids.shape = ', discourse_ids.shape)\n    print('preds.shape = ', preds.shape)\n    print('len(prob_seqs) = ', len(prob_seqs))\n    \n    pred_df = pd.DataFrame()\n    pred_df['discourse_id'] = discourse_ids\n    pred_df['Ineffective'] = preds[:,0]\n    pred_df['Adequate'] = preds[:,1]\n    pred_df['Effective'] = preds[:,2]\n    pred_df['prob_seq'] = prob_seqs\n    \n    pred_df = test_df[['discourse_id']].merge(pred_df, on='discourse_id', how='left')\n    \n    pred_name = args.model.split('/')[-1]\n    if 'fold' in pred_name:\n        pred_name = pred_name.replace('-fold012','').replace('-fold34','')\n        \n    if args.fold!=-1:\n        print(f'saving raw_pred_{pred_name}_fold{args.fold}.csv...')\n        pred_df.to_csv(f'raw_pred_{pred_name}_fold{args.fold}.csv', index=False)\n        print(f'saving raw_pred_{pred_name}_fold{args.fold}.csv, done')\n    elif args.seed!=-1:\n        print(f'saving raw_pred_{pred_name}_seed{args.seed}.csv...')\n        pred_df.to_csv(f'raw_pred_{pred_name}_seed{args.seed}.csv', index=False)\n        print(f'saving raw_pred_{pred_name}_seed{args.seed}.csv, done')\n    print('\\n')","metadata":{"execution":{"iopub.status.busy":"2022-08-22T02:48:40.983652Z","iopub.execute_input":"2022-08-22T02:48:40.984226Z","iopub.status.idle":"2022-08-22T02:48:40.998664Z","shell.execute_reply.started":"2022-08-22T02:48:40.984191Z","shell.execute_reply":"2022-08-22T02:48:40.997247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile shujun_features.py\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\ndef get_xgb_features(train_df, prob_sequences, use_prob_seq=True):\n    '''\n    prob_seq is the sequence of token probs from each discourse\n    '''\n    \n    if use_prob_seq:\n        # 10 features : instability_0,...,instability_3, begin_0,...,begin_2, end_0,...,end_2\n        features2calculate=[f\"instability_{i}\" for i in range(4)]+\\\n        [f\"begin_{i}\" for i in range(3)]+\\\n        [f\"end_{i}\" for i in range(3)]\n        calculated_features=[]\n        for i,prob_seq in tqdm(enumerate(prob_sequences)):\n            tmp=[]\n            prob_seq=np.array(prob_seq)\n            tmp.append(np.diff(prob_seq[:,:],0).mean(0)) # 3\n            tmp.append([(np.diff(prob_seq[:,[1,2]].sum(1))**2).mean()]) # 1\n            tmp.append(prob_seq[:5,:].mean(0)) # 3\n            tmp.append(prob_seq[-5:,:].mean(0)) # 3\n            calculated_features.append(np.concatenate(tmp))\n        calculated_features=np.array(calculated_features)\n        print('calculated_features.shape = ', calculated_features.shape)\n        train_df[features2calculate]=calculated_features # 10 features\n        train_df['len']=[len(s) for s in prob_sequences]\n\n    p_features=[]\n    n_features=[]\n    neighbor_features=['Ineffective','Adequate','Effective','discourse_type']\n    neighbor_features_values=train_df[neighbor_features].values\n    for i in tqdm(range(len(train_df))):\n        if i>1 and train_df['essay_id'].iloc[i]==train_df['essay_id'].iloc[i-1]:\n            p_features.append(neighbor_features_values[i-1])\n        else:\n            p_features.append(neighbor_features_values[i])\n\n        if i<(len(train_df)-1) and train_df['essay_id'].iloc[i]==train_df['essay_id'].iloc[i+1]:\n            n_features.append(neighbor_features_values[i+1])\n        else:\n            n_features.append(neighbor_features_values[i])\n\n    train_df[[f+\"_previous\" for f in neighbor_features]]=p_features\n    train_df[[f+\"_next\" for f in neighbor_features]]=n_features\n\n    train_df['mean_Ineffective']=train_df.groupby(\"essay_id\")[\"Ineffective\"].transform(\"mean\")\n    train_df['mean_Adequate']=train_df.groupby(\"essay_id\")[\"Adequate\"].transform(\"mean\")\n    train_df['mean_Effective']=train_df.groupby(\"essay_id\")[\"Effective\"].transform(\"mean\")\n\n    train_df['std_Ineffective']=train_df.groupby(\"essay_id\")[\"Ineffective\"].transform(\"std\")\n    train_df['std_Adequate']=train_df.groupby(\"essay_id\")[\"Adequate\"].transform(\"std\")\n    train_df['std_Effective']=train_df.groupby(\"essay_id\")[\"Effective\"].transform(\"std\")\n\n    train_df['discourse_count']=train_df.groupby(\"essay_id\")['discourse_type'].transform(\"count\")\n\n    cnts=train_df.groupby('essay_id')['discourse_type'].apply(lambda x: x.value_counts())\n\n    discourse_types=['Claim','Evidence','Concluding Statement','Lead','Position','Counterclaim','Rebuttal']\n    value_count_hash={}\n    for t in discourse_types:\n        value_count_hash[t]={}\n    for key in cnts.keys():\n        value_count_hash[key[1]][key[0]]=cnts[key]\n\n    discourse_cnts=[]    \n    for essay_id in train_df['essay_id'].unique():\n        row=[essay_id]\n        for d in discourse_types:\n            try:\n                row.append(value_count_hash[d][essay_id])\n            except:\n                row.append(0)\n        discourse_cnts.append(row)\n\n    discourse_cnts=pd.DataFrame(discourse_cnts,columns=['essay_id']+[f'{d}_count' for d in discourse_types])    \n\n    train_df=train_df.merge(discourse_cnts,how='left',on='essay_id')\n\n    return train_df","metadata":{"execution":{"iopub.status.busy":"2022-08-22T02:48:41.000834Z","iopub.execute_input":"2022-08-22T02:48:41.00125Z","iopub.status.idle":"2022-08-22T02:48:41.012659Z","shell.execute_reply.started":"2022-08-22T02:48:41.001216Z","shell.execute_reply":"2022-08-22T02:48:41.011366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"DEBUG = 'false'\n#DEBUG = 'true'","metadata":{"execution":{"iopub.status.busy":"2022-08-22T02:48:41.015181Z","iopub.execute_input":"2022-08-22T02:48:41.016258Z","iopub.status.idle":"2022-08-22T02:48:41.024016Z","shell.execute_reply.started":"2022-08-22T02:48:41.016222Z","shell.execute_reply":"2022-08-22T02:48:41.02305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\ntrain_df = pd.read_csv('../input/feedback-prize-effectiveness/train.csv')\ntest_df = pd.read_csv('../input/feedback-prize-effectiveness/test.csv')\n#test_df = train_df.copy()\n\nIS_PRIVATE = True\n# if DEBUG=='false':\n#     if len(test_df)==10:\n#         IS_PRIVATE = False","metadata":{"execution":{"iopub.status.busy":"2022-08-22T03:08:16.585659Z","iopub.execute_input":"2022-08-22T03:08:16.586368Z","iopub.status.idle":"2022-08-22T03:08:16.867715Z","shell.execute_reply.started":"2022-08-22T03:08:16.58633Z","shell.execute_reply":"2022-08-22T03:08:16.866666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_list = [\n    # (model_name, num_fold, weight, stacking_path, num_stacking_fold)\n    ('fb2-34-v2-02-deberta-large',5,0.23678063, 'none','none'), # 2nd round pseudo-label, 5fold, residual lstm\n    ('fb2-34-vl-01-deberta-v3-large',5,0.42236033, 'none','none'), # 2nd round pseudo-label, 5fold, residual lstm\n    ('fb2-29-v2-01-deberta-large',5,0.14750601, f'../input/fb2-999-v1-22',5), # 1st round pseudo-label, 5fold, catboost\n    ('fb2-29-vl-01-deberta-v3-large',5,0.19335303, f'../input/fb2-999-v1-23',5), # 1st round pseudo-label, 5fold, catboost\n]\nmodel_name_list = [name for (name,_,_,_,_) in model_list]\nnum_fold_list = [num_fold for (_,num_fold,_,_,_) in model_list]\nw_list = [w for (_,_,w,_,_) in model_list]\nstacking_path_list = [stacking_path for (_,_,_,stacking_path,_) in model_list]\nnum_stacking_fold_list = [num_stacking_fold for (_,_,_,_,num_stacking_fold) in model_list]\n\n# for debug\nif DEBUG=='true': \n    model_list = [(name,1,w,stacking_path,num_stacking_fold) for (name,nu_mfolds,w,stacking_path,num_stacking_fold) in model_list]","metadata":{"execution":{"iopub.status.busy":"2022-08-22T02:48:41.399104Z","iopub.execute_input":"2022-08-22T02:48:41.399492Z","iopub.status.idle":"2022-08-22T02:48:41.410062Z","shell.execute_reply.started":"2022-08-22T02:48:41.399441Z","shell.execute_reply":"2022-08-22T02:48:41.409025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if IS_PRIVATE:\n    MODEL_NAME = 'microsoft/deberta-v3-large' #seed100 5fold xentropy, residual lstm\n    MODEL = '../input/fb2-34-vl-01-deberta-v3-large'\n    NUM_LABELS = 3\n    WINDOW = 768 #512\n    INNER = 512 #384\n    EDGE = 128 #64\n    RNN = 'lstm'\n    if MODEL.split('/')[-1] in model_name_list:\n        !python inference.py --model_name $MODEL_NAME --model $MODEL --fold 0 --window_size $WINDOW --inner_len $INNER --edge_len $EDGE --rnn $RNN\n        if DEBUG=='false':\n            !python inference.py --model_name $MODEL_NAME --model $MODEL --fold 1 --window_size $WINDOW --inner_len $INNER --edge_len $EDGE --rnn $RNN\n            !python inference.py --model_name $MODEL_NAME --model $MODEL --fold 2 --window_size $WINDOW --inner_len $INNER --edge_len $EDGE --rnn $RNN\n            !python inference.py --model_name $MODEL_NAME --model $MODEL --fold 3 --window_size $WINDOW --inner_len $INNER --edge_len $EDGE --rnn $RNN\n            !python inference.py --model_name $MODEL_NAME --model $MODEL --fold 4 --window_size $WINDOW --inner_len $INNER --edge_len $EDGE --rnn $RNN","metadata":{"execution":{"iopub.status.busy":"2022-08-22T02:48:41.414259Z","iopub.execute_input":"2022-08-22T02:48:41.415455Z","iopub.status.idle":"2022-08-22T02:52:00.086411Z","shell.execute_reply.started":"2022-08-22T02:48:41.415414Z","shell.execute_reply":"2022-08-22T02:52:00.085135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if IS_PRIVATE:\n    MODEL_NAME = 'microsoft/deberta-large' #seed100 5fold xentropy, residual lstm\n    MODEL = '../input/fb2-34-v2-02-deberta-large'\n    NUM_LABELS = 3\n    WINDOW = 768 #512\n    INNER = 512 #384\n    EDGE = 128 #64\n    RNN = 'lstm'\n    if MODEL.split('/')[-1] in model_name_list:\n        !python inference.py --model_name $MODEL_NAME --model $MODEL --fold 0 --window_size $WINDOW --inner_len $INNER --edge_len $EDGE --rnn $RNN\n        if DEBUG=='false':\n            !python inference.py --model_name $MODEL_NAME --model $MODEL --fold 1 --window_size $WINDOW --inner_len $INNER --edge_len $EDGE --rnn $RNN\n            !python inference.py --model_name $MODEL_NAME --model $MODEL --fold 2 --window_size $WINDOW --inner_len $INNER --edge_len $EDGE --rnn $RNN\n            !python inference.py --model_name $MODEL_NAME --model $MODEL --fold 3 --window_size $WINDOW --inner_len $INNER --edge_len $EDGE --rnn $RNN\n            !python inference.py --model_name $MODEL_NAME --model $MODEL --fold 4 --window_size $WINDOW --inner_len $INNER --edge_len $EDGE --rnn $RNN","metadata":{"execution":{"iopub.status.busy":"2022-08-22T02:52:00.08817Z","iopub.execute_input":"2022-08-22T02:52:00.088621Z","iopub.status.idle":"2022-08-22T02:55:31.396445Z","shell.execute_reply.started":"2022-08-22T02:52:00.088558Z","shell.execute_reply":"2022-08-22T02:55:31.39519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if IS_PRIVATE:\n    MODEL_NAME = 'microsoft/deberta-v3-large' #seed100 5fold xentropy\n    MODEL = '../input/fb2-29-vl-01-deberta-v3-large'\n    NUM_LABELS = 3\n    WINDOW = 512\n    INNER = 384\n    EDGE = 64\n    if MODEL.split('/')[-1] in model_name_list:\n        !python inference.py --model_name $MODEL_NAME --model $MODEL --fold 0 --window_size $WINDOW --inner_len $INNER --edge_len $EDGE\n        if DEBUG=='false':\n            !python inference.py --model_name $MODEL_NAME --model $MODEL --fold 1 --window_size $WINDOW --inner_len $INNER --edge_len $EDGE\n            !python inference.py --model_name $MODEL_NAME --model $MODEL --fold 2 --window_size $WINDOW --inner_len $INNER --edge_len $EDGE\n            !python inference.py --model_name $MODEL_NAME --model $MODEL --fold 3 --window_size $WINDOW --inner_len $INNER --edge_len $EDGE\n            !python inference.py --model_name $MODEL_NAME --model $MODEL --fold 4 --window_size $WINDOW --inner_len $INNER --edge_len $EDGE","metadata":{"execution":{"iopub.status.busy":"2022-08-22T02:55:31.399413Z","iopub.execute_input":"2022-08-22T02:55:31.40021Z","iopub.status.idle":"2022-08-22T02:59:36.59305Z","shell.execute_reply.started":"2022-08-22T02:55:31.400163Z","shell.execute_reply":"2022-08-22T02:59:36.591823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if IS_PRIVATE:\n    MODEL_NAME = 'microsoft/deberta-large' #seed100 5fold xentropy\n    MODEL = '../input/fb2-29-v2-01-deberta-large'\n    NUM_LABELS = 3\n    WINDOW = 512\n    INNER = 384\n    EDGE = 64\n    if MODEL.split('/')[-1] in model_name_list:\n        !python inference.py --model_name $MODEL_NAME --model $MODEL --fold 0 --window_size $WINDOW --inner_len $INNER --edge_len $EDGE\n        if DEBUG=='false':\n            !python inference.py --model_name $MODEL_NAME --model $MODEL --fold 1 --window_size $WINDOW --inner_len $INNER --edge_len $EDGE\n            !python inference.py --model_name $MODEL_NAME --model $MODEL --fold 2 --window_size $WINDOW --inner_len $INNER --edge_len $EDGE\n            !python inference.py --model_name $MODEL_NAME --model $MODEL --fold 3 --window_size $WINDOW --inner_len $INNER --edge_len $EDGE\n            !python inference.py --model_name $MODEL_NAME --model $MODEL --fold 4 --window_size $WINDOW --inner_len $INNER --edge_len $EDGE","metadata":{"execution":{"iopub.status.busy":"2022-08-22T02:59:36.595654Z","iopub.execute_input":"2022-08-22T02:59:36.595981Z","iopub.status.idle":"2022-08-22T03:03:23.860306Z","shell.execute_reply.started":"2022-08-22T02:59:36.59594Z","shell.execute_reply":"2022-08-22T03:03:23.859087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Stacking","metadata":{}},{"cell_type":"code","source":"model_list","metadata":{"execution":{"iopub.status.busy":"2022-08-22T03:03:23.863301Z","iopub.execute_input":"2022-08-22T03:03:23.864248Z","iopub.status.idle":"2022-08-22T03:03:23.875708Z","shell.execute_reply.started":"2022-08-22T03:03:23.864212Z","shell.execute_reply":"2022-08-22T03:03:23.874395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from os.path import join as opj\nimport joblib\nimport catboost\nfrom shujun_features import get_xgb_features\n\ndef stacking(_test_df, prob_sequences, stacking_path, num_stacking_fold, i_model, fold, num_fold, weight, use_prob_seq=False):\n    test_df = _test_df.copy()\n    \n    # generate features\n    test_df = get_xgb_features(test_df, prob_sequences, use_prob_seq=use_prob_seq)\n    print('test_df.shape = ', test_df.shape)\n    seq_cols = [f\"instability_{i}\" for i in range(4)] + [f\"begin_{i}\" for i in range(3)] + [f\"end_{i}\" for i in range(3)] + ['len']\n    \n    cols = ['discourse_type','discourse_type_previous','discourse_type_next']\n    cols += ['Ineffective','Adequate','Effective']\n    cols += [\n        'Ineffective_previous', 'Adequate_previous', 'Effective_previous', \n        'Ineffective_next', 'Adequate_next', 'Effective_next',\n        'mean_Ineffective', 'mean_Adequate', 'mean_Effective',\n        'std_Ineffective', 'std_Adequate', 'std_Effective', \n        'discourse_count', 'Claim_count', 'Evidence_count', 'Concluding Statement_count',\n        'Lead_count', 'Position_count', 'Counterclaim_count', 'Rebuttal_count'\n    ]\n    cols += seq_cols\n    print('cols = ', cols)\n\n    num_stacking_model = 1 #2 # catboost, lgb\n    preds = 0\n    # catboost\n    cat_features = [0,1,2]\n    pool_test = catboost.Pool(test_df[cols].values, cat_features=cat_features)\n    for stacking_fold in range(num_stacking_fold):\n        model = joblib.load(opj(stacking_path, f'cat_fold{stacking_fold}.joblib'))\n        pred = model.predict(pool_test, prediction_type='Probability')\n        preds += pred / num_stacking_fold / num_stacking_model * weight / num_fold\n    # lgb\n#     cols_lgb = cols.copy()\n#     le = joblib.load(opj(stacking_path,'label_encoder.joblib'))\n#     test_df['discourse_type_label'] = le.transform(test_df['discourse_type'])\n#     test_df['discourse_type_previous_label'] = le.transform(test_df['discourse_type_previous'])\n#     test_df['discourse_type_next_label'] = le.transform(test_df['discourse_type_next'])\n#     cols_lgb[0] = 'discourse_type_label'\n#     cols_lgb[1] = 'discourse_type_previous_label'\n#     cols_lgb[2] = 'discourse_type_next_label'\n#     for stacking_fold in range(num_stacking_fold):\n#         model = joblib.load(opj(stacking_path, f'lgb_fold{stacking_fold}.joblib'))\n#         pred = model.predict_proba(test_df[cols_lgb].values)\n#         preds += pred / num_stacking_fold / num_stacking_model * weight / num_fold\n\n    test_df[f'Ineffective_{i_model}_{fold}'] = preds[:,0] \n    test_df[f'Adequate_{i_model}_{fold}'] = preds[:,1]\n    test_df[f'Effective_{i_model}_{fold}'] = preds[:,2]\n    \n    return test_df[['discourse_id',f'Ineffective_{i_model}_{fold}',f'Adequate_{i_model}_{fold}',f'Effective_{i_model}_{fold}']]\n\n\ndef convert(_test_df, i_model, fold, num_fold, weight):\n    test_df = _test_df.copy()\n    cols = ['Ineffective','Adequate','Effective']\n    preds = test_df[cols].values\n    \n    test_df[f'Ineffective_{i_model}_{fold}'] = preds[:,0] * weight / num_fold\n    test_df[f'Adequate_{i_model}_{fold}'] = preds[:,1] * weight / num_fold\n    test_df[f'Effective_{i_model}_{fold}'] = preds[:,2] * weight / num_fold\n    \n    return test_df[['discourse_id',f'Ineffective_{i_model}_{fold}',f'Adequate_{i_model}_{fold}',f'Effective_{i_model}_{fold}']]","metadata":{"execution":{"iopub.status.busy":"2022-08-22T03:08:22.076895Z","iopub.execute_input":"2022-08-22T03:08:22.077476Z","iopub.status.idle":"2022-08-22T03:08:22.091178Z","shell.execute_reply.started":"2022-08-22T03:08:22.077439Z","shell.execute_reply":"2022-08-22T03:08:22.089932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if IS_PRIVATE:\n    import pandas as pd\n    import numpy as np\n    \n    #assert len(model_list)==1\n    \n    pred_df = None\n    for i_model,(model,num_fold,w,stacking_path,num_stacking_fold) in enumerate(model_list):\n        for fold in range(num_fold):\n            try:\n                tmp_df = pd.read_csv(f'raw_pred_{model}_fold{fold}.csv')\n                print('fold = ', fold)\n            except:\n                tmp_df = pd.read_csv(f'raw_pred_{model}_seed{(fold+1)*100}.csv')\n                print('seed = ', (fold+1)*100)\n            \n            if stacking_path!='none':\n                # based on https://stackoverflow.com/questions/45704999/how-to-convert-vector-wrapped-as-string-to-numpy-array-in-pandas-dataframe\n                tmp_df['prob_seq'] = tmp_df['prob_seq'].apply(lambda x:np.fromstring(x.replace('\\n','')\n                                                                                       .replace('[','')\n                                                                                       .replace(']','')\n                                                                                       .replace('  ',' '), sep=' '))\n                prob_seqs = [tmp_df['prob_seq'].values[i].reshape(-1,3) for i in range(len(tmp_df))]\n\n                tmp_df = tmp_df.merge(test_df[['essay_id','discourse_id','discourse_type']], on='discourse_id', how='left')\n                tmp_df = stacking(tmp_df, prob_seqs, stacking_path, num_stacking_fold, i_model, fold, num_fold, \n                                  weight=w, use_prob_seq=True)\n            else:\n                tmp_df = convert(tmp_df, i_model, fold, num_fold, weight=w)\n            \n            if pred_df is None:\n                pred_df = tmp_df.copy()\n            else:\n                pred_df = pred_df.merge(tmp_df, on='discourse_id', how='left')\n            display(pred_df.head(2))\n        \n    # sum all weighted predictions\n    for col_name in ['Ineffective','Adequate','Effective']:\n        cols = []\n        for i_model,(_,num_fold,_,_,_) in enumerate(model_list):\n            for fold in range(num_fold):\n                cols.append(f'{col_name}_{i_model}_{fold}')\n        #pred_df[col_name] = pred_df[cols].mean(axis=1)\n        pred_df[col_name] = pred_df[cols].sum(axis=1)\n    \n    test_df = test_df.merge(pred_df, on='discourse_id', how='left')","metadata":{"execution":{"iopub.status.busy":"2022-08-22T03:08:22.390338Z","iopub.execute_input":"2022-08-22T03:08:22.390646Z","iopub.status.idle":"2022-08-22T03:08:24.531588Z","shell.execute_reply.started":"2022-08-22T03:08:22.390618Z","shell.execute_reply":"2022-08-22T03:08:24.530637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-22T03:08:24.533717Z","iopub.execute_input":"2022-08-22T03:08:24.53418Z","iopub.status.idle":"2022-08-22T03:08:24.55848Z","shell.execute_reply.started":"2022-08-22T03:08:24.534139Z","shell.execute_reply":"2022-08-22T03:08:24.557424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Post-processing","metadata":{}},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"sub_df = pd.read_csv('../input/feedback-prize-effectiveness/sample_submission.csv')\n#sub_df = test_df.copy()\n\nif IS_PRIVATE:\n    sub_df = pd.merge(sub_df[['discourse_id']], \n                      test_df[['discourse_id','Ineffective','Adequate','Effective']], \n                      on='discourse_id', \n                      how='left')\n\nsub_df.to_csv('submission.csv', index=False)\nsub_df.head(100)","metadata":{"execution":{"iopub.status.busy":"2022-08-22T03:08:24.560115Z","iopub.execute_input":"2022-08-22T03:08:24.560777Z","iopub.status.idle":"2022-08-22T03:08:24.580699Z","shell.execute_reply.started":"2022-08-22T03:08:24.560741Z","shell.execute_reply":"2022-08-22T03:08:24.579712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}