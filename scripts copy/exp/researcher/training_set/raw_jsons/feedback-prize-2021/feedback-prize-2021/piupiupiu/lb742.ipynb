{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !python ../input/feedback-py/hexian.py\n# !python ../input/wenfeng-models-code/debert_xxlarge_fold12.py\n# !python ../input/wenfeng-models-code/robert_fold34.py\n# !python ../input/wenfeng-models-code/distilbart_mnli_12_9_adv_fold02.py\ndef run_cmd(cmd):\n    import os\n    a = os.system(cmd)\n    if a != 0:\n        raise Exception(cmd+\"文件执行错误\")\n    else:\n        print(cmd+\"文件执行成功！\")\n\nrun_cmd('python ../input/wenfeng-models-code4/distilbart_xsum_12_6_adv_fold5.py')\nrun_cmd('python ../input/wenfeng-models-code4/distilbart_cnn_12_6_adv_fold5.py')\nrun_cmd('python ../input/wenfeng-models-code4/robert_fold5.py')\nrun_cmd('python ../input/wenfeng-models-code4/bart_large_finetuned_squadv1_fold5.py')\nrun_cmd('python ../input/wenfeng-models-code4/distilbart_mnli_12_9_adv_fold5.py')\nrun_cmd('python ../input/feedback-py/hexian.py')\nrun_cmd('python ../input/wenfeng-models-code4/debert_xxlarge_adv_fold5.py')\nrun_cmd('python ../input/wenfeng-models-code4/debert_xlarge_fold5.py')\n# run_cmd('python ../input/wenfeng-models-code3/2longformer_2robert_2dist_2squadv1.py')\nrun_cmd('python ../input/wenfeng-models-code4/longformer_fold5.py')\n","metadata":{"execution":{"iopub.status.busy":"2022-03-13T05:24:56.501614Z","iopub.execute_input":"2022-03-13T05:24:56.502071Z","iopub.status.idle":"2022-03-13T05:30:59.091397Z","shell.execute_reply.started":"2022-03-13T05:24:56.501963Z","shell.execute_reply":"2022-03-13T05:30:59.090446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\ntime.sleep(10)\n! python ../input/wenfeng-models-code3/pred_add.py \\\n--path1 './longformer_fold5.pkl' \\\n--path2 './robert_fold5.pkl' \\\n--path3 './ronghe.pkl' \\\n--align 0 \\\n--weight 1.5\n! rm ./longformer_fold5.pkl\n! rm ./robert_fold5.pkl\n\n! python ../input/wenfeng-models-code3/pred_add.py \\\n--path1 './ronghe.pkl' \\\n--path2 './debert_xxlarge_adv_fold5.pkl' \\\n--path3 './ronghe.pkl' \\\n--align 1 \\\n--weight 2.5\n! rm ./debert_xxlarge_adv_fold5.pkl\n\n! python ../input/wenfeng-models-code3/pred_add.py \\\n--path1 './ronghe.pkl' \\\n--path2 './hexian.pkl' \\\n--path3 './ronghe.pkl' \\\n--align 1 \\\n--weight 1.5\n! rm ./hexian.pkl\n\n! python ../input/wenfeng-models-code3/pred_add.py \\\n--path1 './ronghe.pkl' \\\n--path2 './distilbart_mnli_12_9_adv_fold5.pkl' \\\n--path3 './ronghe.pkl' \\\n--align 0 \\\n--weight 0.5\n! rm ./distilbart_mnli_12_9_adv_fold5.pkl\n\n! python ../input/wenfeng-models-code3/pred_add.py \\\n--path1 './ronghe.pkl' \\\n--path2 './bart_large_finetuned_squadv1_fold5.pkl' \\\n--path3 './ronghe.pkl' \\\n--align 0 \\\n--weight 0.5\n! rm ./bart_large_finetuned_squadv1_fold5.pkl\n\n! python ../input/wenfeng-models-code3/pred_add.py \\\n--path1 './ronghe.pkl' \\\n--path2 './debert_xlarge_fold5.pkl' \\\n--path3 './ronghe.pkl' \\\n--align 1 \\\n--weight 0.5\n! rm ./debert_xlarge_fold5.pkl\n\n! python ../input/wenfeng-models-code3/pred_add.py \\\n--path1 './ronghe.pkl' \\\n--path2 './distilbart_cnn_12_6_adv_fold5.pkl' \\\n--path3 './ronghe.pkl' \\\n--align 0 \\\n--weight 0.5\n! rm ./distilbart_cnn_12_6_adv_fold5.pkl\n\n! python ../input/wenfeng-models-code3/pred_add.py \\\n--path1 './ronghe.pkl' \\\n--path2 './distilbart_xsum_12_6_adv_fold5.pkl' \\\n--path3 './ronghe.pkl' \\\n--align 0 \\\n--weight 0.5\n! rm ./distilbart_xsum_12_6_adv_fold5.pkl","metadata":{"execution":{"iopub.status.busy":"2022-03-13T05:30:59.093823Z","iopub.execute_input":"2022-03-13T05:30:59.094256Z","iopub.status.idle":"2022-03-13T05:31:17.455087Z","shell.execute_reply.started":"2022-03-13T05:30:59.0942Z","shell.execute_reply":"2022-03-13T05:31:17.453873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import os\n# import gc\n# import sys\n# import pickle\n# sys.path.append('../input/wenfeng-models-code2')\n# from utils import after_deal\n\n\n# test_feat = pickle.load(open('./ronghe.pkl','+rb'))\n# test_feat['pred'] = test_feat['pred'].apply(lambda x: x/x.sum(axis=1).reshape(-1,1))  # 重要不要遗漏\n\n\n# discourse_type = ['Claim','Evidence', 'Position','Concluding Statement','Lead','Counterclaim','Rebuttal']\n# i_discourse_type = ['I-'+i for i in discourse_type]\n# b_discourse_type = ['B-'+i for i in discourse_type]\n# labels_to_ids = {k:v for v,k in enumerate(['O']+i_discourse_type+b_discourse_type)}\n# ids_to_labels = {k:v for v,k in labels_to_ids.items()}\n\n# segment_param = {\n# \"Lead\":                 {'min_proba':[0.47,0.41],'begin_proba':1.00,'min_sep':40,'min_length': 5},\n# \"Position\":             {'min_proba':[0.45,0.40],'begin_proba':0.90,'min_sep':21,'min_length': 3},\n# \"Evidence\":             {'min_proba':[0.50,0.40],'begin_proba':0.56,'min_sep': 2,'min_length':21},\n# \"Claim\":                {'min_proba':[0.40,0.30],'begin_proba':0.30,'min_sep':10,'min_length': 1},\n# \"Concluding Statement\": {'min_proba':[0.58,0.25],'begin_proba':0.93,'min_sep':50,'min_length': 5},\n# \"Counterclaim\":         {'min_proba':[0.45,0.25],'begin_proba':0.70,'min_sep':35,'min_length': 6},\n# \"Rebuttal\":             {'min_proba':[0.37,0.34],'begin_proba':0.70,'min_sep':45,'min_length': 5},\n# }\n\n# test_predictionstring = after_deal(test_feat, labels_to_ids, segment_param,print)\n# del test_feat\n# gc.collect()\n\n# print(test_predictionstring.head())\n# test_predictionstring.to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-13T05:31:17.456813Z","iopub.execute_input":"2022-03-13T05:31:17.457189Z","iopub.status.idle":"2022-03-13T05:31:17.464698Z","shell.execute_reply.started":"2022-03-13T05:31:17.457126Z","shell.execute_reply":"2022-03-13T05:31:17.463544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\ndata_pred = pickle.load(open('./ronghe.pkl','+rb'))\ndata_pred['pred'] = data_pred['pred'].apply(lambda x: x/x.sum(axis=1).reshape(-1,1))  # 重要不要遗漏","metadata":{"execution":{"iopub.status.busy":"2022-03-13T05:31:17.468164Z","iopub.execute_input":"2022-03-13T05:31:17.469183Z","iopub.status.idle":"2022-03-13T05:31:17.484551Z","shell.execute_reply.started":"2022-03-13T05:31:17.469133Z","shell.execute_reply":"2022-03-13T05:31:17.483412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport math\nimport re\nfrom numba import jit\ndata_path = '../input/feedback-prize-2021/'\n\nfiles = os.listdir(data_path+'test')\nIDS = [f.replace('.txt','') for f in files if 'txt' in f]\n\nt = {}\nfor f in IDS:\n    t[f.replace('.txt','')] = len(open(f'{data_path}test/{f}.txt', 'r').read().split())\nIDS = sorted(t.keys(), key=lambda x: t[x])\n\nid2label = {0:'Lead', 1:'Position', 2:'Evidence', 3:'Claim', 4:'Concluding Statement',\n             5:'Counterclaim', 6:'Rebuttal', 7:'blank'}\nlabel2id = {v:k for k,v in id2label.items()}\n\nclass CONFIG:\n    def __init__(self):\n        self.max_length = 4096\n        \nconfig = CONFIG() ","metadata":{"execution":{"iopub.status.busy":"2022-03-13T05:31:17.486327Z","iopub.execute_input":"2022-03-13T05:31:17.487092Z","iopub.status.idle":"2022-03-13T05:31:18.143695Z","shell.execute_reply.started":"2022-03-13T05:31:17.487048Z","shell.execute_reply":"2022-03-13T05:31:18.142726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dic_off_map = data_pred[['id','offset_mapping']].set_index('id')['offset_mapping'].to_dict()\ndic_txt = data_pred[['id','text']].set_index('id')['text'].to_dict()\n\ndef change_wenfeng(x):\n    res1  = x[:,8:].sum(axis=1)\n    res2 = np.zeros((len(res1), 8))\n    \n    label_map = {0:5, 1:3, 2:2, 3:1, 4:4, 5:6, 6:7, 7:0}\n    for i in range(8):\n        if i == 7:\n            res2[:,i] = x[:,label_map[i]]\n        else:\n            res2[:,i] = x[:,[label_map[i], label_map[i]+7]].sum(axis=1)\n\n    return res1, res2\n\npreds1_mean = {}\npreds2_mean = {}\nfor irow,row in data_pred.iterrows():\n    t1, t2 = change_wenfeng(row.pred)\n    preds1_mean[row.id] = t1.astype('float64')\n    preds2_mean[row.id] = t2.astype('float64')","metadata":{"execution":{"iopub.status.busy":"2022-03-13T05:31:18.145035Z","iopub.execute_input":"2022-03-13T05:31:18.145327Z","iopub.status.idle":"2022-03-13T05:31:18.168598Z","shell.execute_reply.started":"2022-03-13T05:31:18.145285Z","shell.execute_reply":"2022-03-13T05:31:18.167498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%time\nimport pickle\nimport torch\nimport torch.utils.data as data\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torch.utils.data import Dataset, DataLoader\n\nclass LSTMModel(nn.Module):\n    def __init__(self):\n        super(LSTMModel, self).__init__()\n        self.embedding = nn.Embedding(8,16)\n        self.lstm = nn.LSTM(1, 32, num_layers=1, bidirectional=True)\n        self.fc0 = nn.Linear(64+16, 64+16)\n        self.fc1 = nn.Linear(64+16, 1)\n        self.fc2 = nn.Linear(64+16, 8)\n        self.dropout = nn.Dropout(p=0.2)\n        self.init_parameters()\n\n    def forward(self, x, x2):\n        x = x.unsqueeze(2)\n        sequence_output_l = (x.permute(1, 0, 2))\n        sequence_output_l, _ = self.lstm(sequence_output_l)\n#         print(sequence_output_l.shape)\n        sequence_output_l = sequence_output_l.permute(1, 0, 2)[:,-1,:]\n        sequence_output_l = torch.cat([sequence_output_l,self.embedding(x2).squeeze(1) * 0],1)\n        sequence_output_l = self.dropout(sequence_output_l)\n        sequence_output_l = nn.ReLU()(self.fc0(sequence_output_l))\n        output1 = nn.Sigmoid()(self.fc1(sequence_output_l))\n        output2 = nn.Sigmoid()(self.fc2(sequence_output_l))\n        return output1,output2\n\n    def init_parameters(self):\n        for param in self.parameters():\n            param.data.uniform_(-0.05, 0.05)\n\ndef transform_score(x,n = 32):\n    res = np.zeros(n+1)+0.01\n    res_count = np.zeros(n+1)+0.01\n    for i in range(len(x)):\n        start_index = int(np.floor((i * n)/len(x)))\n        end_index = int(np.ceil(((i + 1) * n)/len(x)))\n        for index in range(start_index,end_index+1):\n            res[index] += x[i]\n            res_count[index] += 1\n    res = res/res_count\n    return res\n\nfrom sklearn.decomposition import PCA,TruncatedSVD\nimport pickle\npca = pickle.load(open('../input/fdddw3/pcamodel.h5', 'rb'))\nlstmmodels = torch.load('../input/fdddw3/lstmmodel1.h5').cuda()\nlstmmodels.eval()\nlstmmodels2 = torch.load('../input/fdddw3/lstmmodel4.h5').cuda()\nlstmmodels2.eval()","metadata":{"execution":{"iopub.status.busy":"2022-03-13T05:31:18.170757Z","iopub.execute_input":"2022-03-13T05:31:18.171399Z","iopub.status.idle":"2022-03-13T05:31:22.177479Z","shell.execute_reply.started":"2022-03-13T05:31:18.171353Z","shell.execute_reply":"2022-03-13T05:31:22.176253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# recall_thre = { \n#     \"Lead\": 0.07,\n#     \"Position\": 0.07,\n#     \"Evidence\": 0.07,\n#     \"Claim\": 0.06,\n#     \"Concluding Statement\": 0.07,\n#     \"Counterclaim\": 0.04,\n#     \"Rebuttal\": 0.03,\n# }\nrecall_thre = { \n    \"Lead\": 0.07,\n    \"Position\": 0.06,\n    \"Evidence\": 0.07,\n    \"Claim\": 0.06,\n    \"Concluding Statement\": 0.07,\n    \"Counterclaim\": 0.03,\n    \"Rebuttal\": 0.02,\n}\n\nL_k = {\n    \"Evidence\": 0.85,\n    \"Rebuttal\": 0.6,\n}\n\ndef deal_predictionstring(df):\n    new_predictionstring = []\n    new_pos_list = []\n    flag_list = []\n    thre = 0.8\n    for id, typ, pos, (start, end) in df.values:\n        flag = 0\n        L = round(max(1, (pos[1]-pos[0]+1)*0.25))\n\n        pos_left = max(0, pos[0]-L)\n        pos_right = min(len(preds1_mean[id]), pos[1]+1+L)\n        \n        if start<10:\n            left_thre = 2\n        else:\n            left_thre = max(preds1_mean[id][pos[0]], 1-preds2_mean[id][pos_left:pos[0],label2id[typ]].min())\n        \n        if pos[1] >= len(preds1_mean[id])-10:\n            right_thre=2\n        else:\n            right_thre = max(preds1_mean[id][pos[1]+1:pos_right].max(), 1-preds2_mean[id][pos[1]+1:pos_right, label2id[typ]].min())\n        \n        if left_thre>thre and right_thre>thre:\n\n            L = math.ceil((pos[1]-pos[0]+1)*L_k.get(typ, 0.65))\n\n            tmp = {}\n            for i in range(pos[0], pos[1]):\n                if i+L>pos[1]:\n                    break\n                tmp[i] = np.sum(preds2_mean[id][i:i+L+1,label2id[typ]])\n            if len(tmp)==0:\n                new_pos = pos\n            else:\n                flag = min(left_thre, right_thre)\n                new_start = max(tmp.keys(), key=lambda x:tmp[x])\n                new_pos = (new_start,new_start+L)\n\n        else:\n            new_pos = pos\n\n        off_map = dic_off_map[id]\n        txt = dic_txt[id]\n        txt_max = len(txt.split())\n\n        start_word = len(txt[:off_map[new_pos[0]][0]].split())\n\n        L = len(txt[off_map[new_pos[0]][0]:off_map[new_pos[1]][1]].split())\n        end_word = min(txt_max, start_word+L) - 1\n\n        new_predictionstring.append([start_word, end_word])\n        new_pos_list.append(new_pos)\n        flag_list.append(flag)\n        \n    df_new = df.copy()\n    df_new['pos'] = new_pos_list\n    df_new['predictionstring'] = new_predictionstring\n    df_new['flag'] = flag_list\n    \n    df_new = pd.concat([df_new, df.loc[df_new[(df_new.flag>=0.8) & (df_new.flag<0.95)].index]])\n    df_new = df_new.reset_index(drop=True)\n    df_new['flag'].fillna(0,inplace=True)\n    \n    return df_new\n\n\ndef get_recall(id):\n    all_predictions = []\n\n    pred1_np = np.array(preds1_mean[id])\n    pred2_np_all = np.array(preds2_mean[id])\n\n    off_map = dic_off_map[id]\n    off_map_len = len(off_map) if off_map[-1][1] != 0 else len(off_map)-1\n    max_length = min(config.max_length, off_map_len)\n    for class_num in range(7):\n        thre = recall_thre[id2label[class_num]]\n        pred2_np = pred2_np_all[:, class_num]\n\n        i_start = 0\n        while i_start < max_length:\n            i = 0\n            if pred1_np[i_start] > thre and pred2_np[i_start:i_start+10].max() > thre: #开头\n                i = i_start + 1\n                if i>=max_length: break\n                while pred1_np[i] < (1-thre) and pred2_np[i:i+10].max() > thre: # 结束\n                    cond = any([\n                        i+1==max_length,\n                        pred1_np[i] > thre,\n                        i+1<max_length and pred2_np[i] < 0.6 and pred2_np[i] - pred2_np[i+1] > thre\n                    ])\n                    if i>i_start+1 and cond:\n                        all_predictions.append((id, id2label[class_num], [i_start, i]))\n                    i += 1\n                    if i>=max_length: break\n\n            if i != 0:\n                if i == max_length:\n                    i -=1\n\n                all_predictions.append((id, id2label[class_num], [i_start, i]))\n            i_start += 1\n                \n    df_recall = pd.DataFrame(all_predictions, columns=['id', 'class', 'pos'])\n    \n    predictionstring = []\n    for cache in df_recall.values:\n        id = cache[0]\n        pos = cache[2]\n        off_map = dic_off_map[id]\n        txt = dic_txt[id]\n        txt_max = len(txt.split())\n\n        start_word = len(txt[:off_map[pos[0]][0]].split())\n\n        L = len(txt[off_map[pos[0]][0]:off_map[pos[1]][1]].split())\n        end_word = min(txt_max, start_word+L) - 1\n\n        predictionstring.append([start_word, end_word])\n\n    df_recall['predictionstring'] = predictionstring\n\n    return deal_predictionstring(df_recall)\n#     return df_recall\n\n\n\n@jit(nopython=True)\ndef feat_speedup(arr):\n    r_max, r_min, r_sum = -1e5,1e5,0\n    for x in arr:\n        r_max = max(r_max, x)\n        r_min = min(r_min, x)\n        r_sum += x\n    return r_max, r_min, r_sum, r_sum/len(arr)\n\nnp_lin = np.linspace(0,1,7)\n\n@jit(nopython=True)\ndef sorted_quantile(array, q):\n    n = len(array)\n    index = (n - 1) * q\n    left = int(index)\n    fraction = index - left\n    right = left\n    right = right + int(fraction > 0)\n    i, j = array[left], array[right]\n    return i + (j - i) * fraction\n\ndef get_percentile(array):\n    x = np.sort(array)\n    n = len(x)-1\n    return x[[int(n*t) for t in np_lin[1:-1]]]\n\n\ndef tuple_map(offset_mapping,threshold):\n    # 无意义0 其他从1 2 3排序\n    paragraph_rk = []\n    rk = 0\n    last = 1\n    for token_index in offset_mapping:\n        if len(threshold) == 0:\n            paragraph_rk.append(1)   # 只有一段\n        elif token_index[1] <= threshold[rk][1]:\n            last = max(rk+1,last)\n            paragraph_rk.append(last)  # 左区间\n        else: \n            last = max(rk+2,last)\n            paragraph_rk.append(last)  # 右区间\n            if rk + 1 < len(threshold) - 1: #判断加一是否溢出\n                rk += 1\n            \n    return paragraph_rk\n\n\ndef get_pos_feat(text, offset_mapping):\n    # 总共几段\n    paragraph_cnt = len(text.split('\\n\\n')) + 1\n    # 第几段\n    paragraph_th = [m.span() for m in re.finditer('\\n\\n',text)]\n    paragraph_rk = tuple_map(offset_mapping,paragraph_th)\n    # 倒数第几段\n    paragraph_rk_r = [paragraph_cnt-rk+1 if rk!=0 else 0 for rk in paragraph_rk]\n    # 总共几句\n    sentence_th = []\n    for i,v in enumerate([m.span() for m in re.finditer('\\n\\n|\\.|,|\\?|\\!',text)]):\n        if i == 0:\n            sentence_th.append(list(v))\n        else:\n            if v[0]==sentence_th[-1][-1]:\n                sentence_th[-1][-1] = v[-1]\n            else:\n                sentence_th.append(list(v))\n    sentence_cnt = len(sentence_th) + 1\n    # 第几句\n    sentence_rk = tuple_map(offset_mapping,sentence_th)\n    # 导数第几句\n    sentence_rk_r = [sentence_cnt-rk+1 if rk!=0 else 0 for rk in sentence_rk]\n\n    # 属于段落的第几句\n    last_garagraph_cnt = 0\n    sentence_rk_of_paragraph = []\n    for i in range(len(offset_mapping)):\n        sentence_rk_of_paragraph.append(sentence_rk[i]-last_garagraph_cnt)\n        if i+1 == len(offset_mapping) or paragraph_rk[i]!=paragraph_rk[i+1]:\n            last_garagraph_cnt = sentence_rk[i]\n\n    # 当前段落几句\n    sentence_cnt_of_paragraph = []\n    last_max = None\n    for i in range(1,len(offset_mapping)+1):\n        if i==1 or paragraph_rk[-i] != paragraph_rk[-i+1]:\n            last_max = sentence_rk_of_paragraph[-i]\n        sentence_cnt_of_paragraph.append(last_max)\n    sentence_cnt_of_paragraph = sentence_cnt_of_paragraph[::-1]\n    # 属于段落的倒数第几句\n    sentence_rk_r_of_paragraph = [s_cnt-rk+1 if rk!=0 else 0 for s_cnt,rk in zip(sentence_cnt_of_paragraph,sentence_rk_of_paragraph)]\n\n    return paragraph_cnt,sentence_cnt,paragraph_rk,paragraph_rk_r,sentence_rk,sentence_rk_r, \\\n            sentence_cnt_of_paragraph,sentence_rk_of_paragraph,sentence_rk_r_of_paragraph\n\n\nlgb_columns = pickle.load(open('../input/feedback-lgb7471/lgb_columns.pkl','rb'))\nlgb_columns = lgb_columns[:14] + ['pca_f'+str(i) for i in range(8)]  + ['pca2_f'+str(i) for i in range(8)] + lgb_columns[14:]\n\ndef fun_get_feat(id):\n    df_feat = []\n    df_feat2 = []\n    data_sub = get_recall(id)\n    txt = dic_txt[id]\n    off_map = dic_off_map[id]\n    txt_feat = get_pos_feat(txt, off_map)\n   \n    preds1_all = preds1_mean[id]\n    preds_type = preds2_mean[id].argmax(axis=-1)\n    \n    text_char_length = len(txt)\n    text_word_length = len(txt.split())\n    text_token_length = len(off_map)\n    repeat_key_set = set()\n    for cache in data_sub.values:\n        id = cache[0]\n        typ = cache[1]\n        start, end = cache[2]\n        prediction = cache[3]\n        repeat_key = id+str(typ)+str(start)+str(end)\n        if repeat_key in repeat_key_set:\n            continue\n        repeat_key_set.add(repeat_key)\n        dic = {k:np.nan for k in lgb_columns}\n#         dic={'id': id}\n        dic['id'] = id\n        dic['pos'] = cache[2]\n        dic['class'] = label2id[typ]\n        dic['post_flag'] = cache[4]\n\n        # 段落特征\n#         txt_feat  = dic_txt_feat[id]\n        dic['paragraph_cnt'] = txt_feat[0]\n        dic['sentence_cnt'] = txt_feat[1]\n        dic['paragraph_rk'] = txt_feat[2][start]\n        dic['paragraph_rk_r'] = txt_feat[3][end]\n        dic['sentence_rk'] = txt_feat[4][start]\n        dic['sentence_rk_r'] = txt_feat[5][end]\n        dic['sentence_cnt_of_paragraph'] = txt_feat[6][start]\n        dic['sentence_cnt_of_paragraph2'] = txt_feat[6][end]\n        dic['sentence_rk_of_paragraph'] = txt_feat[7][start]\n        dic['sentence_rk_r_of_paragraph'] = txt_feat[8][end]\n        dic['sub_paragraph_cnt'] = txt_feat[2][end] - txt_feat[2][start]\n        dic['sub_sentence_cnt'] = txt_feat[4][end] - txt_feat[4][start]\n\n        # 段内统计特征\n        other_type = [t for t in range(8) if t != dic['class']]\n        preds2_all = preds2_mean[id][:, label2id[typ]]\n        preds4_all = preds2_mean[id][:, other_type].max(axis=-1)\n        preds1 = preds1_all[start:end+1]\n        preds2 = preds2_all[start:end+1]\n        preds4 = preds4_all[start:end+1]\n        preds2lstm = preds2_all[max(0,start - 2):min(preds2_all.shape[0] - 1,end+3)]  \n        # preds1lstm = preds1_all[max(0,start - 2):min(preds2_all.shape[0] - 1,end+3)]  \n        # preds2lstm = np.concatenate([preds2lstm,preds1lstm],1)\n        # model = lstmmodels[idlist_lstm_dict[id] % 5]\n        # output1,output2 = model(torch.tensor([([-1] * 128 + preds2lstm.reshape(-1).tolist())[-128:]]).cuda(),\n        # torch.tensor([label2id[typ]]).long().cuda())\n        # output1 = output1.cpu().detach().numpy()[0].tolist()\n        # output2 = output2.cpu().detach().numpy()[0].tolist()\n        # dic[f'lstm_f0'] = output1[0]\n        # for i in range(8):\n        #     dic[f'lstm_f{i}'] = output2[i]\n        # dic2={'id': id, 'x':[preds2lstm.reshape(-1),label2id[typ]]}\n        df_feat2.append([id,preds2lstm.reshape(-1),label2id[typ]])\n\n        word_length = prediction[-1] - prediction[0] + 1\n        \n        pca_res = pca.transform([transform_score(preds2lstm)])[0]\n        for i in range(8):\n            dic['pca_f'+str(i)] = pca_res[i] \n        pca_res = pca.transform([transform_score(preds1_all[max(0,start - 2):min(preds2_all.shape[0] - 1,end+3)])])[0]\n        for i in range(8):\n            dic['pca2_f'+str(i)] = pca_res[i] \n        dic['L1'] = word_length\n        dic['L2'] = end - start + 1\n        dic['text_char_length'] = text_char_length\n        dic['text_word_length'] = text_word_length\n        dic['text_token_length'] = text_token_length\n\n        dic['word_start'] = prediction[0]\n        dic['word_end'] = prediction[-1]\n        dic['token_start'] = start\n        dic['token_start2'] = start / text_token_length\n        dic['token_end'] = end\n        dic['token_end2'] = text_token_length - end\n        dic['token_end3'] = end / text_token_length\n        \n        dic[f'head_preds1'] = preds1[0]\n        dic[f'head2_preds1'] = preds1_all[start-1:start+2].sum()\n        if len(preds1) > 1:\n            dic[f'tail_preds1'] = preds1[-1]\n            dic['max_preds1'], dic['min_preds1'], dic['sum_preds1'], dic['mean_preds1'] = feat_speedup(preds1[1:])\n      \n        sort_idx = preds1[1:].argsort()[::-1]\n        tmp = []\n        for i in range(5):\n            if i < len(sort_idx):\n                dic[f'other_preds1_{i}'] = preds1[1+sort_idx[i]]\n                dic[f'other_preds1_idx_{i}'] = (1+sort_idx[i])/len(preds1)\n                tmp.append(preds1[1+sort_idx[i]])\n        if len(tmp):\n            dic[f'other_preds1_mean'] = np.mean(tmp)\n\n        dic[f'head_preds2'] = preds2[0]\n        dic[f'tail_preds2'] = preds2[-1]\n        dic['max_preds2'], dic['min_preds2'], dic['sum_preds2'], dic['mean_preds2'] = feat_speedup(preds2)\n\n        dic[f'head_preds4'] = preds4[0]\n        dic[f'tail_preds4'] = preds4[-1]\n        dic['max_preds4'], dic['min_preds4'], dic['sum_preds4'], dic['mean_preds4'] = feat_speedup(preds4)\n        \n        sort_idx = preds2.argsort()\n        tmp = []\n        for i in range(5):\n            if i < len(sort_idx):\n                dic[f'other_preds2_{i}'] = preds2[sort_idx[i]]\n                dic[f'other_preds2_idx_{i}'] = (sort_idx[i])/len(preds2)\n                tmp.append(preds2[sort_idx[i]])\n        if len(tmp):\n            dic[f'other_preds2_mean'] = np.mean(tmp)\n            \n            \n        for i,ntile in enumerate([sorted_quantile(preds2,i) for i in np_lin]):\n            dic[f'preds2_trend{i}'] = ntile        # 趋势分布\n        for i,ntile in enumerate(get_percentile(preds2)):\n            dic[f'preds2_ntile{i}'] = ntile        # 分位数\n        for i,ntile in enumerate([sorted_quantile(preds4,i) for i in np_lin]):\n            dic[f'preds4_trend{i}'] = ntile        # 趋势分布\n        for i,ntile in enumerate(get_percentile(preds4)):\n            dic[f'preds4_ntile{i}'] = ntile        # 分位数\n            \n            \n        for i in range(1,4):\n            if start-i >= 0:\n                dic[f'before_head2_prob{i}'] = preds2_all[start-i]\n                dic[f'before_other_prob{i}'] = preds4_all[start-i]\n                dic[f'before_other_type{i}'] = preds_type[start-i]\n                \n            if end+i < len(preds1_all):\n                dic[f'after_head2_prob{i}'] = preds2_all[end+i]\n                dic[f'after_other_prob{i}'] = preds4_all[end+i]\n                dic[f'after_other_type{i}'] = preds_type[end+i]\n\n\n        for mode in ['before', 'after']:\n            for iw, extend_L in enumerate([math.ceil(word_length/2), word_length]):\n                if mode == 'before':\n                    if start-extend_L<0:\n                        continue\n                    preds1_extend = preds1_all[start-extend_L:start]\n                    preds2_extend = preds2_all[start-extend_L:start]\n#                     preds4_extend = preds4_all[start-extend_L:start]\n                else:\n                    if end+extend_L >=len(preds1_all):\n                        continue\n                    preds1_extend = preds1_all[end+1:end+extend_L]\n                    preds2_extend = preds2_all[end+1:end+extend_L]\n#                     preds4_extend = preds4_all[end+1:end+extend_L]\n                    \n                if len(preds1_extend) == 0:\n                    continue\n                dic[f'{mode}{iw}_head_preds1'] = preds1_extend[0]\n                dic[f'{mode}{iw}_max_preds1'], dic[f'{mode}{iw}_min_preds1'], \\\n                dic[f'{mode}{iw}_sum_preds1'], dic[f'{mode}{iw}_mean_preds1'] = feat_speedup(preds1_extend)\n\n                dic[f'{mode}{iw}_head_preds2'] = preds2_extend[0]\n                dic[f'{mode}{iw}_max_preds2'], dic[f'{mode}{iw}_min_preds2'], \\\n                dic[f'{mode}{iw}_sum_preds2'], dic[f'{mode}{iw}_mean_preds2'] = feat_speedup(preds2_extend)\n\n                dic[f'{mode}{iw}_sum_preds1_rate'] = dic[f'{mode}{iw}_sum_preds1'] / dic[f'sum_preds1']\n                dic[f'{mode}{iw}_sum_preds2_rate'] = dic[f'{mode}{iw}_sum_preds2'] / dic[f'sum_preds2']\n                dic[f'{mode}{iw}_max_preds1_rate'] = dic[f'{mode}{iw}_max_preds1'] / dic[f'max_preds1']\n                dic[f'{mode}{iw}_max_preds2_rate'] = dic[f'{mode}{iw}_max_preds2'] / dic[f'max_preds2']\n\n        df_feat.append(dic)\n\n    return [df_feat,df_feat2]\n\n","metadata":{"execution":{"iopub.status.busy":"2022-03-13T05:31:22.179678Z","iopub.execute_input":"2022-03-13T05:31:22.180186Z","iopub.status.idle":"2022-03-13T05:31:22.640188Z","shell.execute_reply.started":"2022-03-13T05:31:22.180139Z","shell.execute_reply":"2022-03-13T05:31:22.639166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%time\nfrom tqdm import tqdm\nfrom joblib import Parallel, delayed\n\nproba_thresh = { \n    \"Lead\": 0.48,\n    \"Position\": 0.4,\n    \"Evidence\": 0.44,\n    \"Claim\": 0.39,\n    \"Concluding Statement\": 0.51,\n    \"Counterclaim\": 0.34,\n    \"Rebuttal\": 0.3,\n}\n\ninter_thresh = { \n    \"Lead\": 0.09,\n    \"Position\": 0.19,\n    \"Evidence\": 0.07,\n    \"Claim\": 0.27,\n    \"Concluding Statement\": 0.15,\n    \"Counterclaim\": 0.25,\n    \"Rebuttal\": 0.21,\n}\n\ndef post_choice(df):\n    rtn = []\n    for k,group in df.groupby(['id','class']):\n        group = group.sort_values('lgb_prob',ascending=False)\n\n        preds_range = []\n        for irow, row in group.iterrows():\n            start = row.word_start\n            end = row.word_end\n            L1 = end-start+1\n            flag = 0\n            for pos_range in preds_range:\n                L2 = pos_range[1] - pos_range[0] + 1\n                intersection = (min(end, pos_range[1]) - max(start, pos_range[0]) + 1) / L1\n                inter_t = inter_thresh[row['class']]\n                if intersection>inter_t and (inter_t<=L1/L2<=1 or inter_t<=L2/L1<=1):\n                    flag = 1\n                    break\n\n            if flag == 0:\n                preds_range.append((start, end, row.lgb_prob))\n                rtn.append((row.id, row['class'], row.pos, row.word_start, row.word_end, row.lgb_prob))\n    rtn = pd.DataFrame(rtn, columns=['id','class','pos','start','end','lgb_prob'])\n    return rtn\n\n\ndata_splits = np.array_split(IDS, round(len(IDS)/2))\n\nlgb_model = pickle.load(open(f'../input/feedback-lgb7471/lgb_fold{0}.pkl','rb'))\nlgb_model1 = pickle.load(open(f'../input/fdddw3/lgb_fold2.pkl','rb'))\nlgb_model2 = pickle.load(open(f'../input/fdddw3/lgb_fold4.pkl','rb'))\n\nsub = pd.DataFrame()\n# for id_sub in data_splits:\n#     results = Parallel(n_jobs=2, backend=\"multiprocessing\")(\n#                 delayed(fun_get_feat)(id) for id in id_sub)\n#     df_feat = []\n#     df_feat2 = []\n#     for res in results:\n#         df_feat += res[0]\n#         df_feat2 += res[1]\nfor id in (IDS):\n#     for ii in tqdm(range(1000)):\n    df_feat,df_feat2 = fun_get_feat(id)\n\n    df_feat = pd.DataFrame(df_feat)\n\n    X = []\n    X2 = []\n    for i in range(len(df_feat2)):\n        X.append(([-1] * 128 + df_feat2[i][1].reshape(-1).tolist())[-128:])\n        X2.append([df_feat2[i][2]])\n\n    model = lstmmodels\n    model2 = lstmmodels2\n    pos = 0\n    df_feat3 = np.zeros((len(df_feat),9))\n    while(True):\n        output1,output2 = model(torch.tensor(X[pos:pos + 32]).cuda(),torch.tensor(X2[pos:pos + 32]).cuda().long())\n\n        df_feat3[pos:pos + output1.shape[0],:1] += output1.cpu().detach().numpy() * 0.5\n        df_feat3[pos:pos + output1.shape[0],1:] += output2.cpu().detach().numpy() * 0.5\n        output1,output2 = model2(torch.tensor(X[pos:pos + 32]).cuda(),torch.tensor(X2[pos:pos + 32]).cuda().long())\n        df_feat3[pos:pos + output1.shape[0],:1] += output1.cpu().detach().numpy() * 0.5\n        df_feat3[pos:pos + output1.shape[0],1:] += output2.cpu().detach().numpy() * 0.5\n        \n        pos += 32\n        if pos >= len(X):\n            break\n\n\n    df_feat3 = pd.DataFrame(df_feat3)\n    df_feat3.columns = ['lstmf' + str(i) for i in range(df_feat3.shape[1])]\n    df_feat = pd.concat([df_feat,df_feat3.iloc[:,:]],1)\n#     lgb_preds = lgb_model1.predict(df_feat.drop(['id','pos'],axis=1))\n    lgb_preds = (lgb_model1.predict(df_feat.drop(['id','pos'],axis=1)) + lgb_model2.predict(df_feat.drop(['id','pos'],axis=1)))/2\n#     print(lgb_model1.predict(df_feat.drop(['id','pos'],axis=1)))\n#     print(lgb_model2.predict(df_feat.drop(['id','pos'],axis=1)))\n    df_final = df_feat[['id', 'class','pos', 'word_start','word_end']].copy()\n    df_final['lgb_prob'] = lgb_preds\n    df_final['class'] = df_final['class'].map(lambda x:id2label[x])\n\n    df_final['thre'] = df_final['class'].map(lambda x: proba_thresh[x])\n    df_final = df_final[df_final.lgb_prob>=df_final.thre]\n    df_final = post_choice(df_final)\n\n    sub = pd.concat([sub, df_final])","metadata":{"execution":{"iopub.status.busy":"2022-03-13T05:31:22.642279Z","iopub.execute_input":"2022-03-13T05:31:22.643052Z","iopub.status.idle":"2022-03-13T05:31:32.780522Z","shell.execute_reply.started":"2022-03-13T05:31:22.643005Z","shell.execute_reply":"2022-03-13T05:31:32.779438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %time\n# from joblib import Parallel, delayed\n# from tqdm import tqdm\n\n# proba_thresh = { \n#     \"Lead\": 0.45,\n#     \"Position\": 0.4,\n#     \"Evidence\": 0.45,\n#     \"Claim\": 0.35,\n#     \"Concluding Statement\": 0.5,\n#     \"Counterclaim\": 0.35,\n#     \"Rebuttal\": 0.3,\n# }\n\n# inter_thresh = { \n#     \"Lead\": 0.15,\n#     \"Position\": 0.15,\n#     \"Evidence\": 0.15,\n#     \"Claim\": 0.25,\n#     \"Concluding Statement\": 0.15,\n#     \"Counterclaim\": 0.25,\n#     \"Rebuttal\": 0.25,\n# }\n\n\n# def post_choice(df):\n#     rtn = []\n#     for k,group in df.groupby(['id','class']):\n#         group = group.sort_values('lgb_prob',ascending=False)\n\n#         preds_range = []\n#         for irow, row in group.iterrows():\n#             start = row.word_start\n#             end = row.word_end\n#             L1 = end-start+1\n#             flag = 0\n#             for pos_range in preds_range:\n#                 L2 = pos_range[1] - pos_range[0] + 1\n#                 intersection = (min(end, pos_range[1]) - max(start, pos_range[0]) + 1) / L1\n#                 inter_t = inter_thresh[row['class']]\n#                 if intersection>inter_t and (inter_t<=L1/L2<=1 or inter_t<=L2/L1<=1):\n#                     flag = 1\n#                     break\n\n#             if flag == 0:\n#                 preds_range.append((start, end, row.lgb_prob))\n#                 rtn.append((row.id, row['class'], row.pos, row.word_start, row.word_end, row.lgb_prob))\n#     rtn = pd.DataFrame(rtn, columns=['id','class','pos','start','end','lgb_prob'])\n#     return rtn\n\n\n# data_splits = np.array_split(IDS, round(len(IDS)/2))\n\n# lgb_model = pickle.load(open(f'../input/feedback-lgb7471/lgb_fold{0}.pkl','rb'))\n\n# sub = pd.DataFrame()\n# for id_sub in data_splits:\n# # for id in tqdm(IDS):\n# #     df_feat = pd.DataFrame(fun_get_feat(id))\n#     for i in tqdm(range(1000)):\n#         results = Parallel(n_jobs=2, backend=\"multiprocessing\")(\n#                     delayed(fun_get_feat)(id) for id in id_sub)\n#         df_feat = []\n#         for res in results:\n#             df_feat += res\n#         df_feat = pd.DataFrame(df_feat)\n\n#         lgb_preds = lgb_model.predict(df_feat.drop(['id','pos'],axis=1))\n\n#         df_final = df_feat[['id', 'class','pos', 'word_start','word_end']].copy()\n#         df_final['lgb_prob'] = lgb_preds\n#         df_final['class'] = df_final['class'].map(lambda x:id2label[x])\n\n#         df_final['thre'] = df_final['class'].map(lambda x: proba_thresh[x])\n#         df_final = df_final[df_final.lgb_prob>=df_final.thre]\n#         df_final = post_choice(df_final)\n\n#         sub = pd.concat([sub, df_final])","metadata":{"execution":{"iopub.status.busy":"2022-03-13T05:31:32.784434Z","iopub.execute_input":"2022-03-13T05:31:32.78495Z","iopub.status.idle":"2022-03-13T05:31:32.793208Z","shell.execute_reply.started":"2022-03-13T05:31:32.784915Z","shell.execute_reply":"2022-03-13T05:31:32.791784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_predictionstring(df):\n    predictionstring = []\n    for cache in df.values:\n        predictionstring.append(' '.join(list(map(str, range(cache[3], cache[4]+1)))))\n    return predictionstring\n\n\nsub['predictionstring'] = get_predictionstring(sub)","metadata":{"execution":{"iopub.status.busy":"2022-03-13T05:31:32.795632Z","iopub.execute_input":"2022-03-13T05:31:32.796053Z","iopub.status.idle":"2022-03-13T05:31:32.808539Z","shell.execute_reply.started":"2022-03-13T05:31:32.796009Z","shell.execute_reply":"2022-03-13T05:31:32.807602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub[['id', 'class', 'predictionstring']].to_csv('submission.csv',index=False)\n# sub.sample(round(0.9*len(sub)))[['id', 'class', 'predictionstring']].to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-13T05:31:32.80995Z","iopub.execute_input":"2022-03-13T05:31:32.810979Z","iopub.status.idle":"2022-03-13T05:31:32.825096Z","shell.execute_reply.started":"2022-03-13T05:31:32.810926Z","shell.execute_reply":"2022-03-13T05:31:32.824025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub[sub['id']=='18409261F5C2']","metadata":{"execution":{"iopub.status.busy":"2022-03-13T05:31:32.826903Z","iopub.execute_input":"2022-03-13T05:31:32.827693Z","iopub.status.idle":"2022-03-13T05:31:32.854439Z","shell.execute_reply.started":"2022-03-13T05:31:32.827589Z","shell.execute_reply":"2022-03-13T05:31:32.853534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}