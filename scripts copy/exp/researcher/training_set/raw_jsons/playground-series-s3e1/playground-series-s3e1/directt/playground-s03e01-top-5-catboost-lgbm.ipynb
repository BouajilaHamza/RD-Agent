{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-01-09T12:57:51.714144Z","iopub.execute_input":"2023-01-09T12:57:51.71456Z","iopub.status.idle":"2023-01-09T12:57:51.72733Z","shell.execute_reply.started":"2023-01-09T12:57:51.714528Z","shell.execute_reply":"2023-01-09T12:57:51.726368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom pandas_profiling import ProfileReport\n\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom tqdm import tqdm\n\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2023-01-09T12:57:51.729343Z","iopub.execute_input":"2023-01-09T12:57:51.729936Z","iopub.status.idle":"2023-01-09T12:57:51.750953Z","shell.execute_reply.started":"2023-01-09T12:57:51.729897Z","shell.execute_reply":"2023-01-09T12:57:51.74934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/playground-series-s3e1/train.csv')\ntest_df = pd.read_csv('/kaggle/input/playground-series-s3e1/test.csv')\nsubmission_df = pd.read_csv('/kaggle/input/playground-series-s3e1/sample_submission.csv')\ntrain_df = train_df.drop('id', axis=1)\ntest_df = test_df.drop('id', axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-01-09T12:57:51.752576Z","iopub.execute_input":"2023-01-09T12:57:51.753239Z","iopub.status.idle":"2023-01-09T12:57:51.984468Z","shell.execute_reply.started":"2023-01-09T12:57:51.753203Z","shell.execute_reply":"2023-01-09T12:57:51.983126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## EDA","metadata":{}},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-09T12:57:51.987088Z","iopub.execute_input":"2023-01-09T12:57:51.987501Z","iopub.status.idle":"2023-01-09T12:57:52.16164Z","shell.execute_reply.started":"2023-01-09T12:57:51.987468Z","shell.execute_reply":"2023-01-09T12:57:52.160152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.shape[0], test_df.shape[0], submission_df.shape[0]","metadata":{"execution":{"iopub.status.busy":"2023-01-09T12:57:52.163626Z","iopub.execute_input":"2023-01-09T12:57:52.164154Z","iopub.status.idle":"2023-01-09T12:57:52.175285Z","shell.execute_reply.started":"2023-01-09T12:57:52.164101Z","shell.execute_reply":"2023-01-09T12:57:52.173778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"profile = ProfileReport(train_df, title=\"Pandas Profiling Report\")\n# profile","metadata":{"execution":{"iopub.status.busy":"2023-01-09T12:57:52.177277Z","iopub.execute_input":"2023-01-09T12:57:52.177786Z","iopub.status.idle":"2023-01-09T12:57:52.186868Z","shell.execute_reply.started":"2023-01-09T12:57:52.17773Z","shell.execute_reply":"2023-01-09T12:57:52.185556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"correlations = profile.description_set[\"correlations\"]\nprint(correlations['pearson']['MedHouseVal'].sort_values(ascending=False), '\\n\\n')\nprint(correlations['spearman']['MedHouseVal'].sort_values(ascending=False), '\\n\\n')","metadata":{"execution":{"iopub.status.busy":"2023-01-09T12:57:52.188296Z","iopub.execute_input":"2023-01-09T12:57:52.188683Z","iopub.status.idle":"2023-01-09T12:58:13.046837Z","shell.execute_reply.started":"2023-01-09T12:57:52.18863Z","shell.execute_reply":"2023-01-09T12:58:13.0454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install dython","metadata":{"execution":{"iopub.status.busy":"2023-01-09T12:58:13.048256Z","iopub.execute_input":"2023-01-09T12:58:13.048607Z","iopub.status.idle":"2023-01-09T12:58:27.766726Z","shell.execute_reply.started":"2023-01-09T12:58:13.048574Z","shell.execute_reply":"2023-01-09T12:58:27.765425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from dython.nominal import associations\n\nassociations(train_df)['corr']","metadata":{"execution":{"iopub.status.busy":"2023-01-09T12:58:27.771022Z","iopub.execute_input":"2023-01-09T12:58:27.771444Z","iopub.status.idle":"2023-01-09T12:58:29.04512Z","shell.execute_reply.started":"2023-01-09T12:58:27.771403Z","shell.execute_reply":"2023-01-09T12:58:29.043887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"TODO:\n- additional training data - DONE\n- additional feature MedInc / Population - DONE\n- additional features long/lat-itude - DONE\n- remove outliers - DONE\n\nTODO v2:\n- categorical features long/lat-itude - DONE\n- scaling for NN - DONE","metadata":{}},{"cell_type":"code","source":"original_df = fetch_california_housing(as_frame=True)['frame']\ntrain_df['is_generated'] = 1\ntest_df['is_generated'] = 1\noriginal_df['is_generated'] = 0\ntrain_df = pd.concat([train_df, original_df]).reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2023-01-09T12:58:29.046635Z","iopub.execute_input":"2023-01-09T12:58:29.047036Z","iopub.status.idle":"2023-01-09T12:58:30.486284Z","shell.execute_reply.started":"2023-01-09T12:58:29.047001Z","shell.execute_reply":"2023-01-09T12:58:30.485083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### (Long/Lat)itude features engineering\n\nbased on: https://bmanikan.medium.com/feature-engineering-all-i-learned-about-geo-spatial-features-649871d16796","metadata":{}},{"cell_type":"code","source":"train_df['r'] = np.sqrt(train_df['Latitude']**2 + train_df['Longitude']**2)\ntrain_df['theta'] = np.arctan2(train_df['Latitude'], train_df['Longitude'])\n\ntest_df['r'] = np.sqrt(test_df['Latitude']**2 + test_df['Longitude']**2)\ntest_df['theta'] = np.arctan2(test_df['Latitude'], test_df['Longitude'])","metadata":{"execution":{"iopub.status.busy":"2023-01-09T12:58:30.487685Z","iopub.execute_input":"2023-01-09T12:58:30.488088Z","iopub.status.idle":"2023-01-09T12:58:30.504675Z","shell.execute_reply.started":"2023-01-09T12:58:30.488054Z","shell.execute_reply":"2023-01-09T12:58:30.503466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['rot_15_x'] = (np.cos(np.radians(15)) * train_df['Longitude']) + (np.sin(np.radians(15)) * train_df['Latitude'])\ntrain_df['rot_15_y'] = (np.cos(np.radians(15)) * train_df['Latitude']) - (np.sin(np.radians(15)) * train_df['Longitude'])\n\ntrain_df['rot_30_x'] = (np.cos(np.radians(30)) * train_df['Longitude']) + (np.sin(np.radians(30)) * train_df['Latitude'])\ntrain_df['rot_30_y'] = (np.cos(np.radians(30)) * train_df['Latitude']) - (np.sin(np.radians(30)) * train_df['Longitude'])\n\ntrain_df['rot_45_x'] = (np.cos(np.radians(45)) * train_df['Longitude']) + (np.sin(np.radians(45)) * train_df['Latitude'])\ntrain_df['rot_45_y'] = (np.cos(np.radians(45)) * train_df['Latitude']) - (np.sin(np.radians(45)) * train_df['Longitude'])","metadata":{"execution":{"iopub.status.busy":"2023-01-09T12:58:30.506023Z","iopub.execute_input":"2023-01-09T12:58:30.506374Z","iopub.status.idle":"2023-01-09T12:58:30.528066Z","shell.execute_reply.started":"2023-01-09T12:58:30.50633Z","shell.execute_reply":"2023-01-09T12:58:30.526742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df['rot_15_x'] = (np.cos(np.radians(15)) * test_df['Longitude']) + (np.sin(np.radians(15)) * test_df['Latitude'])\ntest_df['rot_15_y'] = (np.cos(np.radians(15)) * test_df['Latitude']) - (np.sin(np.radians(15)) * test_df['Longitude'])\n\ntest_df['rot_30_x'] = (np.cos(np.radians(30)) * test_df['Longitude']) + (np.sin(np.radians(30)) * test_df['Latitude'])\ntest_df['rot_30_y'] = (np.cos(np.radians(30)) * test_df['Latitude']) - (np.sin(np.radians(30)) * test_df['Longitude'])\n\ntest_df['rot_45_x'] = (np.cos(np.radians(45)) * test_df['Longitude']) + (np.sin(np.radians(45)) * test_df['Latitude'])\ntest_df['rot_45_y'] = (np.cos(np.radians(45)) * test_df['Latitude']) - (np.sin(np.radians(45)) * test_df['Longitude'])","metadata":{"execution":{"iopub.status.busy":"2023-01-09T12:58:30.529901Z","iopub.execute_input":"2023-01-09T12:58:30.530336Z","iopub.status.idle":"2023-01-09T12:58:30.549391Z","shell.execute_reply.started":"2023-01-09T12:58:30.530303Z","shell.execute_reply":"2023-01-09T12:58:30.547896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import PCA\n\ndef pca(data):\n    '''\n    input: dataframe containing Latitude(x) and Longitude(y)\n    '''\n    coordinates = data[['Latitude','Latitude']].values\n    pca_obj = PCA().fit(coordinates)\n    pca_x = pca_obj.transform(data[['Latitude', 'Longitude']].values)[:,0]\n    pca_y = pca_obj.transform(data[['Latitude', 'Longitude']].values)[:,1]\n    return pca_x, pca_y\n\ntrain_df['pca_x'], train_df['pca_y'] = pca(train_df)\ntest_df['pca_x'], test_df['pca_y'] = pca(test_df)","metadata":{"execution":{"iopub.status.busy":"2023-01-09T12:58:30.552701Z","iopub.execute_input":"2023-01-09T12:58:30.55382Z","iopub.status.idle":"2023-01-09T12:58:30.636086Z","shell.execute_reply.started":"2023-01-09T12:58:30.553771Z","shell.execute_reply":"2023-01-09T12:58:30.634405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install reverse_geocoder --quiet","metadata":{"execution":{"iopub.status.busy":"2023-01-09T12:58:30.638937Z","iopub.execute_input":"2023-01-09T12:58:30.640179Z","iopub.status.idle":"2023-01-09T12:58:46.007604Z","shell.execute_reply.started":"2023-01-09T12:58:30.640107Z","shell.execute_reply":"2023-01-09T12:58:46.005893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import reverse_geocoder as rg\n\ndef geocoder(data):\n    '''\n    input: dataframe containing Latitude(x) and Longitude(y) coordinates\n    output: JSON data containing info on available building or street names.\n    '''\n    coordinates = list(zip(data['Latitude'].values, data['Longitude'].values))\n    results = rg.search(coordinates) # default mode = 2\n    return results","metadata":{"execution":{"iopub.status.busy":"2023-01-09T12:58:46.010422Z","iopub.execute_input":"2023-01-09T12:58:46.01179Z","iopub.status.idle":"2023-01-09T12:58:46.026047Z","shell.execute_reply.started":"2023-01-09T12:58:46.011724Z","shell.execute_reply":"2023-01-09T12:58:46.024859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_values(df, column):\n    return df[column].value_counts(normalize=True)[df[column].value_counts(normalize=True) < 0.005].index.tolist()\n\ngeocoder_data = geocoder(train_df)\n\ntrain_df['admin2'] = [val['admin2'] for val in geocoder_data]\ntrain_df['admin2'][train_df['admin2'].isin(get_values(train_df, 'admin2'))] = 'Other'\ntrain_df['admin2'].value_counts()\n\ngeocoder_data = geocoder(test_df)\n\ntest_df['admin2'] = [val['admin2'] for val in geocoder_data]\ntest_df['admin2'][test_df['admin2'].isin(get_values(test_df, 'admin2'))] = 'Other'\ntest_df['admin2'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-01-09T12:58:46.027937Z","iopub.execute_input":"2023-01-09T12:58:46.028993Z","iopub.status.idle":"2023-01-09T12:58:47.875417Z","shell.execute_reply.started":"2023-01-09T12:58:46.028952Z","shell.execute_reply":"2023-01-09T12:58:47.873737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import geopy.distance\n\n# sacramento = (38.576931, -121.494949)\n# san_francisco = (37.780080, -122.420160)\n# san_jose = (37.334789, -121.888138)\n# los_angeles = (34.052235, -118.243683)\n# san_diego = (32.715759, -117.163818)\n\n# train_df['sacramento_distance'] = [geopy.distance.geodesic(sacramento, row).km for row in (list(zip(train_df['Latitude'].values, train_df['Longitude'].values)))]\n# train_df['san_francisco_distance'] = [geopy.distance.geodesic(san_francisco, row).km for row in (list(zip(train_df['Latitude'].values, train_df['Longitude'].values)))]\n# train_df['san_jose_distance'] = [geopy.distance.geodesic(san_jose, row).km for row in (list(zip(train_df['Latitude'].values, train_df['Longitude'].values)))]\n# train_df['los_angeles_distance'] = [geopy.distance.geodesic(los_angeles, row).km for row in (list(zip(train_df['Latitude'].values, train_df['Longitude'].values)))]\n# train_df['san_diego_distance'] = [geopy.distance.geodesic(san_diego, row).km for row in (list(zip(train_df['Latitude'].values, train_df['Longitude'].values)))]\n\n# test_df['sacramento_distance'] = [geopy.distance.geodesic(sacramento, row).km for row in (list(zip(test_df['Latitude'].values, test_df['Longitude'].values)))]\n# test_df['san_francisco_distance'] = [geopy.distance.geodesic(san_francisco, row).km for row in (list(zip(test_df['Latitude'].values, test_df['Longitude'].values)))]\n# test_df['san_jose_distance'] = [geopy.distance.geodesic(san_jose, row).km for row in (list(zip(test_df['Latitude'].values, test_df['Longitude'].values)))]\n# test_df['los_angeles_distance'] = [geopy.distance.geodesic(los_angeles, row).km for row in (list(zip(test_df['Latitude'].values, test_df['Longitude'].values)))]\n# test_df['san_diego_distance'] = [geopy.distance.geodesic(san_diego, row).km for row in (list(zip(test_df['Latitude'].values, test_df['Longitude'].values)))]","metadata":{"execution":{"iopub.status.busy":"2023-01-08T08:44:23.018661Z","iopub.execute_input":"2023-01-08T08:44:23.019033Z","iopub.status.idle":"2023-01-08T08:44:23.025229Z","shell.execute_reply.started":"2023-01-08T08:44:23.019Z","shell.execute_reply":"2023-01-08T08:44:23.02429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import json\n# import pandas as pd\n# import numpy as np\n# from tqdm.auto import tqdm\n# tqdm.pandas()\n\n# # Reads California coastline coordinates.\n# # (https://earthworks.stanford.edu/catalog/stanford-vx275xn8886) @kaivanbrunt\n# path = r'/kaggle/input/coastline-data/stanford-vx275xn8886-geojson.json'\n# with open(path, 'r') as f:\n#     coastline = json.load(f)\n#     features = coastline['features']\n\n# # Unpacks California coastline coordinates and builds a dataframe. shape=(25693, 2) \n# cstl_coords = [features[i]['geometry']['coordinates'] for i in range(len(features))]\n# cstl_coords = np.hstack(cstl_coords).reshape((-1, 2))\n# cstl_df = pd.DataFrame(cstl_coords, columns=['Longitude', 'Latitude'])\n\n# # Finds the shortest distance to the coastline (Euclidian Distance).\n# def f(lat, lon, df):\n#     return (((df.Latitude - lat)**2 + (df.Longitude - lon)**2)**.5).min()\n\n# train_df['dist_to_cstl'] = train_df.progress_apply(lambda x: f(x.Latitude, x.Longitude, cstl_df), axis=1)\n# test_df['dist_to_cstl'] = test_df.progress_apply(lambda x: f(x.Latitude, x.Longitude, cstl_df), axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-01-08T08:44:23.026644Z","iopub.execute_input":"2023-01-08T08:44:23.02726Z","iopub.status.idle":"2023-01-08T08:44:23.040252Z","shell.execute_reply.started":"2023-01-08T08:44:23.027227Z","shell.execute_reply":"2023-01-08T08:44:23.038989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Number of houses in block : Population / AveOccup (size of block)\n# Total income of block : MedInc * Population (total wealth of each block - could adjust to discount children)\n# Ratio of occupants to bedrooms : AveOccup / AveBedrms (could help identify summer houses)\n# Number of unused bedrooms : AveBedrms - AveOccup (could correspond to guest rooms)\n# Total number of rooms : AveBedrms + AveRooms (indicates size of house)\n# Number of non-bedrooms rooms : AveRooms - AveBedrms (how many bathrooms, kitchens etc.)\n# Ratio of bedrooms to rooms : AveBedrms/AveRooms (could be useful)\n# Ratio of occupants to rooms : AveOccup / AveRooms (could be useful)\n# Distance to nearest block : min_{L2 dist} (Latitude, Longitude) (how remote is this block)\n# Number of blocks in 10 mile/km radius (how connected is this block)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['size_of_block'] = train_df['Population'] / train_df['AveOccup']\ntest_df['size_of_block'] = test_df['Population'] / test_df['AveOccup']\n\ntrain_df['income_of_block'] = train_df['MedInc'] * train_df['Population']\ntest_df['income_of_block'] = test_df['MedInc'] * test_df['Population']\n\ntrain_df['occupants/bedrooms'] = train_df['AveOccup'] / train_df['AveBedrms']\ntest_df['occupants/bedrooms'] = test_df['AveOccup'] / test_df['AveBedrms']\n\ntrain_df['unused_bedrooms'] = train_df['AveBedrms'] - train_df['AveOccup']\ntest_df['unused_bedrooms'] = test_df['AveBedrms'] - test_df['AveOccup']\n\ntrain_df['total_rooms'] = train_df['AveBedrms'] + train_df['AveRooms']\ntest_df['total_rooms'] = test_df['AveBedrms'] + test_df['AveRooms']\n\ntrain_df['non_bedrooms'] = train_df['AveRooms'] - train_df['AveBedrms']\ntest_df['non_bedrooms'] = test_df['AveRooms'] - test_df['AveBedrms']\n\ntrain_df['bedrooms/rooms'] = train_df['AveBedrms'] / train_df['AveRooms']\ntest_df['bedrooms/rooms'] = test_df['AveBedrms'] / test_df['AveRooms']\n\ntrain_df['occupants/rooms'] = train_df['AveOccup'] / train_df['AveRooms']\ntest_df['occupants/rooms'] = test_df['AveOccup'] / test_df['AveRooms']","metadata":{"execution":{"iopub.status.busy":"2023-01-09T13:09:37.16977Z","iopub.execute_input":"2023-01-09T13:09:37.170239Z","iopub.status.idle":"2023-01-09T13:09:37.196762Z","shell.execute_reply.started":"2023-01-09T13:09:37.170199Z","shell.execute_reply":"2023-01-09T13:09:37.195607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modeling","metadata":{}},{"cell_type":"markdown","source":"TODO:\n- CV vs Hold-out - DONE\n- RF (baseline) - DONE\n- CatBoost - DONE\n\nTODO v2:\n- Feedforward NN - DONE (didn't work best tho)\n- Ensemble CatBoost / LightGBM/ NN - PARTLY DONE","metadata":{}},{"cell_type":"code","source":"y = train_df['MedHouseVal']\nX = train_df.drop(['MedHouseVal'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-01-09T13:09:42.590991Z","iopub.execute_input":"2023-01-09T13:09:42.592361Z","iopub.status.idle":"2023-01-09T13:09:42.619114Z","shell.execute_reply.started":"2023-01-09T13:09:42.592309Z","shell.execute_reply":"2023-01-09T13:09:42.618015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cross-validating CatBoost","metadata":{}},{"cell_type":"code","source":"from catboost import CatBoostRegressor\nfrom sklearn.model_selection import KFold\nfrom tqdm import tqdm\n\nrmses = []\nmodels = []\nkf = KFold(n_splits=10, random_state=42, shuffle=True)\n\nfor train_index, val_index in tqdm(kf.split(X)):\n    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n\n    model = CatBoostRegressor(iterations=15_000, loss_function='RMSE')\n    model.fit(X_train, y_train, eval_set=(X_val, y_val), cat_features=['admin2'],\n              early_stopping_rounds=1500, use_best_model=True, verbose=5000)\n    pred = model.predict(X_val)\n    \n    models.append(model)\n    rmses.append(mean_squared_error(y_val, pred, squared=False))\n    \nprint(f'Total RMSE: {np.mean(rmses)}')","metadata":{"execution":{"iopub.status.busy":"2023-01-09T13:12:11.94833Z","iopub.execute_input":"2023-01-09T13:12:11.948828Z","iopub.status.idle":"2023-01-09T13:58:43.254274Z","shell.execute_reply.started":"2023-01-09T13:12:11.948789Z","shell.execute_reply":"2023-01-09T13:58:43.253058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Total RMSE: 0.5090353897181753","metadata":{"execution":{"iopub.status.busy":"2023-01-09T13:58:43.256594Z","iopub.execute_input":"2023-01-09T13:58:43.25745Z","iopub.status.idle":"2023-01-09T13:58:43.263328Z","shell.execute_reply.started":"2023-01-09T13:58:43.257402Z","shell.execute_reply":"2023-01-09T13:58:43.262033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 1.0 basic additional features: \n# 2.0 basic features + distance to cities: \n# 3.0 basic feats + dist to cities + dist to coast: ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CatBoost feature importances","metadata":{}},{"cell_type":"code","source":"feature_importances = np.zeros(28)\nfor model in models[:10]:\n    feature_importances += model.get_feature_importance()\n    \nfeature_names = X_train.columns\nfor score, name in sorted(zip(feature_importances / 10, feature_names), reverse=True):\n    print('{}: {}'.format(name, score))","metadata":{"execution":{"iopub.status.busy":"2023-01-09T13:59:26.586767Z","iopub.execute_input":"2023-01-09T13:59:26.588149Z","iopub.status.idle":"2023-01-09T13:59:27.133512Z","shell.execute_reply.started":"2023-01-09T13:59:26.588099Z","shell.execute_reply":"2023-01-09T13:59:27.132389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cross-validating LightGBM","metadata":{}},{"cell_type":"code","source":"from lightgbm import LGBMRegressor\nimport lightgbm as lgbm\n\nparams= {\n 'lambda_l1': 1.945,\n 'num_leaves': 87,\n 'feature_fraction': 0.79,\n 'bagging_fraction': 0.93,\n 'bagging_freq': 4,\n 'min_data_in_leaf': 103,\n 'max_depth': 17,\n}\n\nrmses = []\nkf = KFold(n_splits=10, random_state=42, shuffle=True)\nX['admin2'] = X['admin2'].astype('category')\n\nfor train_index, val_index in tqdm(kf.split(X)):\n    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n    \n    model = LGBMRegressor(learning_rate=0.01, n_estimators=15_000, metric='rmse', **params)\n    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], categorical_feature=['admin2'],\n              callbacks=[lgbm.early_stopping(1500, verbose=True)])\n    pred = model.predict(X_val)\n    \n    models.append(model)\n    rmses.append(mean_squared_error(y_val, pred, squared=False))\n    \nprint(f'Total RMSE: {np.mean(rmses)}')","metadata":{"execution":{"iopub.status.busy":"2023-01-08T09:27:10.823596Z","iopub.execute_input":"2023-01-08T09:27:10.823943Z","iopub.status.idle":"2023-01-08T09:34:11.211173Z","shell.execute_reply.started":"2023-01-08T09:27:10.823911Z","shell.execute_reply":"2023-01-08T09:34:11.210061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LightGBM feature importances","metadata":{}},{"cell_type":"code","source":"feature_importances = np.zeros(20)\nfor model in models[10:]:\n    feature_importances += model.feature_importances_\n    \nfeature_names = X_train.columns\nfor score, name in sorted(zip(feature_importances / 10, feature_names), reverse=True):\n    print('{}: {}'.format(name, score))","metadata":{"execution":{"iopub.status.busy":"2023-01-08T09:34:11.213486Z","iopub.execute_input":"2023-01-08T09:34:11.213841Z","iopub.status.idle":"2023-01-08T09:34:11.242236Z","shell.execute_reply.started":"2023-01-08T09:34:11.213807Z","shell.execute_reply":"2023-01-08T09:34:11.240819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cross-validating XGBoost","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBRegressor\n\nrmses = []\nkf = KFold(n_splits=10, random_state=42, shuffle=True)\nX['admin2'] = X['admin2'].astype('category')\n\nparams= {\n    'max_depth': 9,\n    'colsample_bytree': 0.66,\n    'subsample': 0.9,\n    'min_child_weight': 22,\n    'reg_lambda': 16,\n    'tree_method': 'hist'\n}\n\nfor train_index, val_index in tqdm(kf.split(X)):\n    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n    \n    model = XGBRegressor(learning_rate=0.01, n_estimators=15_000,\n                         eval_metric='rmse', enable_categorical=True, **params)\n    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=1500, verbose=5000)\n    pred = model.predict(X_val)\n    \n    models.append(model)\n    rmses.append(mean_squared_error(y_val, pred, squared=False))\n    \nprint(f'Total RMSE: {np.mean(rmses)}')","metadata":{"execution":{"iopub.status.busy":"2023-01-09T13:59:50.962124Z","iopub.execute_input":"2023-01-09T13:59:50.962574Z","iopub.status.idle":"2023-01-09T14:14:33.863499Z","shell.execute_reply.started":"2023-01-09T13:59:50.962543Z","shell.execute_reply":"2023-01-09T14:14:33.862579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Total RMSE: 0.5072187072363529","metadata":{"execution":{"iopub.status.busy":"2023-01-09T13:58:43.338421Z","iopub.status.idle":"2023-01-09T13:58:43.339518Z","shell.execute_reply.started":"2023-01-09T13:58:43.339138Z","shell.execute_reply":"2023-01-09T13:58:43.33917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 1.0 basic additional features: 0.5095460287618219\n# 2.0 basic features + distance to cities: \n# 3.0 basic feats + dist to cities + dist to coast: ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## XGBoost feature importances","metadata":{}},{"cell_type":"code","source":"feature_importances = np.zeros(28)\nfor model in models[10:]:\n    feature_importances += model.feature_importances_\n    \nfeature_names = X_train.columns\nfor score, name in sorted(zip(feature_importances / 10, feature_names), reverse=True):\n    print('{}: {}'.format(name, score))","metadata":{"execution":{"iopub.status.busy":"2023-01-09T14:14:47.170773Z","iopub.execute_input":"2023-01-09T14:14:47.171194Z","iopub.status.idle":"2023-01-09T14:14:47.441618Z","shell.execute_reply.started":"2023-01-09T14:14:47.17115Z","shell.execute_reply":"2023-01-09T14:14:47.440381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feedforward NN","metadata":{}},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, BatchNormalization\nfrom keras import backend as K\nfrom sklearn.preprocessing import MinMaxScaler\n\n\ndef root_mean_squared_error(y_true, y_pred):\n        return K.sqrt(K.mean(K.square(y_pred - y_true))) \n    \n\ndef create_nn_model():    \n    model = Sequential()\n\n    model.add(Dense(512, activation='relu', input_shape=(18,)))\n    model.add(Dense(512, activation='relu'))\n    model.add(Dense(512, activation='relu'))\n    model.add(Dense(1))\n\n    model.compile(optimizer='adam',loss=root_mean_squared_error)\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2023-01-06T23:00:11.677574Z","iopub.execute_input":"2023-01-06T23:00:11.678287Z","iopub.status.idle":"2023-01-06T23:00:19.039303Z","shell.execute_reply.started":"2023-01-06T23:00:11.678245Z","shell.execute_reply":"2023-01-06T23:00:19.037953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# models = []\n# rmses = []\n# kf = KFold(n_splits=10, random_state=42, shuffle=True)\n\n# for train_index, val_index in tqdm(kf.split(X)):\n#     X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n#     y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n    \n#     minmax = MinMaxScaler()\n#     X_train = minmax.fit_transform(X_train)\n#     X_val = minmax.transform(X_val)\n    \n#     model = create_nn_model()\n#     model.fit(X_train, y_train, batch_size=128, epochs=100, validation_data=(X_val, y_val))\n#     pred = model.predict(X_val)\n    \n#     models.append(model)\n#     rmses.append(mean_squared_error(y_val, pred, squared=False))","metadata":{"execution":{"iopub.status.busy":"2023-01-06T23:00:19.041242Z","iopub.execute_input":"2023-01-06T23:00:19.042228Z","iopub.status.idle":"2023-01-06T23:00:19.048453Z","shell.execute_reply.started":"2023-01-06T23:00:19.042184Z","shell.execute_reply":"2023-01-06T23:00:19.046884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Unfortunately, didn't work best, so as a final sumbmission let's ensemble CatBoost & LGBM only.","metadata":{}},{"cell_type":"markdown","source":"## Submission","metadata":{}},{"cell_type":"code","source":"# test_df['admin2'] = test_df['admin2'].astype('category')\n\n# cb_pred = np.mean(np.stack([model.predict(test_df) for model in models[:10]]), axis=0)\n# lgbm_pred = np.mean(np.stack([model.predict(test_df) for model in models[10:20]]), axis=0)\n# xgb_pred = np.mean(np.stack([model.predict(test_df) for model in models[20:]]), axis=0)","metadata":{"execution":{"iopub.status.busy":"2023-01-08T09:49:47.258824Z","iopub.execute_input":"2023-01-08T09:49:47.259259Z","iopub.status.idle":"2023-01-08T09:50:40.87851Z","shell.execute_reply.started":"2023-01-08T09:49:47.259224Z","shell.execute_reply":"2023-01-08T09:50:40.877309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_pred = 0.45 * xgb_pred + 0.55 * cb_pred","metadata":{"execution":{"iopub.status.busy":"2023-01-08T10:04:15.686854Z","iopub.execute_input":"2023-01-08T10:04:15.687265Z","iopub.status.idle":"2023-01-08T10:04:15.694838Z","shell.execute_reply.started":"2023-01-08T10:04:15.687233Z","shell.execute_reply":"2023-01-08T10:04:15.693154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df['admin2'] = test_df['admin2'].astype('category')\ntest_pred = np.mean(np.stack([model.predict(test_df) for model in models]), axis=0)\nsubmission = pd.DataFrame(data={'id': submission_df.id, 'MedHouseVal': test_pred})\nsubmission.MedHouseVal.clip(0, train_df.MedHouseVal.max(), inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-01-09T14:14:51.873169Z","iopub.execute_input":"2023-01-09T14:14:51.873558Z","iopub.status.idle":"2023-01-09T14:15:09.409915Z","shell.execute_reply.started":"2023-01-09T14:14:51.873527Z","shell.execute_reply":"2023-01-09T14:15:09.408689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission_cb_xgb_additional_basic_features.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-01-09T14:15:09.412173Z","iopub.execute_input":"2023-01-09T14:15:09.416374Z","iopub.status.idle":"2023-01-09T14:15:09.490326Z","shell.execute_reply.started":"2023-01-09T14:15:09.416318Z","shell.execute_reply":"2023-01-09T14:15:09.488955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}