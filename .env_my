# Global configs:
EMBEDDING_USE_AZURE=True
MAX_RETRY=12000
RETRY_WAIT_SECONDS=5

DUMP_CHAT_CACHE=True
USE_CHAT_CACHE=True
DUMP_EMBEDDING_CACHE=True
USE_EMBEDDING_CACHE=True
LOG_LLM_CHAT_CONTENT=True
CHAT_FREQUENCY_PENALTY=0.1
CHAT_PRESENCE_PENALTY=0.0


# embedding model configs:
EMBEDDING_USE_AZURE_TOKEN_PROVIDER=False
EMBEDDING_MODEL=text-embedding-3-small
EMBEDDING_AZURE_API_BASE=https://amcopenaieastus2.openai.azure.com/
EMBEDDING_AZURE_API_VERSION=2024-05-01-preview
EMBEDDING_OPENAI_API_KEY=e259582eb85648929d668c4b72f1a1e6


# chat model configs:
CHAT_MODEL=gpt-4o
CHAT_MAX_TOKENS=3000
CHAT_TEMPERATURE=0.7
CHAT_STREAM=True
# CHAT_SEED=42
# CHAT_AZURE_API_BASE=https://amcopenaieastus2.openai.azure.com/
# CHAT_OPENAI_API_KEY=e259582eb85648929d668c4b72f1a1e6
# CHAT_AZURE_API_VERSION=2024-05-01-preview
# CHAT_USE_AZURE_TOKEN_PROVIDER=False

CHAT_OPENAI_API_KEY=sk-1234
CHAT_OPENAI_BASE_URL=http://10.150.240.117:38888
OPENAI_API_KEY=sk-1234
OPENAI_BASE_URL=http://10.150.240.117:38888

# document model configs:
# AZURE_DOCUMENT_INTELLIGENCE_KEY=
# AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT=https://llm-document.cognitiveservices.azure.cn/

# use llama2 endpoint
# USE_GCR_ENDPOINT=False
# GCR_ENDPOINT_TYPE=llama3_70b 
# llama2_70b, llama3_70b, phi2, phi3_4k, phi3_128k

# llama-2-70b-chat CONFIG:
# LLAMA2_70B_ENDPOINT=https://gcrllm2-70b-chat.westus3.inference.ml.azure.com/score
# LLAMA2_70B_ENDPOINT_KEY=s0q25E7YeBFbpMmapW4hyi9XLkPsLuCF

# llama-3-70b CONFIG:
# LLAMA3_70B_ENDPOINT=https://gcr-llama3-70b-instruct.westus3.inference.ml.azure.com/score
# LLAMA3_70B_ENDPOINT_KEY=rY3Pet4TqC5UNxuD9GMbn27l15Hi08cn

#下面其实是llama3.1-70b!!!!!!
# LLAMA3_70B_ENDPOINT=https://gcr-llama31-70b-instruct.westus3.inference.ml.azure.com/score
# LLAMA3_70B_ENDPOINT_KEY=FneAHh1OdC5YqdPh7fR7UUNQaX6ZM2WJ

# phi 3 (128k) CONFIG:
# phi3_128K_ENDPOINT=https://gcr-phi-3-medium-128k-instruct.westus3.inference.ml.azure.com/score
# phi3_128K_ENDPOINT_KEY=0xotcUPO4uJimRCBvHZRw4GwipPnD2FK

# GCR_ENDPOINT_TEMPERATURE=0.7
# GCR_ENDPOINT_TOP_P=1.0
# GCR_ENDPOINT_MAX_TOKEN=2000


# Factor coder configs:
FACTOR_CODER_DATA_FOLDER=/data/userdata/v-xuminrui/new/RD-Agent/git_ignore_folder/factor_implementation_source_data_all
FACTOR_CODER_DATA_FOLDER_DEBUG=/data/userdata/v-xuminrui/new/RD-Agent/git_ignore_folder/factor_implementation_source_data_debug
FACTOR_CODER_CACHE_LOCATION=/data/userdata/v-xuminrui/new/RD-Agent/git_ignore_folder/factor_implementation_execution_cache

FAIL_TASK_TRIAL_LIMIT=200

V2_QUERY_COMPONENT_LIMIT=10
V2_QUERY_ERROR_LIMIT=1
V2_QUERY_FORMER_TRACE_LIMIT=1


FACTOR_CODER_FILE_BASED_EXECUTION_TIMEOUT=120

FACTOR_CODER_MAX_LOOP=10
FACTOR_CODER_SELECT_RATIO=1
FACTOR_CODER_CODER_USE_CACHE=True
# FACTOR_CODER_PYTHON_BIN=/data/userdata/xuyang1/miniconda3/envs/rdagent/bin/python

# FACTOR_CODER_KNOWLEDGE_BASE_PATH=/home/xuyang1/workspace/RD-Agent/factor_from_report_knowledge_base_v2.pkl
# FACTOR_CODER_NEW_KNOWLEDGE_BASE_PATH=/home/xuyang1/workspace/RD-Agent/factor_from_report_knowledge_base_v2.pkl

MODEL_CODER_MAX_LOOP=10
MODEL_CODER_CODER_USE_CACHE=True

QLIB_EVOLVING_N=20

# docker related
QLIB_DOCKER_NETWORK=host

MULTI_PROC_N=16

# kaggle related
DS_DOCKER_LOCAL_DATA_PATH=/data/userdata/v-xuminrui/new/RD-Agent/kaggle_data
DS_LOCAL_DATA_PATH=/data/userdata/share/mle_kaggle
 
DS_DOCKER_NETWORK=host
DS_AUTO_SUBMIT=False

DS_IF_USING_GRAPH_RAG=True
DS_IF_USING_MLE_DATA=True
KG_LOCAL_DATA_PATH=/data/userdata/v-xuminrui/new/RD-Agent/kaggle_data