## Competition Scenario
------Background of the scenario------
You are a world-class data scientist and machine learning engineer with deep expertise in statistics, mathematics, and computer science. 
Your knowledge spans cutting-edge data analysis techniques, advanced machine learning algorithms, and their practical applications to solve complex real-world problems.
You are dedicated to producing accurate, efficient, and innovative solutions.
The task type for this competition is Classification.
The data type used in this competition is Audio.
Briefly, the competition involves: The competition aims to classify audio recordings to determine whether they contain right whale calls or noise, utilizing archival data recorded with Marine Autonomous Recording Units (MARUs)..
The dataset used in this competition is: The dataset includes audio recordings in .aif format. The training set consists of recordings labeled as whale calls or noise, while the test set includes recordings for which participants need to predict the probability of containing whale calls. The processed data folder includes 22,692 training audio files and 25,149 test audio files. Each file name in the training set indicates its label, with '_1.aif' for whale calls and '_0.aif' for noise..
Your goal in this competition is to: Submissions are evaluated based on the Area Under the ROC Curve (AUC). This metric assesses the model's ability to distinguish between positive (whale call) and negative (noise) examples..
------The expected output & submission format specifications------
Submission files should include two columns: 'clip' and 'probability'. The 'clip' column contains the file names of the test recordings, and the 'probability' column contains the predicted probability that each recording contains a right whale call. The file should have a header.
------Evaluation------
Submissions are evaluated based on the Area Under the ROC Curve (AUC). This metric assesses the model's ability to distinguish between positive (whale call) and negative (noise) examples.
The evaluation metrics used is directed as:
The metric is better when it is bigger. 

## Solution Notebook

File Path: ensemble.py
```
from typing import Dict
import numpy as np
import pandas as pd
from sklearn.metrics import roc_auc_score

def ensemble_workflow(test_preds_dict: Dict[str, np.array], val_preds_dict: Dict[str, np.array], val_label: np.array):
    """
    Perform ensemble of multiple model predictions for test data using validation evaluations.
    This function aggregates predictions from various models to enhance predictive performance, leveraging a defined ensemble strategy, and evaluates each model's performance on validation data.
    Parameters:
    - test_preds_dict (Dict[str, np.array]): A dictionary of test predictions from different models. The key is the model file name, and each value is a prediction array of shape [-1].
    - val_preds_dict (Dict[str, np.array]): A dictionary of validation predictions from different models. The key is the model file name, and each value is a prediction array of shape [-1].
    - val_label (np.array): Validation label. Shape: [-1].
    Output:
    - final_pred (np.array): Ensemble prediction for the test data. Shape: [-1].
    Notes:
    - All predictions must be validated for consistent shapes and dimensions.
    - Calculate and save each model's and the ensemble's metric scores in 'scores.csv'.
    - Clearly define the ensemble strategy used for aggregating predictions, such as weighted average.
    - Ensure that the ensemble approach does not introduce bias or overfitting.
    """
    # Validate predictions
    for model_name, preds in val_preds_dict.items():
        assert len(preds) == len(val_label), f"Validation predictions from {model_name} do not match the number of validation labels."

    # Calculate individual model AUC scores
    model_scores = {}
    for model_name, preds in val_preds_dict.items():
        auc_score = roc_auc_score(val_label, preds)
        model_scores[model_name] = auc_score

    # Calculate weights for ensemble based on AUC scores
    total_auc = sum(model_scores.values())
    weights = {model_name: score / total_auc for model_name, score in model_scores.items()}

    # Perform weighted average ensemble on test predictions
    final_pred = np.zeros_like(next(iter(test_preds_dict.values())), dtype=np.float64)
    for model_name, preds in test_preds_dict.items():
        final_pred += weights[model_name] * preds

    # Save scores to CSV
    scores_df = pd.DataFrame(list(model_scores.items()), columns=['Model', 'AUC'])
    scores_df.to_csv('scores.csv', index=False)

    return final_pred
```
File Path: load_data.py
```
import os
import numpy as np
import librosa

def load_data():
    train_path = '/kaggle/input/train2/train2'
    test_path = '/kaggle/input/test2/test2'
    X_train = []
    y_train = []
    X_test = []
    test_ids = []
    feature_dim = 22050  # Example fixed length for 1 second audio at 22050 Hz
    
    # Load training data
    for file_name in os.listdir(train_path):
        if file_name.endswith('.aif'):
            file_path = os.path.join(train_path, file_name)
            audio_data, _ = librosa.load(file_path, sr=None)
            if len(audio_data) < feature_dim:
                # Pad sequences shorter than feature_dim
                audio_data = np.pad(audio_data, (0, feature_dim - len(audio_data)), 'constant')
            elif len(audio_data) > feature_dim:
                # Truncate sequences longer than feature_dim
                audio_data = audio_data[:feature_dim]
            X_train.append(audio_data)
            label = 1 if '_1.aif' in file_name else 0
            y_train.append(label)

    # Load test data
    for file_name in os.listdir(test_path):
        if file_name.endswith('.aif'):
            file_path = os.path.join(test_path, file_name)
            audio_data, _ = librosa.load(file_path, sr=None)
            if len(audio_data) < feature_dim:
                audio_data = np.pad(audio_data, (0, feature_dim - len(audio_data)), 'constant')
            elif len(audio_data) > feature_dim:
                audio_data = audio_data[:feature_dim]
            X_test.append(audio_data)
            test_ids.append(file_name)

    # Convert lists to numpy arrays
    X_train = np.array(X_train)
    y_train = np.array(y_train)
    X_test = np.array(X_test)
    
    return X_train, y_train, X_test, test_ids
```
File Path: feature.py
```
import numpy as np

def feat_eng(X, y, X_test):
    """
    Perform feature engineering on the audio dataset for classification tasks.
    This function takes in the raw train and test data, along with their labels, and applies various feature transformations to enhance the dataset's quality for model training and prediction. It ensures that the transformations do not cause data leakage and maintains consistency between train and test data.
    Parameters:
    - X (np.array): Train data to be transformed. Shape: [-1, feature_dim].
    - y (np.array): Train label data. Shape: [-1].
    - X_test (np.array): Test data to be transformed. Shape: [-1, feature_dim].
    Output:
    - X_transformed (np.array): Transformed train data. Shape: [-1, transformed_feature_dim]. The number of features might change depending on the transformations applied.
    - y_transformed (np.array): Transformed train label data. Shape: [-1].
    - X_test_transformed (np.array): Transformed test data. Shape: [-1, transformed_feature_dim].
    Notes:
    - Ensure the sample size remains consistent across train and test datasets.
    - All transformations should be applied uniformly to both train and test datasets to maintain consistency.
    - Integrate domain-specific transformations like audio feature extraction (e.g., MFCCs) or augmentation if applicable.
    - Leverage GPU resources and parallel processing to expedite feature engineering for large datasets.
    - Handle any missing or anomalous values appropriately to ensure data quality.
    """
    # For this baseline, we will simply pass through the data without any transformations
    # as specified in the task description.

    # Check and fill missing or anomalous values with zeros
    X_transformed = np.nan_to_num(X, nan=0.0)
    X_test_transformed = np.nan_to_num(X_test, nan=0.0)

    # Labels remain unchanged
    y_transformed = y

    return X_transformed, y_transformed, X_test_transformed

```
File Path: model_baseline_logistic_regression.py
```
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from typing import Optional, Dict

def model_workflow(X, y, val_X: Optional[np.array] = None, val_y: Optional[np.array] = None, test_X: Optional[np.array] = None, hyper_params: Optional[Dict] = None):
    """
    Build and train a machine learning model for audio classification, with optional validation and test predictions.

    This function orchestrates the model training process, utilizing the provided training data and hyperparameters. It can optionally evaluate the model on validation data and generate predictions for test data, if provided.

    Parameters:
    - X (np.array): Training feature data. Shape: [-1, feature_dim].
    - y (np.array): Training label data. Shape: [-1].
    - val_X (Optional[np.array]): Validation feature data. Shape: [-1, feature_dim].
    - val_y (Optional[np.array]): Validation label data. Shape: [-1].
    - test_X (Optional[np.array]): Test feature data. Shape: [-1, feature_dim].
    - hyper_params (dict): Dictionary of hyperparameters for model configuration. If not provided, default hyperparameters will be used.

    Output:
    - pred_val (Optional[np.array]): Predictions on validation data. Shape: [-1] if val_X is provided; otherwise, None.
    - pred_test (Optional[np.array]): Predictions on test data. Shape: [-1] if test_X is provided; otherwise, None.
    - hyper_params (dict): Updated dictionary of hyperparameters after training.

    Notes:
    - Ensure input arrays have consistent dimensions and shapes.
    - The function supports GPU acceleration for training to improve performance.
    - Model evaluation will be conducted using the validation data if provided, enabling performance assessment and potential hyperparameter tuning.
    """
    # Set default hyperparameters if none are provided
    if hyper_params is None:
        hyper_params = {'solver': 'liblinear', 'max_iter': 100, 'random_state': 42}

    # Initialize the Logistic Regression model
    model = LogisticRegression(**hyper_params)

    # Train the model
    model.fit(X, y)

    # Initialize predictions
    pred_val = None
    pred_test = None

    # Predict on validation data if provided
    if val_X is not None and val_y is not None:
        pred_val = model.predict(val_X)
        val_accuracy = accuracy_score(val_y, pred_val)
        print(f'Validation Accuracy: {val_accuracy}')

    # Predict on test data if provided
    if test_X is not None:
        pred_test = model.predict(test_X)

    return pred_val, pred_test, hyper_params

```
File Path: main.py
```
import os
import pandas as pd
from load_data import load_data
from feature import feat_eng
from ensemble import ensemble_workflow
from sklearn.model_selection import train_test_split

# Load data
X, y, X_test, test_ids = load_data()

# Feature engineering
X_transformed, y_transformed, X_test_transformed = feat_eng(X, y, X_test)

# Split the data into training and validation sets
train_X, val_X, train_y, val_y = train_test_split(X_transformed, y_transformed, test_size=0.2, random_state=42)

# Identify available model files
available_models = [f[:-3] for f in os.listdir('.') if f.startswith('model_') and f.endswith('.py') and 'test' not in f]

# Initialize dictionaries to store predictions
val_preds_dict = {}
test_preds_dict = {}

# Iterate over each model file and get predictions
for model_name in available_models:
    module = __import__(model_name)
    workflow_function = getattr(module, 'model_workflow')
    val_preds, test_preds, _ = workflow_function(
        X=train_X,
        y=train_y,
        val_X=val_X,
        val_y=val_y,
        test_X=X_test_transformed
    )
    val_preds_dict[model_name] = val_preds
    test_preds_dict[model_name] = test_preds

# Perform ensemble
final_pred = ensemble_workflow(test_preds_dict, val_preds_dict, val_y)

# Prepare the submission file
submission = pd.DataFrame({'clip': test_ids, 'probability': final_pred})
submission.to_csv('submission.csv', index=False)

# Print shape and information of the output
print(f'Final prediction shape: {final_pred.shape}')
print(submission.head())
```
File Path: submission_check.py
```
from pathlib import Path
import pandas as pd


"""
find . | grep -i sample | grep -i submission | grep -v sample_submission.csv | grep -v zip_files  | grep -v 'sample/'
./denoising-dirty-documents/sampleSubmission.csv
./the-icml-2013-whale-challenge-right-whale-redux/sampleSubmission.csv
./text-normalization-challenge-russian-language/ru_sample_submission_2.csv.zip
./text-normalization-challenge-russian-language/ru_sample_submission_2.csv
./random-acts-of-pizza/sampleSubmission.csv
./text-normalization-challenge-english-language/en_sample_submission_2.csv.zip
./text-normalization-challenge-english-language/en_sample_submission_2.csv
./detecting-insults-in-social-commentary/sample_submission_null.csv
"""

# Find sample submission file dynamically
input_dir = Path("/kaggle/input")
# Look for common variations of sample submission filenames
sample_submission_files = list(input_dir.glob("*sample_submission*.csv")) + \
                         list(input_dir.glob("*sampleSubmission*.csv"))

if not sample_submission_files:
    print("Error: No sample submission file found in /kaggle/input/")
    exit(1)

# Use first matching file
sample_submission_name = sample_submission_files[0].name
SAMPLE_SUBMISSION_PATH = str(sample_submission_files[0])
print(f"Using sample submission file: {sample_submission_name}")

# Check if the sample submission file exists
if not Path(SAMPLE_SUBMISSION_PATH).exists():
    print(f"Error: {sample_submission_name} not found at {SAMPLE_SUBMISSION_PATH}")
    exit(0)


sample_submission = pd.read_csv(SAMPLE_SUBMISSION_PATH)
our_submission = pd.read_csv('submission.csv')

success = True
# Print the columns of the sample submission file
print(f"Columns in {sample_submission_name}:", sample_submission.columns)
print("Columns in our_submission.csv:", our_submission.columns)

for col in sample_submission.columns:
    if col not in our_submission.columns:
        success = False
        print(f'Column {col} not found in submission.csv')

if success:
    print(f'submission.csv\'s columns aligns with {sample_submission_name} .')


# Print the first 5 rows of the two submission files, with columns separated by commas.
def print_first_rows(file_path, file_name, num_rows=5):
    print(f"\nFirst {num_rows} rows of {file_name}:")
    try:
        with open(file_path, 'r') as file:
            for i, line in enumerate(file):
                if i < num_rows:
                    print(line.strip())
                else:
                    break
    except FileNotFoundError:
        print(f"Error: {file_name} not found.")

print_first_rows(SAMPLE_SUBMISSION_PATH, sample_submission_name)
print_first_rows('submission.csv', 'submission.csv')

print(f"\nPlease Checked the content of the submission file(submission.csv should align with {sample_submission_name}). ")

```
File Path: mle_submission_check.py
```
from pathlib import Path

from mlebench.grade import validate_submission
from mlebench.registry import registry

COMPETITION_ID = "the-icml-2013-whale-challenge-right-whale-redux"
new_registry = registry.set_data_dir(Path("/mle/data"))
competition = new_registry.get_competition(COMPETITION_ID)

is_valid, message = validate_submission(Path("submission.csv"), competition)

print(message)
```