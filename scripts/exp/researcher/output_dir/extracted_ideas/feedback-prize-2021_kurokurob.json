[
    {
        "idea": "Ensemble Learning",
        "method": "Use a weighted ensemble of multiple transformer models (Longformer, Funnel Transformer, DeBERTa) for prediction.",
        "context": "The notebook combines predictions from Longformer, Funnel Transformer, and DeBERTa models with specified weights to improve the overall performance.",
        "hypothesis": {
            "problem": "The task is to classify segments of text with different discourse elements.",
            "data": "The dataset consists of student essays with annotations for discourse elements.",
            "method": "Transformer models like Longformer, Funnel Transformer, and DeBERTa are used for their capability to handle long sequences and capture complex language patterns.",
            "reason": "The scenario involves complex language understanding where different models may capture different aspects of the text, thus combining them can provide a more robust prediction."
        }
    },
    {
        "idea": "Data Augmentation via Text Splitting",
        "method": "Split longer texts into smaller overlapping segments (striding) to ensure they fit within the maximum input length of the model.",
        "context": "The notebook splits essays into overlapping segments using a stride to fit within the model's input size constraints.",
        "hypothesis": {
            "problem": "The model has a maximum token limit that might be exceeded by the length of some essays.",
            "data": "Essays are often longer than the token limit of transformer models.",
            "method": "By splitting the text into overlapping segments, the model can process each segment without losing context.",
            "reason": "The scenario involves processing long sequences which exceed model limits, so segmenting them allows for efficient processing without truncation."
        }
    },
    {
        "idea": "Removing Inappropriate Tokens",
        "method": "Remove specific tokens from predictions that are likely to be incorrect, such as certain stopwords or common words at the ends of prediction strings.",
        "context": "The notebook filters out specific words from the end of prediction strings to improve accuracy.",
        "hypothesis": {
            "problem": "Predictions may include irrelevant or incorrect tokens, affecting accuracy.",
            "data": "The text data includes many common words that are not significant for discourse classification.",
            "method": "Certain stopwords or common words are removed from predictions based on their position in the prediction string.",
            "reason": "The scenario involves frequent prediction errors at string boundaries due to common words; removing these improves precision."
        }
    },
    {
        "idea": "Threshold-based Post-processing",
        "method": "Adjust prediction strings by extending them based on predefined thresholds for different discourse classes.",
        "context": "The notebook applies specific rules to extend or adjust prediction strings based on their length and class probability score.",
        "hypothesis": {
            "problem": "Predicted segments may not align perfectly with the annotated segments, requiring adjustments.",
            "data": "Predictions often need fine-tuning to better match ground truth annotations.",
            "method": "Extending predictions based on thresholds helps capture more relevant text while maintaining class integrity.",
            "reason": "The scenario involves predicting text spans that may need adjustment to more accurately capture discourse elements."
        }
    },
    {
        "idea": "Seed Everything for Reproducibility",
        "method": "Set seeds for all random number generators to ensure reproducible results across runs.",
        "context": "The notebook sets seeds for numpy, torch, and python random modules to maintain consistent results between runs.",
        "hypothesis": {
            "problem": "Machine learning experiments should be reproducible to verify results.",
            "data": "Consistent initialization and operation is critical for reproducibility in stochastic processes.",
            "method": "Setting seeds controls the randomization process, ensuring reproducibility across different runs and environments.",
            "reason": "The scenario requires reproducible experiments which can be achieved by controlling random number generation."
        }
    },
    {
        "idea": "Model Calibration with Softmax",
        "method": "Apply softmax normalization to model outputs to obtain calibrated probabilities for class labels.",
        "context": "The notebook applies softmax to the output logits of transformer models before making final predictions.",
        "hypothesis": {
            "problem": "Raw model outputs (logits) need calibration to reflect probabilities for classification tasks.",
            "data": "Model outputs are logits which do not represent probabilities directly.",
            "method": "Softmax converts logits into a probability distribution over predicted classes.",
            "reason": "The scenario involves using model outputs for decision-making, which requires them to be in probability form for threshold-based decisions."
        }
    }
]