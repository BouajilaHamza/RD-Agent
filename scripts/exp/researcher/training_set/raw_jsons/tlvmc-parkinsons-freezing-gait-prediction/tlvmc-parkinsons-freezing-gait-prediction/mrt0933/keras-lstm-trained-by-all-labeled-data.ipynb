{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys, os\nimport numpy as np\nimport pandas as pd\nimport pickle\nimport gc\nimport matplotlib.pyplot as plt\nimport pathlib\nimport lightgbm as lgb\nimport logging\nimport warnings\nimport tensorflow as tf\nimport scipy.ndimage as ndi\nwarnings.simplefilter('ignore')\nfrom tqdm.notebook import tqdm\nfrom glob import glob\nfrom sklearn.model_selection import GroupKFold, KFold, StratifiedKFold, StratifiedGroupKFold\nfrom sklearn import cluster\nfrom sklearn.metrics import precision_score, average_precision_score\nfrom sklearn.metrics import precision_score, average_precision_score\nfrom tensorflow import keras\nfrom tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler, ModelCheckpoint, ReduceLROnPlateau\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay, CosineDecay\n\nDATA_DIR = \"/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/\"\ntrain = False","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-09T12:30:43.730576Z","iopub.execute_input":"2023-06-09T12:30:43.731745Z","iopub.status.idle":"2023-06-09T12:30:43.743701Z","shell.execute_reply.started":"2023-06-09T12:30:43.731702Z","shell.execute_reply":"2023-06-09T12:30:43.742383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"subjects = pd.read_csv(f\"{DATA_DIR}subjects.csv\")\nmeta_tdcs = pd.read_csv(f\"{DATA_DIR}tdcsfog_metadata.csv\")\nmeta_defog = pd.read_csv(f\"{DATA_DIR}defog_metadata.csv\")\n\nids, folds, subs = [], [], []\nfor si in subjects['Subject'].values:\n    idi = meta_tdcs[meta_tdcs['Subject']==si]['Id'].tolist() + meta_defog[meta_defog['Subject']==si]['Id'].tolist()\n    ids += idi\n    subs += [si] * len(idi)\n\nif train:\n    id_tdcs = set([os.path.basename(f).split('.')[0] for f in  glob(f\"{DATA_DIR}train/tdcsfog/*.csv\")])\n    id_defog = set([os.path.basename(f).split('.')[0] for f in  glob(f\"{DATA_DIR}train/defog/*.csv\")])\n    id_notype = set([os.path.basename(f).split('.')[0] for f in  glob(f\"{DATA_DIR}train/notype/*.csv\")])\nelse:\n    id_tdcs = set([os.path.basename(f).split('.')[0] for f in  glob(f\"{DATA_DIR}test/tdcsfog/*.csv\")])\n    id_defog = set([os.path.basename(f).split('.')[0] for f in  glob(f\"{DATA_DIR}test/defog/*.csv\")])\n    id_notype = set([os.path.basename(f).split('.')[0] for f in  glob(f\"{DATA_DIR}test/notype/*.csv\")])\ndatanames = []\nfor i in ids:\n    if i in id_defog:\n        datanames.append('defog')\n    elif i in id_tdcs:\n        datanames.append('tdcsfog')\n    elif i in id_notype:\n        datanames.append('notype')\n    else:\n        datanames.append('')\n        \nid_info = pd.DataFrame({'Id':ids, 'Subject':subs, 'Data':datanames})\nid_info.drop_duplicates(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-06-09T12:30:43.746284Z","iopub.execute_input":"2023-06-09T12:30:43.746706Z","iopub.status.idle":"2023-06-09T12:30:44.000412Z","shell.execute_reply.started":"2023-06-09T12:30:43.746659Z","shell.execute_reply":"2023-06-09T12:30:43.999282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Common func","metadata":{}},{"cell_type":"code","source":"def precision(preds, gts):\n    metrics = []\n    metrics.append(average_precision_score(gts[:, 0].flatten()>0, preds[:, 0].flatten()))\n    metrics.append(average_precision_score(gts[:, 1].flatten()>0, preds[:, 1].flatten()))\n    metrics.append(average_precision_score(gts[:, 2].flatten()>0, preds[:, 2].flatten()))\n\n    print(metrics)\n    print(np.mean(metrics))\n    return np.mean(metrics)","metadata":{"execution":{"iopub.status.busy":"2023-06-09T12:30:44.00234Z","iopub.execute_input":"2023-06-09T12:30:44.002998Z","iopub.status.idle":"2023-06-09T12:30:44.010749Z","shell.execute_reply.started":"2023-06-09T12:30:44.002963Z","shell.execute_reply":"2023-06-09T12:30:44.009553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def resize_func(x, target_size, use_percentile_feat=False):\n    ch = x.shape[0] \n    input_size = x.shape[1]\n    \n    pad = target_size - input_size % target_size\n    factor = (input_size + pad) / input_size\n\n    x = np.array([ndi.zoom(xi, zoom=factor, mode='reflect') for xi in x])\n    x = x.reshape((ch, target_size, -1))\n\n    res = {} \n    res['mean'] = np.mean(x, axis=2).reshape(ch, -1)\n    res['max'] = np.max(x, axis=2).reshape(ch, -1)\n    res['min'] = np.min(x, axis=2).reshape(ch, -1)\n    res['med'] = np.median(x, axis=2).reshape(ch, -1)\n    res['std'] = np.sqrt(np.var(x, axis=2).reshape(ch, -1))\n    if use_percentile_feat:\n        for p in [15, 30, 45, 60, 75, 90]:\n            res[f\"p{p}\"] = np.percentile(x, [p], axis=2).reshape(ch, -1)\n\n    return res\n\ndef resize_sequence(filename, target_size):\n    f = pd.read_csv(filename)\n    data = filename.split('/')[-2]\n    \n    x_cols = ['AccV', 'AccML', 'AccAP']\n    y_cols = ['Event'] if data == 'notype' else ['StartHesitation', 'Turn', 'Walking']\n    p_cols = ['p15', 'p30', 'p45', 'p60', 'p75', 'p90']\n    res_cols = ['mean', 'max', 'min', 'med', 'std']\n    \n    res_x = resize_func(f[x_cols].values.transpose(), target_size, CFG.use_percentile_feat)\n    if CFG.train:\n        res_y = resize_func(f[y_cols].values.transpose(), target_size, False)\n        \n    res = {}\n    for k in res_cols:\n        res[f\"{x_cols[0]}{k}\"] = res_x[k][0]\n        res[f\"{x_cols[1]}{k}\"] = res_x[k][1]\n        res[f\"{x_cols[2]}{k}\"] = res_x[k][2]\n        \n        if CFG.train:\n            # there are just dummy. validation loss calculated by those values are not monitored. \n            if data == 'notype':\n                res[f\"StartHesitation{k}\"] = np.zeros_like(res_y[k][0]).astype(res_y[k][0].dtype)\n                res[f\"Turn{k}\"] = res_y[k][0]\n                res[f\"Walking{k}\"] = np.zeros_like(res_y[k][0]).astype(res_y[k][0].dtype)\n            else:\n                res[f\"{y_cols[0]}{k}\"] = res_y[k][0]\n                res[f\"{y_cols[1]}{k}\"] = res_y[k][1]\n                res[f\"{y_cols[2]}{k}\"] = res_y[k][2]\n        \n        if CFG.use_percentile_feat: \n            for k in p_cols:\n                res[f\"{x_cols[0]}{k}\"] = res_x[k][0]\n                res[f\"{x_cols[1]}{k}\"] = res_x[k][1]\n                res[f\"{x_cols[2]}{k}\"] = res_x[k][2]\n        \n    \n    res = pd.DataFrame(res)\n    return res\n\ndef save_resized_data(data, target_size):\n    out = f\"resized_sequences/{data}/\"\n    os.makedirs(out, exist_ok=True)\n    files =  glob(f\"{DATA_DIR}/train/{data}/*.csv\") if CFG.train else glob(f\"{DATA_DIR}/test/{data}/*.csv\")\n    for fi in tqdm(files):\n        name = fi.split('/')[-1]\n        res = resize_sequence(fi, target_size)\n        res.to_csv(f\"{out}{name}\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-06-09T12:30:44.012801Z","iopub.execute_input":"2023-06-09T12:30:44.01363Z","iopub.status.idle":"2023-06-09T12:30:44.041166Z","shell.execute_reply.started":"2023-06-09T12:30:44.013585Z","shell.execute_reply":"2023-06-09T12:30:44.039913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_data(valid=False):\n    use_cols = []\n    cols = ['mean', 'max', 'min', 'med', 'std']\n    if CFG.use_percentile_feat:\n        cols += ['p15', 'p30', 'p45', 'p60', 'p75', 'p90'] \n    for c in cols:\n        use_cols += [f\"AccV{c}\", f\"AccML{c}\", f\"AccAP{c}\"]\n        \n    bools = id_info['Data'] == 'notype' if valid else id_info['Data'] != 'notype'\n    x_data, y_data, names = [], [], []\n    for di, idi in tqdm(id_info[bools][['Data', 'Id']].values):\n        if di == '':continue\n        f = pd.read_csv(f\"resized_sequences/{di}/{idi}.csv\")\n        f.fillna(method=\"ffill\", inplace=True)    \n        x = f[use_cols].values.astype(np.float32).reshape(-1, CFG.target_size, len(use_cols))\n        names.append(f\"{di}/{idi}.csv\")\n        x_data.append(x)\n        \n        if CFG.train:\n            y = f[['StartHesitationmax', 'Turnmax', 'Walkingmax']].values.astype(np.float32).reshape(-1, CFG.target_size, 3)\n            y_data.append(y)\n            \n    x_data = np.concatenate(x_data)\n    if CFG.train:\n        y_data = np.concatenate(y_data)\n        # add noevent  \n        noevent = np.expand_dims(np.sum(y_data, axis=2) < 1, axis=2).astype('float32')\n        y_data = np.concatenate([y_data, noevent], axis=2)\n        y_data[np.sum(y_data, axis=2) == 2] /= 2\n        y_data[np.sum(y_data, axis=2) == 3] /= 3\n        \n        return x_data, y_data, names\n\n    return x_data, names","metadata":{"execution":{"iopub.status.busy":"2023-06-09T12:30:44.046362Z","iopub.execute_input":"2023-06-09T12:30:44.046692Z","iopub.status.idle":"2023-06-09T12:30:44.062945Z","shell.execute_reply.started":"2023-06-09T12:30:44.046665Z","shell.execute_reply":"2023-06-09T12:30:44.061754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate only presence of Event.\ndef loss_func(y_true, preds):\n    return tf.reduce_mean(keras.losses.binary_crossentropy(1.0 - y_true[:, :, -1], 1.0 - preds[:, :, -1]))","metadata":{"execution":{"iopub.status.busy":"2023-06-09T12:30:44.065863Z","iopub.execute_input":"2023-06-09T12:30:44.066281Z","iopub.status.idle":"2023-06-09T12:30:44.08112Z","shell.execute_reply.started":"2023-06-09T12:30:44.066242Z","shell.execute_reply":"2023-06-09T12:30:44.079896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Models","metadata":{}},{"cell_type":"code","source":"def load_bidLSTM(seq_len, n_feat, hidden0, hidden1, hidden2, hidden3, dense, n_class=4):\n    model = keras.models.Sequential([\n            keras.layers.Input(shape = (seq_len, n_feat)),\n            keras.layers.Bidirectional(keras.layers.LSTM(hidden0, return_sequences = True)),\n            keras.layers.Bidirectional(keras.layers.LSTM(hidden1, return_sequences = True)),\n            keras.layers.Bidirectional(keras.layers.LSTM(hidden2, return_sequences = True)),\n            keras.layers.Bidirectional(keras.layers.LSTM(hidden3, return_sequences = True)),\n            keras.layers.Dense(dense, activation = 'selu'),\n            keras.layers.Dense(n_class, activation='softmax'),\n        ])\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-06-09T12:30:44.084823Z","iopub.execute_input":"2023-06-09T12:30:44.085477Z","iopub.status.idle":"2023-06-09T12:30:44.094958Z","shell.execute_reply.started":"2023-06-09T12:30:44.085427Z","shell.execute_reply":"2023-06-09T12:30:44.093503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Implementation of https://www.kaggle.com/competitions/ventilator-pressure-prediction/discussion/285330\ndef load_1dCNN(seq_len, n_feat, n_class=4):\n    inputs_x = keras.layers.Input(shape=(seq_len, n_feat))\n\n    c0 = keras.layers.Conv1D(n_feat, 2, padding='same')(inputs_x)\n    c1 = keras.layers.Conv1D(n_feat, 3, padding='same')(inputs_x)\n    c2 = keras.layers.Conv1D(n_feat, 4, padding='same')(inputs_x)\n\n    x = tf.keras.layers.Concatenate(axis=2)([inputs_x, c0, c1, c2])\n    x = keras.layers.Bidirectional(keras.layers.LSTM(1024, return_sequences=True, dropout=0.1))(x)\n    x = keras.layers.Bidirectional(keras.layers.LSTM(512, return_sequences=True, dropout=0.1))(x)\n    x = keras.layers.Bidirectional(keras.layers.LSTM(256, return_sequences=True, dropout=0.1))(x)\n    x = keras.layers.Bidirectional(keras.layers.LSTM(128, return_sequences=True, dropout=0.1))(x)\n\n    x = tf.keras.layers.Dense(n_class, activation=\"softmax\")(x)\n    model = tf.keras.Model(inputs=inputs_x, outputs=x)\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-06-09T12:30:44.097918Z","iopub.execute_input":"2023-06-09T12:30:44.098403Z","iopub.status.idle":"2023-06-09T12:30:44.112568Z","shell.execute_reply.started":"2023-06-09T12:30:44.098364Z","shell.execute_reply":"2023-06-09T12:30:44.111508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training\nAll labeled data are used as training data. Validation score is calucrated by only 'NoEvent' data. ","metadata":{}},{"cell_type":"code","source":"# 1dCNN\nclass CFG:\n    exp = '001'\n    target_size = 2048\n    lr = 3e-4\n    early_stop_patience = 30\n    epoch = 100\n    batch_size = 32\n    model_no = 0\n    use_percentile_feat = False\n    train = train\n    \nif CFG.train:\n    save_resized_data('tdcsfog', CFG.target_size)\n    save_resized_data('defog', CFG.target_size)\n    save_resized_data('notype', CFG.target_size)\n    \n    gpu_strategy = tf.distribute.get_strategy()\n    with gpu_strategy.scope():\n        valid_x, valid_y, _ = get_data(True)\n        train_x, train_y, _ = get_data(False)\n\n        model = load_1dCNN(train_x.shape[1], train_x.shape[2])\n        scheduler = CosineDecay(CFG.lr, CFG.epoch * (train_x.shape[0] // CFG.batch_size))\n        optimizer = keras.optimizers.Adam(learning_rate = scheduler)\n        model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=loss_func)\n\n        callbacks = []\n        callbacks.append(\n            EarlyStopping(\n                monitor = \"val_loss_func\",\n                patience = CFG.early_stop_patience,\n                verbose =1,\n                mode = \"min\",\n                restore_best_weights = True))\n\n        model.fit(train_x, train_y, validation_data=(valid_x, valid_y), epochs=CFG.epoch, batch_size=CFG.batch_size, callbacks=callbacks) \n        save_locally = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\n        model.save(f'model-{CFG.model_no}', options=save_locally)\n\n        preds = 1.0 - model.predict(valid_x, batch_size=CFG.batch_size, verbose=2)[:, :, -1]\n        # It is not a full size score. If you want to see the actual score, you need to resize it.\n        print(average_precision_score((1.0 - valid_y[:, :, -1]).flatten() > 0, preds.flatten()))","metadata":{"execution":{"iopub.status.busy":"2023-06-09T12:30:44.115222Z","iopub.execute_input":"2023-06-09T12:30:44.115614Z","iopub.status.idle":"2023-06-09T12:30:44.131899Z","shell.execute_reply.started":"2023-06-09T12:30:44.115573Z","shell.execute_reply":"2023-06-09T12:30:44.129958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Bidirectional LSTM\nclass CFG:\n    exp = '001'\n    target_size = 2048\n    lr = 3e-4\n    early_stop_patience = 30\n    epoch = 100\n    batch_size = 32\n    model_no = 1\n    use_percentile_feat = False\n    train = train\n    \nif CFG.train:\n    save_resized_data('tdcsfog', CFG.target_size)\n    save_resized_data('defog', CFG.target_size)\n    save_resized_data('notype', CFG.target_size)\n    \n    gpu_strategy = tf.distribute.get_strategy()\n    with gpu_strategy.scope():\n        valid_x, valid_y, _ = get_data(True)\n        train_x, train_y, _ = get_data(False)\n\n        model = load_bidLSTM(train_x.shape[1], train_x.shape[2], 1024, 512, 256, 128, 128)\n        scheduler = CosineDecay(CFG.lr, CFG.epoch * (train_x.shape[0] // CFG.batch_size))\n        optimizer = keras.optimizers.Adam(learning_rate = scheduler)\n        model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=loss_func)\n\n        callbacks = []\n        callbacks.append(\n            EarlyStopping(\n                monitor = \"val_loss_func\",\n                patience = CFG.early_stop_patience,\n                verbose =1,\n                mode = \"min\",\n                restore_best_weights = True))\n\n        model.fit(train_x, train_y, validation_data=(valid_x, valid_y), epochs=CFG.epoch, batch_size=CFG.batch_size, callbacks=callbacks) \n        save_locally = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\n        model.save(f'model-{CFG.model_no}', options=save_locally)\n\n        preds = 1.0 - model.predict(valid_x, batch_size=CFG.batch_size, verbose=2)[:, :, -1]\n        # It is not a full size score. If you want to see the actual score, you need to resize it.\n        print(average_precision_score((1.0 - valid_y[:, :, -1]).flatten() > 0, preds.flatten()))","metadata":{"execution":{"iopub.status.busy":"2023-06-09T12:30:44.135386Z","iopub.execute_input":"2023-06-09T12:30:44.135899Z","iopub.status.idle":"2023-06-09T12:30:44.151882Z","shell.execute_reply.started":"2023-06-09T12:30:44.135837Z","shell.execute_reply":"2023-06-09T12:30:44.150597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 1dCNN (percentile features are added)\nclass CFG:\n    exp = '001'\n    target_size = 2048\n    lr = 3e-4\n    early_stop_patience = 30\n    epoch = 100\n    batch_size = 16\n    model_no = 2\n    use_percentile_feat = True\n    train = train\n    \nif CFG.train:\n    save_resized_data('tdcsfog', CFG.target_size)\n    save_resized_data('defog', CFG.target_size)\n    save_resized_data('notype', CFG.target_size)\n    \n    gpu_strategy = tf.distribute.get_strategy()\n    with gpu_strategy.scope():\n        valid_x, valid_y, _ = get_data(True)\n        train_x, train_y, _ = get_data(False)\n\n        model = load_bidLSTM(train_x.shape[1], train_x.shape[2], 1024, 512, 256, 128, 128)\n        scheduler = CosineDecay(CFG.lr, CFG.epoch * (train_x.shape[0] // CFG.batch_size))\n        optimizer = keras.optimizers.Adam(learning_rate = scheduler)\n        model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=loss_func)\n\n        callbacks = []\n        callbacks.append(\n            EarlyStopping(\n                monitor = \"val_loss_func\",\n                patience = CFG.early_stop_patience,\n                verbose =1,\n                mode = \"min\",\n                restore_best_weights = True))\n\n        model.fit(train_x, train_y, validation_data=(valid_x, valid_y), epochs=CFG.epoch, batch_size=CFG.batch_size, callbacks=callbacks) \n        save_locally = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\n        model.save(f'model-{CFG.model_no}', options=save_locally)\n\n        preds = 1.0 - model.predict(valid_x, batch_size=CFG.batch_size, verbose=2)[:, :, -1]\n        # It is not a full size score. If you want to see the actual score, you need to resize it.\n        print(average_precision_score((1.0 - valid_y[:, :, -1]).flatten() > 0, preds.flatten()))","metadata":{"execution":{"iopub.status.busy":"2023-06-09T12:30:44.154794Z","iopub.execute_input":"2023-06-09T12:30:44.155241Z","iopub.status.idle":"2023-06-09T12:30:44.172884Z","shell.execute_reply.started":"2023-06-09T12:30:44.155201Z","shell.execute_reply":"2023-06-09T12:30:44.17166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Long1dCNN\nclass CFG:\n    exp = '001'\n    target_size = 4096\n    lr = 3e-4\n    early_stop_patience = 30\n    epoch = 100\n    batch_size = 16\n    model_no = 3\n    use_percentile_feat = False\n    train = train\n    \nif CFG.train:\n    save_resized_data('tdcsfog', CFG.target_size)\n    save_resized_data('defog', CFG.target_size)\n    save_resized_data('notype', CFG.target_size)\n    \n    gpu_strategy = tf.distribute.get_strategy()\n    with gpu_strategy.scope():\n        valid_x, valid_y, _ = get_data(True)\n        train_x, train_y, _ = get_data(False)\n\n        model = load_1dCNN(train_x.shape[1], train_x.shape[2])\n        scheduler = CosineDecay(CFG.lr, CFG.epoch * (train_x.shape[0] // CFG.batch_size))\n        optimizer = keras.optimizers.Adam(learning_rate = scheduler)\n        model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=loss_func)\n\n        callbacks = []\n        callbacks.append(\n            EarlyStopping(\n                monitor = \"val_loss_func\",\n                patience = CFG.early_stop_patience,\n                verbose =1,\n                mode = \"min\",\n                restore_best_weights = True))\n\n        model.fit(train_x, train_y, validation_data=(valid_x, valid_y), epochs=CFG.epoch, batch_size=CFG.batch_size, callbacks=callbacks) \n        save_locally = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\n        model.save(f'model-{CFG.model_no}', options=save_locally)\n\n        preds = 1.0 - model.predict(valid_x, batch_size=CFG.batch_size, verbose=2)[:, :, -1]\n        # It is not a full size score. If you want to see the actual score, you need to resize it.\n        print(average_precision_score((1.0 - valid_y[:, :, -1]).flatten() > 0, preds.flatten()))","metadata":{"execution":{"iopub.status.busy":"2023-06-09T12:30:44.175348Z","iopub.execute_input":"2023-06-09T12:30:44.176194Z","iopub.status.idle":"2023-06-09T12:30:44.192483Z","shell.execute_reply.started":"2023-06-09T12:30:44.176151Z","shell.execute_reply":"2023-06-09T12:30:44.191329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference\nI chosen models that score of NoEvent are higher than 0.28.","metadata":{}},{"cell_type":"code","source":"preds= []\nexpid = '035'","metadata":{"execution":{"iopub.status.busy":"2023-06-09T12:30:44.196545Z","iopub.execute_input":"2023-06-09T12:30:44.196901Z","iopub.status.idle":"2023-06-09T12:30:44.206251Z","shell.execute_reply.started":"2023-06-09T12:30:44.196872Z","shell.execute_reply":"2023-06-09T12:30:44.205078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model 0: 1dCNN\nclass CFG:\n    exp = expid\n    target_size = 2048\n    use_percentile_feat = False\n    model_no = 0\n    batch_size = 32\n    train = train\n    \nsave_resized_data('tdcsfog', CFG.target_size)\nsave_resized_data('defog', CFG.target_size)\n\ntest_x, names = get_data(False)\n\ngpu_strategy = tf.distribute.get_strategy()\nwith gpu_strategy.scope():\n    model = keras.models.load_model(f\"/kaggle/input/pfgp-exp{CFG.exp}/model-{CFG.model_no}\", custom_objects={'loss_func':loss_func})\n    preds.append(model.predict(test_x, batch_size=CFG.batch_size, verbose=2))\n        \ndel test_x\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-06-09T12:30:44.230025Z","iopub.execute_input":"2023-06-09T12:30:44.230629Z","iopub.status.idle":"2023-06-09T12:31:30.379651Z","shell.execute_reply.started":"2023-06-09T12:30:44.230597Z","shell.execute_reply":"2023-06-09T12:31:30.378376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model 3: BiLSTM\nclass CFG:\n    exp = expid\n    target_size = 2048\n    use_percentile_feat = False\n    model_no = 3\n    batch_size = 32\n    train = train\n    \nsave_resized_data('tdcsfog', CFG.target_size)\nsave_resized_data('defog', CFG.target_size)\ntest_x, names = get_data(False)\n\ngpu_strategy = tf.distribute.get_strategy()\nwith gpu_strategy.scope():\n    model = keras.models.load_model(f\"/kaggle/input/pfgp-exp{CFG.exp}/model-{CFG.model_no}\", custom_objects={'loss_func':loss_func})\n    preds.append(model.predict(test_x, batch_size=CFG.batch_size, verbose=2))\n        \ndel test_x\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-06-09T12:32:02.516834Z","iopub.execute_input":"2023-06-09T12:32:02.517373Z","iopub.status.idle":"2023-06-09T12:32:44.520942Z","shell.execute_reply.started":"2023-06-09T12:32:02.517328Z","shell.execute_reply":"2023-06-09T12:32:44.519733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 1dCNN (percentile features are added)\nclass CFG:\n    exp = expid\n    target_size = 2048\n    use_percentile_feat = True\n    model_no = 5\n    batch_size = 32\n    train = train\n    \nsave_resized_data('tdcsfog', CFG.target_size)\nsave_resized_data('defog', CFG.target_size)\ntest_x, names = get_data(False)\n\ngpu_strategy = tf.distribute.get_strategy()\nwith gpu_strategy.scope():\n    model = keras.models.load_model(f\"/kaggle/input/pfgp-exp{CFG.exp}/model-{CFG.model_no}\", custom_objects={'loss_func':loss_func})\n    preds.append(model.predict(test_x, batch_size=CFG.batch_size, verbose=2))\n        \ndel test_x\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-06-09T12:37:47.502631Z","iopub.execute_input":"2023-06-09T12:37:47.503031Z","iopub.status.idle":"2023-06-09T12:38:25.766483Z","shell.execute_reply.started":"2023-06-09T12:37:47.502996Z","shell.execute_reply":"2023-06-09T12:38:25.765107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model 7: long 1dCNN\nclass CFG:\n    exp = expid\n    target_size = 4096\n    use_percentile_feat = False\n    model_no = 7\n    batch_size = 16\n    train = train\n    \nsave_resized_data('tdcsfog', CFG.target_size)\nsave_resized_data('defog', CFG.target_size)\ntest_x, names = get_data(False)\n\ngpu_strategy = tf.distribute.get_strategy()\nwith gpu_strategy.scope():\n    model = keras.models.load_model(f\"/kaggle/input/pfgp-exp{CFG.exp}/model-{CFG.model_no}\", custom_objects={'loss_func':loss_func})\n    preds.append(model.predict(test_x, batch_size=CFG.batch_size, verbose=2))\n        \ndel test_x\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-06-09T12:38:25.768731Z","iopub.execute_input":"2023-06-09T12:38:25.769119Z","iopub.status.idle":"2023-06-09T12:39:06.366496Z","shell.execute_reply.started":"2023-06-09T12:38:25.769049Z","shell.execute_reply":"2023-06-09T12:39:06.365098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ensemble","metadata":{}},{"cell_type":"code","source":"tcols = ['StartHesitation', 'Turn', 'Walking']\nscols = ['Id', 'StartHesitation', 'Turn' , 'Walking']\n\ndef reconst_sequence(x, target_size):\n    res = np.array([\n        ndi.zoom(x[:, 0], zoom=target_size/len(x), mode='nearest'),\n        ndi.zoom(x[:, 1], zoom=target_size/len(x), mode='nearest'),\n        ndi.zoom(x[:, 2], zoom=target_size/len(x), mode='nearest')])\n    return res\n\ndfs = []\nfor p in tqdm(preds):\n    dfi = []\n    for pi, ni in zip(p, names):\n        ori = pd.read_csv(f\"{DATA_DIR}test/{ni}\")\n\n        res = reconst_sequence(pi, len(ori))\n        df = pd.DataFrame(res.transpose(), columns=tcols)\n        df['Id'] = ni.split('/')[-1].replace('.csv', '') + '_' + df.index.astype(str)\n        dfi.append(df[scols])\n        \n    dfs.append(pd.concat(dfi))\n\nfor c in tcols:\n    dfs[0][c] = np.mean([dfs[i][c].values for i in range(len(preds))], axis=0)","metadata":{"execution":{"iopub.status.busy":"2023-06-09T12:39:06.369359Z","iopub.execute_input":"2023-06-09T12:39:06.370527Z","iopub.status.idle":"2023-06-09T12:39:09.475543Z","shell.execute_reply.started":"2023-06-09T12:39:06.370479Z","shell.execute_reply":"2023-06-09T12:39:09.474177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.read_csv(f'{DATA_DIR}sample_submission.csv')\nsub['t'] = 0\nres = pd.merge(sub[['Id','t']], dfs[0], how='left', on='Id').fillna(0.0)\nres[scols].to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-06-09T12:39:09.478732Z","iopub.execute_input":"2023-06-09T12:39:09.479239Z","iopub.status.idle":"2023-06-09T12:39:11.936898Z","shell.execute_reply.started":"2023-06-09T12:39:09.479189Z","shell.execute_reply":"2023-06-09T12:39:11.935618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"subs = pd.read_csv('submission.csv')\nsubs","metadata":{"execution":{"iopub.status.busy":"2023-06-09T12:39:11.939007Z","iopub.execute_input":"2023-06-09T12:39:11.939469Z","iopub.status.idle":"2023-06-09T12:39:12.252581Z","shell.execute_reply.started":"2023-06-09T12:39:11.939421Z","shell.execute_reply":"2023-06-09T12:39:12.249614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}