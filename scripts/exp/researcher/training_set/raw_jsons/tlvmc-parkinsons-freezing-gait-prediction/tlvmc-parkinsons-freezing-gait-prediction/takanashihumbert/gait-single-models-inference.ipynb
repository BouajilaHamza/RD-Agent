{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport gc\nimport pickle\nimport joblib\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\nimport pandas as pd\nfrom tqdm.auto import tqdm\nimport glob\nfrom os.path import basename, dirname, join, exists\nfrom time import perf_counter\nfrom collections import defaultdict as dd\nfrom functools import partial\nfrom colorama import Fore, Back, Style\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, StratifiedGroupKFold\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.preprocessing import StandardScaler as Scaler\nfrom sklearn.cluster import KMeans\nfrom scipy.special import expit\nimport pywt\nfrom statsmodels.robust import mad\nimport scipy\nfrom scipy import signal\nfrom scipy.signal import butter\n\nimport catboost as ctb\nimport xgboost as xgb\nimport lightgbm as lgb\n\nimport tensorflow as tf\nprint(f\"TF version: {tf.__version__}\")\nAUTO = tf.data.experimental.AUTOTUNE","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-07T06:01:53.212461Z","iopub.execute_input":"2023-06-07T06:01:53.213216Z","iopub.status.idle":"2023-06-07T06:02:03.418884Z","shell.execute_reply.started":"2023-06-07T06:01:53.213179Z","shell.execute_reply":"2023-06-07T06:02:03.416311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def madev(d, axis=None):\n    \"\"\" Mean absolute deviation of a signal \"\"\"\n    return np.mean(np.absolute(d - np.mean(d, axis)), axis)\n\ndef wavelet_denoising_1(x, wavelet='db4', level=1):\n    coeffs = pywt.wavedec(x, wavelet, mode=\"per\")\n    sigma = (1/0.6745) * madev(coeffs[-level])\n    uthresh = sigma * np.sqrt(2 * np.log(len(x)))\n    coeffs[1:] = (pywt.threshold(i, value=uthresh, mode='hard') for i in coeffs[1:])\n    result = pywt.waverec(coeffs, wavelet, mode='per')\n    if len(x)%2==1:\n        result = result[:-1]\n    return result\n\ndef wavelet_denoising_2(x, wavelet='db4'):\n    coeffs = pywt.wavedec(x, wavelet, mode=\"per\")\n    coeffs[len(coeffs)-1] *= 0\n    coeffs[len(coeffs)-2] *= 0\n    result = pywt.waverec(coeffs, wavelet, mode='per')\n    if len(x)%2==1:\n        result = result[:-1]\n    return result\n\ndef sgn(num):\n    if(num > 0.0):\n        return 1.0\n    elif(num == 0.0):\n        return 0.0\n    else:\n        return -1.0\n\ndef wavelet_denoising_3(x, wavelet='dB10'):\n    ca3, cd3, cd2, cd1 = pywt.wavedec(x, wavelet, level=3, mode=\"per\")  # 3层小波分解\n\n    abs_cd1 = np.abs(np.array(cd1))\n    median_cd1 = np.median(abs_cd1)\n\n    length0 = len(x)\n    sigma = (1.0 / 0.6745) * median_cd1\n    lamda = sigma * math.sqrt(2.0 * math.log(float(length0), math.e))\n    usecoeffs = []\n    usecoeffs.append(ca3)\n\n    length1 = len(cd1)\n    for k in range(length1):\n        if (abs(cd1[k]) >= lamda/np.log2(2)):\n            cd1[k] = sgn(cd1[k]) * (abs(cd1[k]) - lamda/np.log2(2))\n        else:\n            cd1[k] = 0.0\n\n    length2 = len(cd2)\n    for k in range(length2):\n        if (abs(cd2[k]) >= lamda/np.log2(3)):\n            cd2[k] = sgn(cd2[k]) * (abs(cd2[k]) - lamda/np.log2(3))\n        else:\n            cd2[k] = 0.0\n\n    length3 = len(cd3)\n    for k in range(length3):\n        if (abs(cd3[k]) >= lamda/np.log2(4)):\n            cd3[k] = sgn(cd3[k]) * (abs(cd3[k]) - lamda/np.log2(4))\n        else:\n            cd3[k] = 0.0\n\n    usecoeffs.append(cd3)\n    usecoeffs.append(cd2)\n    usecoeffs.append(cd1)\n    result = pywt.waverec(usecoeffs, wavelet, mode=\"per\") #信号重构\n    \n    if len(x)%2==1:\n        result = result[:-1]\n    return result","metadata":{"execution":{"iopub.status.busy":"2023-06-07T06:02:03.421052Z","iopub.execute_input":"2023-06-07T06:02:03.42219Z","iopub.status.idle":"2023-06-07T06:02:03.442425Z","shell.execute_reply.started":"2023-06-07T06:02:03.422154Z","shell.execute_reply":"2023-06-07T06:02:03.441358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Constants\nBASE_DIR = \"/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction\"\nTRAIN_DIR = join(BASE_DIR, \"train\")\nTEST_DIR = join(BASE_DIR, \"test\")\n\nIS_PUBLIC = len(glob.glob(join(TEST_DIR, \"*/*.csv\")))==2","metadata":{"execution":{"iopub.status.busy":"2023-06-07T06:02:03.443737Z","iopub.execute_input":"2023-06-07T06:02:03.444271Z","iopub.status.idle":"2023-06-07T06:02:03.466964Z","shell.execute_reply.started":"2023-06-07T06:02:03.444238Z","shell.execute_reply":"2023-06-07T06:02:03.465823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Config:\n    train_sub_dirs = [\n        join(TRAIN_DIR, \"defog\"),\n        join(TRAIN_DIR, \"tdcsfog\")\n    ]\n    \n    metadata_paths = [\n        join(BASE_DIR, \"defog_metadata.csv\"),\n        join(BASE_DIR, \"tdcsfog_metadata.csv\")\n    ]\n    \n    splits = 5\n    batch_size = 1024\n    defog_window_size = 100\n    defog_window_future = 25\n    defog_window_past = defog_window_size - defog_window_future\n    tdcsfog_window_size = 128\n    tdcsfog_window_future = 32\n    tdcsfog_window_past = tdcsfog_window_size - tdcsfog_window_future\n    \n    wx = 3\n    \n    model_dropout = 0.2\n    model_hidden = 128\n    model_nblocks = 2\n    \n    lr = 0.0002\n    num_epochs = 5\n    \n    feature_list = ['Time_frac', 'AccV', 'AccML', 'AccAP', 'V_ML', 'V_AP', 'ML_AP']\n    label_list = ['StartHesitation', 'Turn', 'Walking', 'Normal']\n    \n    n_features = len(feature_list)\n    n_labels = len(label_list)    \n    \ncfg = Config()","metadata":{"execution":{"iopub.status.busy":"2023-06-07T06:02:03.469875Z","iopub.execute_input":"2023-06-07T06:02:03.470243Z","iopub.status.idle":"2023-06-07T06:02:03.477667Z","shell.execute_reply.started":"2023-06-07T06:02:03.470209Z","shell.execute_reply":"2023-06-07T06:02:03.476615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FOGSequence(tf.keras.utils.Sequence):\n\n    def __init__(self, df_paths, module, cfg=cfg, split=\"train\"):\n        _time = perf_counter()\n        \n        self.rng = default_rng(42)\n        self.cfg = cfg\n        self.split = split\n        self.module = module\n        \n        if self.module=='defog':\n            self.past_pad = self.cfg.wx*(self.cfg.defog_window_past-1)\n            self.future_pad = self.cfg.wx*self.cfg.defog_window_future\n        else:\n            self.past_pad = self.cfg.wx*(self.cfg.tdcsfog_window_past-1)\n            self.future_pad = self.cfg.wx*self.cfg.tdcsfog_window_future\n        \n        if self.split == \"test\":\n            self.Ids = []\n            self.Time_frac = []\n        _values = [self._read(f) for f in df_paths]\n        \n        self.mapping = []\n        _length = 0\n        for _value in _values:\n            _shape = _value.shape[0]\n            self.mapping.extend(range(_length+self.past_pad, _length+_shape-self.future_pad))\n            _length += _shape\n            \n        self.values = np.concatenate(_values, axis=0)\n        self.mapping = np.array(self.mapping)\n        if self.split != \"test\":\n            # Keep only vaild and task rows\n            _valid_pos = self.values[self.mapping, self.valid_position] > 0\n            _task_pos = self.values[self.mapping, self.task_position] > 0\n            self.mapping = self.mapping[_valid_pos&_task_pos]\n        self.length = self.mapping.shape[0]\n        \n        if split==\"train\":\n            print(f\"Train Dataset of size {self.length:,} initialized in {perf_counter() - _time:.3f} secs!\")\n        if split==\"valid\":\n            print(f\"Valid Dataset of size {self.length:,} initialized in {perf_counter() - _time:.3f} secs!\") \n        gc.collect()\n    \n    def _read(self, path):\n        _is_tdcs = basename(dirname(path)).startswith('tdcs')\n        df = pd.read_csv(path)\n        #########\n        df['Time_frac'] = (df.index/df.index.max()).values\n        \n        if self.split == \"test\":\n            _ids = basename(path).split('.')[0] + '_' + df.Time.astype(str)\n            self.Ids.extend(_ids.tolist())\n            self.Time_frac.extend(df.Time_frac.tolist())\n            return self._df_to_array(df, self.cfg.feature_list)\n        \n        _cols = [*self.cfg.feature_list, *self.cfg.label_list, 'Valid', 'Task']\n        self.valid_position = self.cfg.n_features + self.cfg.n_labels\n        self.task_position = self.valid_position + 1\n        \n        if _is_tdcs:\n            # Fill Valid and Task columns for tdcsfog\n            df['Valid'] = 1\n            df['Task'] = 1\n            \n        return self._df_to_array(df, _cols)\n    \n    def _df_to_array(self, df, cols):\n        # Pads past and future rows to dataframe values for indexing\n        df['AccV'] = wavelet_denoising_2(df['AccV'], wavelet='db4')\n        df['AccML'] = wavelet_denoising_2(df['AccML'], wavelet='db4')\n        df['AccAP'] = wavelet_denoising_2(df['AccAP'], wavelet='db4')\n        df['V_ML'] = df['AccV'] - df['AccML']\n        df['V_AP'] = df['AccV'] - df['AccAP']\n        df['ML_AP'] = df['AccML'] - df['AccAP']\n        \n        df['AccV'] = (df['AccV'] - df['AccV'].mean()) / df['AccV'].std()\n        df['AccML'] = (df['AccML'] - df['AccML'].mean()) / df['AccML'].std()\n        df['AccAP'] = (df['AccAP'] - df['AccAP'].mean()) / df['AccAP'].std()\n        df['V_ML'] = (df['V_ML'] - df['V_ML'].mean()) / df['V_ML'].std()\n        df['V_AP'] = (df['V_AP'] - df['V_AP'].mean()) / df['V_AP'].std()\n        df['ML_AP'] = (df['ML_AP'] - df['ML_AP'].mean()) / df['ML_AP'].std()\n        \n        _values = df[cols].values.astype(np.float32)  #np.float32\n        return np.pad(_values, ((self.past_pad, self.future_pad),(0,0)), 'edge')\n    \n    def __len__(self):\n        return int(np.ceil(self.length / self.cfg.batch_size))\n    \n    def __getitem__(self, idx):\n        \n        if self.split == \"train\":\n            # Onlt train set has randomly selected batches\n            _idxs = self.rng.choice(self.mapping, size=self.cfg.batch_size, replace=False)\n        else:\n            _idxs = self._get_indices(idx)\n            \n        # For test return only features\n        if self.split == \"test\":\n            return self._get_X(_idxs)\n        # For train and val splits return y also\n        return self._get_X_y(_idxs)\n    \n    def _get_indices(self, idx):\n        _low = idx * self.cfg.batch_size\n        # Cap high at self.length so overflow does not occur\n        _high = min(_low + self.cfg.batch_size, self.length)\n        return self.mapping[_low:_high]\n    \n    def _get_X(self, indices):\n        if self.module=='defog':\n            _X = np.empty((len(indices), self.cfg.defog_window_size, self.cfg.n_features), dtype=np.float32)\n        else:\n            _X = np.empty((len(indices), self.cfg.tdcsfog_window_size, self.cfg.n_features), dtype=np.float32)\n        for i, idx in enumerate(indices):\n            _X[i] = self.values[idx-self.past_pad:idx+self.future_pad+1:self.cfg.wx, :self.cfg.n_features]\n        return _X\n    \n    def _get_X_y(self, indices):\n        if self.module=='defog':\n            _X = np.empty((len(indices), self.cfg.defog_window_size, self.cfg.n_features), dtype=np.float32)\n        else:\n            _X = np.empty((len(indices), self.cfg.tdcsfog_window_size, self.cfg.n_features), dtype=np.float32)\n        for i, idx in enumerate(indices):\n            _X[i] = self.values[idx-self.past_pad: idx+self.future_pad+1:self.cfg.wx, :self.cfg.n_features]\n        return _X, self.values[indices, self.cfg.n_features:self.cfg.n_features+self.cfg.n_labels]","metadata":{"execution":{"iopub.status.busy":"2023-06-07T06:02:03.479485Z","iopub.execute_input":"2023-06-07T06:02:03.479866Z","iopub.status.idle":"2023-06-07T06:02:03.513053Z","shell.execute_reply.started":"2023-06-07T06:02:03.479831Z","shell.execute_reply":"2023-06-07T06:02:03.512028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"def get_model(module, checkpoint_path = None):\n    if module=='defog':\n        window_size = cfg.defog_window_size\n    else:\n        window_size = cfg.tdcsfog_window_size\n        \n    model = tf.keras.models.Sequential()\n    model.add(tf.keras.Input(shape=(window_size, cfg.n_features), dtype='float32'))\n    for i in range(cfg.model_nblocks):\n        model.add(tf.keras.layers.Conv1D(filters=cfg.model_hidden, strides=i+1, kernel_size=16-6*i, padding=\"same\"))\n        model.add(tf.keras.layers.BatchNormalization())\n        model.add(tf.keras.layers.ReLU())\n        model.add(tf.keras.layers.Dropout(cfg.model_dropout))\n    model.add(tf.keras.layers.GlobalAveragePooling1D())\n    model.add(tf.keras.layers.Dense(cfg.n_labels, activation='sigmoid'))\n    if checkpoint_path is not None:\n        model.load_weights(checkpoint_path)\n    \n    model.compile(\n        tf.keras.optimizers.Adam(learning_rate=cfg.lr), \n        loss = tf.keras.losses.BinaryCrossentropy()\n    )\n    return model\n\ntf.keras.backend.clear_session()\nget_model('defog').summary()","metadata":{"execution":{"iopub.status.busy":"2023-06-02T08:34:56.558988Z","iopub.execute_input":"2023-06-02T08:34:56.55986Z","iopub.status.idle":"2023-06-02T08:34:59.267722Z","shell.execute_reply.started":"2023-06-02T08:34:56.559828Z","shell.execute_reply":"2023-06-02T08:34:59.26693Z"}}},{"cell_type":"code","source":"# Model adapted from https://keras.io/examples/timeseries/timeseries_classification_from_scratch/\ndef get_model(module, checkpoint_path = None):\n    if module=='defog':\n        window_size = cfg.defog_window_size\n    else:\n        window_size = cfg.tdcsfog_window_size\n        \n    inputs =  tf.keras.Input(shape=(window_size, cfg.n_features), dtype='float32')\n    \n    left = tf.keras.layers.Conv1D(filters=cfg.model_hidden, strides=1, kernel_size=4, padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.01))(inputs)\n    left = tf.keras.layers.BatchNormalization()(left)\n\n    right = tf.keras.layers.Conv1D(filters=cfg.model_hidden, strides=3, kernel_size=8, padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.01))(inputs)\n    right = tf.keras.layers.BatchNormalization()(right)\n    \n    mid = tf.keras.layers.Conv1D(filters=cfg.model_hidden, strides=5, kernel_size=16, padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.01))(inputs)\n    mid = tf.keras.layers.BatchNormalization()(mid)\n    \n    conb = tf.keras.layers.Concatenate(axis=1)([left, mid, right])\n    conb = tf.keras.layers.ReLU()(conb)\n    #conb = tf.keras.layers.GlobalAveragePooling1D()(conb)\n    conb = tf.keras.layers.Flatten()(conb)\n    conb = tf.keras.layers.Dropout(0.2)(conb)\n    outputs = tf.keras.layers.Dense(cfg.n_labels, activation='sigmoid')(conb)\n    \n    model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n\n    if checkpoint_path is not None:\n        model.load_weights(checkpoint_path)\n        \n    model.compile(\n        tf.keras.optimizers.Adam(learning_rate=cfg.lr), \n        loss = tf.keras.losses.BinaryCrossentropy(),\n    )\n    return model\n\nget_model('defog').summary()","metadata":{"execution":{"iopub.status.busy":"2023-06-07T06:02:03.514718Z","iopub.execute_input":"2023-06-07T06:02:03.515331Z","iopub.status.idle":"2023-06-07T06:02:06.216023Z","shell.execute_reply.started":"2023-06-07T06:02:03.515297Z","shell.execute_reply":"2023-06-07T06:02:06.215292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model adapted from https://keras.io/examples/timeseries/timeseries_classification_from_scratch/\ndef get_model(module, checkpoint_path = None):\n    if module=='defog':\n        window_size = cfg.defog_window_size\n    else:\n        window_size = cfg.tdcsfog_window_size\n        \n    inputs =  tf.keras.Input(shape=(window_size, cfg.n_features), dtype='float32')\n    \n    left = tf.keras.layers.Conv1D(filters=cfg.model_hidden, strides=1, kernel_size=4, padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.01))(inputs)\n    left = tf.keras.layers.BatchNormalization()(left)\n\n    right = tf.keras.layers.Conv1D(filters=cfg.model_hidden, strides=3, kernel_size=8, padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.01))(inputs)\n    right = tf.keras.layers.BatchNormalization()(right)\n    \n    mid = tf.keras.layers.Conv1D(filters=cfg.model_hidden, strides=5, kernel_size=16, padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.01))(inputs)\n    mid = tf.keras.layers.BatchNormalization()(mid)\n    \n    conb = tf.keras.layers.Concatenate(axis=1)([left, mid, right])\n    conb = tf.keras.layers.ReLU()(conb)\n\n    conb = tf.keras.layers.Flatten()(conb)\n    conb = tf.keras.layers.Dropout(0.2)(conb)\n    outputs = tf.keras.layers.Dense(cfg.n_labels, activation='sigmoid')(conb)\n    \n    model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n\n    if checkpoint_path is not None:\n        model.load_weights(checkpoint_path)\n        \n    model.compile(\n        tf.keras.optimizers.Adam(learning_rate=cfg.lr), \n        loss = tf.keras.losses.BinaryCrossentropy(),\n    )\n    return model\n\ntf.keras.backend.clear_session()\nget_model('defog').summary()","metadata":{"execution":{"iopub.status.busy":"2023-06-02T17:10:42.987452Z","iopub.execute_input":"2023-06-02T17:10:42.987788Z","iopub.status.idle":"2023-06-02T17:10:45.387903Z","shell.execute_reply.started":"2023-06-02T17:10:42.987757Z","shell.execute_reply":"2023-06-02T17:10:45.387162Z"}}},{"cell_type":"markdown","source":"#### conv1d model","metadata":{}},{"cell_type":"code","source":"model_paths = {\n    'defog': [f for f in glob.glob(\"/kaggle/input/gait-cov1d-models/defog_ver25/*.h5\")],\n    'tdcsfog': [f for f in glob.glob(\"/kaggle/input/gait-cov1d-models/tdcsfog_ver25/*.h5\")],\n}\ndisplay(model_paths)","metadata":{"execution":{"iopub.status.busy":"2023-06-07T06:02:06.217094Z","iopub.execute_input":"2023-06-07T06:02:06.217426Z","iopub.status.idle":"2023-06-07T06:02:06.241082Z","shell.execute_reply.started":"2023-06-07T06:02:06.217392Z","shell.execute_reply":"2023-06-07T06:02:06.239028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\ntest_defog_paths = glob.glob(join(TEST_DIR, \"defog/*.csv\"))\ntest_tdcsfog_paths = glob.glob(join(TEST_DIR, \"tdcsfog/*.csv\"))\n\ntest_ds_dict = {\n    'defog': FOGSequence(test_defog_paths, module='defog', split=\"test\"), \n    'tdcsfog': FOGSequence(test_tdcsfog_paths, module='tdcsfog', split=\"test\")\n}\n\n# Get test predictions\ndf_list = []\nfor module, test_ds in test_ds_dict.items():\n    y_pred_list = []\n    for model_path in tqdm(model_paths[module]):\n        model = get_model(module, model_path)\n        y_pred_list.append(model.predict(test_ds, verbose=0, batch_size=256))\n    y_pred = np.mean(y_pred_list, axis=0)\n    df_list.append(pd.DataFrame(\n        {'Id': test_ds.Ids, 'module': [module]*len(y_pred), 'Time_frac': test_ds.Time_frac, 'StartHesitation': y_pred[:,0], 'Turn': y_pred[:,1], 'Walking': y_pred[:,2]}))","metadata":{"execution":{"iopub.status.busy":"2023-06-07T06:02:06.242299Z","iopub.execute_input":"2023-06-07T06:02:06.242743Z","iopub.status.idle":"2023-06-07T06:02:26.653265Z","shell.execute_reply.started":"2023-06-07T06:02:06.242705Z","shell.execute_reply":"2023-06-07T06:02:26.652265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Concatenate Prediction to DataFrames\nsubmission = pd.concat(df_list)\n\n#submission.loc[((submission.Time_frac<0.01)|(submission.Time_frac>0.99))&(submission.module=='tdcsfog'), 'Walking'] = 0\n#submission.loc[(submission.Time_frac<0.01)&(submission.module=='tdcsfog'), 'Turn'] = 0\n#submission.loc[(submission.Time_frac<0.01)&(submission.module=='defog'), 'Turn'] = 0\n\n# Only keep Ids in sample_submission\nsample_submission = pd.read_csv(join(BASE_DIR, \"sample_submission.csv\"))\nsubmission = pd.merge(sample_submission[['Id']], submission, how='left', on='Id').fillna(0.0)\nsubmission[['Id','StartHesitation','Turn','Walking']].to_csv(\"submission.csv\", index=False, float_format='%.5f') # round to 5 decimal places while keeping point notation\n\ndisplay(submission.head())\ndisplay(submission.tail())","metadata":{"execution":{"iopub.status.busy":"2023-06-07T06:02:26.65505Z","iopub.execute_input":"2023-06-07T06:02:26.655726Z","iopub.status.idle":"2023-06-07T06:02:30.404706Z","shell.execute_reply.started":"2023-06-07T06:02:26.655691Z","shell.execute_reply":"2023-06-07T06:02:30.403743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#submission[submission.Turn>0.8].head(100)","metadata":{"execution":{"iopub.status.busy":"2023-06-07T06:02:30.408042Z","iopub.execute_input":"2023-06-07T06:02:30.408747Z","iopub.status.idle":"2023-06-07T06:02:30.412649Z","shell.execute_reply.started":"2023-06-07T06:02:30.408712Z","shell.execute_reply":"2023-06-07T06:02:30.411745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del test_ds_dict\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-06-07T06:02:30.414271Z","iopub.execute_input":"2023-06-07T06:02:30.415045Z","iopub.status.idle":"2023-06-07T06:02:30.725453Z","shell.execute_reply.started":"2023-06-07T06:02:30.414996Z","shell.execute_reply":"2023-06-07T06:02:30.724542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}