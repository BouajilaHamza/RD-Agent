{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install ../input/pip-wheels-feedback/transformers-4.20.1-py3-none-any.whl","metadata":{"execution":{"iopub.status.busy":"2022-09-02T20:09:24.829423Z","iopub.execute_input":"2022-09-02T20:09:24.830154Z","iopub.status.idle":"2022-09-02T20:10:03.636792Z","shell.execute_reply.started":"2022-09-02T20:09:24.830058Z","shell.execute_reply":"2022-09-02T20:10:03.635569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport torch\nimport transformers\nfrom transformers import AutoTokenizer, AutoConfig, AutoModel\nimport os\nimport json\nimport shutil\nfrom types import SimpleNamespace  \nimport yaml\nimport multiprocessing as mp\nfrom tqdm.auto import tqdm\nimport os\nimport gc\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-09-02T20:10:03.640123Z","iopub.execute_input":"2022-09-02T20:10:03.64099Z","iopub.status.idle":"2022-09-02T20:10:05.834201Z","shell.execute_reply.started":"2022-09-02T20:10:03.640951Z","shell.execute_reply":"2022-09-02T20:10:05.833284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N_CORES = mp.cpu_count()\n\nID_SAMPLE = 0.01\n\nNUM_MODELS = 3\n\nLABEL_MEANS = np.array([0.57056984, 0.25366517, 0.17576499])\n\nLABEL_MEANS[0] *= 1.025\nLABEL_MEANS[1] *= 0.875\nLABEL_MEANS = LABEL_MEANS / LABEL_MEANS.sum()\n\nsample_submission = pd.read_csv(\"../input/feedback-prize-effectiveness/sample_submission.csv\")\n\n\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    data_folder = \"test\"\n    df = pd.read_csv(\"../input/feedback-prize-effectiveness/test.csv\")\n    CALC_SCORE = False\nelse:\n    data_folder = \"train\"\n    df = pd.read_csv(\"../input/feedback-prize-effectiveness/train.csv\")\n    df.loc[df.discourse_id == \"56744a66949a\", \"discourse_text\"] = \"This whole thing is point less how they have us in here for two days im missing my education. We could have finished this in one day and had the rest of the week to get back on the track of learning. I've missed both days of weight lifting, algebra, and my world history that i do not want to fail again! If their are any people actually gonna sit down and take the time to read this then\\n\\nDO NOT DO THIS NEXT YEAR\\n\\n.\\n\\nThey are giving us cold lunches. ham and cheese and an apple, I am 16 years old and my body needs proper food. I wouldnt be complaining if they served actual breakfast. but because of Michelle Obama and her healthy diet rule they surve us 1 poptart in the moring. How does the school board expect us to last from 7:05-12:15 on a pop tart? then expect us to get A's, we are more focused on lunch than anything else. I am about done so if you have the time to read this even though this does not count. Bring PROPER_NAME a big Mac from mc donalds, SCHOOL_NAME, (idk area code but its in LOCATION_NAME)       \\xa0    \"\n\n    ids = df.essay_id.unique()\n    np.random.seed(1337)\n    val_ids = np.random.choice(ids, size=int(ID_SAMPLE*len(ids)), replace=False)\n    df = df[df.essay_id.isin(val_ids)]\n    df = df.reset_index(drop=True)\n    CALC_SCORE = True\n    \nprint(CALC_SCORE)\n    \ndf[\"discourse_type_essay\"] = df.groupby(\"essay_id\")[\"discourse_type\"].transform(lambda x: \" \".join(x)).values","metadata":{"execution":{"iopub.status.busy":"2022-09-02T20:10:09.151862Z","iopub.execute_input":"2022-09-02T20:10:09.1526Z","iopub.status.idle":"2022-09-02T20:10:09.518708Z","shell.execute_reply.started":"2022-09-02T20:10:09.152558Z","shell.execute_reply":"2022-09-02T20:10:09.517602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from glob import glob\nfrom tqdm import tqdm\nimport pandas as pd\n\nessay_texts = {}\nfor fname in tqdm(glob(f\"../input/feedback-prize-effectiveness/{data_folder}/*.txt\")):\n    with open(fname) as f:\n        lines = f.read()\n        \n    essay_texts[fname.split(\"/\")[-1][:-4]] = lines\n\ndf[\"essay_text\"] = df.essay_id.map(essay_texts)\n\ndel essay_texts\ngc.collect()\n\ndf[\"count\"] = df[\"essay_text\"].apply(lambda x: len(x))\ndf[\"orig_order\"] = range(len(df))\ndf = df.sort_values([\"count\", \"essay_id\", \"orig_order\"], ascending=True).reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-09-02T20:10:09.520784Z","iopub.execute_input":"2022-09-02T20:10:09.521149Z","iopub.status.idle":"2022-09-02T20:10:33.296687Z","shell.execute_reply.started":"2022-09-02T20:10:09.521113Z","shell.execute_reply":"2022-09-02T20:10:33.295768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"initial_df = df.copy()","metadata":{"execution":{"iopub.status.busy":"2022-09-02T20:10:33.299464Z","iopub.execute_input":"2022-09-02T20:10:33.299827Z","iopub.status.idle":"2022-09-02T20:10:33.305876Z","shell.execute_reply.started":"2022-09-02T20:10:33.299798Z","shell.execute_reply":"2022-09-02T20:10:33.304841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nimport torch\nimport collections\n\nclass FeedbackDataset(Dataset):\n\n    def __init__(self, df, mode, cfg):\n        self.df = df.reset_index(drop=True)\n        self.mode = mode\n        self.cfg = cfg\n        \n        self.tokenizer = AutoTokenizer.from_pretrained(cfg.architecture.cache_dir)\n        if self.tokenizer.pad_token is None:\n            self.tokenizer.pad_token = self.tokenizer.eos_token\n\n        if self.tokenizer.sep_token is None:\n            self.tokenizer.sep_token = \" \"\n            \n        if hasattr(cfg.dataset, \"separator\") and len(cfg.dataset.separator):\n            self.cfg._tokenizer_sep_token = cfg.dataset.separator\n        else:\n            self.cfg._tokenizer_sep_token = self.tokenizer.sep_token\n                                                       \n        self.text = self.get_texts(self.df, self.cfg, self.tokenizer.sep_token)\n        \n        if self.cfg.tokenizer.lowercase:\n            self.df[\"essay_text\"] = self.df[\"essay_text\"].str.lower()\n            self.df[\"discourse_text\"] = self.df[\"discourse_text\"].str.lower()\n\n        if self.cfg.dataset.group_discourse:\n            grps = self.df.groupby(\"essay_id\", sort=False)\n            self.grp_texts = []\n            \n            s = 0\n\n            for grp in grps.groups:\n                g = grps.get_group(grp)\n                t = g.essay_text.values[0]\n                \n                end = 0\n                for j in range(len(g)):\n\n                    d = g.discourse_text.values[j]\n                    start = t[end:].find(d.strip()) \n                    if start == -1:\n                        print(\"ERROR\")\n                    \n                    start = start + end\n                    end = start + len(d.strip())\n                    if self.cfg.architecture.aux_type:\n                        t = t[:start] + f\" [START] \" + t[start:end] + \" [END] \" + t[end:] \n                    elif self.cfg.architecture.use_type:\n                        t = t[:start] + f\" [START_{g.discourse_type.values[j]}]  \" + t[start:end] + f\" [END_{g.discourse_type.values[j]}] \" + t[end:] \n                    else:\n                        t = t[:start] + f\" [START] {g.discourse_type.values[j]} \" + t[start:end] + \" [END] \" + t[end:] \n\n                if hasattr(self.cfg.dataset, \"add_group_types\") and self.cfg.dataset.add_group_types:\n                    t = \" \".join(g.discourse_type.values) + f\" {self.cfg._tokenizer_sep_token} \" + t\n                        \n                self.grp_texts.append(t)\n\n                s += len(g)\n\n            if self.cfg.dataset.group_discourse:\n                \n                self.cfg._tokenizer_start_token_id = []\n                self.cfg._tokenizer_end_token_id = []\n\n                if self.cfg.architecture.use_type:\n                    for type in sorted(self.df.discourse_type.unique()):\n                        self.tokenizer.add_tokens([f\"[START_{type}]\"], special_tokens=True)\n                        self.cfg._tokenizer_start_token_id.append(self.tokenizer.encode(f\"[START_{type}]\")[1])\n                    \n                    for type in sorted(self.df.discourse_type.unique()):\n                        self.tokenizer.add_tokens([f\"[END_{type}]\"], special_tokens=True)\n                        self.cfg._tokenizer_end_token_id.append(self.tokenizer.encode(f\"[END_{type}]\")[1])\n\n                else:\n                    self.tokenizer.add_tokens([\"[START]\", \"[END]\"], special_tokens=True)\n                    self.cfg._tokenizer_start_token_id.append(self.tokenizer.encode(f\"[START]\")[1])\n                    self.cfg._tokenizer_end_token_id.append(self.tokenizer.encode(f\"[END]]\")[1])\n\n                print(self.cfg._tokenizer_start_token_id)\n                print(self.cfg._tokenizer_end_token_id)\n\n            if hasattr(self.cfg.tokenizer, \"add_newline_token\") and self.cfg.tokenizer.add_newline_token:\n                self.tokenizer.add_tokens([f\"\\n\"], special_tokens=True)\n\n            self.cfg._tokenizer_size = len(self.tokenizer)\n            \n    def __len__(self):\n        if self.cfg.dataset.group_discourse:\n            return len(self.grp_texts)\n        else:\n            return len(self.df)\n        \n    @staticmethod\n    def collate_fn(batch):\n        elem = batch[0]\n\n        ret = {}\n        for key in elem:\n            if key in {\"target\", \"weight\"}:\n                ret[key] = [d[key].float() for d in batch]\n            elif key in {\"target_aux\"}:\n\n                ret[key] = [d[key].float() for d in batch]\n            else:\n                ret[key] = torch.stack([d[key] for d in batch], 0)\n        return ret\n            \n    def batch_to_device(batch, device):\n\n        if isinstance(batch, torch.Tensor):\n            return batch.to(device)\n        elif isinstance(batch, collections.abc.Mapping):\n            return {\n                key: FeedbackDataset.batch_to_device(value, device)\n                for key, value in batch.items()\n            }\n        elif isinstance(batch, collections.abc.Sequence):\n            return [FeedbackDataset.batch_to_device(value, device) for value in batch]\n        else:\n            raise ValueError(f\"Can not move {type(batch)} to device.\")\n    \n    @staticmethod\n    def _lowercase(sample):\n        if isinstance(sample, str):\n            return sample.lower()\n        elif isinstance(sample, Iterable):\n            return [x.lower() for x in sample]\n    \n    def get_texts(cls, df, cfg, separator):\n        if separator is None:\n            if hasattr(cfg.dataset, \"separator\") and len(cfg.dataset.separator):\n                separator = cfg.dataset.separator\n            else:\n                separator = getattr(cfg, \"_tokenizer_sep_token\", \"<SEPARATOR>\")\n\n        lowercase = hasattr(cfg, \"tokenizer\") and cfg.tokenizer.lowercase\n        if isinstance(cfg.dataset.text_column, str):\n            texts = df[cfg.dataset.text_column].astype(str)\n            if lowercase:\n                texts = texts.apply(cls._lowercase)\n            texts = texts.values\n        else:\n            columns = list(cfg.dataset.text_column)\n            join_str = f\" {separator} \"\n            texts = df[columns].astype(str)\n            if lowercase:\n                texts = texts.apply(cls._lowercase)\n            texts = texts.apply(lambda x: join_str.join(x), axis=1).values\n\n        return texts\n        \n    def _read_data(self, idx, sample):\n\n        if self.cfg.dataset.group_discourse:\n            text = self.grp_texts[idx]\n        else:\n            text = self.text[idx]\n\n        if idx == 0:\n            print(text)\n            \n        sample.update(self.encode(text))\n        return sample\n    \n    def encode(self, text):\n        sample = dict()\n        encodings = self.tokenizer(\n            text,\n            return_tensors=\"pt\",\n            padding=\"max_length\",\n            truncation=True,\n            max_length=self.cfg.tokenizer.max_length,\n        )\n        sample[\"input_ids\"] = encodings[\"input_ids\"][0]\n        sample[\"attention_mask\"] = encodings[\"attention_mask\"][0]\n        return sample\n\n    def __getitem__(self, idx):\n        sample = dict()\n            \n        sample = self._read_data(idx=idx, sample=sample)\n\n        return sample","metadata":{"execution":{"iopub.status.busy":"2022-09-02T20:10:33.308656Z","iopub.execute_input":"2022-09-02T20:10:33.309458Z","iopub.status.idle":"2022-09-02T20:10:33.34271Z","shell.execute_reply.started":"2022-09-02T20:10:33.309418Z","shell.execute_reply":"2022-09-02T20:10:33.341762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch import nn\nfrom torch.nn.parameter import Parameter\nclass NLPAllclsTokenPooling(nn.Module):\n\n    def __init__(self, dim, cfg):\n        super(NLPAllclsTokenPooling, self).__init__()\n\n        self.dim = dim\n        self.feat_mult = 1\n        if cfg.dataset.group_discourse:\n            self.feat_mult = 3\n\n    def forward(self, x, attention_mask, input_ids, cfg):\n\n        if not cfg.dataset.group_discourse:\n            input_ids_expanded = input_ids.clone().unsqueeze(-1).expand(x.shape)\n            attention_mask_expanded = torch.zeros_like(input_ids_expanded)\n\n            attention_mask_expanded[(input_ids_expanded == cfg._tokenizer_cls_token_id) | (input_ids_expanded == cfg._tokenizer_sep_token_id)] = 1\n\n            sum_features = (x * attention_mask_expanded).sum(self.dim)\n            ret = sum_features / attention_mask_expanded.sum(self.dim).clip(min=1e-8)\n\n        else:\n            ret = []\n\n            for j in range(x.shape[0]):\n\n\n                idx0 = torch.where((input_ids[j] >= min(cfg._tokenizer_start_token_id)) & (input_ids[j] <= max(cfg._tokenizer_start_token_id)))[0]\n                idx1 = torch.where((input_ids[j] >= min(cfg._tokenizer_end_token_id)) & (input_ids[j] <= max(cfg._tokenizer_end_token_id)))[0]\n\n                xx = []\n                for jj in range(len(idx0)):\n                    xx0 = x[j, idx0[jj]]\n                    xx1 = x[j, idx1[jj]]\n                    xx2 = x[j, idx0[jj]+1:idx1[jj]].mean(dim=0)\n                    xxx = torch.cat([xx0, xx1, xx2]).unsqueeze(0)\n                    xx.append(xxx)\n                xx = torch.cat(xx)\n                ret.append(xx)\n        \n        return ret\n\nclass GeMText(nn.Module):\n    def __init__(self, dim, cfg, p=3, eps=1e-6):\n        super(GeMText, self).__init__()\n        self.dim = dim\n        self.p = Parameter(torch.ones(1) * p)\n        self.eps = eps\n        self.feat_mult = 1\n\n    def forward(self, x, attention_mask, input_ids, cfg):\n        attention_mask_expanded = attention_mask.unsqueeze(-1).expand(x.shape)\n        x = (x.clamp(min=self.eps) * attention_mask_expanded).pow(self.p).sum(self.dim)\n        ret = x / attention_mask_expanded.sum(self.dim).clip(min=self.eps)\n        ret = ret.pow(1 / self.p)\n        return ret\n    \nclass NLPPoolings:\n    _poolings = {\n        \"All [CLS] token\": NLPAllclsTokenPooling,\n        \"GeM\": GeMText\n    }\n    @classmethod\n    def get(cls, name):\n        return cls._poolings.get(name)\n\nclass FeedbackModel(nn.Module):\n\n\n    def __init__(self, cfg):\n\n        super(FeedbackModel, self).__init__()\n\n        self.cfg = cfg\n        self.n_classes = 3\n        config = AutoConfig.from_pretrained(cfg.architecture.cache_dir)\n        self.backbone = AutoModel.from_config(config)\n    \n        self.backbone.pooler = None\n        \n        if self.cfg.dataset.group_discourse:\n            self.backbone.resize_token_embeddings(cfg._tokenizer_size)\n        \n        self.pooling = NLPPoolings.get(self.cfg.architecture.pool)\n        self.pooling = self.pooling(dim=1, cfg=cfg)  # init pooling and pool over token dimension\n        \n        self.head = nn.Linear(self.backbone.config.hidden_size*self.pooling.feat_mult, self.n_classes)\n\n    def get_features(self, batch):\n        attention_mask = batch[\"attention_mask\"]\n        input_ids = batch[\"input_ids\"]\n\n        x = self.backbone(\n            input_ids=input_ids, attention_mask=attention_mask\n        ).last_hidden_state\n\n        x = self.pooling(x, attention_mask, input_ids, cfg=self.cfg)\n\n        if self.cfg.dataset.group_discourse:\n            x = torch.cat(x)\n        \n        if self.cfg.architecture.dropout > 0.0:\n            x = F.dropout(x, p=self.cfg.architecture.dropout, training=self.training)\n\n        return x\n\n    def forward(self, batch, calculate_loss=False):\n        \n        idx = int(torch.where(batch[\"attention_mask\"] == 1)[1].max())\n        idx += 1\n        batch[\"attention_mask\"] = batch[\"attention_mask\"][:, :idx]\n        batch[\"input_ids\"] = batch[\"input_ids\"][:, :idx]\n        \n        x = self.get_features(batch)\n                \n        logits = self.head(x)\n        outputs = {}\n\n        outputs[\"logits\"] = logits\n\n        return outputs","metadata":{"execution":{"iopub.status.busy":"2022-09-02T20:10:33.344185Z","iopub.execute_input":"2022-09-02T20:10:33.345934Z","iopub.status.idle":"2022-09-02T20:10:33.370269Z","shell.execute_reply.started":"2022-09-02T20:10:33.345902Z","shell.execute_reply":"2022-09-02T20:10:33.369155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoConfig, AutoModel, PreTrainedModel\n\ndef create_nlp_backbone(\n    cfg, model_class=AutoModel, remove_pooling_layer=False\n):\n\n    config = AutoConfig.from_pretrained(\n        cfg[\"backbone\"], cache_dir=cfg[\"cache_dir\"]\n    )\n\n    kwargs = dict(add_pooling_layer=False) if remove_pooling_layer else dict()\n    \n    try:\n        backbone = model_class.from_config(config, **kwargs)\n    except TypeError:\n        backbone = model_class.from_config(config)\n\n    return backbone\n\ndef glorot_uniform(parameter):\n    nn.init.xavier_uniform_(parameter.data, gain=1.0)\n\n\nclass NBMEHead(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(NBMEHead, self).__init__()\n        self.dropout1 = nn.Dropout(0.1)\n        self.dropout2 = nn.Dropout(0.2)\n        self.dropout3 = nn.Dropout(0.3)\n        self.dropout4 = nn.Dropout(0.4)\n        self.dropout5 = nn.Dropout(0.5)\n        self.classifier = nn.Linear(input_dim, output_dim)\n        glorot_uniform(self.classifier.weight)\n\n    def forward(self, x):\n        # x is B x S x C\n        logits1 = self.classifier(self.dropout1(x))\n        logits2 = self.classifier(self.dropout2(x))\n        logits3 = self.classifier(self.dropout3(x))\n        logits4 = self.classifier(self.dropout4(x))\n        logits5 = self.classifier(self.dropout5(x))\n\n        logits = ((logits1 + logits2 + logits3 + logits4 + logits5) / 5)\n\n        return logits\n\nclass ModelYauhen(nn.Module):\n    def __init__(self, cfg):\n        super(ModelYauhen, self).__init__()\n\n        self.cfg = cfg\n        self.n_classes = 3\n        self.backbone = create_nlp_backbone(\n            self.cfg,\n            model_class=AutoModel,\n            remove_pooling_layer=False,\n        )\n        self.head = nn.Linear(self.backbone.config.hidden_size, 3)\n        if self.cfg[\"add_wide_dropout\"]:\n            self.token_type_head = NBMEHead(self.backbone.config.hidden_size, 3)\n\n    def forward(self, batch, calculate_loss=True):\n        outputs = {}\n        \n        idx = int(torch.where(batch[\"attention_mask\"] == 1)[1].max()) \n        idx += 1\n        batch[\"attention_mask\"] = batch[\"attention_mask\"][:, :idx]\n        batch[\"input_ids\"] = batch[\"input_ids\"][:, :idx]\n        batch[\"word_start_mask\"] = batch[\"word_start_mask\"][:, :idx]\n        batch[\"word_ids\"] = batch[\"word_ids\"][:, :idx]\n\n        outputs[\"word_start_mask\"] = batch[\"word_start_mask\"]\n\n        x = self.backbone(\n            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n        ).last_hidden_state\n\n        for obs_id in range(x.size()[0]):\n            for w_id in range(int(torch.max(batch[\"word_ids\"][obs_id]).item()) + 1):\n                chunk_mask = batch[\"word_ids\"][obs_id] == w_id\n                chunk_logits = x[obs_id] * chunk_mask.unsqueeze(-1)\n                chunk_logits = chunk_logits.sum(dim=0) / chunk_mask.sum()\n                x[obs_id][chunk_mask] = chunk_logits\n\n        if self.cfg[\"add_wide_dropout\"]:\n            logits = self.token_type_head(x)\n        else:\n            logits = self.head(x)\n        outputs[\"logits\"] = logits      \n\n        return outputs\n    \n    \nfrom torch.utils.data import Dataset\nfrom transformers import AutoTokenizer\n\n\nclass FeedbackDatasetYauhen(Dataset):\n    @staticmethod\n    def _lowercase(sample):\n        if isinstance(sample, str):\n            return sample.lower()\n        elif isinstance(sample, Iterable):\n            return [x.lower() for x in sample]\n\n    def __init__(self, df, cfg):\n        self.cfg = cfg\n        self.df = df\n\n        self.tokenizer = AutoTokenizer.from_pretrained(\n            self.cfg[\"backbone\"],\n            add_prefix_space=True,\n            use_fast=True,\n            cache_dir=self.cfg[\"cache_dir\"],\n        )\n\n        self.text = self.get_texts(self.df, self.cfg, self.tokenizer.sep_token)\n        self.labels = self.df[\"tokens\"].values\n\n    @classmethod\n    def get_texts(cls, df, cfg, separator=None):\n        texts = df[cfg[\"text_column\"]].values\n\n        if cfg[\"lowercase\"]:\n            texts = [cls._lowercase(x) for x in texts]\n\n        return texts\n\n    def __getitem__(self, idx):\n        sample = dict()\n            \n        text = self.text[idx]\n        \n        if \"deberta-v3\" in self.cfg[\"backbone\"]:\n            text = [x.replace(\"\\n\", \"[NL_HYDRO]\") for x in list(text)]\n            text = [x if not x.isspace() else \"[SP_HYDRO]\" * len(x) for x in text]\n            tokenizer_input = [text]\n            raise ValueError(f\"BES {text}\")\n        else:\n            if \"add_types\" in self.cfg and self.cfg[\"add_types\"]:\n                tokenizer_input = [x if x_idx > 0 else x + self.tokenizer.sep_token for x_idx, x in enumerate(list(text))]\n            else:\n                tokenizer_input = [list(text)]\n\n        encodings = self.tokenizer(\n            tokenizer_input,\n            return_tensors=\"pt\",\n            padding=\"max_length\",\n            truncation=True,\n            max_length=2048,\n            is_split_into_words=True,\n        )\n\n        sample[\"input_ids\"] = encodings[\"input_ids\"][0]\n        sample[\"attention_mask\"] = encodings[\"attention_mask\"][0]\n\n        word_ids = encodings.word_ids(0)\n        word_ids = [-1 if x is None else x for x in word_ids]\n        sample[\"word_ids\"] = torch.tensor(word_ids)\n\n        word_start_mask = []\n        lab_idx = -1\n        for i, word in enumerate(word_ids):\n            word_start = word > -1 and (i == 0 or word_ids[i - 1] != word)\n            if word_start:\n                lab_idx += 1\n                if self.labels[idx][lab_idx] != 1:\n                    word_start_mask.append(True)\n                    continue\n\n            word_start_mask.append(False)\n\n        sample[\"word_start_mask\"] = torch.tensor(word_start_mask)\n\n        return sample\n    \n    def __len__(self):\n        return len(self.df)","metadata":{"execution":{"iopub.status.busy":"2022-09-02T20:10:33.371918Z","iopub.execute_input":"2022-09-02T20:10:33.372383Z","iopub.status.idle":"2022-09-02T20:10:34.31712Z","shell.execute_reply.started":"2022-09-02T20:10:33.372348Z","shell.execute_reply":"2022-09-02T20:10:34.316009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = df.copy()","metadata":{"execution":{"iopub.status.busy":"2022-09-02T20:10:34.319859Z","iopub.execute_input":"2022-09-02T20:10:34.320225Z","iopub.status.idle":"2022-09-02T20:10:34.326605Z","shell.execute_reply.started":"2022-09-02T20:10:34.320188Z","shell.execute_reply":"2022-09-02T20:10:34.325635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\n\nall_obs = []\n\nfor name, gr in tqdm(test.groupby(\"essay_id\", sort=False)):\n    essay_text_start_end = gr.essay_text.values[0]\n    token_labels = []\n    token_obs = []\n    end_pos = 0\n    \n    for idx, row in gr.reset_index(drop=True).iterrows():\n        target_text = row[\"discourse_type\"] + \" \" + row[\"discourse_text\"].strip()\n        \n        essay_text_start_end = essay_text_start_end[:end_pos] + essay_text_start_end[end_pos:].replace(row[\"discourse_text\"].strip(), target_text, 1)\n        \n        start_pos = essay_text_start_end[end_pos:].find(target_text)\n        if start_pos == -1:\n            raise ValueError()\n        start_pos += end_pos\n        \n        if idx == 0 and start_pos > 0:\n            token_labels.append(1)\n            token_obs.append(essay_text_start_end[:start_pos])\n        \n        if start_pos > end_pos and end_pos > 0:\n            token_labels.append(1)\n            token_obs.append(essay_text_start_end[end_pos:start_pos])\n  \n        end_pos = start_pos + len(target_text)\n        token_labels.append(0)\n        token_obs.append(essay_text_start_end[start_pos: end_pos])\n            \n        if idx == len(gr) - 1 and end_pos < len(essay_text_start_end):\n            token_labels.append(1)\n            token_obs.append(essay_text_start_end[end_pos:])\n            \n    if len(token_labels) != len(token_obs):\n        raise ValueError()\n            \n    all_obs.append((name, token_labels, token_obs))\n\ntt = pd.DataFrame(all_obs, columns=[\"essay_id\", \"tokens\", \"essay_text\"])","metadata":{"execution":{"iopub.status.busy":"2022-09-02T20:10:34.328131Z","iopub.execute_input":"2022-09-02T20:10:34.329226Z","iopub.status.idle":"2022-09-02T20:10:34.388579Z","shell.execute_reply.started":"2022-09-02T20:10:34.329187Z","shell.execute_reply":"2022-09-02T20:10:34.387658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\n\nall_obs = []\n\nfor name, gr in tqdm(test.groupby(\"essay_id\", sort=False)):\n    essay_text_start_end = gr.essay_text.values[0]\n    token_labels = []\n    token_obs = []\n    end_pos = 0\n    \n    token_obs.append(\" \".join(gr.discourse_type.to_list()))\n    token_labels.append(1)\n    \n    for idx, row in gr.reset_index(drop=True).iterrows():\n        target_text = row[\"discourse_type\"] + \" \" + row[\"discourse_text\"].strip()\n        \n        essay_text_start_end = essay_text_start_end[:end_pos] + essay_text_start_end[end_pos:].replace(row[\"discourse_text\"].strip(), target_text, 1)\n        \n        start_pos = essay_text_start_end[end_pos:].find(target_text)\n        if start_pos == -1:\n            raise ValueError()\n        start_pos += end_pos\n        \n        if idx == 0 and start_pos > 0:\n            token_labels.append(1)\n            token_obs.append(essay_text_start_end[:start_pos])\n        \n        if start_pos > end_pos and end_pos > 0:\n            token_labels.append(1)\n            token_obs.append(essay_text_start_end[end_pos:start_pos])\n  \n        end_pos = start_pos + len(target_text)\n        token_labels.append(0)\n        token_obs.append(essay_text_start_end[start_pos: end_pos])\n            \n        if idx == len(gr) - 1 and end_pos < len(essay_text_start_end):\n            token_labels.append(1)\n            token_obs.append(essay_text_start_end[end_pos:])\n            \n    if len(token_labels) != len(token_obs):\n        raise ValueError()\n            \n    all_obs.append((name, token_labels, token_obs))\n\ntt_v2 = pd.DataFrame(all_obs, columns=[\"essay_id\", \"tokens\", \"essay_text\"])","metadata":{"execution":{"iopub.status.busy":"2022-09-02T20:10:34.39195Z","iopub.execute_input":"2022-09-02T20:10:34.392811Z","iopub.status.idle":"2022-09-02T20:10:34.452469Z","shell.execute_reply.started":"2022-09-02T20:10:34.392776Z","shell.execute_reply":"2022-09-02T20:10:34.451534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_predictions_philipp(exp_name, df, BS=1, num_models=NUM_MODELS):\n    \n    cfg = yaml.safe_load(open(f\"../input/{exp_name}/cfg.yaml\").read())\n    for k,v in cfg.items():\n        if type(v) == dict:\n            cfg[k] = SimpleNamespace(**v)\n    cfg = SimpleNamespace(**cfg)\n\n    if cfg.architecture.backbone == 'microsoft/deberta-v3-large':\n        cfg.architecture.cache_dir = \"../input/deberta-v3-large/\"\n    elif cfg.architecture.backbone == 'microsoft/deberta-v3-small':\n        cfg.architecture.cache_dir = \"../input/deberta-v3-small/\"\n    elif cfg.architecture.backbone == 'microsoft/deberta-v3-base':\n        cfg.architecture.cache_dir = \"../input/deberta-v3-lbase/\"\n\n    ds = FeedbackDataset(df.iloc[:], mode=\"test\", cfg=cfg)\n    \n    preds_all = []\n    for fold in range(num_models):\n        print(f\"running model {fold}\")\n        \n        model = FeedbackModel(cfg).to(\"cuda\").eval()\n    \n        d = torch.load(f\"../input/{exp_name}/checkpoint-fold{fold}.pth\", map_location=\"cpu\")\n\n        model_weights = d[\"model\"]\n        model_weights = {k.replace(\"module.\", \"\"): v for k, v in model_weights.items()}\n        \n        for k in list(model_weights.keys()):\n            if \"aux\" in k or \"loss_fn\" in k:\n                del model_weights[k]\n\n        model.load_state_dict(collections.OrderedDict(model_weights), strict=True)\n        \n        del d\n        del model_weights\n        gc.collect() \n    \n        batch_size = BS\n        dl = DataLoader(ds, shuffle=False, batch_size = batch_size, num_workers = N_CORES)\n\n        with torch.no_grad():    \n            preds = []\n            for batch in tqdm(dl):\n\n                batch = FeedbackDataset.batch_to_device(batch, \"cuda\")\n                out = model(batch)\n                preds.append(out[\"logits\"].float().softmax(dim=1).detach().cpu().numpy())\n\n        preds_all.append(np.concatenate(preds, axis=0))\n        \n        del model\n        del dl\n        gc.collect()\n        \n    del ds\n    \n    \n    preds = np.mean(preds_all, axis=0)\n    \n    return preds\n\ndef run_predictions_yauhen(all_cfgs, df, yauhen_batch_size=1):\n    ds = FeedbackDatasetYauhen(df=df, cfg=all_cfgs[0])\n        \n    preds_all = []\n\n    for params in all_cfgs:\n\n        model = ModelYauhen(params).to(\"cuda\").eval()\n\n        d = torch.load(params[\"path\"], map_location=\"cpu\")\n\n        model_weights = d[\"model\"]\n        model_weights = {k.replace(\"module.\", \"\"): v for k, v in model_weights.items()}\n        model.load_state_dict(collections.OrderedDict(model_weights), strict=True)\n        \n        del d\n        del model_weights\n        gc.collect() \n    \n        dl = DataLoader(ds, shuffle=False, batch_size = yauhen_batch_size, num_workers = N_CORES)\n\n        with torch.no_grad():\n            \n            preds = []\n            for batch in tqdm(dl):\n                texts = {\n                key: value.to(\"cuda\")\n                for key, value in batch.items()\n            }\n                output = model.forward(texts, calculate_loss=False)\n\n                val = (\n                        torch.softmax(output[\"logits\"][output[\"word_start_mask\"]], dim=1).detach().cpu().numpy()\n                    )\n\n                preds.append(val)\n\n        preds_all.append(np.concatenate(preds, axis=0))\n        \n        del model\n        del dl\n        gc.collect()\n        \n    del ds\n    \n    preds = np.mean(preds_all, axis=0)\n    \n    return preds","metadata":{"execution":{"iopub.status.busy":"2022-09-02T20:10:34.453908Z","iopub.execute_input":"2022-09-02T20:10:34.454409Z","iopub.status.idle":"2022-09-02T20:10:34.473641Z","shell.execute_reply.started":"2022-09-02T20:10:34.454374Z","shell.execute_reply":"2022-09-02T20:10:34.472645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = []\nweights = []","metadata":{"execution":{"iopub.status.busy":"2022-09-02T20:10:34.474918Z","iopub.execute_input":"2022-09-02T20:10:34.475338Z","iopub.status.idle":"2022-09-02T20:10:34.488992Z","shell.execute_reply.started":"2022-09-02T20:10:34.4753Z","shell.execute_reply":"2022-09-02T20:10:34.488043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def scale_probs(pp_single):\n    pp = pp_single.copy()\n\n    for _ in range(100):\n\n        pp = pp * (LABEL_MEANS.reshape(1,3) / pp.mean(axis=0))\n\n        pp = pp / pp.sum(axis=1, keepdims=True)\n        \n    return pp","metadata":{"execution":{"iopub.status.busy":"2022-09-02T20:10:34.490335Z","iopub.execute_input":"2022-09-02T20:10:34.490997Z","iopub.status.idle":"2022-09-02T20:10:34.499816Z","shell.execute_reply.started":"2022-09-02T20:10:34.490962Z","shell.execute_reply":"2022-09-02T20:10:34.498853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import lightgbm\ndef get_features(df, vects):\n    vect_discourse = vects[\"vect_discourse\"]\n    X = vect_discourse.transform(df[\"discourse_text\"]).A\n\n    X = np.concatenate([X, X.sum(axis=1).reshape(-1,1)], axis=1)\n\n    vect_essay = vects[\"vect_essay\"]\n    XX = vect_essay.transform(df[\"essay_text\"]).A\n    XX = np.concatenate([XX, XX.sum(axis=1).reshape(-1,1)], axis=1)\n    X = np.concatenate([X, XX], axis=1)\n\n    vect_type = vects[\"vect_type\"]\n    X = np.concatenate([X, vect_type.transform(df[\"discourse_type\"]).A], axis=1)\n\n    vect_type_essay = vects[\"vect_type_essay\"]\n    X = np.concatenate([X, vect_type_essay.transform(df[\"discourse_type_essay\"]).A], axis=1)\n\n    f = \"rel_rank\"\n    X = np.concatenate([X, df[f].values.reshape(-1,1)], axis=1)\n    print(X.shape)\n\n    return X\n\ndef run_predictions_lgb(exp_name, df):\n    df[\"rank\"] = df.groupby(\"essay_id\")[\"discourse_type\"].transform(lambda x: np.arange(len(x))).values\n    df[\"length\"] = df.groupby(\"essay_id\")[\"discourse_type\"].transform(lambda x: len(x)).values\n    df[\"rel_rank\"] = df[\"rank\"] / df[\"length\"]\n    \n    pps = []\n    for fold in [-1]:\n    \n        vects = pd.read_pickle(f\"../input/{exp_name}/fold{fold}/vectorizers.p\")\n\n        X = get_features(df, vects)\n\n        clf = lightgbm.Booster(model_file=f\"../input/{exp_name}/fold{fold}/model_seed0.txt\")\n\n        preds = clf.predict(X)\n        pps.append(preds)\n    preds = np.mean(pps, axis=0)\n    \n    print(preds.mean(axis=0))\n    \n    return preds\n\npreds.append(scale_probs(run_predictions_lgb(\"lgb-v0\", df)))\nweights.append(1.131471)","metadata":{"execution":{"iopub.status.busy":"2022-09-02T20:10:34.50136Z","iopub.execute_input":"2022-09-02T20:10:34.502057Z","iopub.status.idle":"2022-09-02T20:10:38.35769Z","shell.execute_reply.started":"2022-09-02T20:10:34.502023Z","shell.execute_reply":"2022-09-02T20:10:38.356731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cfg_1 = {\"backbone\": \"../input/debertalarge\",\n       \"lowercase\": False,\n       \"text_column\": \"essay_text\",\n       \"cache_dir\": \"../input/debertalarge\",\n       \"path\": \"../input/pseudo-75-datasets-v4/olivine-spaniel-ff/checkpoint-fold0.pth\",\n         \"add_wide_dropout\": False,\n      }\n\ncfg_2 = {\"backbone\": \"../input/debertalarge\",\n       \"lowercase\": False,\n       \"text_column\": \"essay_text\",\n       \"cache_dir\": \"../input/debertalarge\",\n       \"path\": \"../input/pseudo-75-datasets-v4/olivine-spaniel-ff/checkpoint-fold1.pth\",\n         \"add_wide_dropout\": False,\n      }\n\ncfg_3 = {\"backbone\": \"../input/debertalarge\",\n       \"lowercase\": False,\n       \"text_column\": \"essay_text\",\n       \"cache_dir\": \"../input/debertalarge\",\n       \"path\": \"../input/pseudo-75-datasets-v4/olivine-spaniel-ff/checkpoint-fold2.pth\",\n         \"add_wide_dropout\": False,\n      }\n\ncfg_4 = {\"backbone\": \"../input/debertalarge\",\n       \"lowercase\": False,\n       \"text_column\": \"essay_text\",\n       \"cache_dir\": \"../input/debertalarge\",\n       \"path\": \"../input/pseudo-75-datasets-v4/olivine-spaniel-ff/checkpoint-fold3.pth\",\n         \"add_wide_dropout\": False,\n      }\n\ncfg_5 = {\"backbone\": \"../input/debertalarge\",\n       \"lowercase\": False,\n       \"text_column\": \"essay_text\",\n       \"cache_dir\": \"../input/debertalarge\",\n       \"path\": \"../input/pseudo-75-datasets-v4/olivine-spaniel-ff/checkpoint-fold4.pth\",\n         \"add_wide_dropout\": False,\n      }\n\nall_cfgs = [cfg_1, cfg_2, cfg_3, cfg_4, cfg_5]\n\npreds.append(scale_probs(run_predictions_yauhen(all_cfgs, tt, yauhen_batch_size=4)))\nweights.append(-1.795161)","metadata":{"execution":{"iopub.status.busy":"2022-08-22T13:53:34.793909Z","iopub.execute_input":"2022-08-22T13:53:34.795156Z","iopub.status.idle":"2022-08-22T13:54:11.837632Z","shell.execute_reply.started":"2022-08-22T13:53:34.79511Z","shell.execute_reply":"2022-08-22T13:54:11.836213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cfg_1 = {\"backbone\": \"../input/debertalarge\",\n       \"lowercase\": False,\n       \"text_column\": \"essay_text\",\n       \"cache_dir\": \"../input/debertalarge\",\n       \"path\": \"../input/meteoric-bettong-v2-ff/checkpoint-fold0.pth\",\n         \"add_wide_dropout\": True,\n      }\n\ncfg_2 = {\"backbone\": \"../input/debertalarge\",\n       \"lowercase\": False,\n       \"text_column\": \"essay_text\",\n       \"cache_dir\": \"../input/debertalarge\",\n       \"path\": \"../input/meteoric-bettong-v2-ff/checkpoint-fold1.pth\",\n         \"add_wide_dropout\": True,\n      }\n\ncfg_3 = {\"backbone\": \"../input/debertalarge\",\n       \"lowercase\": False,\n       \"text_column\": \"essay_text\",\n       \"cache_dir\": \"../input/debertalarge\",\n       \"path\": \"../input/meteoric-bettong-v2-ff/checkpoint-fold2.pth\",\n         \"add_wide_dropout\": True,\n      }\n\nall_cfgs = [cfg_1, cfg_2, cfg_3]\n\npreds.append(scale_probs(run_predictions_yauhen(all_cfgs, tt)))\nweights.append(-0.455578)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cfg_1 = {\"backbone\": \"../input/debertalarge\",\n       \"lowercase\": False,\n       \"text_column\": \"essay_text\",\n       \"cache_dir\": \"../input/debertalarge\",\n       \"path\": \"../input/saffron-rook-ff/checkpoint_fold_0.pth\",\n         \"add_wide_dropout\": True,\n         \"add_types\": True,\n      }\n\ncfg_2 = {\"backbone\": \"../input/debertalarge\",\n       \"lowercase\": False,\n       \"text_column\": \"essay_text\",\n       \"cache_dir\": \"../input/debertalarge\",\n       \"path\": \"../input/saffron-rook-ff/checkpoint_fold_1.pth\",\n         \"add_wide_dropout\": True,\n         \"add_types\": True,\n      }\n\ncfg_3 = {\"backbone\": \"../input/debertalarge\",\n       \"lowercase\": False,\n       \"text_column\": \"essay_text\",\n       \"cache_dir\": \"../input/debertalarge\",\n       \"path\": \"../input/saffron-rook-ff/checkpoint_fold_2.pth\",\n         \"add_wide_dropout\": True,\n         \"add_types\": True,\n      }\n\nall_cfgs = [cfg_1, cfg_2, cfg_3]\n\nseed_1 = run_predictions_yauhen(all_cfgs, tt_v2, yauhen_batch_size=4)\n\ncfg_1 = {\"backbone\": \"../input/debertalarge\",\n       \"lowercase\": False,\n       \"text_column\": \"essay_text\",\n       \"cache_dir\": \"../input/debertalarge\",\n       \"path\": \"../input/saffron-rook-v2-ff/checkpoint_fold_0.pth\",\n         \"add_wide_dropout\": True,\n         \"add_types\": True,\n      }\n\ncfg_2 = {\"backbone\": \"../input/debertalarge\",\n       \"lowercase\": False,\n       \"text_column\": \"essay_text\",\n       \"cache_dir\": \"../input/debertalarge\",\n       \"path\": \"../input/saffron-rook-v2-ff/checkpoint_fold_1.pth\",\n         \"add_wide_dropout\": True,\n         \"add_types\": True,\n      }\n\ncfg_3 = {\"backbone\": \"../input/debertalarge\",\n       \"lowercase\": False,\n       \"text_column\": \"essay_text\",\n       \"cache_dir\": \"../input/debertalarge\",\n       \"path\": \"../input/saffron-rook-v2-ff/checkpoint_fold_2.pth\",\n         \"add_wide_dropout\": True,\n         \"add_types\": True,\n      }\n\nall_cfgs = [cfg_1, cfg_2, cfg_3]\n\nseed_2 = run_predictions_yauhen(all_cfgs, tt_v2, yauhen_batch_size=4)\n\nseeds = (seed_1 + seed_2) / 2\n\npreds.append(scale_probs(seeds))\nweights.append(0.639652)","metadata":{"execution":{"iopub.status.busy":"2022-08-22T13:54:40.750798Z","iopub.execute_input":"2022-08-22T13:54:40.75128Z","iopub.status.idle":"2022-08-22T13:55:44.258572Z","shell.execute_reply.started":"2022-08-22T13:54:40.751231Z","shell.execute_reply":"2022-08-22T13:55:44.257079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cfg_1 = {\"backbone\": \"../input/debertaxlarge\",\n       \"lowercase\": False,\n       \"text_column\": \"essay_text\",\n       \"cache_dir\": \"../input/debertaxlarge\",\n       \"path\": \"../input/big-ocelot-ff/checkpoint_fold_0.pth\",\n         \"add_wide_dropout\": False,\n         \"add_types\": True,\n      }\n\ncfg_2 = {\"backbone\": \"../input/debertaxlarge\",\n       \"lowercase\": False,\n       \"text_column\": \"essay_text\",\n       \"cache_dir\": \"../input/debertaxlarge\",\n       \"path\": \"../input/big-ocelot-ff/checkpoint_fold_1.pth\",\n         \"add_wide_dropout\": False,\n         \"add_types\": True,\n      }\n\ncfg_3 = {\"backbone\": \"../input/debertaxlarge\",\n       \"lowercase\": False,\n       \"text_column\": \"essay_text\",\n       \"cache_dir\": \"../input/debertaxlarge\",\n       \"path\": \"../input/big-ocelot-ff/checkpoint_fold_2.pth\",\n         \"add_wide_dropout\": False,\n         \"add_types\": True,\n      }\n\ncfg_4 = {\"backbone\": \"../input/debertaxlarge\",\n       \"lowercase\": False,\n       \"text_column\": \"essay_text\",\n       \"cache_dir\": \"../input/debertaxlarge\",\n       \"path\": \"../input/big-ocelot-ff-v3/checkpoint-fold0.pth\",\n         \"add_wide_dropout\": False,\n         \"add_types\": True,\n      }\n\ncfg_5 = {\"backbone\": \"../input/debertaxlarge\",\n       \"lowercase\": False,\n       \"text_column\": \"essay_text\",\n       \"cache_dir\": \"../input/debertaxlarge\",\n       \"path\": \"../input/big-ocelot-ff-v3/checkpoint-fold1.pth\",\n         \"add_wide_dropout\": False,\n         \"add_types\": True,\n      }\n\nall_cfgs = [cfg_1, cfg_2, cfg_3, cfg_4, cfg_5]\n\npreds.append(scale_probs(run_predictions_yauhen(all_cfgs, tt_v2, yauhen_batch_size=4)))\nweights.append(1.586749)","metadata":{"execution":{"iopub.status.busy":"2022-08-22T13:55:44.260579Z","iopub.execute_input":"2022-08-22T13:55:44.261042Z","iopub.status.idle":"2022-08-22T13:56:44.205062Z","shell.execute_reply.started":"2022-08-22T13:55:44.260942Z","shell.execute_reply":"2022-08-22T13:56:44.203527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cfg_1 = {\"backbone\": \"../input/debertalarge\",\n       \"lowercase\": False,\n       \"text_column\": \"essay_text\",\n       \"cache_dir\": \"../input/debertalarge\",\n       \"path\": \"../input/shrewd-rook-3ep-ff/checkpoint-fold0.pth\",\n         \"add_wide_dropout\": False,\n      }\n\ncfg_2 = {\"backbone\": \"../input/debertalarge\",\n       \"lowercase\": False,\n       \"text_column\": \"essay_text\",\n       \"cache_dir\": \"../input/debertalarge\",\n       \"path\": \"../input/shrewd-rook-3ep-ff/checkpoint-fold1.pth\",\n         \"add_wide_dropout\": False,\n      }\n\ncfg_3 = {\"backbone\": \"../input/debertalarge\",\n       \"lowercase\": False,\n       \"text_column\": \"essay_text\",\n       \"cache_dir\": \"../input/debertalarge\",\n       \"path\": \"../input/shrewd-rook-3ep-ff/checkpoint-fold2.pth\",\n         \"add_wide_dropout\": False,\n      }\n\ncfg_4 = {\"backbone\": \"../input/debertalarge\",\n       \"lowercase\": False,\n       \"text_column\": \"essay_text\",\n       \"cache_dir\": \"../input/debertalarge\",\n       \"path\": \"../input/shrewd-rook-3ep-ff-v2/checkpoint-fold0.pth\",\n         \"add_wide_dropout\": False,\n      }\n\ncfg_5 = {\"backbone\": \"../input/debertalarge\",\n       \"lowercase\": False,\n       \"text_column\": \"essay_text\",\n       \"cache_dir\": \"../input/debertalarge\",\n       \"path\": \"../input/shrewd-rook-3ep-ff-v2/checkpoint-fold1.pth\",\n         \"add_wide_dropout\": False,\n      }\n\ncfg_6 = {\"backbone\": \"../input/debertalarge\",\n       \"lowercase\": False,\n       \"text_column\": \"essay_text\",\n       \"cache_dir\": \"../input/debertalarge\",\n       \"path\": \"../input/shrewd-rook-3ep-ff-v2/checkpoint-fold2.pth\",\n         \"add_wide_dropout\": False,\n      }\n\nall_cfgs = [cfg_1, cfg_2, cfg_3, cfg_4, cfg_5, cfg_6]\n\npreds.append(scale_probs(run_predictions_yauhen(all_cfgs, tt, yauhen_batch_size=4)))\nweights.append(0.983297)","metadata":{"execution":{"iopub.status.busy":"2022-08-22T13:56:44.207558Z","iopub.execute_input":"2022-08-22T13:56:44.208097Z","iopub.status.idle":"2022-08-22T13:57:13.179098Z","shell.execute_reply.started":"2022-08-22T13:56:44.207996Z","shell.execute_reply":"2022-08-22T13:57:13.177676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cfg_1 = {\"backbone\": \"../input/debertalarge\",\n       \"lowercase\": False,\n       \"text_column\": \"essay_text\",\n       \"cache_dir\": \"../input/debertalarge\",\n       \"path\": \"../input/conscious-uakari-ff/checkpoint-fold0.pth\",\n         \"add_wide_dropout\": True,\n         \"add_types\": True,\n      }\n\ncfg_2 = {\"backbone\": \"../input/debertalarge\",\n       \"lowercase\": False,\n       \"text_column\": \"essay_text\",\n       \"cache_dir\": \"../input/debertalarge\",\n       \"path\": \"../input/conscious-uakari-ff/checkpoint-fold1.pth\",\n         \"add_wide_dropout\": True,\n         \"add_types\": True,\n      }\n\ncfg_3 = {\"backbone\": \"../input/debertalarge\",\n       \"lowercase\": False,\n       \"text_column\": \"essay_text\",\n       \"cache_dir\": \"../input/debertalarge\",\n       \"path\": \"../input/conscious-uakari-ff/checkpoint-fold2.pth\",\n         \"add_wide_dropout\": True,\n         \"add_types\": True,\n      }\n\ncfg_4 = {\"backbone\": \"../input/debertalarge\",\n       \"lowercase\": False,\n       \"text_column\": \"essay_text\",\n       \"cache_dir\": \"../input/debertalarge\",\n       \"path\": \"../input/conscious-uakari-ff-v2/checkpoint-fold0.pth\",\n         \"add_wide_dropout\": True,\n         \"add_types\": True,\n      }\n\ncfg_5 = {\"backbone\": \"../input/debertalarge\",\n       \"lowercase\": False,\n       \"text_column\": \"essay_text\",\n       \"cache_dir\": \"../input/debertalarge\",\n       \"path\": \"../input/conscious-uakari-ff-v2/checkpoint-fold1.pth\",\n         \"add_wide_dropout\": True,\n         \"add_types\": True,\n      }\n\ncfg_6 = {\"backbone\": \"../input/debertalarge\",\n       \"lowercase\": False,\n       \"text_column\": \"essay_text\",\n       \"cache_dir\": \"../input/debertalarge\",\n       \"path\": \"../input/conscious-uakari-ff-v2/checkpoint-fold2.pth\",\n         \"add_wide_dropout\": True,\n         \"add_types\": True,\n      }\n\nall_cfgs = [cfg_1, cfg_2, cfg_3, cfg_4, cfg_5, cfg_6]\n\n\npreds.append(scale_probs(run_predictions_yauhen(all_cfgs, tt_v2, yauhen_batch_size=4)))\nweights.append(3.707194)","metadata":{"execution":{"iopub.status.busy":"2022-08-22T13:57:13.181274Z","iopub.execute_input":"2022-08-22T13:57:13.182441Z","iopub.status.idle":"2022-08-22T13:57:40.341582Z","shell.execute_reply.started":"2022-08-22T13:57:13.182389Z","shell.execute_reply":"2022-08-22T13:57:40.340084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds.append(scale_probs(run_predictions_philipp(\"valiant-degu-ff-2\", df, BS=8)))\nweights.append(0.590190)","metadata":{"execution":{"iopub.status.busy":"2022-09-02T20:41:00.007848Z","iopub.execute_input":"2022-09-02T20:41:00.008544Z","iopub.status.idle":"2022-09-02T20:42:32.933241Z","shell.execute_reply.started":"2022-09-02T20:41:00.008509Z","shell.execute_reply":"2022-09-02T20:42:32.93212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds.append(scale_probs(run_predictions_philipp(\"axiomatic-vulture-ff-v2\", df, BS=8, num_models=5)))\nweights.append(0.964377)","metadata":{"execution":{"iopub.status.busy":"2022-09-02T20:10:38.359338Z","iopub.execute_input":"2022-09-02T20:10:38.359708Z","iopub.status.idle":"2022-09-02T20:13:19.248605Z","shell.execute_reply.started":"2022-09-02T20:10:38.359654Z","shell.execute_reply":"2022-09-02T20:13:19.247547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds.append(scale_probs(run_predictions_philipp(\"smart-bumblebee-ff\", df)))\nweights.append(0.366988)","metadata":{"execution":{"iopub.status.busy":"2022-09-02T20:42:32.935381Z","iopub.execute_input":"2022-09-02T20:42:32.935706Z","iopub.status.idle":"2022-09-02T20:44:02.751739Z","shell.execute_reply.started":"2022-09-02T20:42:32.935678Z","shell.execute_reply":"2022-09-02T20:44:02.750692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds.append(scale_probs(run_predictions_philipp(\"awesome-rose-ff-v2\", df, BS=8, num_models=5)))\nweights.append(1.162001)","metadata":{"execution":{"iopub.status.busy":"2022-09-02T20:13:19.250586Z","iopub.execute_input":"2022-09-02T20:13:19.251013Z","iopub.status.idle":"2022-09-02T20:15:54.20071Z","shell.execute_reply.started":"2022-09-02T20:13:19.250974Z","shell.execute_reply":"2022-09-02T20:15:54.19944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds.append(scale_probs(run_predictions_philipp(\"honest-apple-ff\", df, BS=8)))\nweights.append(0.543224)","metadata":{"execution":{"iopub.status.busy":"2022-09-02T20:15:54.202929Z","iopub.execute_input":"2022-09-02T20:15:54.203342Z","iopub.status.idle":"2022-09-02T20:17:29.03939Z","shell.execute_reply.started":"2022-09-02T20:15:54.203303Z","shell.execute_reply":"2022-09-02T20:17:29.038309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds.append(scale_probs(run_predictions_philipp(\"funky-funk-ff\", df, BS=8)))\nweights.append(1.455657)","metadata":{"execution":{"iopub.status.busy":"2022-09-02T20:17:29.041228Z","iopub.execute_input":"2022-09-02T20:17:29.041622Z","iopub.status.idle":"2022-09-02T20:18:58.96168Z","shell.execute_reply.started":"2022-09-02T20:17:29.041593Z","shell.execute_reply":"2022-09-02T20:18:58.960555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds.append(scale_probs(run_predictions_philipp(\"lame-flame-ff-v2\", df, BS=8, num_models=5)))\nweights.append(1.981731)","metadata":{"execution":{"iopub.status.busy":"2022-09-02T20:18:58.963183Z","iopub.execute_input":"2022-09-02T20:18:58.964051Z","iopub.status.idle":"2022-09-02T20:21:31.699996Z","shell.execute_reply.started":"2022-09-02T20:18:58.963991Z","shell.execute_reply":"2022-09-02T20:21:31.698953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds.append(scale_probs(run_predictions_philipp(\"pastel-frog-ff\", df, BS=8)))\nweights.append(-1.005743)","metadata":{"execution":{"iopub.status.busy":"2022-09-02T20:24:43.51137Z","iopub.execute_input":"2022-09-02T20:24:43.511933Z","iopub.status.idle":"2022-09-02T20:25:19.677085Z","shell.execute_reply.started":"2022-09-02T20:24:43.511881Z","shell.execute_reply":"2022-09-02T20:25:19.676087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_orig = np.array(preds).copy()","metadata":{"execution":{"iopub.status.busy":"2022-08-22T14:02:10.001638Z","iopub.execute_input":"2022-08-22T14:02:10.002506Z","iopub.status.idle":"2022-08-22T14:02:10.011618Z","shell.execute_reply.started":"2022-08-22T14:02:10.00246Z","shell.execute_reply":"2022-08-22T14:02:10.009869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_ind_models = df.copy()\n\nfor model_idx in range(len(preds)):\n    df_ind_models[f\"Adequate_{model_idx}\"] = preds[model_idx][:,0]\n    df_ind_models[f\"Effective_{model_idx}\"] = preds[model_idx][:,1]\n    df_ind_models[f\"Ineffective_{model_idx}\"] = preds[model_idx][:,2]","metadata":{"execution":{"iopub.status.busy":"2022-08-22T14:02:10.014226Z","iopub.execute_input":"2022-08-22T14:02:10.014923Z","iopub.status.idle":"2022-08-22T14:02:10.052471Z","shell.execute_reply.started":"2022-08-22T14:02:10.014875Z","shell.execute_reply":"2022-08-22T14:02:10.051154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = np.average(preds, weights=weights, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-08-22T14:02:10.056383Z","iopub.execute_input":"2022-08-22T14:02:10.056697Z","iopub.status.idle":"2022-08-22T14:02:10.0666Z","shell.execute_reply.started":"2022-08-22T14:02:10.05667Z","shell.execute_reply":"2022-08-22T14:02:10.064814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"assert len(preds) == len(df)","metadata":{"execution":{"iopub.status.busy":"2022-08-22T14:02:10.087299Z","iopub.execute_input":"2022-08-22T14:02:10.087946Z","iopub.status.idle":"2022-08-22T14:02:10.097549Z","shell.execute_reply.started":"2022-08-22T14:02:10.087906Z","shell.execute_reply":"2022-08-22T14:02:10.095315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pp = preds.copy()\n\neps = 0.0001\npp = pp.clip(eps, 1 - eps)\npp = pp / pp.sum(axis=1, keepdims=True)\n\npp = scale_probs(pp)","metadata":{"execution":{"iopub.status.busy":"2022-08-22T14:02:10.128302Z","iopub.execute_input":"2022-08-22T14:02:10.129287Z","iopub.status.idle":"2022-08-22T14:02:10.142605Z","shell.execute_reply.started":"2022-08-22T14:02:10.129174Z","shell.execute_reply":"2022-08-22T14:02:10.141163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"Adequate\"] = pp[:, 0] \ndf[\"Effective\"] = pp[:, 1] \ndf[\"Ineffective\"] = pp[:, 2] ","metadata":{"execution":{"iopub.status.busy":"2022-08-22T14:02:10.174673Z","iopub.execute_input":"2022-08-22T14:02:10.175978Z","iopub.status.idle":"2022-08-22T14:02:10.188587Z","shell.execute_reply.started":"2022-08-22T14:02:10.175934Z","shell.execute_reply":"2022-08-22T14:02:10.186655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_ind_models[\"Adequate\"] = pp[:, 0] \ndf_ind_models[\"Effective\"] = pp[:, 1] \ndf_ind_models[\"Ineffective\"] = pp[:, 2] ","metadata":{"execution":{"iopub.status.busy":"2022-08-22T14:02:10.191183Z","iopub.execute_input":"2022-08-22T14:02:10.191703Z","iopub.status.idle":"2022-08-22T14:02:10.202215Z","shell.execute_reply.started":"2022-08-22T14:02:10.191624Z","shell.execute_reply":"2022-08-22T14:02:10.200538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"orig_preds = pp.copy()","metadata":{"execution":{"iopub.status.busy":"2022-08-22T14:02:10.204849Z","iopub.execute_input":"2022-08-22T14:02:10.205428Z","iopub.status.idle":"2022-08-22T14:02:10.214849Z","shell.execute_reply.started":"2022-08-22T14:02:10.205335Z","shell.execute_reply":"2022-08-22T14:02:10.213339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if CALC_SCORE:\n    from sklearn.metrics import log_loss\n    \n    label_cols = [\"Adequate\", \"Effective\", \"Ineffective\"]\n    \n    y = np.zeros_like(preds)\n    \n    for ii, jj in enumerate([label_cols.index(x) for x in df[\"discourse_effectiveness\"].values]):\n        y[ii,jj] = 1\n        \n    print(log_loss(y, pp))","metadata":{"execution":{"iopub.status.busy":"2022-08-22T14:02:10.217915Z","iopub.execute_input":"2022-08-22T14:02:10.219517Z","iopub.status.idle":"2022-08-22T14:02:10.22989Z","shell.execute_reply.started":"2022-08-22T14:02:10.219442Z","shell.execute_reply":"2022-08-22T14:02:10.228089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_cols = [\"Adequate\", \"Effective\", \"Ineffective\"]\noof_cols = []\nfor j, l in enumerate(label_cols):\n\n    df[f\"oof_{l}\"] = pp[:,j]\n    oof_cols.append(f\"oof_{l}\")","metadata":{"execution":{"iopub.status.busy":"2022-08-22T14:02:10.233943Z","iopub.execute_input":"2022-08-22T14:02:10.234313Z","iopub.status.idle":"2022-08-22T14:02:10.248145Z","shell.execute_reply.started":"2022-08-22T14:02:10.234286Z","shell.execute_reply":"2022-08-22T14:02:10.246539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import StandardScaler\nimport torch\n\nimport torch.nn as nn\n\nclass FeedbackStackerModel(nn.Module):\n    def __init__(self, n_features):\n        super(FeedbackStackerModel, self).__init__()\n        \n        self.sizes = [256, 128, 64]\n        \n        self.features = nn.Sequential(\n            nn.utils.weight_norm(nn.Linear(n_features, self.sizes[0])),\n            nn.PReLU(),\n            nn.Linear(self.sizes[0], self.sizes[1]),\n            nn.PReLU(),\n            nn.Linear(self.sizes[1], self.sizes[2]),\n            nn.PReLU(),\n        )\n        \n        self.head = nn.Linear(self.sizes[-1], 3)\n        \n        self.loss_fn = nn.CrossEntropyLoss()\n        \n    def forward(self, x, y):\n        \n        x = self.features(x)\n        x = self.head(x)\n        \n        \n        output = {}\n        \n        output[\"logits\"] = x\n        \n        if self.training:\n            output[\"loss\"] = self.loss_fn(x, y.argmax(dim=1))\n        \n        return output\n\nclass FeedbackStackerDataset(Dataset):\n\n    def __init__(self, df, mode):\n        self.df = df.copy().reset_index(drop=True)\n        self.mode = mode\n\n        self.feature_cols = oof_cols.copy()\n        self.label_cols = label_cols.copy()\n        \n        df = self.df\n        \n        df[f\"len\"] = df.groupby(\"essay_id\")[f\"discourse_id\"].transform(\"count\") / 10\n        self.feature_cols.append(f\"len\")\n        \n        for j, l in enumerate(label_cols):\n            df[f\"oof_{l}_mean\"] = df.groupby(\"essay_id\")[f\"oof_{l}\"].transform(\"mean\")\n            self.feature_cols.append(f\"oof_{l}_mean\")\n            \n            df[f\"oof_{l}_t_mean\"] = df.groupby([\"essay_id\", \"discourse_type\"])[f\"oof_{l}\"].transform(\"mean\")\n            self.feature_cols.append(f\"oof_{l}_t_mean\")\n\n        self.num_features = len(self.feature_cols)\n\n        self.X = self.df[self.feature_cols].values\n        self.y = self.df[self.label_cols].values\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        \n        X = self.X[idx]\n        y = self.y[idx]\n        \n        return torch.FloatTensor(X), torch.FloatTensor(y)\n\n\n    def __len__(self):\n        return self.df.shape[0]\n    \nds = FeedbackStackerDataset(df.copy(), mode=\"val\")\nds[0][0].shape\n\ndef run_nn_stacker(exp_name, df, BS=64):\n\n\n    ds = FeedbackStackerDataset(df.iloc[:].copy(), mode=\"test\")\n    \n    checkpoints = glob(f\"../input/{exp_name}/*.pth\")\n    \n    preds_all = []\n    for checkpoint in checkpoints:\n        print(f\"running model {checkpoint}\")\n        \n        model = FeedbackStackerModel(n_features=ds.num_features).to(\"cuda\").eval()\n    \n        model_weights = torch.load(checkpoint, map_location=\"cpu\")\n\n        model.load_state_dict(collections.OrderedDict(model_weights), strict=True)\n        \n        del model_weights\n        gc.collect() \n    \n        batch_size = BS\n        dl = DataLoader(ds, shuffle=False, batch_size = batch_size, num_workers = N_CORES)\n\n        with torch.no_grad():\n            preds = []\n            for batch in tqdm(dl):\n\n                data = [x.to(\"cuda\") for x in batch]\n                inputs, target = data\n                out = model(inputs, target)\n                preds.append(out[\"logits\"].float().softmax(dim=1).detach().cpu().numpy())\n\n        preds_all.append(np.concatenate(preds, axis=0))\n        \n        del model\n        del dl\n        gc.collect()\n        \n    del ds\n    \n    \n    preds = np.mean(preds_all, axis=0)\n    \n    return preds\n\nnn_stacker_preds_1 = run_nn_stacker(\"feedback-nn-v8-blend151-ff\", df, BS=64)","metadata":{"execution":{"iopub.status.busy":"2022-08-22T14:02:10.250727Z","iopub.execute_input":"2022-08-22T14:02:10.25126Z","iopub.status.idle":"2022-08-22T14:02:15.242869Z","shell.execute_reply.started":"2022-08-22T14:02:10.251232Z","shell.execute_reply":"2022-08-22T14:02:15.24141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import StandardScaler\nimport torch\n\nimport torch.nn as nn\n\nclass FeedbackStackerDataset(Dataset):\n\n    def __init__(self, df, mode):\n        self.df = df.copy()\n        self.mode = mode\n\n        self.label_cols = label_cols.copy()\n        \n        p = [p[self.df.index.values] for p in preds_orig.copy()]\n        p = np.stack(p)\n        \n        df = self.df\n        \n        X = []\n        for j in range(p.shape[0]):\n            cols = []\n            for jj, l in enumerate(label_cols):\n\n                df[f\"oof_{l}\"] = p[j,:,jj]\n                cols.append(f\"oof_{l}\")\n                \n                df[f\"oof_{l}_mean\"] = df.groupby(\"essay_id\")[f\"oof_{l}\"].transform(\"mean\")\n                cols.append(f\"oof_{l}_mean\")\n\n                df[f\"oof_{l}_t_mean\"] = df.groupby([\"essay_id\", \"discourse_type\"])[f\"oof_{l}\"].transform(\"mean\")\n                cols.append(f\"oof_{l}_t_mean\")\n                \n            df[f\"len\"] = df.groupby(\"essay_id\")[f\"discourse_id\"].transform(\"count\") / 10\n            cols.append(f\"len\")\n        \n            \n            X.append(df[cols].values)\n         \n        X = np.stack(X).transpose(1,2,0)\n        print(X.shape)\n        \n        self.num_features = X.shape[1]\n\n        self.X = X\n        self.y = self.df[self.label_cols].values\n\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        \n        X = self.X[idx]\n        y = self.y[idx]\n        \n        return torch.FloatTensor(X), torch.FloatTensor(y)\n\n\n    def __len__(self):\n        return self.df.shape[0]\n\nclass FeedbackStackerModel(nn.Module):\n    def __init__(self, n_features):\n        super(FeedbackStackerModel, self).__init__()\n        \n        self.sizes = [256, 128, 64]\n        \n        layers = []\n        for j,s in enumerate(self.sizes):\n            if j == 0:\n                layers.append(nn.Conv1d(n_features, s, 1))\n            else:\n                layers.append(nn.Conv1d(self.sizes[j-1], s, 1))\n            layers.append(nn.PReLU())\n            layers.append(nn.Dropout(0.2))\n        \n        self.features = nn.Sequential(*layers)\n        self.head = nn.Linear(self.sizes[-1], 3)\n        \n        self.loss_fn = nn.CrossEntropyLoss()\n        \n    def forward(self, x, y):\n        x = self.features(x)\n        x = x.mean(dim=2)\n        x = self.head(x)\n        \n        output = {}\n        \n        output[\"logits\"] = x\n        \n        if self.training:\n            output[\"loss\"] = self.loss_fn(x, y.argmax(dim=1))\n        \n        return output\n    \nds = FeedbackStackerDataset(df.copy(), mode=\"val\")\nds[0][0].shape\n\nnn_stacker_preds_2 = run_nn_stacker(\"feedback-nn-v11-blend151-ff\", df, BS=64)","metadata":{"execution":{"iopub.status.busy":"2022-08-22T14:02:15.245706Z","iopub.execute_input":"2022-08-22T14:02:15.246302Z","iopub.status.idle":"2022-08-22T14:02:26.056226Z","shell.execute_reply.started":"2022-08-22T14:02:15.246241Z","shell.execute_reply":"2022-08-22T14:02:26.05448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def gen_x(values):\n    range = 1\n    return np.histogram(np.clip(values, 0.001, 0.999*range), bins=3, density=True, range=(0,range))[0]","metadata":{"execution":{"iopub.status.busy":"2022-08-22T14:02:26.086947Z","iopub.execute_input":"2022-08-22T14:02:26.088179Z","iopub.status.idle":"2022-08-22T14:02:26.096062Z","shell.execute_reply.started":"2022-08-22T14:02:26.088138Z","shell.execute_reply":"2022-08-22T14:02:26.094415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_groups = []\n\ngb = df.groupby('essay_id', sort=False)\nfor name, group in tqdm(gb):\n    group[\"n_types\"] = group.discourse_type.nunique()\n    for class_name in [\"Adequate\", \"Effective\", \"Ineffective\"]:\n        if class_name in [\"Adequate\", \"Effective\"]:\n            continue\n        for idx, val in enumerate(gen_x(group[class_name].values)):\n            group[f\"{class_name}_bin_{idx}\"] = val \n        group[f\"mean_{class_name}\"] = group[class_name].mean()    \n\n    all_groups.append(group)\n\ndf = pd.concat(all_groups).reset_index(drop=True)\n\ndisc_types_mapping = {'Lead': 0,\n'Position': 1,\n'Claim': 2,\n'Evidence': 3,\n'Counterclaim': 4,\n'Rebuttal': 5,\n'Concluding Statement': 6}\ndf[\"len_disc\"] = df.discourse_text.str.len()\n\ndf[\"discourse_type\"] = df[\"discourse_type\"].map(disc_types_mapping)\n\ndf[\"paragraph_cnt\"] = df.essay_text.map(lambda x: len(x.split(\"\\n\\n\")))","metadata":{"execution":{"iopub.status.busy":"2022-08-22T14:02:26.098099Z","iopub.execute_input":"2022-08-22T14:02:26.098911Z","iopub.status.idle":"2022-08-22T14:02:26.129977Z","shell.execute_reply.started":"2022-08-22T14:02:26.098873Z","shell.execute_reply":"2022-08-22T14:02:26.128566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import lightgbm\n\nlgb_stacker_preds = []\n\nfor fold in range(5):\n    print(fold)\n    gbm = lightgbm.Booster(model_file=f\"../input/lightgbm151/model_fold_{fold}.txt\")\n    valid_pred = gbm.predict(df[['discourse_type', 'Adequate', 'Effective', 'Ineffective', 'n_types',\n       'Ineffective_bin_0', 'Ineffective_bin_1', 'Ineffective_bin_2',\n       'mean_Ineffective', 'len_disc', 'paragraph_cnt']])\n    lgb_stacker_preds.append(valid_pred)","metadata":{"execution":{"iopub.status.busy":"2022-08-22T14:02:26.131976Z","iopub.execute_input":"2022-08-22T14:02:26.13277Z","iopub.status.idle":"2022-08-22T14:02:26.32916Z","shell.execute_reply.started":"2022-08-22T14:02:26.13273Z","shell.execute_reply":"2022-08-22T14:02:26.327747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgb_stacker_preds = np.array(lgb_stacker_preds).mean(axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-08-22T14:02:26.331208Z","iopub.execute_input":"2022-08-22T14:02:26.331662Z","iopub.status.idle":"2022-08-22T14:02:26.33892Z","shell.execute_reply.started":"2022-08-22T14:02:26.331607Z","shell.execute_reply":"2022-08-22T14:02:26.33748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_groups = []\n\ngb = df_ind_models.groupby('essay_id', sort=False)\nfor name, group in tqdm(gb):\n    group[\"n_types\"] = group.discourse_type.nunique()\n\n    for class_name in [\"Adequate\", \"Effective\", \"Ineffective\"]:\n        if class_name in [\"Adequate\", \"Effective\"]:\n            continue\n\n        for idx, val in enumerate(gen_x(group[class_name].values)):\n            group[f\"{class_name}_bin_{idx}\"] = val\n\n        group[f\"mean_{class_name}\"] = group[class_name].mean()\n        group[f\"max_{class_name}\"] = group[class_name].max()\n\n    for class_name in [f\"Ineffective_{i}\" for i in range(15)]:\n        group[f\"mean_{class_name}\"] = group[class_name].mean()\n\n    for class_name in [f\"Effective_{i}\" for i in range(15)]:\n        group[f\"mean_{class_name}\"] = group[class_name].mean()\n\n    all_groups.append(group)\n\ndf_ind_models = pd.concat(all_groups).reset_index(drop=True)\n\ndf_ind_models[\"paragraph_cnt\"] = df_ind_models.essay_text.map(lambda x: len(x.split(\"\\n\\n\")))\n\ndisc_types_mapping = {'Lead': 0,\n'Position': 1,\n'Claim': 2,\n'Evidence': 3,\n'Counterclaim': 4,\n'Rebuttal': 5,\n'Concluding Statement': 6}\n\ndf_ind_models[\"discourse_type\"] = df_ind_models[\"discourse_type\"].map(disc_types_mapping)\n\nfor i in range(15):\n    df_ind_models = df_ind_models.drop([f'Adequate_{i}'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-08-22T14:02:26.341165Z","iopub.execute_input":"2022-08-22T14:02:26.342171Z","iopub.status.idle":"2022-08-22T14:02:26.410491Z","shell.execute_reply.started":"2022-08-22T14:02:26.342128Z","shell.execute_reply":"2022-08-22T14:02:26.408697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import lightgbm\n\nlgb_stacker_preds_2 = []\n\nfor fold in range(5):\n    print(fold)\n    gbm = lightgbm.Booster(model_file=f\"../input/lightgbm151v2/model_fold_{fold}.txt\")\n    valid_pred = gbm.predict(df_ind_models[['discourse_type', 'Adequate', 'Effective', 'Ineffective', 'Effective_0',\n       'Ineffective_0', 'Effective_1', 'Ineffective_1', 'Effective_2',\n       'Ineffective_2', 'Effective_3', 'Ineffective_3', 'Effective_4',\n       'Ineffective_4', 'Effective_5', 'Ineffective_5', 'Effective_6',\n       'Ineffective_6', 'Effective_7', 'Ineffective_7', 'Effective_8',\n       'Ineffective_8', 'Effective_9', 'Ineffective_9', 'Effective_10',\n       'Ineffective_10', 'Effective_11', 'Ineffective_11', 'Effective_12',\n       'Ineffective_12', 'Effective_13', 'Ineffective_13', 'Effective_14',\n       'Ineffective_14', 'n_types', 'Ineffective_bin_0', 'Ineffective_bin_1',\n       'Ineffective_bin_2', 'mean_Ineffective', 'max_Ineffective',\n       'mean_Ineffective_0', 'mean_Ineffective_1', 'mean_Ineffective_2',\n       'mean_Ineffective_3', 'mean_Ineffective_4', 'mean_Ineffective_5',\n       'mean_Ineffective_6', 'mean_Ineffective_7', 'mean_Ineffective_8',\n       'mean_Ineffective_9', 'mean_Ineffective_10', 'mean_Ineffective_11',\n       'mean_Ineffective_12', 'mean_Ineffective_13', 'mean_Ineffective_14',\n       'mean_Effective_0', 'mean_Effective_1', 'mean_Effective_2',\n       'mean_Effective_3', 'mean_Effective_4', 'mean_Effective_5',\n       'mean_Effective_6', 'mean_Effective_7', 'mean_Effective_8',\n       'mean_Effective_9', 'mean_Effective_10', 'mean_Effective_11',\n       'mean_Effective_12', 'mean_Effective_13', 'mean_Effective_14',\n       'paragraph_cnt']])\n    lgb_stacker_preds_2.append(valid_pred)","metadata":{"execution":{"iopub.status.busy":"2022-08-22T14:02:26.412714Z","iopub.execute_input":"2022-08-22T14:02:26.413659Z","iopub.status.idle":"2022-08-22T14:02:26.653878Z","shell.execute_reply.started":"2022-08-22T14:02:26.413616Z","shell.execute_reply":"2022-08-22T14:02:26.652578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgb_stacker_preds_2 = np.array(lgb_stacker_preds_2).mean(axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-08-22T14:02:26.65658Z","iopub.execute_input":"2022-08-22T14:02:26.657098Z","iopub.status.idle":"2022-08-22T14:02:26.664383Z","shell.execute_reply.started":"2022-08-22T14:02:26.657052Z","shell.execute_reply":"2022-08-22T14:02:26.662563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_preds = [\n    orig_preds.copy(),\n    lgb_stacker_preds.copy(),\n    nn_stacker_preds_1.copy(),\n    nn_stacker_preds_2.copy(),\n    lgb_stacker_preds_2.copy(),\n]\n\nall_preds = np.average(all_preds, axis=0, weights=[2.17532521, 0.96247677, 1.15351147, 0.62746974, 0.47835051])","metadata":{"execution":{"iopub.status.busy":"2022-08-22T14:02:26.693955Z","iopub.execute_input":"2022-08-22T14:02:26.695048Z","iopub.status.idle":"2022-08-22T14:02:26.717544Z","shell.execute_reply.started":"2022-08-22T14:02:26.69497Z","shell.execute_reply":"2022-08-22T14:02:26.716065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"Adequate\"] = all_preds[:, 0]\ndf[\"Effective\"] = all_preds[:, 1]\ndf[\"Ineffective\"] = all_preds[:, 2]","metadata":{"execution":{"iopub.status.busy":"2022-08-22T14:02:26.774168Z","iopub.execute_input":"2022-08-22T14:02:26.774726Z","iopub.status.idle":"2022-08-22T14:02:26.784611Z","shell.execute_reply.started":"2022-08-22T14:02:26.774686Z","shell.execute_reply":"2022-08-22T14:02:26.783067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[['discourse_id', 'Ineffective', 'Adequate', 'Effective']].to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-08-22T14:02:26.787105Z","iopub.execute_input":"2022-08-22T14:02:26.787839Z","iopub.status.idle":"2022-08-22T14:02:26.795845Z","shell.execute_reply.started":"2022-08-22T14:02:26.7878Z","shell.execute_reply":"2022-08-22T14:02:26.794164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if CALC_SCORE:\n    from sklearn.metrics import log_loss\n    \n    label_cols = [\"Adequate\", \"Effective\", \"Ineffective\"]\n    \n    y = np.zeros_like(all_preds)\n    \n    for ii, jj in enumerate([label_cols.index(x) for x in df[\"discourse_effectiveness\"].values]):\n        y[ii,jj] = 1\n        \n    print(log_loss(y, all_preds))","metadata":{"execution":{"iopub.status.busy":"2022-08-22T14:02:26.798275Z","iopub.execute_input":"2022-08-22T14:02:26.800556Z","iopub.status.idle":"2022-08-22T14:02:26.809738Z","shell.execute_reply.started":"2022-08-22T14:02:26.800513Z","shell.execute_reply":"2022-08-22T14:02:26.8075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}