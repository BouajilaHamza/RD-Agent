{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Packages","metadata":{"_uuid":"5e4c8a91-7788-4fba-9d8a-a58828373835","_cell_guid":"1f3625f8-d03b-49ad-a25c-246872d2fc31","trusted":true}},{"cell_type":"code","source":"# use transformer version 4.19.2\n!pip install -qq ../input/transformers-4-19-2/transformers-4.19.2-py3-none-any.whl","metadata":{"_uuid":"a76ae4a2-2a30-4d51-b8f7-cef8c2e2a0cd","_cell_guid":"f2e2b3fe-b4b2-4c66-8db7-a25ced94b1c3","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:03:32.232886Z","iopub.execute_input":"2022-08-23T12:03:32.233433Z","iopub.status.idle":"2022-08-23T12:04:09.595089Z","shell.execute_reply.started":"2022-08-23T12:03:32.233324Z","shell.execute_reply":"2022-08-23T12:04:09.594004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Imports","metadata":{"_uuid":"1ae6f7af-ffa7-4b47-829e-05e24ee8a892","_cell_guid":"6a04ee98-8ca7-4fda-b8c8-0d15f622a1e1","trusted":true}},{"cell_type":"code","source":"# basics\nimport os\nimport gc\nimport sys\nimport json\nfrom copy import deepcopy\nfrom dataclasses import dataclass\nfrom itertools import chain\n\n# Processing\nimport numpy as np\nimport pandas as pd\n\n# PyTorch\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\n\n# huggingface\nfrom datasets import Dataset\nfrom accelerate import Accelerator\nfrom transformers import AutoConfig, AutoModel, AutoTokenizer, DataCollatorWithPadding\nfrom transformers.models.deberta_v2.modeling_deberta_v2 import ContextPooler, StableDropout\n\n# misc\nfrom joblib import Parallel, delayed\nfrom tqdm.auto import tqdm\n\n# ipython\nfrom IPython.display import display\nfrom IPython.core.debugger import set_trace\nfrom tokenizers import AddedToken","metadata":{"_uuid":"b6331a04-a9dc-48a0-970a-5fae2438dc4d","_cell_guid":"f3d83d4b-5b43-4921-af3e-e35ce27190f2","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:04:12.375999Z","iopub.execute_input":"2022-08-23T12:04:12.376418Z","iopub.status.idle":"2022-08-23T12:04:20.310779Z","shell.execute_reply.started":"2022-08-23T12:04:12.376377Z","shell.execute_reply":"2022-08-23T12:04:20.309653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Enable other models","metadata":{"_uuid":"1ebe1a70-f3c9-4eab-8552-78d795ffbcbf","_cell_guid":"d81afdc5-be59-4337-a94b-4c0ed9ea580c","trusted":true}},{"cell_type":"code","source":"# debv3-l 8 fold\nuse_exp1 = True\n\n# debv3-l multihead lstm\nuse_exp3 = False\n\n# debv3-l resolved data\nuse_exp4 = False\n\n# dexl \nuse_exp6 = False\n\n# debl kd from dexl\nuse_exp8 = False\n\n# debv3-l revist\nuse_exp10 = False\n\n# debv3-l uda \nuse_exp11 = False\n\n# debv3-l 10 fold\nuse_exp16 = True\n\n# v3L+b \nuse_exp102 = False\n\n# debv3-l prompt 8 fold\nuse_exp205 = False\n\n# debv3-l prompt 10 fold LB 0.565\nuse_exp209 = True\n\n# longformer\nuse_exp212 = True\n\nuse_exp214 = False\n\nuse_full_data_models = True\n\n# enable for running sampled (3k) train vs test\ndebug = False","metadata":{"_uuid":"07713218-3b46-4ec4-801b-3236ba31209d","_cell_guid":"0aa43898-dfed-4d17-ba1d-45ced72c6926","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:04:20.313026Z","iopub.execute_input":"2022-08-23T12:04:20.314143Z","iopub.status.idle":"2022-08-23T12:04:20.320772Z","shell.execute_reply.started":"2022-08-23T12:04:20.314098Z","shell.execute_reply":"2022-08-23T12:04:20.319256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\nfrom textblob import TextBlob\n\n# functions for separating the POS Tags\ndef adjectives(text):\n    blob = TextBlob(text)\n    return len([word for (word,tag) in blob.tags if tag == 'JJ'])\ndef verbs(text):\n    blob = TextBlob(text)\n    return len([word for (word,tag) in blob.tags if tag.startswith('VB')])\ndef adverbs(text):\n    blob = TextBlob(text)\n    return len([word for (word,tag) in blob.tags if tag.startswith('RB')])\ndef nouns(text):\n    blob = TextBlob(text)\n    return len([word for (word,tag) in blob.tags if tag.startswith('NN')])","metadata":{"execution":{"iopub.status.busy":"2022-08-23T12:04:20.322119Z","iopub.execute_input":"2022-08-23T12:04:20.322705Z","iopub.status.idle":"2022-08-23T12:04:21.004264Z","shell.execute_reply.started":"2022-08-23T12:04:20.322671Z","shell.execute_reply":"2022-08-23T12:04:21.003397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Data","metadata":{"_uuid":"a0a8766d-901e-4a1c-aade-d17e88d90415","_cell_guid":"3f2a554a-868f-4b2d-82b0-5303f19a51ad","trusted":true}},{"cell_type":"code","source":"# Read in test data and assign uid for tracking discourse elements\nif debug:\n    test_df = pd.read_csv(\"../input/feedback-prize-effectiveness/train.csv\")\n    test_df = test_df.sample(n=3000).reset_index(drop=True)\nelse:\n    test_df = pd.read_csv(\"../input/feedback-prize-effectiveness/test.csv\")\n\n\nall_ids = test_df[\"discourse_id\"].unique().tolist()\ndiscourse2idx = {discourse: pos for pos, discourse in enumerate(all_ids)}\nidx2discourse = {v:k for k, v in discourse2idx.items()}\ntest_df[\"uid\"] = test_df[\"discourse_id\"].map(discourse2idx)\n\n# Load test essays\ndef _load_essay(essay_id):\n    if debug:\n        filename = os.path.join(\"../input/feedback-prize-effectiveness/train\", f\"{essay_id}.txt\")\n    else:\n        filename = os.path.join(\"../input/feedback-prize-effectiveness/test\", f\"{essay_id}.txt\")\n    with open(filename, \"r\") as f:\n        text = f.read()\n    return [essay_id, text]\n\ndef read_essays(essay_ids, num_jobs=12):\n    train_essays = []\n    results = Parallel(n_jobs=num_jobs, verbose=1)(delayed(_load_essay)(essay_id) for essay_id in essay_ids)\n    for result in results:\n        train_essays.append(result)\n\n    result_dict = dict()\n    for e in train_essays:\n        result_dict[e[0]] = e[1]\n\n    essay_df = pd.Series(result_dict).reset_index()\n    essay_df.columns = [\"essay_id\", \"essay_text\"]\n    return essay_df\n\nessay_ids = test_df[\"essay_id\"].unique().tolist()\nessay_df = read_essays(essay_ids)\n\n# Display sample test data\ndisplay(test_df.sample())","metadata":{"_uuid":"3c719a50-5f91-4d01-84be-4a6e9fe86fae","_cell_guid":"f4006a5b-e105-4185-98cd-298bd68bcc27","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:04:21.007779Z","iopub.execute_input":"2022-08-23T12:04:21.008084Z","iopub.status.idle":"2022-08-23T12:04:23.103267Z","shell.execute_reply.started":"2022-08-23T12:04:21.008051Z","shell.execute_reply":"2022-08-23T12:04:23.099251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Topics","metadata":{}},{"cell_type":"code","source":"import sys\n#sys.path.append('../input/k/trushk/feedback-topics-identification-with-bertopic/site-packages/site-packages/')\nsys.path.append('../input/k/lextoumbourou/feedback-topics-identification-with-bertopic/site-packages/')\nfrom bertopic import BERTopic\nimport glob, pandas as pd, numpy as np, re\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\nfrom tqdm import tqdm\n\ntopic_model = BERTopic.load(\"../input/feedback-topics-identification-with-bertopic/feedback_2021_topic_model\")\ntopic_meta_df = pd.read_csv('../input/feedback-topics-identification-with-bertopic/topic_model_metadata.csv')\n\n#topic_model = BERTopic.load(\"../input/fdbk-topic-model/feedback_2021_topic_model\")\n#topic_meta_df = pd.read_csv('../input/fdbk-topic-model/topic_model_metadata.csv')\n\n\ntopic_meta_df = topic_meta_df.rename(columns={'Topic': 'topic', 'Name': 'topic_name'}).drop(columns=['Count'])\ntopic_meta_df.topic_name = topic_meta_df.topic_name.apply(lambda n: ' '.join(n.split('_')[1:]))\n\nsws = stopwords.words(\"english\") + [\"n't\",  \"'s\", \"'ve\"]\nfls = glob.glob(\"../input/feedback-prize-effectiveness/test/*.txt\")\ndocs = []\nfor fl in tqdm(fls):\n    with open(fl) as f:\n        txt = f.read()\n        word_tokens = word_tokenize(txt)\n        txt = \" \".join([w for w in word_tokens if not w.lower() in sws])\n    docs.append(txt)\n\ntopics, probs = topic_model.transform(docs)\n\npred_topics = pd.DataFrame()\ndids = list(map(lambda fl: fl.split(\"/\")[-1].split(\".\")[0], fls))\npred_topics[\"id\"] = dids\npred_topics[\"topic\"] = topics\npred_topics['prob'] = probs\npred_topics = pred_topics.drop(columns={'prob'})\npred_topics = pred_topics.rename(columns={'id': 'essay_id'})\n\npred_topics = pred_topics.merge(topic_meta_df, left_on='topic', right_on='topic', how='left')\npred_topics.rename(columns={'topic': 'topic_num', 'topic_name': 'topic'}, inplace=True)\npred_topics","metadata":{"execution":{"iopub.status.busy":"2022-08-23T12:04:23.106655Z","iopub.execute_input":"2022-08-23T12:04:23.107054Z","iopub.status.idle":"2022-08-23T12:05:16.51395Z","shell.execute_reply.started":"2022-08-23T12:04:23.107013Z","shell.execute_reply":"2022-08-23T12:05:16.513042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"topic_map = {\n\t'seagoing luke animals cowboys': 'Should you join the Seagoing Cowboys program?',\n\t'driving phone phones cell' :  'Should drivers be allowed to use cell phones while driving?',\n\t 'phones cell cell phones school': 'Should students be allowed to use cell phones in school?',\n\t 'straights state welfare wa' : ' State welfare' ,\n\t 'summer students project projects': 'Should school summer projects be designed by students or teachers?',\n\t 'students online school classes': 'Is distance learning or online schooling beneficial to students?',\n\t 'car cars usage pollution': 'Should car usage be limited to help reduce pollution?',\n\t 'cars driverless car driverless cars': 'Are driverless cars going to be helpful?',\n\t 'emotions technology facial computer' : 'Should computers read the emotional expressions of students in a classroom?',\n\t 'community service community service help': 'Should community service be mandatory for all students?',\n\t 'sports average school students' : 'Should students be allowed to participate in sports  unless they have at least a grade B average?',\n\t 'advice people ask multiple': 'Should you ask multiple people for advice?',\n\t 'extracurricular activities activity students': 'Should all students participate in at least one extracurricular activity?',\n\t 'electoral college electoral college vote':  'Should the electoral college be abolished in favor of popular vote?' ,\n\t 'electoral vote college electoral college' : 'Should the electoral college be abolished in favor of popular vote?' ,\n\t 'face mars landform aliens' : 'Is the face on Mars  a natural landform or made by Aliens?',\n     'venus planet author earth': 'Is Studying Venus a worthy pursuit?',\n}\nessay_df = essay_df.merge(pred_topics, on='essay_id', how='left')\nessay_df['prompt'] = essay_df['topic'].map(topic_map)\n\nessay_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-23T12:05:16.515458Z","iopub.execute_input":"2022-08-23T12:05:16.515813Z","iopub.status.idle":"2022-08-23T12:05:16.533773Z","shell.execute_reply.started":"2022-08-23T12:05:16.515779Z","shell.execute_reply":"2022-08-23T12:05:16.532997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EXP 214 - Debv2-XL + Aug data","metadata":{}},{"cell_type":"code","source":"config = \"\"\"{\n    \"debug\": false,\n\n    \"base_model_path\": \"../input/deberta-v2-xlarge/\",\n    \"add_new_tokens\": true,\n    \"model_dir\": \"./outputs\",\n\n    \"max_length\": 1024,\n    \"stride\": 256,\n    \"num_labels\": 3,\n    \"dropout\": 0.1,\n    \"infer_bs\": 8\n}\n\"\"\"\nconfig = json.loads(config)","metadata":{"execution":{"iopub.status.busy":"2022-08-23T12:05:16.53504Z","iopub.execute_input":"2022-08-23T12:05:16.535883Z","iopub.status.idle":"2022-08-23T12:05:16.554201Z","shell.execute_reply.started":"2022-08-23T12:05:16.535843Z","shell.execute_reply":"2022-08-23T12:05:16.553245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#--------------- Tokenizer ---------------------------------------------#\ndef get_tokenizer(config):\n    \"\"\"load the tokenizer\"\"\"\n\n    print(\"using auto tokenizer\")\n    tokenizer = AutoTokenizer.from_pretrained(config[\"base_model_path\"])\n\n    print(\"==\"*40)\n    print(f\"tokenizer len: {len(tokenizer)}\")\n    print(\n        f\"tokenizer test: {tokenizer.tokenize('Starts: [SOE] [LEAD] [CLAIM] [POSITION] [COUNTER_CLAIM]')}\")\n    print(\n        f\"tokenizer test: {tokenizer.tokenize('Starts: [EOE] [LEAD_END] [POSITION_END] [CLAIM_END]')}\")\n\n    print(\"==\"*40)\n    return tokenizer\n\n\n#--------------- Processing ---------------------------------------------#\n\n\nDISCOURSE_START_TOKENS = [\n    \"[LEAD]\",\n    \"[POSITION]\",\n    \"[CLAIM]\",\n    \"[COUNTER_CLAIM]\",\n    \"[REBUTTAL]\",\n    \"[EVIDENCE]\",\n    \"[CONCLUDING_STATEMENT]\"\n]\n\nTOKEN_MAP = {\n    \"topic\": [\"Topic [TOPIC]\", \"[TOPIC END]\"],\n    \"Lead\": [\"Lead [LEAD]\", \"[LEAD END]\"],\n    \"Position\": [\"Position [POSITION]\", \"[POSITION END]\"],\n    \"Claim\": [\"Claim [CLAIM]\", \"[CLAIM END]\"],\n    \"Counterclaim\": [\"Counterclaim [COUNTER_CLAIM]\", \"[COUNTER_CLAIM END]\"],\n    \"Rebuttal\": [\"Rebuttal [REBUTTAL]\", \"[REBUTTAL END]\"],\n    \"Evidence\": [\"Evidence [EVIDENCE]\", \"[EVIDENCE END]\"],\n    \"Concluding Statement\": [\"Concluding Statement [CONCLUDING_STATEMENT]\", \"[CONCLUDING_STATEMENT END]\"]\n}\n\n\nDISCOURSE_END_TOKENS = [\n    \"[LEAD END]\",\n    \"[POSITION END]\",\n    \"[CLAIM END]\",\n    \"[COUNTER_CLAIM END]\",\n    \"[REBUTTAL END]\",\n    \"[EVIDENCE END]\",\n    \"[CONCLUDING_STATEMENT END]\",\n]\n\n\n\ndef relaxed_search(text, substring, min_length=2, fraction=0.99999):\n    \"\"\"\n    Returns substring's span from the given text with the certain precision.\n    \"\"\"\n\n    position = text.find(substring)\n    substring_length = len(substring)\n    if position == -1:\n        half_length = int(substring_length * fraction)\n        half_substring = substring[:half_length]\n        half_substring_length = len(half_substring)\n        if half_substring_length < min_length:\n            return [-1, 0]\n        else:\n            return relaxed_search(text=text,\n                                  substring=half_substring,\n                                  min_length=min_length,\n                                  fraction=fraction)\n\n    span = [position, position+substring_length]\n    return span\n\n\ndef build_span_map(discourse_list, essay_text):\n    reading_head = 0\n    to_return = dict()\n\n    for cur_discourse in discourse_list:\n        if cur_discourse not in to_return:\n            to_return[cur_discourse] = []\n\n        matches = re.finditer(re.escape(r'{}'.format(cur_discourse)), essay_text)\n        for match in matches:\n            span_start, span_end = match.span()\n            if span_end <= reading_head:\n                continue\n            to_return[cur_discourse].append(match.span())\n            reading_head = span_end\n            break\n\n    # post process\n    for cur_discourse in discourse_list:\n        if not to_return[cur_discourse]:\n            print(\"resorting to relaxed search...\")\n            to_return[cur_discourse] = [relaxed_search(essay_text, cur_discourse)]\n    return to_return\n\n\ndef get_substring_span(texts, mapping):\n    result = []\n    for text in texts:\n        ans = mapping[text].pop(0)\n        result.append(ans)\n    return result\n\n\ndef process_essay(essay_id, essay_text, topic, prompt, anno_df):\n    \"\"\"insert newly added tokens in the essay text\n    \"\"\"\n    tmp_df = anno_df[anno_df[\"essay_id\"] == essay_id].copy()\n    tmp_df = tmp_df.sort_values(by=\"discourse_start\")\n    buffer = 0\n\n    for _, row in tmp_df.iterrows():\n        s, e, d_type = int(row.discourse_start) + buffer, int(row.discourse_end) + buffer, row.discourse_type\n        s_tok, e_tok = TOKEN_MAP[d_type]\n        essay_text = \" \".join([essay_text[:s], s_tok, essay_text[s:e], e_tok, essay_text[e:]])\n        buffer += len(s_tok) + len(e_tok) + 4\n\n    essay_text = \"[SOE]\" + \" [TOPIC] \" + prompt + \" [TOPIC END] \" +  essay_text + \"[EOE]\"\n    return essay_text\n\n\ndef process_input_df(anno_df, notes_df):\n    \"\"\"pre-process input dataframe\n\n    :param df: input dataframe\n    :type df: pd.DataFrame\n    :return: processed dataframe\n    :rtype: pd.DataFrame\n    \"\"\"\n    notes_df = deepcopy(notes_df)\n    anno_df = deepcopy(anno_df)\n\n    #------------------- Pre-Process Essay Text --------------------------#\n    anno_df[\"discourse_text\"] = anno_df[\"discourse_text\"].apply(lambda x: x.strip())  # pre-process\n    if \"discourse_effectiveness\" in anno_df.columns:\n        anno_df = anno_df[[\"discourse_id\", \"essay_id\", \"discourse_text\",\n                           \"discourse_type\", \"discourse_effectiveness\", \"uid\"]].copy()\n    else:\n        anno_df = anno_df[[\"discourse_id\", \"essay_id\", \"discourse_text\", \"discourse_type\", \"uid\"]].copy()\n\n    tmp_df = anno_df.groupby(\"essay_id\")[[\"discourse_id\", \"discourse_text\"]].agg(list).reset_index()\n    tmp_df = pd.merge(tmp_df, notes_df, on=\"essay_id\", how=\"left\")\n    tmp_df[\"span_map\"] = tmp_df[[\"discourse_text\", \"essay_text\"]].apply(\n        lambda x: build_span_map(x[0], x[1]), axis=1)\n    tmp_df[\"span\"] = tmp_df[[\"discourse_text\", \"span_map\"]].apply(\n        lambda x: get_substring_span(x[0], x[1]), axis=1)\n\n    all_discourse_ids = list(chain(*tmp_df[\"discourse_id\"].values))\n    all_discourse_spans = list(chain(*tmp_df[\"span\"].values))\n    span_df = pd.DataFrame()\n    span_df[\"discourse_id\"] = all_discourse_ids\n    span_df[\"span\"] = all_discourse_spans\n    span_df[\"discourse_start\"] = span_df[\"span\"].apply(lambda x: x[0])\n    span_df[\"discourse_end\"] = span_df[\"span\"].apply(lambda x: x[1])\n    span_df = span_df.drop(columns=\"span\")\n\n    anno_df = pd.merge(anno_df, span_df, on=\"discourse_id\", how=\"left\")\n    # anno_df = pd.merge(anno_df, notes_df, on=\"essay_id\", how=\"left\")\n\n    print(\"==\"*40)\n    print(\"processing essay text and inserting new tokens at span boundaries\")\n    notes_df[\"essay_text\"] = notes_df[[\"essay_id\", \"essay_text\", \"topic\", \"prompt\"]].apply(\n        lambda x: process_essay(x[0], x[1], x[2], x[3], anno_df), axis=1\n    )\n    print(\"==\"*40)\n\n    anno_df = anno_df.drop(columns=[\"discourse_start\", \"discourse_end\"])\n    notes_df = notes_df.drop_duplicates(subset=[\"essay_id\"])[[\"essay_id\", \"essay_text\"]].copy()\n\n    anno_df = pd.merge(anno_df, notes_df, on=\"essay_id\", how=\"left\")\n\n    if \"discourse_effectiveness\" in anno_df.columns:\n        grouped_df = anno_df.groupby(\"essay_id\")[[\"uid\", \"discourse_effectiveness\", \"discourse_type\"]].agg(list).reset_index()\n    else:\n        grouped_df = anno_df.groupby(\"essay_id\")[[\"uid\", \"discourse_type\"]].agg(list).reset_index()\n\n    grouped_df = pd.merge(grouped_df, notes_df, on=\"essay_id\", how=\"left\")\n    grouped_df = grouped_df.rename(columns={\"uid\": \"uids\"})\n\n    return grouped_df\n\n\n#--------------- Dataset ----------------------------------------------#\nNEW_TOKENS = [\n        \"[LEAD]\",\n        \"[POSITION]\",\n        \"[CLAIM]\",\n        \"[COUNTER_CLAIM]\",\n        \"[REBUTTAL]\",\n        \"[EVIDENCE]\",\n        \"[CONCLUDING_STATEMENT]\",\n        \"[TOPIC]\",  # 12808\n        \"[SOE]\",  # 12809\n        \"[EOE]\",  # 12810\n        \"[LEAD END]\",\n        \"[POSITION END]\",\n        \"[CLAIM END]\",\n        \"[COUNTER_CLAIM END]\",\n        \"[REBUTTAL END]\",\n        \"[EVIDENCE END]\",\n        \"[CONCLUDING_STATEMENT END]\",\n        \"[TOPIC END]\",  # 128018\n    ]\n    \n\nclass AuxFeedbackDataset:\n    \"\"\"Dataset class for feedback prize effectiveness task\n    \"\"\"\n\n    def __init__(self, config):\n        self.config = config\n\n        self.label2id = {\n            \"Ineffective\": 0,\n            \"Adequate\": 1,\n            \"Effective\": 2,\n        }\n\n        self.discourse_type2id = {\n            \"Lead\": 1,\n            \"Position\": 2,\n            \"Claim\": 3,\n            \"Counterclaim\": 4,\n            \"Rebuttal\": 5,\n            \"Evidence\": 6,\n            \"Concluding Statement\": 7,\n        }\n\n        self.id2label = {v: k for k, v in self.label2id.items()}\n        self.load_tokenizer()\n\n    def load_tokenizer(self):\n        \"\"\"load tokenizer as per config \n        \"\"\"\n        self.tokenizer = get_tokenizer(self.config)\n        print(\"==\"*40)\n        print(\"token maps...\")\n        print(TOKEN_MAP)\n        print(\"==\"*40)\n\n        if self.config[\"add_new_tokens\"]:\n            print(\"adding new tokens...\")\n            tokens_to_add = []\n            for this_tok in NEW_TOKENS:\n                 tokens_to_add.append(AddedToken(this_tok, lstrip=True, rstrip=False))\n            self.tokenizer.add_tokens(tokens_to_add)\n        print(f\"tokenizer len: {len(self.tokenizer)}\")\n\n        self.discourse_token_ids = set(self.tokenizer.convert_tokens_to_ids(DISCOURSE_START_TOKENS))\n        self.discourse_end_ids = set(self.tokenizer.convert_tokens_to_ids(DISCOURSE_END_TOKENS))\n        self.global_tokens = self.discourse_token_ids.union(self.discourse_end_ids)\n\n    def tokenize_function(self, examples):\n        tz = self.tokenizer(\n            examples[\"essay_text\"],\n            padding=False,\n            truncation=False,  # no truncation at first\n            add_special_tokens=True,\n            return_offsets_mapping=True,\n        )\n        return tz\n\n    def process_spans(self, examples):\n\n        span_head_char_start_idxs, span_tail_char_end_idxs = [], []\n        span_head_idxs, span_tail_idxs = [], []\n\n        for example_input_ids, example_offset_mapping, example_uids in zip(examples[\"input_ids\"], examples[\"offset_mapping\"], examples[\"uids\"]):\n            example_span_head_idxs = [pos for pos, this_id in enumerate(\n                example_input_ids) if this_id in self.discourse_token_ids]\n            example_span_tail_idxs = [pos for pos, this_id in enumerate(\n                example_input_ids) if this_id in self.discourse_end_ids]\n\n            example_span_head_char_start_idxs = [example_offset_mapping[pos][0] for pos in example_span_head_idxs]\n            example_span_tail_char_end_idxs = [example_offset_mapping[pos][1] for pos in example_span_tail_idxs]\n\n            span_head_char_start_idxs.append(example_span_head_char_start_idxs)\n            span_tail_char_end_idxs.append(example_span_tail_char_end_idxs)\n\n            span_head_idxs.append(example_span_head_idxs)\n            span_tail_idxs.append(example_span_tail_idxs)\n\n        return {\n            \"span_head_idxs\": span_head_idxs,\n            \"span_tail_idxs\": span_tail_idxs,\n            \"span_head_char_start_idxs\": span_head_char_start_idxs,\n            \"span_tail_char_end_idxs\": span_tail_char_end_idxs,\n        }\n\n    def generate_labels(self, examples):\n        labels = []\n        for example_labels, example_uids in zip(examples[\"discourse_effectiveness\"], examples[\"uids\"]):\n            labels.append([self.label2id[l] for l in example_labels])\n        return {\"labels\": labels}\n\n    def generate_discourse_type_ids(self, examples):\n        discourse_type_ids = []\n        for example_discourse_types in examples[\"discourse_type\"]:\n            discourse_type_ids.append([self.discourse_type2id[dt] for dt in example_discourse_types])\n        return {\"discourse_type_ids\": discourse_type_ids}\n\n    def compute_input_length(self, examples):\n        return {\"input_length\": [len(x) for x in examples[\"input_ids\"]]}\n\n    def sanity_check_head_tail(self, examples):\n        for head_idxs, tail_idxs in zip(examples[\"span_head_idxs\"], examples[\"span_tail_idxs\"]):\n            assert len(head_idxs) == len(tail_idxs)\n            for head, tail in zip(head_idxs, tail_idxs):\n                assert tail > head + 1\n\n    def sanity_check_head_labels(self, examples):\n        for head_idxs, head_labels in zip(examples[\"span_head_idxs\"], examples[\"labels\"]):\n            assert len(head_idxs) == len(head_labels)\n\n    def get_dataset(self, df, essay_df, mode='train'):\n        \"\"\"main api for creating the Feedback dataset\n\n        :param df: input annotation dataframe\n        :type df: pd.DataFrame\n        :param essay_df: dataframe with essay texts\n        :type essay_df: pd.DataFrame\n        :param mode: check if required for train or infer, defaults to 'train'\n        :type mode: str, optional\n        :return: the created dataset\n        :rtype: Dataset\n        \"\"\"\n        df = process_input_df(df, essay_df)\n\n        # save a sample for sanity checks\n        sample_df = df.sample(min(16, len(df)))\n        sample_df.to_csv(os.path.join(self.config[\"model_dir\"], f\"{mode}_df_processed.csv\"), index=False)\n\n        task_dataset = Dataset.from_pandas(df)\n        task_dataset = task_dataset.map(self.tokenize_function, batched=True)\n        task_dataset = task_dataset.map(self.compute_input_length, batched=True)\n        task_dataset = task_dataset.map(self.process_spans, batched=True)\n        print(task_dataset)\n        # todo check edge cases\n        task_dataset = task_dataset.filter(lambda example: len(example['span_head_idxs']) == len(\n            example['span_tail_idxs']))  # no need to run on empty set\n        print(task_dataset)\n        task_dataset = task_dataset.map(self.generate_discourse_type_ids, batched=True)\n        task_dataset = task_dataset.map(self.sanity_check_head_tail, batched=True)\n\n        if mode != \"infer\":\n            task_dataset = task_dataset.map(self.generate_labels, batched=True)\n            task_dataset = task_dataset.map(self.sanity_check_head_labels, batched=True)\n\n        try:\n            task_dataset = task_dataset.remove_columns(column_names=[\"__index_level_0__\"])\n        except Exception as e:\n            pass\n        return df, task_dataset\n\n#--------------- dataset with truncation ---------------------------------------------#\n\n\ndef get_fast_dataset(config, df, essay_df, mode=\"train\"):\n    \"\"\"Function to get fast approach dataset with truncation & sliding window\n    \"\"\"\n    dataset_creator = AuxFeedbackDataset(config)\n    _, task_dataset = dataset_creator.get_dataset(df, essay_df, mode=mode)\n\n    original_dataset = deepcopy(task_dataset)\n    tokenizer = dataset_creator.tokenizer\n    START_IDS = dataset_creator.discourse_token_ids\n    END_IDS = dataset_creator.discourse_end_ids\n    GLOBAL_IDS = dataset_creator.global_tokens\n\n    def tokenize_with_truncation(examples):\n        tz = tokenizer(\n            examples[\"essay_text\"],\n            padding=False,\n            truncation=True,\n            add_special_tokens=True,\n            return_offsets_mapping=True,\n            max_length=config[\"max_length\"],\n            stride=config[\"stride\"],\n            return_overflowing_tokens=True,\n            return_token_type_ids=True,\n        )\n        return tz\n\n    def process_span(examples):\n        span_head_idxs, span_tail_idxs = [], []\n        span_head_char_start_idxs, span_tail_char_end_idxs = [], []\n\n        buffer = 25  # do not include a head if it is within buffer distance away from last token\n\n        for example_input_ids, example_offset_mapping in zip(examples[\"input_ids\"], examples[\"offset_mapping\"]):\n            # ------------------- Span Heads -----------------------------------------#\n            if len(example_input_ids) < config[\"max_length\"]:  # no truncation\n                head_candidate = [pos for pos, this_id in enumerate(example_input_ids) if this_id in START_IDS]\n            else:\n                head_candidate = [pos for pos, this_id in enumerate(example_input_ids) if (\n                    (this_id in START_IDS) & (pos <= config[\"max_length\"]-buffer))]\n\n            n_heads = len(head_candidate)\n\n            # ------------------- Span Tails -----------------------------------------#\n            tail_candidate = [pos for pos, this_id in enumerate(example_input_ids) if this_id in END_IDS]\n\n            # ------------------- Edge Cases -----------------------------------------#\n            # 1. A tail occurs before the first head in the sequence due to truncation\n            if (len(tail_candidate) > 0) & (len(head_candidate) > 0):\n                if tail_candidate[0] < head_candidate[0]:  # truncation effect\n                    # print(f\"check: heads: {head_candidate}, tails {tail_candidate}\")\n                    tail_candidate = tail_candidate[1:]  # shift by one\n\n            # 2. Tail got chopped off due to truncation but the corresponding head is still there\n            if len(tail_candidate) < n_heads:\n                assert len(tail_candidate) + 1 == n_heads\n                assert len(example_input_ids) == config[\"max_length\"]  # should only happen if input text is truncated\n                tail_candidate.append(config[\"max_length\"]-2)  # the token before [SEP] token\n\n            # 3. Additional tails remain in the buffer region\n            if len(tail_candidate) > len(head_candidate):\n                tail_candidate = tail_candidate[:len(head_candidate)]\n\n            # ------------------- Create the fields ------------------------------------#\n            example_span_head_char_start_idxs = [example_offset_mapping[pos][0] for pos in head_candidate]\n            example_span_tail_char_end_idxs = [example_offset_mapping[pos][1] for pos in tail_candidate]\n\n            span_head_idxs.append(head_candidate)\n            span_tail_idxs.append(tail_candidate)\n            span_head_char_start_idxs.append(example_span_head_char_start_idxs)\n            span_tail_char_end_idxs.append(example_span_tail_char_end_idxs)\n\n        return {\n            \"span_head_idxs\": span_head_idxs,\n            \"span_tail_idxs\": span_tail_idxs,\n            \"span_head_char_start_idxs\": span_head_char_start_idxs,\n            \"span_tail_char_end_idxs\": span_tail_char_end_idxs,\n        }\n\n    def enforce_alignment(examples):\n        uids = []\n\n        for example_span_head_char_start_idxs, example_overflow_to_sample_mapping in zip(\n                examples[\"span_head_char_start_idxs\"], examples[\"overflow_to_sample_mapping\"]):\n\n            original_example = original_dataset[example_overflow_to_sample_mapping]\n            original_example_span_head_char_start_idxs = original_example[\"span_head_char_start_idxs\"]\n            original_example_uids = original_example[\"uids\"]\n            char2uid = {k: v for k, v in zip(original_example_span_head_char_start_idxs, original_example_uids)}\n            current_example_uids = [char2uid[char_idx] for char_idx in example_span_head_char_start_idxs]\n            uids.append(current_example_uids)\n        return {\"uids\": uids}\n\n    def recompute_labels(examples):\n        labels = []\n        for example_span_head_char_start_idxs, example_overflow_to_sample_mapping in zip(\n                examples[\"span_head_char_start_idxs\"], examples[\"overflow_to_sample_mapping\"]):\n\n            original_example = original_dataset[example_overflow_to_sample_mapping]\n            original_example_span_head_char_start_idxs = original_example[\"span_head_char_start_idxs\"]\n            original_example_labels = original_example[\"labels\"]\n            char2label = {k: v for k, v in zip(original_example_span_head_char_start_idxs, original_example_labels)}\n            current_example_labels = [char2label[char_idx] for char_idx in example_span_head_char_start_idxs]\n            labels.append(current_example_labels)\n        return {\"labels\": labels}\n\n    def recompute_discourse_type_ids(examples):\n        discourse_type_ids = []\n        for example_span_head_char_start_idxs, example_overflow_to_sample_mapping in zip(\n                examples[\"span_head_char_start_idxs\"], examples[\"overflow_to_sample_mapping\"]):\n\n            original_example = original_dataset[example_overflow_to_sample_mapping]\n            original_example_span_head_char_start_idxs = original_example[\"span_head_char_start_idxs\"]\n            original_example_discourse_type_ids = original_example[\"discourse_type_ids\"]\n            char2discourse_id = {k: v for k, v in zip(\n                original_example_span_head_char_start_idxs, original_example_discourse_type_ids)}\n            current_example_discourse_type_ids = [char2discourse_id[char_idx]\n                                                  for char_idx in example_span_head_char_start_idxs]\n            discourse_type_ids.append(current_example_discourse_type_ids)\n        return {\"discourse_type_ids\": discourse_type_ids}\n\n    def compute_input_length(examples):\n        return {\"input_length\": [len(x) for x in examples[\"input_ids\"]]}\n\n    def sanity_check_head_tail(examples):\n        for head_idxs, tail_idxs in zip(examples[\"span_head_idxs\"], examples[\"span_tail_idxs\"]):\n            assert len(head_idxs) == len(tail_idxs)\n            for head, tail in zip(head_idxs, tail_idxs):\n                assert tail > head + 1, f\"head idxs: {head_idxs}, tail idxs {tail_idxs}\"\n\n    task_dataset = task_dataset.map(\n        tokenize_with_truncation,\n        batched=True,\n        remove_columns=task_dataset.column_names,\n        batch_size=len(task_dataset)\n    )\n\n    task_dataset = task_dataset.map(process_span, batched=True)\n    task_dataset = task_dataset.map(enforce_alignment, batched=True)\n    task_dataset = task_dataset.map(recompute_discourse_type_ids, batched=True)\n    task_dataset = task_dataset.map(sanity_check_head_tail, batched=True)\n\n    # no need to run on empty set\n    task_dataset = task_dataset.filter(lambda example: len(example['span_head_idxs']) != 0)\n    task_dataset = task_dataset.map(compute_input_length, batched=True)\n\n    if mode != \"infer\":\n        task_dataset = task_dataset.map(recompute_labels, batched=True)\n\n    to_return = dict()\n    to_return[\"dataset\"] = task_dataset\n    to_return[\"original_dataset\"] = original_dataset\n    to_return[\"tokenizer\"] = tokenizer\n    return to_return\n\nif use_exp214:\n    os.makedirs(config[\"model_dir\"], exist_ok=True)\n\n    print(\"creating the inference datasets...\")\n    infer_ds_dict = get_fast_dataset(config, test_df, essay_df, mode=\"infer\")\n    tokenizer = infer_ds_dict[\"tokenizer\"]\n    infer_dataset = infer_ds_dict[\"dataset\"]\n    print(infer_dataset)\n\n    config[\"len_tokenizer\"] = len(tokenizer)\n\n    infer_dataset = infer_dataset.sort(\"input_length\")\n\n    infer_dataset.set_format(\n        type=None,\n        columns=['input_ids', 'attention_mask', 'token_type_ids', 'span_head_idxs',\n                 'span_tail_idxs', 'discourse_type_ids', 'uids']\n    )\n\n# %% [markdown]\n# ## Data Loader\n\nfrom copy import deepcopy\nfrom dataclasses import dataclass\n\nimport torch\nfrom transformers import DataCollatorWithPadding\n\n\n@dataclass\nclass CustomDataCollatorWithPadding(DataCollatorWithPadding):\n    \"\"\"\n    data collector for seq classification\n    \"\"\"\n\n    tokenizer = None\n    padding = True\n    max_length = None\n    pad_to_multiple_of = 512\n    return_tensors = \"pt\"\n\n    def __call__(self, features):\n        uids = [feature[\"uids\"] for feature in features]\n        discourse_type_ids = [feature[\"discourse_type_ids\"] for feature in features]\n        span_head_idxs = [feature[\"span_head_idxs\"] for feature in features]\n        span_tail_idxs = [feature[\"span_tail_idxs\"] for feature in features]\n        span_attention_mask = [[1]*len(feature[\"span_head_idxs\"]) for feature in features]\n\n        labels = None\n        if \"labels\" in features[0].keys():\n            labels = [feature[\"labels\"] for feature in features]\n\n        batch = self.tokenizer.pad(\n            features,\n            padding=self.padding,\n            max_length=self.max_length,\n            pad_to_multiple_of=self.pad_to_multiple_of,\n            return_tensors=None,\n        )\n\n        b_max = max([len(l) for l in span_head_idxs])\n        max_len = len(batch[\"input_ids\"][0])\n\n        default_head_idx = max(max_len - 10, 1)  # for padding\n        default_tail_idx = max(max_len - 4, 1)  # for padding\n\n        batch[\"span_head_idxs\"] = [\n            ex_span_head_idxs + [default_head_idx] * (b_max - len(ex_span_head_idxs)) for ex_span_head_idxs in span_head_idxs\n        ]\n\n        batch[\"uids\"] = [ex_uids + [-1] * (b_max - len(ex_uids)) for ex_uids in uids]\n        batch[\"discourse_type_ids\"] = [ex_discourse_type_ids + [0] *\n                                       (b_max - len(ex_discourse_type_ids)) for ex_discourse_type_ids in discourse_type_ids]\n\n        batch[\"span_tail_idxs\"] = [\n            ex_span_tail_idxs + [default_tail_idx] * (b_max - len(ex_span_tail_idxs)) for ex_span_tail_idxs in span_tail_idxs\n        ]\n\n        batch[\"span_attention_mask\"] = [\n            ex_discourse_masks + [0] * (b_max - len(ex_discourse_masks)) for ex_discourse_masks in span_attention_mask\n        ]\n\n        if labels is not None:\n            batch[\"labels\"] = [ex_labels + [-1] * (b_max - len(ex_labels)) for ex_labels in labels]\n\n        # multitask labels\n        def _get_additional_labels(label_id):\n            if label_id == 0:\n                vec = [0, 0]\n            elif label_id == 1:\n                vec = [1, 0]\n            elif label_id == 2:\n                vec = [1, 1]\n            elif label_id == -1:\n                vec = [-1, -1]\n            else:\n                raise\n            return vec\n\n        if labels is not None:\n            additional_labels = []\n            for ex_labels in batch[\"labels\"]:\n                ex_additional_labels = [_get_additional_labels(el) for el in ex_labels]\n                additional_labels.append(ex_additional_labels)\n            batch[\"multitask_labels\"] = additional_labels\n        # pdb.set_trace()\n\n        batch = {k: (torch.tensor(v, dtype=torch.int64) if k != \"multitask_labels\" else torch.tensor(\n            v, dtype=torch.float32)) for k, v in batch.items()}\n        return batch\n\n\nif use_exp214:\n    data_collector = CustomDataCollatorWithPadding(tokenizer=tokenizer)\n\n    infer_dl = DataLoader(\n        infer_dataset,\n        batch_size=config[\"infer_bs\"],\n        shuffle=False,\n        collate_fn=data_collector\n    )\n\n# %% [markdown]\n# ## Model\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nfrom torch.nn import LayerNorm\nfrom transformers import AutoConfig, AutoModel, BertConfig\nfrom transformers.models.bert.modeling_bert import BertAttention\n\nfrom transformers.models.deberta_v2.modeling_deberta_v2 import ContextPooler, StableDropout, DebertaV2Attention\n\n\n#-------- Model ------------------------------------------------------------------#\nclass FeedbackModel(nn.Module):\n    \"\"\"The feedback prize effectiveness baseline model\n    \"\"\"\n\n    def __init__(self, config):\n        super(FeedbackModel, self).__init__()\n        self.config = config\n\n        # base transformer\n        base_config = AutoConfig.from_pretrained(self.config[\"base_model_path\"])\n        base_config.update({\"add_pooling_layer\": False, \"max_position_embeddings\": 1024})\n        self.base_model = AutoModel.from_pretrained(self.config[\"base_model_path\"], config=base_config)\n\n         # resize model embeddings\n        if config[\"add_new_tokens\"]:\n            print(\"resizing model embeddings...\")\n            print(f\"tokenizer length = {config['len_tokenizer']}\")\n            self.base_model.resize_token_embeddings(config[\"len_tokenizer\"])\n        \n        # dropouts\n        self.dropout = StableDropout(self.config[\"dropout\"])\n        \n        # multi-head attention\n        attention_config = deepcopy(self.base_model.config)\n        attention_config.update({\"relative_attention\": False})\n        self.fpe_span_attention = DebertaV2Attention(attention_config)\n        \n        # classification\n        hidden_size = self.base_model.config.hidden_size\n        feature_size = hidden_size\n        self.layer_norm = LayerNorm(feature_size, self.base_model.config.layer_norm_eps)\n        \n        # # LSTM Head\n        self.fpe_lstm_layer = nn.LSTM(\n            input_size=feature_size,\n            hidden_size=hidden_size//2,\n            num_layers=1,\n            batch_first=True,\n            bidirectional=True,\n        )\n\n        self.num_labels = self.config[\"num_labels\"]\n        self.classifier = nn.Linear(feature_size, self.config[\"num_labels\"])\n\n    def forward(self, input_ids, attention_mask, span_head_idxs, span_tail_idxs, span_attention_mask, **kwargs):\n        \n        bs = input_ids.shape[0]  # batch size\n        outputs = self.base_model(input_ids, attention_mask=attention_mask)\n        encoder_layer = outputs[0]\n        \n        self.fpe_lstm_layer.flatten_parameters()\n        encoder_layer = self.fpe_lstm_layer(encoder_layer)[0]\n\n        mean_feature_vector = []\n        for i in range(bs):  # TODO: vectorize\n            span_vec_i = []\n\n            for head, tail in zip(span_head_idxs[i], span_tail_idxs[i]):\n                # span feature\n                tmp = torch.mean(encoder_layer[i, head+1:tail], dim=0)  # [h]\n                span_vec_i.append(tmp)\n            span_vec_i = torch.stack(span_vec_i)  # (num_disourse, h)\n            mean_feature_vector.append(span_vec_i)\n\n        mean_feature_vector = torch.stack(mean_feature_vector)  # (bs, num_disourse, h)\n        mean_feature_vector = self.layer_norm(mean_feature_vector)\n\n        # attend to other features\n        extended_span_attention_mask = span_attention_mask.unsqueeze(1).unsqueeze(2)\n        span_attention_mask = extended_span_attention_mask * extended_span_attention_mask.squeeze(-2).unsqueeze(-1)\n        span_attention_mask = span_attention_mask.byte()\n        feature_vector = self.fpe_span_attention(mean_feature_vector, span_attention_mask)\n\n        feature_vector = self.dropout(feature_vector)\n\n        logits = self.classifier(feature_vector)\n\n        return logits\n\n# %% [markdown]\n# ## Inference\n\ncheckpoints = [\n    \"../input/exp214-debv2-xl-prompt/fpe_model_fold_0_best.pth.tar\",\n    \"../input/exp214-debv2-xl-prompt/fpe_model_fold_1_best.pth.tar\",\n    \"../input/exp214-debv2-xl-prompt/fpe_model_fold_2_best.pth.tar\",\n    \"../input/exp214-debv2-xl-prompt/fpe_model_fold_3_best.pth.tar\",\n]\n\ndef inference_fn(model, infer_dl, model_id):\n    all_preds = []\n    all_uids = []\n    accelerator = Accelerator()\n    model, infer_dl = accelerator.prepare(model, infer_dl)\n    \n    model.eval()\n    tk0 = tqdm(infer_dl, total=len(infer_dl))\n    \n    for batch in tk0:\n        with torch.no_grad():\n            logits = model(**batch) # (b, nd, 3)\n            batch_preds = F.softmax(logits, dim=-1)\n            batch_uids = batch[\"uids\"]\n        all_preds.append(batch_preds)\n        all_uids.append(batch_uids)\n    \n    all_preds = [p.to('cpu').detach().numpy().tolist() for p in all_preds]\n    all_preds = list(chain(*all_preds))\n    flat_preds = list(chain(*all_preds))\n    \n    all_uids = [p.to('cpu').detach().numpy().tolist() for p in all_uids]\n    all_uids = list(chain(*all_uids))\n    flat_uids = list(chain(*all_uids))    \n    \n    preds_df = pd.DataFrame(flat_preds)\n    preds_df.columns = [\"Ineffective\", \"Adequate\", \"Effective\"]\n    preds_df[\"span_uid\"] = flat_uids # SORTED_DISCOURSE_IDS\n    preds_df = preds_df[preds_df[\"span_uid\"]>=0].copy()\n    preds_df[\"discourse_id\"] = preds_df[\"span_uid\"].map(idx2discourse)\n    preds_df = preds_df[[\"discourse_id\", \"Ineffective\", \"Adequate\", \"Effective\"]].copy()\n    preds_df.to_csv(f\"exp214_model_preds_{model_id}.csv\", index=False)\n    \nif use_exp214:\n    for model_id, checkpoint in enumerate(checkpoints):\n        print(f\"infering from {checkpoint}\")\n        model = FeedbackModel(config)\n        ckpt = torch.load(checkpoint)\n        print(f\"validation score for fold {model_id} = {ckpt['loss']}\")\n        model.load_state_dict(ckpt['state_dict'])\n        inference_fn(model, infer_dl, model_id)\n\n\n    del model\n    del tokenizer, infer_dataset, infer_ds_dict, data_collector, infer_dl\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    import glob\n    import pandas as pd\n\n    csvs = glob.glob(\"exp214_model_preds_*.csv\")\n\n    idx = []\n    preds = []\n\n\n    for csv_idx, csv in enumerate(csvs):\n\n        print(\"==\"*40)\n        print(f\"preds in {csv}\")\n        df = pd.read_csv(csv)\n        df = df.sort_values(by=[\"discourse_id\"])\n        print(df.head(10))\n        print(\"==\"*40)\n\n        temp_preds = df.drop([\"discourse_id\"], axis=1).values\n        if csv_idx == 0:\n            idx = list(df[\"discourse_id\"])\n            preds = temp_preds\n        else:\n            preds += temp_preds\n\n    preds = preds / len(csvs)\n\n    exp214_df = pd.DataFrame()\n    exp214_df[\"discourse_id\"] = idx\n    exp214_df[\"Ineffective\"]  = preds[:, 0]\n    exp214_df[\"Adequate\"]     = preds[:, 1]\n    exp214_df[\"Effective\"]    = preds[:, 2]\n\n    exp214_df = exp214_df.groupby(\"discourse_id\")[[\"Ineffective\", \"Adequate\", \"Effective\"]].agg(np.mean).reset_index()\n\n","metadata":{"execution":{"iopub.status.busy":"2022-08-23T12:05:16.558251Z","iopub.execute_input":"2022-08-23T12:05:16.558595Z","iopub.status.idle":"2022-08-23T12:05:16.670428Z","shell.execute_reply.started":"2022-08-23T12:05:16.558567Z","shell.execute_reply":"2022-08-23T12:05:16.66925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if use_exp214:\n    print(exp214_df.head())","metadata":{"execution":{"iopub.status.busy":"2022-08-23T12:05:16.672272Z","iopub.execute_input":"2022-08-23T12:05:16.672649Z","iopub.status.idle":"2022-08-23T12:05:16.676977Z","shell.execute_reply.started":"2022-08-23T12:05:16.672603Z","shell.execute_reply":"2022-08-23T12:05:16.676003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EXP 19 Model - DEXL","metadata":{"_uuid":"036b765e-fbc3-451f-b750-2ca9a01608b8","_cell_guid":"2a982c70-8d53-46cd-a248-4f657231488d","trusted":true}},{"cell_type":"code","source":"config = \"\"\"{\n    \"debug\": false,\n\n    \"base_model_path\": \"../input/tapt-fpe-dexl\",\n    \"model_dir\": \"./outputs\",\n\n    \"max_length\": 1024,\n    \"stride\": 256,\n    \"num_labels\": 5,\n    \"dropout\": 0.1,\n    \"infer_bs\": 8\n}\n\"\"\"\nconfig = json.loads(config)","metadata":{"_uuid":"03dc8553-a4b6-4eb6-96a1-31d7d1bc0109","_cell_guid":"2c1aae33-42fc-4470-845f-87f9940b8d99","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:05:16.680906Z","iopub.execute_input":"2022-08-23T12:05:16.681502Z","iopub.status.idle":"2022-08-23T12:05:16.686484Z","shell.execute_reply.started":"2022-08-23T12:05:16.681467Z","shell.execute_reply":"2022-08-23T12:05:16.685673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport re\nfrom copy import deepcopy\nfrom itertools import chain\n\nimport pandas as pd\nfrom datasets import Dataset\nfrom tokenizers import AddedToken\nfrom transformers import AutoTokenizer\n\n\n#--------------- Tokenizer ---------------------------------------------#\ndef get_tokenizer(config):\n    \"\"\"load the tokenizer\"\"\"\n\n    print(\"using auto tokenizer\")\n    tokenizer = AutoTokenizer.from_pretrained(config[\"base_model_path\"])\n\n    print(\"==\"*40)\n    print(f\"tokenizer len: {len(tokenizer)}\")\n    print(\n        f\"tokenizer test: {tokenizer.tokenize('Starts: [SOE] [LEAD] [CLAIM] [POSITION] [COUNTER_CLAIM]')}\")\n    print(\n        f\"tokenizer test: {tokenizer.tokenize('Starts: [EOE] [LEAD_END] [POSITION_END] [CLAIM_END]')}\")\n\n    print(\"==\"*40)\n    return tokenizer\n\n\n#--------------- Processing ---------------------------------------------#\n\nTOKEN_MAP = {\n    \"Lead\": [\"Lead [LEAD]\", \"[LEAD_END]\"],\n    \"Position\": [\"Position [POSITION]\", \"[POSITION_END]\"],\n    \"Claim\": [\"Claim [CLAIM]\", \"[CLAIM_END]\"],\n    \"Counterclaim\": [\"Counterclaim [COUNTER_CLAIM]\", \"[COUNTER_CLAIM_END]\"],\n    \"Rebuttal\": [\"Rebuttal [REBUTTAL]\", \"[REBUTTAL_END]\"],\n    \"Evidence\": [\"Evidence [EVIDENCE]\", \"[EVIDENCE_END]\"],\n    \"Concluding Statement\": [\"Concluding Statement [CONCLUDING_STATEMENT]\", \"[CONCLUDING_STATEMENT_END]\"]\n}\n\nDISCOURSE_START_TOKENS = [\n    \"[LEAD]\",\n    \"[POSITION]\",\n    \"[CLAIM]\",\n    \"[COUNTER_CLAIM]\",\n    \"[REBUTTAL]\",\n    \"[EVIDENCE]\",\n    \"[CONCLUDING_STATEMENT]\"\n]\n\nDISCOURSE_END_TOKENS = [\n    \"[LEAD_END]\",\n    \"[POSITION_END]\",\n    \"[CLAIM_END]\",\n    \"[COUNTER_CLAIM_END]\",\n    \"[REBUTTAL_END]\",\n    \"[EVIDENCE_END]\",\n    \"[CONCLUDING_STATEMENT_END]\"\n]\n\n# NEW_TOKENS = [\n#     \"[LEAD]\",\n#     \"[POSITION]\",\n#     \"[CLAIM]\",\n#     \"[COUNTER_CLAIM]\",\n#     \"[REBUTTAL]\",\n#     \"[EVIDENCE]\",\n#     \"[CONCLUDING_STATEMENT]\",\n#     \"[LEAD_END]\",\n#     \"[POSITION_END]\",\n#     \"[CLAIM_END]\",\n#     \"[COUNTER_CLAIM_END]\",\n#     \"[REBUTTAL_END]\",\n#     \"[EVIDENCE_END]\",\n#     \"[CONCLUDING_STATEMENT_END]\",\n#     \"[SOE]\",\n#     \"[EOE]\",\n# ]\n\n\ndef relaxed_search(text, substring, min_length=2, fraction=0.99999):\n    \"\"\"\n    Returns substring's span from the given text with the certain precision.\n    \"\"\"\n\n    position = text.find(substring)\n    substring_length = len(substring)\n    if position == -1:\n        half_length = int(substring_length * fraction)\n        half_substring = substring[:half_length]\n        half_substring_length = len(half_substring)\n        if half_substring_length < min_length:\n            return [-1, 0]\n        else:\n            return relaxed_search(text=text,\n                                  substring=half_substring,\n                                  min_length=min_length,\n                                  fraction=fraction)\n\n    span = [position, position+substring_length]\n    return span\n\n\ndef build_span_map(discourse_list, essay_text):\n    reading_head = 0\n    to_return = dict()\n\n    for cur_discourse in discourse_list:\n        if cur_discourse not in to_return:\n            to_return[cur_discourse] = []\n\n        matches = re.finditer(re.escape(r'{}'.format(cur_discourse)), essay_text)\n        for match in matches:\n            span_start, span_end = match.span()\n            if span_end <= reading_head:\n                continue\n            to_return[cur_discourse].append(match.span())\n            reading_head = span_end\n            break\n\n    # post process\n    for cur_discourse in discourse_list:\n        if not to_return[cur_discourse]:\n            print(\"resorting to relaxed search...\")\n            to_return[cur_discourse] = [relaxed_search(essay_text, cur_discourse)]\n    return to_return\n\n\ndef get_substring_span(texts, mapping):\n    result = []\n    for text in texts:\n        ans = mapping[text].pop(0)\n        result.append(ans)\n    return result\n\n\ndef process_essay(essay_id, essay_text, anno_df):\n    \"\"\"insert newly added tokens in the essay text\n    \"\"\"\n    tmp_df = anno_df[anno_df[\"essay_id\"] == essay_id].copy()\n    tmp_df = tmp_df.sort_values(by=\"discourse_start\")\n    buffer = 0\n\n    for _, row in tmp_df.iterrows():\n        s, e, d_type = int(row.discourse_start) + buffer, int(row.discourse_end) + buffer, row.discourse_type\n        s_tok, e_tok = TOKEN_MAP[d_type]\n        essay_text = \" \".join([essay_text[:s], s_tok, essay_text[s:e], e_tok, essay_text[e:]])\n        buffer += len(s_tok) + len(e_tok) + 4\n\n    essay_text = \"[SOE]\" + essay_text + \"[EOE]\"\n    return essay_text\n\n\ndef process_input_df(anno_df, notes_df):\n    \"\"\"pre-process input dataframe\n\n    :param df: input dataframe\n    :type df: pd.DataFrame\n    :return: processed dataframe\n    :rtype: pd.DataFrame\n    \"\"\"\n    notes_df = deepcopy(notes_df)\n    anno_df = deepcopy(anno_df)\n\n    #------------------- Pre-Process Essay Text --------------------------#\n    anno_df[\"discourse_text\"] = anno_df[\"discourse_text\"].apply(lambda x: x.strip())  # pre-process\n    if \"discourse_effectiveness\" in anno_df.columns:\n        anno_df = anno_df[[\"discourse_id\", \"essay_id\", \"discourse_text\",\n                           \"discourse_type\", \"discourse_effectiveness\", \"uid\"]].copy()\n    else:\n        anno_df = anno_df[[\"discourse_id\", \"essay_id\", \"discourse_text\", \"discourse_type\", \"uid\"]].copy()\n\n    tmp_df = anno_df.groupby(\"essay_id\")[[\"discourse_id\", \"discourse_text\"]].agg(list).reset_index()\n    tmp_df = pd.merge(tmp_df, notes_df, on=\"essay_id\", how=\"left\")\n    tmp_df[\"span_map\"] = tmp_df[[\"discourse_text\", \"essay_text\"]].apply(\n        lambda x: build_span_map(x[0], x[1]), axis=1)\n    tmp_df[\"span\"] = tmp_df[[\"discourse_text\", \"span_map\"]].apply(\n        lambda x: get_substring_span(x[0], x[1]), axis=1)\n\n    all_discourse_ids = list(chain(*tmp_df[\"discourse_id\"].values))\n    all_discourse_spans = list(chain(*tmp_df[\"span\"].values))\n    span_df = pd.DataFrame()\n    span_df[\"discourse_id\"] = all_discourse_ids\n    span_df[\"span\"] = all_discourse_spans\n    span_df[\"discourse_start\"] = span_df[\"span\"].apply(lambda x: x[0])\n    span_df[\"discourse_end\"] = span_df[\"span\"].apply(lambda x: x[1])\n    span_df = span_df.drop(columns=\"span\")\n\n    anno_df = pd.merge(anno_df, span_df, on=\"discourse_id\", how=\"left\")\n    # anno_df = pd.merge(anno_df, notes_df, on=\"essay_id\", how=\"left\")\n\n    print(\"==\"*40)\n    print(\"processing essay text and inserting new tokens at span boundaries\")\n    notes_df[\"essay_text\"] = notes_df[[\"essay_id\", \"essay_text\"]].apply(\n        lambda x: process_essay(x[0], x[1], anno_df), axis=1\n    )\n    print(\"==\"*40)\n\n    anno_df = anno_df.drop(columns=[\"discourse_start\", \"discourse_end\"])\n    notes_df = notes_df.drop_duplicates(subset=[\"essay_id\"])[[\"essay_id\", \"essay_text\"]].copy()\n\n    anno_df = pd.merge(anno_df, notes_df, on=\"essay_id\", how=\"left\")\n\n    if \"discourse_effectiveness\" in anno_df.columns:\n        grouped_df = anno_df.groupby(\"essay_id\")[[\"uid\", \"discourse_effectiveness\", \"discourse_type\"]].agg(list).reset_index()\n    else:\n        grouped_df = anno_df.groupby(\"essay_id\")[[\"uid\", \"discourse_type\"]].agg(list).reset_index()\n\n    grouped_df = pd.merge(grouped_df, notes_df, on=\"essay_id\", how=\"left\")\n    grouped_df = grouped_df.rename(columns={\"uid\": \"uids\"})\n\n    return grouped_df\n\n\n#--------------- Dataset ----------------------------------------------#\n\n\nclass AuxFeedbackDataset:\n    \"\"\"Dataset class for feedback prize effectiveness task\n    \"\"\"\n\n    def __init__(self, config):\n        self.config = config\n\n        self.label2id = {\n            \"Ineffective\": 0,\n            \"Adequate\": 1,\n            \"Effective\": 2,\n        }\n\n        self.discourse_type2id = {\n            \"Lead\": 1,\n            \"Position\": 2,\n            \"Claim\": 3,\n            \"Counterclaim\": 4,\n            \"Rebuttal\": 5,\n            \"Evidence\": 6,\n            \"Concluding Statement\": 7,\n        }\n\n        self.id2label = {v: k for k, v in self.label2id.items()}\n        self.load_tokenizer()\n\n    def load_tokenizer(self):\n        \"\"\"load tokenizer as per config \n        \"\"\"\n        self.tokenizer = get_tokenizer(self.config)\n        print(\"==\"*40)\n        print(\"token maps...\")\n        print(TOKEN_MAP)\n        print(\"==\"*40)\n\n        # print(\"adding new tokens...\")\n        # tokens_to_add = []\n        # for this_tok in NEW_TOKENS:\n        #     tokens_to_add.append(AddedToken(this_tok, lstrip=True, rstrip=False))\n        # self.tokenizer.add_tokens(tokens_to_add)\n        print(f\"tokenizer len: {len(self.tokenizer)}\")\n\n        self.discourse_token_ids = set(self.tokenizer.convert_tokens_to_ids(DISCOURSE_START_TOKENS))\n        self.discourse_end_ids = set(self.tokenizer.convert_tokens_to_ids(DISCOURSE_END_TOKENS))\n        self.global_tokens = self.discourse_token_ids.union(self.discourse_end_ids)\n\n    def tokenize_function(self, examples):\n        tz = self.tokenizer(\n            examples[\"essay_text\"],\n            padding=False,\n            truncation=False,  # no truncation at first\n            add_special_tokens=True,\n            return_offsets_mapping=True,\n        )\n        return tz\n\n    def process_spans(self, examples):\n\n        span_head_char_start_idxs, span_tail_char_end_idxs = [], []\n        span_head_idxs, span_tail_idxs = [], []\n\n        for example_input_ids, example_offset_mapping, example_uids in zip(examples[\"input_ids\"], examples[\"offset_mapping\"], examples[\"uids\"]):\n            example_span_head_idxs = [pos for pos, this_id in enumerate(\n                example_input_ids) if this_id in self.discourse_token_ids]\n            example_span_tail_idxs = [pos for pos, this_id in enumerate(\n                example_input_ids) if this_id in self.discourse_end_ids]\n\n            example_span_head_char_start_idxs = [example_offset_mapping[pos][0] for pos in example_span_head_idxs]\n            example_span_tail_char_end_idxs = [example_offset_mapping[pos][1] for pos in example_span_tail_idxs]\n\n            span_head_char_start_idxs.append(example_span_head_char_start_idxs)\n            span_tail_char_end_idxs.append(example_span_tail_char_end_idxs)\n\n            span_head_idxs.append(example_span_head_idxs)\n            span_tail_idxs.append(example_span_tail_idxs)\n\n        return {\n            \"span_head_idxs\": span_head_idxs,\n            \"span_tail_idxs\": span_tail_idxs,\n            \"span_head_char_start_idxs\": span_head_char_start_idxs,\n            \"span_tail_char_end_idxs\": span_tail_char_end_idxs,\n        }\n\n    def generate_labels(self, examples):\n        labels = []\n        for example_labels, example_uids in zip(examples[\"discourse_effectiveness\"], examples[\"uids\"]):\n            labels.append([self.label2id[l] for l in example_labels])\n        return {\"labels\": labels}\n\n    def generate_discourse_type_ids(self, examples):\n        discourse_type_ids = []\n        for example_discourse_types in examples[\"discourse_type\"]:\n            discourse_type_ids.append([self.discourse_type2id[dt] for dt in example_discourse_types])\n        return {\"discourse_type_ids\": discourse_type_ids}\n\n    def compute_input_length(self, examples):\n        return {\"input_length\": [len(x) for x in examples[\"input_ids\"]]}\n\n    def sanity_check_head_tail(self, examples):\n        for head_idxs, tail_idxs in zip(examples[\"span_head_idxs\"], examples[\"span_tail_idxs\"]):\n            assert len(head_idxs) == len(tail_idxs)\n            for head, tail in zip(head_idxs, tail_idxs):\n                assert tail > head + 1\n\n    def sanity_check_head_labels(self, examples):\n        for head_idxs, head_labels in zip(examples[\"span_head_idxs\"], examples[\"labels\"]):\n            assert len(head_idxs) == len(head_labels)\n\n    def get_dataset(self, df, essay_df, mode='train'):\n        \"\"\"main api for creating the Feedback dataset\n\n        :param df: input annotation dataframe\n        :type df: pd.DataFrame\n        :param essay_df: dataframe with essay texts\n        :type essay_df: pd.DataFrame\n        :param mode: check if required for train or infer, defaults to 'train'\n        :type mode: str, optional\n        :return: the created dataset\n        :rtype: Dataset\n        \"\"\"\n        df = process_input_df(df, essay_df)\n\n        # save a sample for sanity checks\n        sample_df = df.sample(min(16, len(df)))\n        sample_df.to_csv(os.path.join(self.config[\"model_dir\"], f\"{mode}_df_processed.csv\"), index=False)\n\n        task_dataset = Dataset.from_pandas(df)\n        task_dataset = task_dataset.map(self.tokenize_function, batched=True)\n        task_dataset = task_dataset.map(self.compute_input_length, batched=True)\n        task_dataset = task_dataset.map(self.process_spans, batched=True)\n        print(task_dataset)\n        # todo check edge cases\n        task_dataset = task_dataset.filter(lambda example: len(example['span_head_idxs']) == len(\n            example['span_tail_idxs']))  # no need to run on empty set\n        print(task_dataset)\n        task_dataset = task_dataset.map(self.generate_discourse_type_ids, batched=True)\n        task_dataset = task_dataset.map(self.sanity_check_head_tail, batched=True)\n\n        if mode != \"infer\":\n            task_dataset = task_dataset.map(self.generate_labels, batched=True)\n            task_dataset = task_dataset.map(self.sanity_check_head_labels, batched=True)\n\n        try:\n            task_dataset = task_dataset.remove_columns(column_names=[\"__index_level_0__\"])\n        except Exception as e:\n            pass\n        return df, task_dataset\n\n#--------------- dataset with truncation ---------------------------------------------#\n\n\ndef get_fast_dataset(config, df, essay_df, mode=\"train\"):\n    \"\"\"Function to get fast approach dataset with truncation & sliding window\n    \"\"\"\n    dataset_creator = AuxFeedbackDataset(config)\n    _, task_dataset = dataset_creator.get_dataset(df, essay_df, mode=mode)\n\n    original_dataset = deepcopy(task_dataset)\n    tokenizer = dataset_creator.tokenizer\n    START_IDS = dataset_creator.discourse_token_ids\n    END_IDS = dataset_creator.discourse_end_ids\n    GLOBAL_IDS = dataset_creator.global_tokens\n\n    def tokenize_with_truncation(examples):\n        tz = tokenizer(\n            examples[\"essay_text\"],\n            padding=False,\n            truncation=True,\n            add_special_tokens=True,\n            return_offsets_mapping=True,\n            max_length=config[\"max_length\"],\n            stride=config[\"stride\"],\n            return_overflowing_tokens=True,\n            return_token_type_ids=True,\n        )\n        return tz\n\n    def process_span(examples):\n        span_head_idxs, span_tail_idxs = [], []\n        span_head_char_start_idxs, span_tail_char_end_idxs = [], []\n\n        buffer = 25  # do not include a head if it is within buffer distance away from last token\n\n        for example_input_ids, example_offset_mapping in zip(examples[\"input_ids\"], examples[\"offset_mapping\"]):\n            # ------------------- Span Heads -----------------------------------------#\n            if len(example_input_ids) < config[\"max_length\"]:  # no truncation\n                head_candidate = [pos for pos, this_id in enumerate(example_input_ids) if this_id in START_IDS]\n            else:\n                head_candidate = [pos for pos, this_id in enumerate(example_input_ids) if (\n                    (this_id in START_IDS) & (pos <= config[\"max_length\"]-buffer))]\n\n            n_heads = len(head_candidate)\n\n            # ------------------- Span Tails -----------------------------------------#\n            tail_candidate = [pos for pos, this_id in enumerate(example_input_ids) if this_id in END_IDS]\n\n            # ------------------- Edge Cases -----------------------------------------#\n            # 1. A tail occurs before the first head in the sequence due to truncation\n            if (len(tail_candidate) > 0) & (len(head_candidate) > 0):\n                if tail_candidate[0] < head_candidate[0]:  # truncation effect\n                    # print(f\"check: heads: {head_candidate}, tails {tail_candidate}\")\n                    tail_candidate = tail_candidate[1:]  # shift by one\n\n            # 2. Tail got chopped off due to truncation but the corresponding head is still there\n            if len(tail_candidate) < n_heads:\n                assert len(tail_candidate) + 1 == n_heads\n                assert len(example_input_ids) == config[\"max_length\"]  # should only happen if input text is truncated\n                tail_candidate.append(config[\"max_length\"]-2)  # the token before [SEP] token\n\n            # 3. Additional tails remain in the buffer region\n            if len(tail_candidate) > len(head_candidate):\n                tail_candidate = tail_candidate[:len(head_candidate)]\n\n            # ------------------- Create the fields ------------------------------------#\n            example_span_head_char_start_idxs = [example_offset_mapping[pos][0] for pos in head_candidate]\n            example_span_tail_char_end_idxs = [example_offset_mapping[pos][1] for pos in tail_candidate]\n\n            span_head_idxs.append(head_candidate)\n            span_tail_idxs.append(tail_candidate)\n            span_head_char_start_idxs.append(example_span_head_char_start_idxs)\n            span_tail_char_end_idxs.append(example_span_tail_char_end_idxs)\n\n        return {\n            \"span_head_idxs\": span_head_idxs,\n            \"span_tail_idxs\": span_tail_idxs,\n            \"span_head_char_start_idxs\": span_head_char_start_idxs,\n            \"span_tail_char_end_idxs\": span_tail_char_end_idxs,\n        }\n\n    def enforce_alignment(examples):\n        uids = []\n\n        for example_span_head_char_start_idxs, example_overflow_to_sample_mapping in zip(\n                examples[\"span_head_char_start_idxs\"], examples[\"overflow_to_sample_mapping\"]):\n\n            original_example = original_dataset[example_overflow_to_sample_mapping]\n            original_example_span_head_char_start_idxs = original_example[\"span_head_char_start_idxs\"]\n            original_example_uids = original_example[\"uids\"]\n            char2uid = {k: v for k, v in zip(original_example_span_head_char_start_idxs, original_example_uids)}\n            current_example_uids = [char2uid[char_idx] for char_idx in example_span_head_char_start_idxs]\n            uids.append(current_example_uids)\n        return {\"uids\": uids}\n\n    def recompute_labels(examples):\n        labels = []\n        for example_span_head_char_start_idxs, example_overflow_to_sample_mapping in zip(\n                examples[\"span_head_char_start_idxs\"], examples[\"overflow_to_sample_mapping\"]):\n\n            original_example = original_dataset[example_overflow_to_sample_mapping]\n            original_example_span_head_char_start_idxs = original_example[\"span_head_char_start_idxs\"]\n            original_example_labels = original_example[\"labels\"]\n            char2label = {k: v for k, v in zip(original_example_span_head_char_start_idxs, original_example_labels)}\n            current_example_labels = [char2label[char_idx] for char_idx in example_span_head_char_start_idxs]\n            labels.append(current_example_labels)\n        return {\"labels\": labels}\n\n    def recompute_discourse_type_ids(examples):\n        discourse_type_ids = []\n        for example_span_head_char_start_idxs, example_overflow_to_sample_mapping in zip(\n                examples[\"span_head_char_start_idxs\"], examples[\"overflow_to_sample_mapping\"]):\n\n            original_example = original_dataset[example_overflow_to_sample_mapping]\n            original_example_span_head_char_start_idxs = original_example[\"span_head_char_start_idxs\"]\n            original_example_discourse_type_ids = original_example[\"discourse_type_ids\"]\n            char2discourse_id = {k: v for k, v in zip(\n                original_example_span_head_char_start_idxs, original_example_discourse_type_ids)}\n            current_example_discourse_type_ids = [char2discourse_id[char_idx]\n                                                  for char_idx in example_span_head_char_start_idxs]\n            discourse_type_ids.append(current_example_discourse_type_ids)\n        return {\"discourse_type_ids\": discourse_type_ids}\n\n    def compute_input_length(examples):\n        return {\"input_length\": [len(x) for x in examples[\"input_ids\"]]}\n\n    def sanity_check_head_tail(examples):\n        for head_idxs, tail_idxs in zip(examples[\"span_head_idxs\"], examples[\"span_tail_idxs\"]):\n            assert len(head_idxs) == len(tail_idxs)\n            for head, tail in zip(head_idxs, tail_idxs):\n                assert tail > head + 1, f\"head idxs: {head_idxs}, tail idxs {tail_idxs}\"\n\n    task_dataset = task_dataset.map(\n        tokenize_with_truncation,\n        batched=True,\n        remove_columns=task_dataset.column_names,\n        batch_size=len(task_dataset)\n    )\n\n    task_dataset = task_dataset.map(process_span, batched=True)\n    task_dataset = task_dataset.map(enforce_alignment, batched=True)\n    task_dataset = task_dataset.map(recompute_discourse_type_ids, batched=True)\n    task_dataset = task_dataset.map(sanity_check_head_tail, batched=True)\n\n    # no need to run on empty set\n    task_dataset = task_dataset.filter(lambda example: len(example['span_head_idxs']) != 0)\n    task_dataset = task_dataset.map(compute_input_length, batched=True)\n\n    if mode != \"infer\":\n        task_dataset = task_dataset.map(recompute_labels, batched=True)\n\n    to_return = dict()\n    to_return[\"dataset\"] = task_dataset\n    to_return[\"original_dataset\"] = original_dataset\n    to_return[\"tokenizer\"] = tokenizer\n    return to_return","metadata":{"_uuid":"418f372c-0a34-4176-8cb6-4cfd2b514726","_cell_guid":"11d5751a-2524-458e-b158-6bdb75a15920","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:05:16.688501Z","iopub.execute_input":"2022-08-23T12:05:16.688975Z","iopub.status.idle":"2022-08-23T12:05:16.759775Z","shell.execute_reply.started":"2022-08-23T12:05:16.68894Z","shell.execute_reply":"2022-08-23T12:05:16.758674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reuse for exp7,6,8\nos.makedirs(config[\"model_dir\"], exist_ok=True)\n\nprint(\"creating the inference datasets...\")\ninfer_ds_dict = get_fast_dataset(config, test_df, essay_df, mode=\"infer\")\ntokenizer = infer_ds_dict[\"tokenizer\"]\ninfer_dataset = infer_ds_dict[\"dataset\"]\nprint(infer_dataset)","metadata":{"_uuid":"0590a9f5-48be-40cd-92c7-f43e5ed85d97","_cell_guid":"ad247b76-bff3-4974-aed1-d53c8161711d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:05:16.761343Z","iopub.execute_input":"2022-08-23T12:05:16.761915Z","iopub.status.idle":"2022-08-23T12:05:17.588516Z","shell.execute_reply.started":"2022-08-23T12:05:16.761767Z","shell.execute_reply":"2022-08-23T12:05:17.587706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config[\"len_tokenizer\"] = len(tokenizer)\n\ninfer_dataset = infer_dataset.sort(\"input_length\")\n\ninfer_dataset.set_format(\n    type=None,\n    columns=['input_ids', 'attention_mask', 'token_type_ids', 'span_head_idxs',\n             'span_tail_idxs', 'discourse_type_ids', 'uids']\n)","metadata":{"_uuid":"737ea474-b5c6-4312-a871-282ce916098e","_cell_guid":"e2de67dd-85df-4b07-8f7b-eec19a91b701","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:05:17.5899Z","iopub.execute_input":"2022-08-23T12:05:17.590464Z","iopub.status.idle":"2022-08-23T12:05:17.609026Z","shell.execute_reply.started":"2022-08-23T12:05:17.590424Z","shell.execute_reply":"2022-08-23T12:05:17.608062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from copy import deepcopy\nfrom dataclasses import dataclass\n\nimport torch\nfrom transformers import DataCollatorWithPadding\n\n\n@dataclass\nclass CustomDataCollatorWithPadding(DataCollatorWithPadding):\n    \"\"\"\n    data collector for seq classification\n    \"\"\"\n\n    tokenizer = None\n    padding = True\n    max_length = None\n    pad_to_multiple_of = 512\n    return_tensors = \"pt\"\n\n    def __call__(self, features):\n        uids = [feature[\"uids\"] for feature in features]\n        discourse_type_ids = [feature[\"discourse_type_ids\"] for feature in features]\n        span_head_idxs = [feature[\"span_head_idxs\"] for feature in features]\n        span_tail_idxs = [feature[\"span_tail_idxs\"] for feature in features]\n        span_attention_mask = [[1]*len(feature[\"span_head_idxs\"]) for feature in features]\n\n        labels = None\n        if \"labels\" in features[0].keys():\n            labels = [feature[\"labels\"] for feature in features]\n\n        batch = self.tokenizer.pad(\n            features,\n            padding=self.padding,\n            max_length=self.max_length,\n            pad_to_multiple_of=self.pad_to_multiple_of,\n            return_tensors=None,\n        )\n\n        b_max = max([len(l) for l in span_head_idxs])\n        max_len = len(batch[\"input_ids\"][0])\n\n        default_head_idx = max(max_len - 10, 1)  # for padding\n        default_tail_idx = max(max_len - 4, 1)  # for padding\n\n        batch[\"span_head_idxs\"] = [\n            ex_span_head_idxs + [default_head_idx] * (b_max - len(ex_span_head_idxs)) for ex_span_head_idxs in span_head_idxs\n        ]\n\n        batch[\"uids\"] = [ex_uids + [-1] * (b_max - len(ex_uids)) for ex_uids in uids]\n        batch[\"discourse_type_ids\"] = [ex_discourse_type_ids + [0] *\n                                       (b_max - len(ex_discourse_type_ids)) for ex_discourse_type_ids in discourse_type_ids]\n\n        batch[\"span_tail_idxs\"] = [\n            ex_span_tail_idxs + [default_tail_idx] * (b_max - len(ex_span_tail_idxs)) for ex_span_tail_idxs in span_tail_idxs\n        ]\n\n        batch[\"span_attention_mask\"] = [\n            ex_discourse_masks + [0] * (b_max - len(ex_discourse_masks)) for ex_discourse_masks in span_attention_mask\n        ]\n\n        if labels is not None:\n            batch[\"labels\"] = [ex_labels + [-1] * (b_max - len(ex_labels)) for ex_labels in labels]\n\n        # multitask labels\n        def _get_additional_labels(label_id):\n            if label_id == 0:\n                vec = [0, 0]\n            elif label_id == 1:\n                vec = [1, 0]\n            elif label_id == 2:\n                vec = [1, 1]\n            elif label_id == -1:\n                vec = [-1, -1]\n            else:\n                raise\n            return vec\n\n        if labels is not None:\n            additional_labels = []\n            for ex_labels in batch[\"labels\"]:\n                ex_additional_labels = [_get_additional_labels(el) for el in ex_labels]\n                additional_labels.append(ex_additional_labels)\n            batch[\"multitask_labels\"] = additional_labels\n        # pdb.set_trace()\n\n        batch = {k: (torch.tensor(v, dtype=torch.int64) if k != \"multitask_labels\" else torch.tensor(\n            v, dtype=torch.float32)) for k, v in batch.items()}\n        return batch","metadata":{"_uuid":"dc996906-0973-4ea8-a4d6-5992202e18fa","_cell_guid":"93ca5e4d-a852-4060-9caf-c573fc686dc7","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:05:17.610945Z","iopub.execute_input":"2022-08-23T12:05:17.611325Z","iopub.status.idle":"2022-08-23T12:05:17.629022Z","shell.execute_reply.started":"2022-08-23T12:05:17.611289Z","shell.execute_reply":"2022-08-23T12:05:17.627594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_collector = CustomDataCollatorWithPadding(tokenizer=tokenizer)\n\ninfer_dl = DataLoader(\n    infer_dataset,\n    batch_size=config[\"infer_bs\"],\n    shuffle=False,\n    collate_fn=data_collector\n)","metadata":{"_uuid":"9b701f61-989a-455d-8344-9b85b7dbab96","_cell_guid":"52433b1a-f59a-4296-a44d-d2537b89ecf7","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:05:17.630865Z","iopub.execute_input":"2022-08-23T12:05:17.631647Z","iopub.status.idle":"2022-08-23T12:05:17.642881Z","shell.execute_reply.started":"2022-08-23T12:05:17.631605Z","shell.execute_reply":"2022-08-23T12:05:17.641994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nfrom torch.nn import LayerNorm\nfrom transformers import AutoConfig, AutoModel, BertConfig\nfrom transformers.models.bert.modeling_bert import BertAttention\n\n\n#-------- Model ------------------------------------------------------------------#\nclass FeedbackModel(nn.Module):\n    \"\"\"The feedback prize effectiveness baseline model\n    \"\"\"\n\n    def __init__(self, config):\n        super(FeedbackModel, self).__init__()\n        self.config = config\n\n        # base transformer\n        base_config = AutoConfig.from_pretrained(self.config[\"base_model_path\"])\n        self.base_model = AutoModel.from_pretrained(self.config[\"base_model_path\"], config=base_config)\n\n        # dropouts\n        self.dropout = nn.Dropout(self.config[\"dropout\"])\n        \n        # multi-head attention over span representations\n        attention_config = BertConfig()\n        attention_config.update(\n            {\n                \"num_attention_heads\": self.base_model.config.num_attention_heads,\n                \"hidden_size\": self.base_model.config.hidden_size,\n                \"attention_probs_dropout_prob\": self.base_model.config.attention_probs_dropout_prob,\n                \"is_decoder\": False,\n\n            }\n        )\n        self.fpe_span_attention = BertAttention(attention_config, position_embedding_type=\"relative_key\")\n        \n        # classification\n        hidden_size = self.base_model.config.hidden_size\n        feature_size = hidden_size\n        self.layer_norm = LayerNorm(feature_size, self.base_model.config.layer_norm_eps)\n        \n        # # LSTM Head\n        self.fpe_lstm_layer = nn.LSTM(\n            input_size=feature_size,\n            hidden_size=hidden_size//2,\n            num_layers=1,\n            batch_first=True,\n            bidirectional=True,\n        )\n\n\n        self.num_labels = self.config[\"num_labels\"]\n        self.classifier = nn.Linear(feature_size, self.config[\"num_labels\"])\n\n    def forward(self, input_ids, token_type_ids, attention_mask, span_head_idxs, span_tail_idxs, span_attention_mask, **kwargs):\n        bs = input_ids.shape[0]  # batch size\n\n        outputs = self.base_model(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n        )\n        encoder_layer = outputs[0]\n\n        self.fpe_lstm_layer.flatten_parameters()\n        encoder_layer = self.fpe_lstm_layer(encoder_layer)[0]  # LSTM layer outputs\n\n        mean_feature_vector = []\n\n        for i in range(bs):\n            span_vec_i = []\n\n            for head, tail in zip(span_head_idxs[i], span_tail_idxs[i]):\n                # span feature\n                tmp = torch.mean(encoder_layer[i, head+1:tail], dim=0)  # [h]\n                span_vec_i.append(tmp)\n            span_vec_i = torch.stack(span_vec_i)  # (num_disourse, h)\n            mean_feature_vector.append(span_vec_i)\n\n        mean_feature_vector = torch.stack(mean_feature_vector)  # (bs, num_disourse, h)\n        mean_feature_vector = self.layer_norm(mean_feature_vector)\n\n        # attention mechanism\n        extended_span_attention_mask = span_attention_mask[:, None, None, :]\n        # extended_span_attention_mask = extended_span_attention_mask.to(dtype=self.dtype)  # fp16 compatibility\n        extended_span_attention_mask = (1.0 - extended_span_attention_mask) * -10000.0\n        feature_vector = self.fpe_span_attention(mean_feature_vector, extended_span_attention_mask)[0]\n\n        feature_vector = self.dropout(feature_vector) # span-atten\n        logits = self.classifier(feature_vector)\n        \n        ######\n        \n        logits = logits[:,:, :3] # main logits\n        return logits","metadata":{"_uuid":"51884024-3cf2-4ec5-ba5d-69319c672a54","_cell_guid":"712fe9b3-648b-4585-8c8d-6ad3f48b2cab","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:05:17.645996Z","iopub.execute_input":"2022-08-23T12:05:17.646303Z","iopub.status.idle":"2022-08-23T12:05:17.66438Z","shell.execute_reply.started":"2022-08-23T12:05:17.646277Z","shell.execute_reply":"2022-08-23T12:05:17.663539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoints = [\n    \"../input/exp19-dexl-dataset-part-1/exp-19-dexl-revisit-part-1/fpe_model_fold_0_best.pth.tar\",\n    \"../input/exp19-dexl-dataset-part-1/exp-19-dexl-revisit-part-1/fpe_model_fold_1_best.pth.tar\",\n    \"../input/exp19-dexl-dataset-part-1/exp-19-dexl-revisit-part-1/fpe_model_fold_2_best.pth.tar\",\n    \"../input/exp19-dexl-dataset-part-1/exp-19-dexl-revisit-part-1/fpe_model_fold_3_best.pth.tar\",\n    \"../input/exp19-dexl-revisit-dataset-part-2/exp-19-dexl-revisit-part-2/fpe_model_fold_4_best.pth.tar\",\n    \"../input/exp19-dexl-revisit-dataset-part-2/exp-19-dexl-revisit-part-2/fpe_model_fold_5_best.pth.tar\",\n    \"../input/exp19-dexl-revisit-dataset-part-2/exp-19-dexl-revisit-part-2/fpe_model_fold_6_best.pth.tar\",\n    \"../input/exp19-dexl-revisit-dataset-part-2/exp-19-dexl-revisit-part-2/fpe_model_fold_7_best.pth.tar\",\n]","metadata":{"_uuid":"2b54c449-c853-4fd4-9d57-a94b6c1188d5","_cell_guid":"c4c5d06a-97e8-47bf-98b4-0380ffc0c29f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:05:17.665867Z","iopub.execute_input":"2022-08-23T12:05:17.666863Z","iopub.status.idle":"2022-08-23T12:05:17.676908Z","shell.execute_reply.started":"2022-08-23T12:05:17.666826Z","shell.execute_reply":"2022-08-23T12:05:17.675922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def inference_fn(model, infer_dl, model_id):\n    all_preds = []\n    all_uids = []\n    accelerator = Accelerator()\n    model, infer_dl = accelerator.prepare(model, infer_dl)\n    \n    model.eval()\n    tk0 = tqdm(infer_dl, total=len(infer_dl))\n    \n    for batch in tk0:\n        with torch.no_grad():\n            logits = model(**batch) # (b, nd, 3)\n            batch_preds = F.softmax(logits, dim=-1)\n            batch_uids = batch[\"uids\"]\n        all_preds.append(batch_preds)\n        all_uids.append(batch_uids)\n    \n    all_preds = [p.to('cpu').detach().numpy().tolist() for p in all_preds]\n    all_preds = list(chain(*all_preds))\n    flat_preds = list(chain(*all_preds))\n    \n    all_uids = [p.to('cpu').detach().numpy().tolist() for p in all_uids]\n    all_uids = list(chain(*all_uids))\n    flat_uids = list(chain(*all_uids))    \n    \n    preds_df = pd.DataFrame(flat_preds)\n    preds_df.columns = [\"Ineffective\", \"Adequate\", \"Effective\"]\n    preds_df[\"span_uid\"] = flat_uids # SORTED_DISCOURSE_IDS\n    preds_df = preds_df[preds_df[\"span_uid\"]>=0].copy()\n    preds_df[\"discourse_id\"] = preds_df[\"span_uid\"].map(idx2discourse)\n    preds_df = preds_df[[\"discourse_id\", \"Ineffective\", \"Adequate\", \"Effective\"]].copy()\n    preds_df.to_csv(f\"exp19_dexl_model_preds_{model_id}.csv\", index=False)\n    \n\nfor model_id, checkpoint in enumerate(checkpoints):\n    print(f\"infering from {checkpoint}\")\n    model = FeedbackModel(config)\n    ckpt = torch.load(checkpoint)\n    print(f\"validation score for fold {model_id} = {ckpt['loss']}\")\n    model.load_state_dict(ckpt['state_dict'])\n    inference_fn(model, infer_dl, model_id)\n    \ndel model\n# del tokenizer, infer_dataset, infer_ds_dict, data_collector, infer_dl\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"_uuid":"4a1f0e5c-188f-401e-8352-6e66cb50a784","_cell_guid":"b0f3a71e-3b77-4470-b024-bf43152c0473","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:05:17.680306Z","iopub.execute_input":"2022-08-23T12:05:17.680573Z","iopub.status.idle":"2022-08-23T12:13:24.288707Z","shell.execute_reply.started":"2022-08-23T12:05:17.680547Z","shell.execute_reply":"2022-08-23T12:13:24.287285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import glob\nimport pandas as pd\n\ncsvs = glob.glob(\"exp19_dexl_model_preds_*.csv\")\n\nidx = []\npreds = []\n\n\nfor csv_idx, csv in enumerate(csvs):\n    \n    print(\"==\"*40)\n    print(f\"preds in {csv}\")\n    df = pd.read_csv(csv)\n    df = df.sort_values(by=[\"discourse_id\"])\n    print(df.head(10))\n    print(\"==\"*40)\n    \n    temp_preds = df.drop([\"discourse_id\"], axis=1).values\n    if csv_idx == 0:\n        idx = list(df[\"discourse_id\"])\n        preds = temp_preds\n    else:\n        preds += temp_preds\n\npreds = preds / len(csvs)\n\nexp19_df = pd.DataFrame()\nexp19_df[\"discourse_id\"] = idx\nexp19_df[\"Ineffective\"]  = preds[:, 0]\nexp19_df[\"Adequate\"]     = preds[:, 1]\nexp19_df[\"Effective\"]    = preds[:, 2]\n\nexp19_df = exp19_df.groupby(\"discourse_id\")[[\"Ineffective\", \"Adequate\", \"Effective\"]].agg(np.mean).reset_index()\n\n","metadata":{"_uuid":"e4c99ccf-aa5d-4a44-917d-519652e0faf6","_cell_guid":"6bd132eb-e207-4f90-a7c8-56ae90c0c685","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:13:24.291718Z","iopub.execute_input":"2022-08-23T12:13:24.292804Z","iopub.status.idle":"2022-08-23T12:13:24.398019Z","shell.execute_reply.started":"2022-08-23T12:13:24.292726Z","shell.execute_reply":"2022-08-23T12:13:24.397166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"exp19_df.head()","metadata":{"_uuid":"da4ad14a-fac0-43b6-8587-ec88709fd377","_cell_guid":"acdf5358-60ee-4792-9040-b5fb28239784","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:13:24.399235Z","iopub.execute_input":"2022-08-23T12:13:24.39984Z","iopub.status.idle":"2022-08-23T12:13:24.420253Z","shell.execute_reply.started":"2022-08-23T12:13:24.399793Z","shell.execute_reply":"2022-08-23T12:13:24.419038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### DEXL all data","metadata":{}},{"cell_type":"code","source":"checkpoints = [\n    \"../input/exp-19f-dexl-revisit-all-data/fpe_model_all_data_seed_464.pth.tar\",\n    \"../input/exp-19f-dexl-revisit-all-data/fpe_model_all_data_seed_446.pth.tar\",\n]\n\ndef inference_fn(model, infer_dl, model_id):\n    all_preds = []\n    all_uids = []\n    accelerator = Accelerator()\n    model, infer_dl = accelerator.prepare(model, infer_dl)\n    \n    model.eval()\n    tk0 = tqdm(infer_dl, total=len(infer_dl))\n    \n    for batch in tk0:\n        with torch.no_grad():\n            logits = model(**batch) # (b, nd, 3)\n            batch_preds = F.softmax(logits, dim=-1)\n            batch_uids = batch[\"uids\"]\n        all_preds.append(batch_preds)\n        all_uids.append(batch_uids)\n    \n    all_preds = [p.to('cpu').detach().numpy().tolist() for p in all_preds]\n    all_preds = list(chain(*all_preds))\n    flat_preds = list(chain(*all_preds))\n    \n    all_uids = [p.to('cpu').detach().numpy().tolist() for p in all_uids]\n    all_uids = list(chain(*all_uids))\n    flat_uids = list(chain(*all_uids))    \n    \n    preds_df = pd.DataFrame(flat_preds)\n    preds_df.columns = [\"Ineffective\", \"Adequate\", \"Effective\"]\n    preds_df[\"span_uid\"] = flat_uids # SORTED_DISCOURSE_IDS\n    preds_df = preds_df[preds_df[\"span_uid\"]>=0].copy()\n    preds_df[\"discourse_id\"] = preds_df[\"span_uid\"].map(idx2discourse)\n    preds_df = preds_df[[\"discourse_id\", \"Ineffective\", \"Adequate\", \"Effective\"]].copy()\n    preds_df.to_csv(f\"19f_dexl_model_preds_{model_id}.csv\", index=False)\n    \n\nfor model_id, checkpoint in enumerate(checkpoints):\n    print(f\"infering from {checkpoint}\")\n    model = FeedbackModel(config)\n    ckpt = torch.load(checkpoint)\n    print(f\"validation score for fold {model_id} = {ckpt['loss']}\")\n    model.load_state_dict(ckpt['state_dict'])\n    inference_fn(model, infer_dl, model_id)\n    \ndel model\ngc.collect()\ntorch.cuda.empty_cache()\n\n\nimport glob\nimport pandas as pd\n\ncsvs = glob.glob(\"19f_dexl_model_preds_*.csv\")\n\nidx = []\npreds = []\n\n\nfor csv_idx, csv in enumerate(csvs):\n    \n    print(\"==\"*40)\n    print(f\"preds in {csv}\")\n    df = pd.read_csv(csv)\n    df = df.sort_values(by=[\"discourse_id\"])\n    print(df.head(10))\n    print(\"==\"*40)\n    \n    temp_preds = df.drop([\"discourse_id\"], axis=1).values\n    if csv_idx == 0:\n        idx = list(df[\"discourse_id\"])\n        preds = temp_preds\n    else:\n        preds += temp_preds\n\npreds = preds / len(csvs)\n\nexp19f_df = pd.DataFrame()\nexp19f_df[\"discourse_id\"] = idx\nexp19f_df[\"Ineffective\"]  = preds[:, 0]\nexp19f_df[\"Adequate\"]     = preds[:, 1]\nexp19f_df[\"Effective\"]    = preds[:, 2]\n\nexp19f_df = exp19f_df.groupby(\"discourse_id\")[[\"Ineffective\", \"Adequate\", \"Effective\"]].agg(np.mean).reset_index()\n\nexp19f_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-23T12:13:24.423163Z","iopub.execute_input":"2022-08-23T12:13:24.424131Z","iopub.status.idle":"2022-08-23T12:15:23.040013Z","shell.execute_reply.started":"2022-08-23T12:13:24.424085Z","shell.execute_reply":"2022-08-23T12:15:23.039214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### DEL KD All Data","metadata":{}},{"cell_type":"code","source":"config = \"\"\"{\n    \"debug\": false,\n\n    \"base_model_path\": \"../input/tapt-fpe-del-wiki\",\n    \"model_dir\": \"./outputs\",\n\n    \"max_length\": 1024,\n    \"stride\": 256,\n    \"num_labels\": 3,\n    \"dropout\": 0.1,\n    \"infer_bs\": 8,\n    \n    \"use_multitask\": true,\n    \"num_additional_labels\": 2\n}\n\"\"\"\nconfig = json.loads(config)\nconfig[\"len_tokenizer\"] = len(tokenizer)\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nfrom torch.nn import LayerNorm\nfrom transformers import AutoConfig, AutoModel, BertConfig\nfrom transformers.models.bert.modeling_bert import BertAttention\n\n\n#-------- Model ------------------------------------------------------------------#\n\nclass FeedbackModel(nn.Module):\n    \"\"\"\n    The feedback prize effectiveness model for fast approach\n    \"\"\"\n\n    def __init__(self, config):\n        print(\"==\"*40)\n        print(\"initializing the feedback model...\")\n\n        super(FeedbackModel, self).__init__()\n        self.config = config\n\n        # base transformer\n        base_config = AutoConfig.from_pretrained(self.config[\"base_model_path\"])\n        self.base_model = AutoModel.from_pretrained(self.config[\"base_model_path\"], config=base_config)\n\n        # resize model embeddings\n        print(\"resizing model embeddings...\")\n        print(f\"tokenizer length = {config['len_tokenizer']}\")\n        self.base_model.resize_token_embeddings(config[\"len_tokenizer\"])\n\n\n        # dropouts\n        self.dropout = nn.Dropout(self.config[\"dropout\"])\n        self.num_labels = self.num_original_labels = self.config[\"num_labels\"]\n\n        if self.config[\"use_multitask\"]:\n            print(\"using multi-task approach...\")\n            self.num_labels += self.config[\"num_additional_labels\"]\n\n        # multi-head attention over span representations\n        attention_config = BertConfig()\n        attention_config.update(\n            {\n                \"num_attention_heads\": self.base_model.config.num_attention_heads,\n                \"hidden_size\": self.base_model.config.hidden_size,\n                \"attention_probs_dropout_prob\": self.base_model.config.attention_probs_dropout_prob,\n                \"is_decoder\": False,\n\n            }\n        )\n        self.fpe_span_attention = BertAttention(attention_config, position_embedding_type=\"relative_key\")\n\n        # classification\n        hidden_size = self.base_model.config.hidden_size\n        feature_size = hidden_size\n        self.layer_norm = LayerNorm(feature_size, self.base_model.config.layer_norm_eps)\n\n\n        self.fpe_lstm_layer = nn.LSTM(\n            input_size=feature_size,\n            hidden_size=hidden_size//2,\n            num_layers=1,\n            batch_first=True,\n            bidirectional=True,\n        )\n\n        self.classifier = nn.Linear(feature_size, self.num_labels)\n\n    def forward(\n        self,\n        input_ids,\n        token_type_ids,\n        attention_mask,\n        span_head_idxs,\n        span_tail_idxs,\n        span_attention_mask,\n        labels=None,\n        multitask_labels=None,\n        **kwargs\n    ):\n\n        bs = input_ids.shape[0]  # batch size\n\n        outputs = self.base_model(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n        )\n        encoder_layer = outputs[0]\n\n\n        self.fpe_lstm_layer.flatten_parameters()\n        encoder_layer = self.fpe_lstm_layer(encoder_layer)[0]  # LSTM layer outputs\n\n        mean_feature_vector = []\n\n        for i in range(bs):\n            span_vec_i = []\n\n            for head, tail in zip(span_head_idxs[i], span_tail_idxs[i]):\n                # span feature\n                tmp = torch.mean(encoder_layer[i, head+1:tail], dim=0)  # [h]\n                span_vec_i.append(tmp)\n            span_vec_i = torch.stack(span_vec_i)  # (num_disourse, h)\n            mean_feature_vector.append(span_vec_i)\n\n        mean_feature_vector = torch.stack(mean_feature_vector)  # (bs, num_disourse, h)\n        mean_feature_vector = self.layer_norm(mean_feature_vector)\n\n        # attention mechanism\n        extended_span_attention_mask = span_attention_mask[:, None, None, :]\n        # extended_span_attention_mask = extended_span_attention_mask.to(dtype=self.dtype)  # fp16 compatibility\n        extended_span_attention_mask = (1.0 - extended_span_attention_mask) * -10000.0\n        feature_vector = self.fpe_span_attention(mean_feature_vector, extended_span_attention_mask)[0]\n\n        feature_vector = self.dropout(feature_vector)\n        logits = self.classifier(feature_vector)\n        \n        logits = logits[:,:,:3]\n\n        return logits\n    \ncheckpoints = [\n    \"../input/exp-20-del-kd-all-data-train/fpe_model_kd_seed_1.pth.tar\",\n#     \"../input/exp-20-del-kd-all-data-train/fpe_model_kd_seed_2.pth.tar\",\n]\n\ndef inference_fn(model, infer_dl, model_id):\n    all_preds = []\n    all_uids = []\n    accelerator = Accelerator()\n    model, infer_dl = accelerator.prepare(model, infer_dl)\n    \n    model.eval()\n    tk0 = tqdm(infer_dl, total=len(infer_dl))\n    \n    for batch in tk0:\n        with torch.no_grad():\n            logits = model(**batch) # (b, nd, 3)\n            batch_preds = F.softmax(logits, dim=-1)\n            batch_uids = batch[\"uids\"]\n        all_preds.append(batch_preds)\n        all_uids.append(batch_uids)\n    \n    all_preds = [p.to('cpu').detach().numpy().tolist() for p in all_preds]\n    all_preds = list(chain(*all_preds))\n    flat_preds = list(chain(*all_preds))\n    \n    all_uids = [p.to('cpu').detach().numpy().tolist() for p in all_uids]\n    all_uids = list(chain(*all_uids))\n    flat_uids = list(chain(*all_uids))    \n    \n    preds_df = pd.DataFrame(flat_preds)\n    preds_df.columns = [\"Ineffective\", \"Adequate\", \"Effective\"]\n    preds_df[\"span_uid\"] = flat_uids # SORTED_DISCOURSE_IDS\n    preds_df = preds_df[preds_df[\"span_uid\"]>=0].copy()\n    preds_df[\"discourse_id\"] = preds_df[\"span_uid\"].map(idx2discourse)\n    preds_df = preds_df[[\"discourse_id\", \"Ineffective\", \"Adequate\", \"Effective\"]].copy()\n    preds_df.to_csv(f\"exp20_del_kd_model_preds_{model_id}.csv\", index=False)\n    \n\nfor model_id, checkpoint in enumerate(checkpoints):\n    print(f\"infering from {checkpoint}\")\n    model = FeedbackModel(config)\n    ckpt = torch.load(checkpoint)\n    print(f\"validation score for fold {model_id} = {ckpt['loss']}\")\n    model.load_state_dict(ckpt['state_dict'])\n    inference_fn(model, infer_dl, model_id)\n    \ndel model\ngc.collect()\ntorch.cuda.empty_cache()\n\nimport glob\nimport pandas as pd\n\ncsvs = glob.glob(\"exp20_del_kd_model_preds_*.csv\")\n\nidx = []\npreds = []\n\n\nfor csv_idx, csv in enumerate(csvs):\n    \n    print(\"==\"*40)\n    print(f\"preds in {csv}\")\n    df = pd.read_csv(csv)\n    df = df.sort_values(by=[\"discourse_id\"])\n    print(df.head(10))\n    print(\"==\"*40)\n    \n    temp_preds = df.drop([\"discourse_id\"], axis=1).values\n    if csv_idx == 0:\n        idx = list(df[\"discourse_id\"])\n        preds = temp_preds\n    else:\n        preds += temp_preds\n\npreds = preds / len(csvs)\n\nexp20f_df = pd.DataFrame()\nexp20f_df[\"discourse_id\"] = idx\nexp20f_df[\"Ineffective\"]  = preds[:, 0]\nexp20f_df[\"Adequate\"]     = preds[:, 1]\nexp20f_df[\"Effective\"]    = preds[:, 2]\n\nexp20f_df = exp20f_df.groupby(\"discourse_id\")[[\"Ineffective\", \"Adequate\", \"Effective\"]].agg(np.mean).reset_index()\n\nexp20f_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-23T12:15:23.041543Z","iopub.execute_input":"2022-08-23T12:15:23.042086Z","iopub.status.idle":"2022-08-23T12:15:57.745344Z","shell.execute_reply.started":"2022-08-23T12:15:23.042048Z","shell.execute_reply":"2022-08-23T12:15:57.744461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EXP 6 - Model - DEXL","metadata":{"_uuid":"ac176d52-d528-45e6-9f1a-d6ccde0e4967","_cell_guid":"2340244e-f126-4a0d-938c-23fd91f7f506","trusted":true}},{"cell_type":"markdown","source":"## Config","metadata":{"_uuid":"cd3170bf-31e8-41bb-a5ee-19dbb18fb535","_cell_guid":"5f3121c9-aa25-4fc5-aadf-59acdbab2727","trusted":true}},{"cell_type":"code","source":"config = \"\"\"{\n    \"debug\": false,\n\n    \"base_model_path\": \"../input/tapt-fpe-dexl\",\n    \"model_dir\": \"./outputs\",\n\n    \"max_length\": 1024,\n    \"stride\": 256,\n    \"num_labels\": 5,\n    \"dropout\": 0.1,\n    \"infer_bs\": 8\n}\n\"\"\"\nconfig = json.loads(config)","metadata":{"_uuid":"da291716-4f7d-4871-af96-ba42a86c2b2e","_cell_guid":"f0f24b52-d585-4ba0-913f-d4644d406918","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:15:57.746756Z","iopub.execute_input":"2022-08-23T12:15:57.747133Z","iopub.status.idle":"2022-08-23T12:15:57.751767Z","shell.execute_reply.started":"2022-08-23T12:15:57.747097Z","shell.execute_reply":"2022-08-23T12:15:57.750663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{"_uuid":"63215e5d-edf1-4e8c-8bf8-131230309e59","_cell_guid":"dfe4cd5c-72ff-4d1f-b876-8d40bb022321","trusted":true}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nfrom torch.nn import LayerNorm\nfrom transformers import AutoConfig, AutoModel, BertConfig\nfrom transformers.models.bert.modeling_bert import BertAttention\n\n\n#-------- Model ------------------------------------------------------------------#\nclass FeedbackModel(nn.Module):\n    \"\"\"The feedback prize effectiveness baseline model\n    \"\"\"\n\n    def __init__(self, config):\n        super(FeedbackModel, self).__init__()\n        self.config = config\n\n        # base transformer\n        base_config = AutoConfig.from_pretrained(self.config[\"base_model_path\"])\n        self.base_model = AutoModel.from_pretrained(self.config[\"base_model_path\"], config=base_config)\n\n        # dropouts\n        self.dropout = nn.Dropout(self.config[\"dropout\"])\n        \n        # multi-head attention over span representations\n        attention_config = BertConfig()\n        attention_config.update(\n            {\n                \"num_attention_heads\": self.base_model.config.num_attention_heads,\n                \"hidden_size\": self.base_model.config.hidden_size,\n                \"attention_probs_dropout_prob\": self.base_model.config.attention_probs_dropout_prob,\n                \"is_decoder\": False,\n\n            }\n        )\n        self.fpe_span_attention = BertAttention(attention_config, position_embedding_type=\"relative_key\")\n        \n        # classification\n        hidden_size = self.base_model.config.hidden_size\n        feature_size = hidden_size\n        self.layer_norm = LayerNorm(feature_size, self.base_model.config.layer_norm_eps)\n        \n        # # LSTM Head\n        self.fpe_lstm_layer = nn.LSTM(\n            input_size=feature_size,\n            hidden_size=hidden_size//2,\n            num_layers=1,\n            batch_first=True,\n            bidirectional=True,\n        )\n\n\n        self.num_labels = self.config[\"num_labels\"]\n        self.classifier = nn.Linear(feature_size, self.config[\"num_labels\"])\n\n    def forward(self, input_ids, token_type_ids, attention_mask, span_head_idxs, span_tail_idxs, span_attention_mask, **kwargs):\n        bs = input_ids.shape[0]  # batch size\n\n        outputs = self.base_model(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n        )\n        encoder_layer = outputs[0]\n\n        self.fpe_lstm_layer.flatten_parameters()\n        encoder_layer = self.fpe_lstm_layer(encoder_layer)[0]  # LSTM layer outputs\n\n        mean_feature_vector = []\n\n        for i in range(bs):\n            span_vec_i = []\n\n            for head, tail in zip(span_head_idxs[i], span_tail_idxs[i]):\n                # span feature\n                tmp = torch.mean(encoder_layer[i, head+1:tail], dim=0)  # [h]\n                span_vec_i.append(tmp)\n            span_vec_i = torch.stack(span_vec_i)  # (num_disourse, h)\n            mean_feature_vector.append(span_vec_i)\n\n        mean_feature_vector = torch.stack(mean_feature_vector)  # (bs, num_disourse, h)\n        mean_feature_vector = self.layer_norm(mean_feature_vector)\n\n        # attention mechanism\n        extended_span_attention_mask = span_attention_mask[:, None, None, :]\n        # extended_span_attention_mask = extended_span_attention_mask.to(dtype=self.dtype)  # fp16 compatibility\n        extended_span_attention_mask = (1.0 - extended_span_attention_mask) * -10000.0\n        feature_vector = self.fpe_span_attention(mean_feature_vector, extended_span_attention_mask)[0]\n\n        feature_vector = self.dropout(mean_feature_vector)\n        logits = self.classifier(feature_vector)\n        \n        ######\n        \n        logits = logits[:,:, :3] # main logits\n        return logits","metadata":{"_uuid":"2239ded3-82be-411c-a1ca-e22c2e2145ea","_cell_guid":"179eae8f-74b7-4e41-a40f-4b872ab46904","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:15:57.753291Z","iopub.execute_input":"2022-08-23T12:15:57.753698Z","iopub.status.idle":"2022-08-23T12:15:57.772115Z","shell.execute_reply.started":"2022-08-23T12:15:57.75366Z","shell.execute_reply":"2022-08-23T12:15:57.771333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference","metadata":{"_uuid":"6888cdb4-33c8-4350-951e-d80f92c5fe4b","_cell_guid":"8a53e3e0-a626-43ff-802e-97567d6cd72f","trusted":true}},{"cell_type":"code","source":"checkpoints = [\n    \"../input/01-a-prod-fpe-dexl-4fold-dataset/a-prod-fpe-dexl/fpe_model_fold_0_best.pth.tar\",\n    \"../input/01-a-prod-fpe-dexl-4fold-dataset/a-prod-fpe-dexl/fpe_model_fold_1_best.pth.tar\",\n    \"../input/01-a-prod-fpe-dexl-4fold-dataset/a-prod-fpe-dexl/fpe_model_fold_2_best.pth.tar\",\n    \"../input/01-a-prod-fpe-dexl-4fold-dataset/a-prod-fpe-dexl/fpe_model_fold_3_best.pth.tar\",\n]","metadata":{"_uuid":"8562caad-cc5e-41fc-a99f-44981321801e","_cell_guid":"65787916-8590-4d73-92ab-4279b44fb2fb","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:15:57.773565Z","iopub.execute_input":"2022-08-23T12:15:57.773927Z","iopub.status.idle":"2022-08-23T12:15:57.785412Z","shell.execute_reply.started":"2022-08-23T12:15:57.773894Z","shell.execute_reply":"2022-08-23T12:15:57.784663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def inference_fn(model, infer_dl, model_id):\n    all_preds = []\n    all_uids = []\n    accelerator = Accelerator()\n    model, infer_dl = accelerator.prepare(model, infer_dl)\n    \n    model.eval()\n    tk0 = tqdm(infer_dl, total=len(infer_dl))\n    \n    for batch in tk0:\n        with torch.no_grad():\n            logits = model(**batch) # (b, nd, 3)\n            batch_preds = F.softmax(logits, dim=-1)\n            batch_uids = batch[\"uids\"]\n        all_preds.append(batch_preds)\n        all_uids.append(batch_uids)\n    \n    all_preds = [p.to('cpu').detach().numpy().tolist() for p in all_preds]\n    all_preds = list(chain(*all_preds))\n    flat_preds = list(chain(*all_preds))\n    \n    all_uids = [p.to('cpu').detach().numpy().tolist() for p in all_uids]\n    all_uids = list(chain(*all_uids))\n    flat_uids = list(chain(*all_uids))    \n    \n    preds_df = pd.DataFrame(flat_preds)\n    preds_df.columns = [\"Ineffective\", \"Adequate\", \"Effective\"]\n    preds_df[\"span_uid\"] = flat_uids # SORTED_DISCOURSE_IDS\n    preds_df = preds_df[preds_df[\"span_uid\"]>=0].copy()\n    preds_df[\"discourse_id\"] = preds_df[\"span_uid\"].map(idx2discourse)\n    preds_df = preds_df[[\"discourse_id\", \"Ineffective\", \"Adequate\", \"Effective\"]].copy()\n    preds_df.to_csv(f\"exp06_dexl_model_preds_{model_id}.csv\", index=False)\n    \n\nif use_exp6:\n    for model_id, checkpoint in enumerate(checkpoints):\n        print(f\"infering from {checkpoint}\")\n        model = FeedbackModel(config)\n        ckpt = torch.load(checkpoint)\n        print(f\"validation score for fold {model_id} = {ckpt['loss']}\")\n        model.load_state_dict(ckpt['state_dict'])\n        inference_fn(model, infer_dl, model_id)\n    \n    del model\n    # del tokenizer, infer_dataset, infer_ds_dict, data_collector, infer_dl\n    gc.collect()\n    torch.cuda.empty_cache()","metadata":{"_uuid":"295d24b4-6800-46ec-b1c4-e0e4155774dc","_cell_guid":"be02b850-3768-401e-8d3a-5e970e1557ed","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:15:57.788137Z","iopub.execute_input":"2022-08-23T12:15:57.788455Z","iopub.status.idle":"2022-08-23T12:15:57.802299Z","shell.execute_reply.started":"2022-08-23T12:15:57.78843Z","shell.execute_reply":"2022-08-23T12:15:57.80135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if use_exp6:\n    import glob\n    import pandas as pd\n\n    csvs = glob.glob(\"exp06_dexl_model_preds_*.csv\")\n\n    idx = []\n    preds = []\n\n\n    for csv_idx, csv in enumerate(csvs):\n\n        print(\"==\"*40)\n        print(f\"preds in {csv}\")\n        df = pd.read_csv(csv)\n        df = df.sort_values(by=[\"discourse_id\"])\n        print(df.head(10))\n        print(\"==\"*40)\n\n        temp_preds = df.drop([\"discourse_id\"], axis=1).values\n        if csv_idx == 0:\n            idx = list(df[\"discourse_id\"])\n            preds = temp_preds\n        else:\n            preds += temp_preds\n\n    preds = preds / len(csvs)\n\n    exp06_df = pd.DataFrame()\n    exp06_df[\"discourse_id\"] = idx\n    exp06_df[\"Ineffective\"]  = preds[:, 0]\n    exp06_df[\"Adequate\"]     = preds[:, 1]\n    exp06_df[\"Effective\"]    = preds[:, 2]\n\n    exp06_df = exp06_df.groupby(\"discourse_id\")[[\"Ineffective\", \"Adequate\", \"Effective\"]].agg(np.mean).reset_index()","metadata":{"_uuid":"91dd737c-f4c1-41d5-b612-c5daa36e2093","_cell_guid":"84498eae-4e7d-46c4-9fd9-63eacdc1744b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:15:57.806311Z","iopub.execute_input":"2022-08-23T12:15:57.808374Z","iopub.status.idle":"2022-08-23T12:15:57.824742Z","shell.execute_reply.started":"2022-08-23T12:15:57.808336Z","shell.execute_reply":"2022-08-23T12:15:57.823708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if use_exp6:\n    exp06_df.head()","metadata":{"_uuid":"76a17b68-a9b0-4ef1-9948-b2dd92b4f8b8","_cell_guid":"c0cdc101-6712-4c19-a4cd-e34338d6ae8a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:15:57.829079Z","iopub.execute_input":"2022-08-23T12:15:57.829632Z","iopub.status.idle":"2022-08-23T12:15:57.840079Z","shell.execute_reply.started":"2022-08-23T12:15:57.829606Z","shell.execute_reply":"2022-08-23T12:15:57.839216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EXP8 - Fast Model - Distilled Deberta Large","metadata":{"_uuid":"21d1c446-30b3-45ad-be53-b57965c9fd70","_cell_guid":"a58b8330-03d7-46b4-8389-3273fea1e591","trusted":true}},{"cell_type":"code","source":"config = \"\"\"{\n    \"debug\": false,\n\n    \"base_model_path\": \"../input/tapt-fpe-del-wiki\",\n    \"model_dir\": \"./outputs\",\n\n    \"max_length\": 1024,\n    \"stride\": 256,\n    \"num_labels\": 5,\n    \"dropout\": 0.1,\n    \"infer_bs\": 8\n}\n\"\"\"\nconfig = json.loads(config)","metadata":{"_uuid":"9ce18c70-a84e-4f21-9d25-bd579b499ec1","_cell_guid":"cb2a15f1-64bc-4d2c-a3b1-e4074c1cabdc","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:15:57.841318Z","iopub.execute_input":"2022-08-23T12:15:57.841724Z","iopub.status.idle":"2022-08-23T12:15:57.850709Z","shell.execute_reply.started":"2022-08-23T12:15:57.84169Z","shell.execute_reply":"2022-08-23T12:15:57.849958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{"_uuid":"0f29d9c3-69b7-4c94-8f85-b647c5c902e4","_cell_guid":"e66f731d-14e4-44fc-87b5-d955a9754726","trusted":true}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nfrom torch.nn import LayerNorm\nfrom transformers import AutoConfig, AutoModel, BertConfig\nfrom transformers.models.bert.modeling_bert import BertAttention\n\n\n#-------- Model ------------------------------------------------------------------#\nclass FeedbackModel(nn.Module):\n    \"\"\"The feedback prize effectiveness baseline model\n    \"\"\"\n\n    def __init__(self, config):\n        super(FeedbackModel, self).__init__()\n        self.config = config\n\n        # base transformer\n        base_config = AutoConfig.from_pretrained(self.config[\"base_model_path\"])\n        self.base_model = AutoModel.from_pretrained(self.config[\"base_model_path\"], config=base_config)\n\n        # dropouts\n        self.dropout = nn.Dropout(self.config[\"dropout\"])\n        \n        # multi-head attention over span representations\n        attention_config = BertConfig()\n        attention_config.update(\n            {\n                \"num_attention_heads\": self.base_model.config.num_attention_heads,\n                \"hidden_size\": self.base_model.config.hidden_size,\n                \"attention_probs_dropout_prob\": self.base_model.config.attention_probs_dropout_prob,\n                \"is_decoder\": False,\n\n            }\n        )\n        self.fpe_span_attention = BertAttention(attention_config, position_embedding_type=\"relative_key\")\n        \n        # classification\n        hidden_size = self.base_model.config.hidden_size\n        feature_size = hidden_size\n        self.layer_norm = LayerNorm(feature_size, self.base_model.config.layer_norm_eps)\n        \n        # # LSTM Head\n        self.fpe_lstm_layer = nn.LSTM(\n            input_size=feature_size,\n            hidden_size=hidden_size//2,\n            num_layers=1,\n            batch_first=True,\n            bidirectional=True,\n        )\n\n\n        self.num_labels = self.config[\"num_labels\"]\n        self.classifier = nn.Linear(feature_size, self.config[\"num_labels\"])\n\n    def forward(self, input_ids, token_type_ids, attention_mask, span_head_idxs, span_tail_idxs, span_attention_mask, **kwargs):\n        bs = input_ids.shape[0]  # batch size\n\n        outputs = self.base_model(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n        )\n        encoder_layer = outputs[0]\n\n        self.fpe_lstm_layer.flatten_parameters()\n        encoder_layer = self.fpe_lstm_layer(encoder_layer)[0]  # LSTM layer outputs\n\n        mean_feature_vector = []\n\n        for i in range(bs):\n            span_vec_i = []\n\n            for head, tail in zip(span_head_idxs[i], span_tail_idxs[i]):\n                # span feature\n                tmp = torch.mean(encoder_layer[i, head+1:tail], dim=0)  # [h]\n                span_vec_i.append(tmp)\n            span_vec_i = torch.stack(span_vec_i)  # (num_disourse, h)\n            mean_feature_vector.append(span_vec_i)\n\n        mean_feature_vector = torch.stack(mean_feature_vector)  # (bs, num_disourse, h)\n        mean_feature_vector = self.layer_norm(mean_feature_vector)\n\n        # attention mechanism\n        extended_span_attention_mask = span_attention_mask[:, None, None, :]\n        # extended_span_attention_mask = extended_span_attention_mask.to(dtype=self.dtype)  # fp16 compatibility\n        extended_span_attention_mask = (1.0 - extended_span_attention_mask) * -10000.0\n        feature_vector = self.fpe_span_attention(mean_feature_vector, extended_span_attention_mask)[0]\n\n        feature_vector = self.dropout(feature_vector) # span-atten\n        logits = self.classifier(feature_vector)\n        \n        ######\n        \n        logits = logits[:,:, :3] # main logits\n        return logits","metadata":{"_uuid":"51b2f84f-9e94-471f-ba27-5ed64c5b63fb","_cell_guid":"7f35a98c-2c7f-4daf-8ed4-fc3cfcdeec90","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:15:57.853035Z","iopub.execute_input":"2022-08-23T12:15:57.853764Z","iopub.status.idle":"2022-08-23T12:15:57.869733Z","shell.execute_reply.started":"2022-08-23T12:15:57.853724Z","shell.execute_reply":"2022-08-23T12:15:57.868793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference","metadata":{"_uuid":"4edebe4d-a2d6-4290-8dd6-c2a62dbfdc31","_cell_guid":"fe4db632-bb75-4aba-9f9f-17c7e3160571","trusted":true}},{"cell_type":"code","source":"checkpoints = [\n    \"../input/exp08-del-dataset-part-1/exp-08-del-8folds-kd-part-1/fpe_model_fold_0_best.pth.tar\",\n    \"../input/exp08-del-dataset-part-1/exp-08-del-8folds-kd-part-1/fpe_model_fold_1_best.pth.tar\",\n    \"../input/exp08-del-dataset-part-1/exp-08-del-8folds-kd-part-1/fpe_model_fold_2_best.pth.tar\",\n    \"../input/exp08-del-dataset-part-1/exp-08-del-8folds-kd-part-1/fpe_model_fold_3_best.pth.tar\",\n    \"../input/exp08-del-dataset-part-2/exp-08-del-8folds-kd-part-2/fpe_model_fold_4_best.pth.tar\",\n    \"../input/exp08-del-dataset-part-2/exp-08-del-8folds-kd-part-2/fpe_model_fold_5_best.pth.tar\",\n    \"../input/exp08-del-dataset-part-2/exp-08-del-8folds-kd-part-2/fpe_model_fold_6_best.pth.tar\",\n    \"../input/exp08-del-dataset-part-2/exp-08-del-8folds-kd-part-2/fpe_model_fold_7_best.pth.tar\",\n]","metadata":{"_uuid":"e2bea428-3764-40e8-ba51-3ad4e8f98af4","_cell_guid":"25ab68c3-aadf-480c-b161-9016a1419b97","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:15:57.870752Z","iopub.execute_input":"2022-08-23T12:15:57.871005Z","iopub.status.idle":"2022-08-23T12:15:57.885584Z","shell.execute_reply.started":"2022-08-23T12:15:57.870982Z","shell.execute_reply":"2022-08-23T12:15:57.884518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def inference_fn(model, infer_dl, model_id):\n    all_preds = []\n    all_uids = []\n    accelerator = Accelerator()\n    model, infer_dl = accelerator.prepare(model, infer_dl)\n    \n    model.eval()\n    tk0 = tqdm(infer_dl, total=len(infer_dl))\n    \n    for batch in tk0:\n        with torch.no_grad():\n            logits = model(**batch) # (b, nd, 3)\n            batch_preds = F.softmax(logits, dim=-1)\n            batch_uids = batch[\"uids\"]\n        all_preds.append(batch_preds)\n        all_uids.append(batch_uids)\n    \n    all_preds = [p.to('cpu').detach().numpy().tolist() for p in all_preds]\n    all_preds = list(chain(*all_preds))\n    flat_preds = list(chain(*all_preds))\n    \n    all_uids = [p.to('cpu').detach().numpy().tolist() for p in all_uids]\n    all_uids = list(chain(*all_uids))\n    flat_uids = list(chain(*all_uids))    \n    \n    preds_df = pd.DataFrame(flat_preds)\n    preds_df.columns = [\"Ineffective\", \"Adequate\", \"Effective\"]\n    preds_df[\"span_uid\"] = flat_uids # SORTED_DISCOURSE_IDS\n    preds_df = preds_df[preds_df[\"span_uid\"]>=0].copy()\n    preds_df[\"discourse_id\"] = preds_df[\"span_uid\"].map(idx2discourse)\n    preds_df = preds_df[[\"discourse_id\", \"Ineffective\", \"Adequate\", \"Effective\"]].copy()\n    preds_df.to_csv(f\"exp08_del_kd_model_preds_{model_id}.csv\", index=False)\n\nif use_exp8:\n\n    for model_id, checkpoint in enumerate(checkpoints):\n        print(f\"infering from {checkpoint}\")\n        model = FeedbackModel(config)\n        ckpt = torch.load(checkpoint)\n        print(f\"validation score for fold {model_id} = {ckpt['loss']}\")\n        model.load_state_dict(ckpt['state_dict'])\n        inference_fn(model, infer_dl, model_id)\n\n    del model\n    gc.collect()\n    torch.cuda.empty_cache()","metadata":{"_uuid":"97020b40-8e4d-4b39-9d58-475701aff911","_cell_guid":"2290a688-77e9-4349-9b0d-519213ecc9be","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:15:57.887116Z","iopub.execute_input":"2022-08-23T12:15:57.887472Z","iopub.status.idle":"2022-08-23T12:15:57.901043Z","shell.execute_reply.started":"2022-08-23T12:15:57.887438Z","shell.execute_reply":"2022-08-23T12:15:57.900175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if use_exp8:\n\n    import glob\n    import pandas as pd\n\n    csvs = glob.glob(\"exp08_del_kd_model_preds_*.csv\")\n\n    idx = []\n    preds = []\n\n\n    for csv_idx, csv in enumerate(csvs):\n\n        print(\"==\"*40)\n        print(f\"preds in {csv}\")\n        df = pd.read_csv(csv)\n        df = df.sort_values(by=[\"discourse_id\"])\n        print(df.head(10))\n        print(\"==\"*40)\n\n        temp_preds = df.drop([\"discourse_id\"], axis=1).values\n        if csv_idx == 0:\n            idx = list(df[\"discourse_id\"])\n            preds = temp_preds\n        else:\n            preds += temp_preds\n\n    preds = preds / len(csvs)\n\n    exp_08_df = pd.DataFrame()\n    exp_08_df[\"discourse_id\"] = idx\n    exp_08_df[\"Ineffective\"]  = preds[:, 0]\n    exp_08_df[\"Adequate\"]     = preds[:, 1]\n    exp_08_df[\"Effective\"]    = preds[:, 2]\n\n    exp_08_df = exp_08_df.groupby(\"discourse_id\")[[\"Ineffective\", \"Adequate\", \"Effective\"]].agg(np.mean).reset_index()","metadata":{"_uuid":"8c9f7ae6-5bb3-4e47-a36e-533fbeb2fa77","_cell_guid":"484dc04a-8fb7-4175-8023-5b2d0341df46","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:15:57.901989Z","iopub.execute_input":"2022-08-23T12:15:57.904112Z","iopub.status.idle":"2022-08-23T12:15:57.915401Z","shell.execute_reply.started":"2022-08-23T12:15:57.902262Z","shell.execute_reply":"2022-08-23T12:15:57.914357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if use_exp8:\n    display(exp_08_df.head())","metadata":{"_uuid":"edb3f8dd-2ac9-4074-b9cb-07210b9fb4c7","_cell_guid":"68963490-ef1f-4481-8400-bd4bb4c8a3bb","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:15:57.918784Z","iopub.execute_input":"2022-08-23T12:15:57.919147Z","iopub.status.idle":"2022-08-23T12:15:57.929666Z","shell.execute_reply.started":"2022-08-23T12:15:57.919122Z","shell.execute_reply":"2022-08-23T12:15:57.928767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EXP212 - Fast Model - Longformer","metadata":{"_uuid":"dd0054d2-b13f-4aaa-a91d-434d7b4ecfcd","_cell_guid":"aac4ad8a-5dc1-4a01-ba58-8f9ef2356803","trusted":true}},{"cell_type":"code","source":"config = \"\"\"{\n    \"debug\": false,\n\n    \"base_model_path\": \"../input/exp212-longformer-l-prompt-mlm50/mlm_model\",\n    \"model_dir\": \"./outputs\",\n\n    \"max_length\": 1024,\n    \"stride\": 256,\n    \"num_labels\": 3,\n    \"dropout\": 0.1,\n    \"infer_bs\": 8\n}\n\"\"\"\nconfig = json.loads(config)","metadata":{"_uuid":"9b49d692-81a6-4656-9ac7-f976fff30dc3","_cell_guid":"c2dd4b5a-3d96-472c-a86b-09be138319b4","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:16:18.907643Z","iopub.execute_input":"2022-08-23T12:16:18.908222Z","iopub.status.idle":"2022-08-23T12:16:18.9139Z","shell.execute_reply.started":"2022-08-23T12:16:18.908157Z","shell.execute_reply":"2022-08-23T12:16:18.912725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset","metadata":{"_uuid":"91af6bb2-17a3-4622-a25c-b3d9ff88c7f0","_cell_guid":"3138c61e-f892-4813-aca0-223f082ff417","trusted":true}},{"cell_type":"code","source":"import os\nimport re\nfrom copy import deepcopy\nfrom itertools import chain\n\nimport pandas as pd\nfrom datasets import Dataset\nfrom tokenizers import AddedToken\nfrom transformers import AutoTokenizer\n\n\n#--------------- Tokenizer ---------------------------------------------#\ndef get_tokenizer(config):\n    \"\"\"load the tokenizer\"\"\"\n\n    print(\"using auto tokenizer\")\n    tokenizer = AutoTokenizer.from_pretrained(config[\"base_model_path\"])\n\n    print(\"==\"*40)\n    print(f\"tokenizer len: {len(tokenizer)}\")\n    print(\n        f\"tokenizer test: {tokenizer.tokenize('Starts: [SOE] [LEAD] [CLAIM] [POSITION] [COUNTER_CLAIM]')}\")\n    print(\n        f\"tokenizer test: {tokenizer.tokenize('Starts: [EOE] [LEAD_END] [POSITION_END] [CLAIM_END]')}\")\n\n    print(\"==\"*40)\n    return tokenizer\n\n\n#--------------- Processing ---------------------------------------------#\n\n\nDISCOURSE_START_TOKENS = [\n    \"[LEAD]\",\n    \"[POSITION]\",\n    \"[CLAIM]\",\n    \"[COUNTER_CLAIM]\",\n    \"[REBUTTAL]\",\n    \"[EVIDENCE]\",\n    \"[CONCLUDING_STATEMENT]\"\n]\n\nTOKEN_MAP = {\n    \"topic\": [\"Topic [TOPIC]\", \"[TOPIC END]\"],\n    \"Lead\": [\"Lead [LEAD]\", \"[LEAD END]\"],\n    \"Position\": [\"Position [POSITION]\", \"[POSITION END]\"],\n    \"Claim\": [\"Claim [CLAIM]\", \"[CLAIM END]\"],\n    \"Counterclaim\": [\"Counterclaim [COUNTER_CLAIM]\", \"[COUNTER_CLAIM END]\"],\n    \"Rebuttal\": [\"Rebuttal [REBUTTAL]\", \"[REBUTTAL END]\"],\n    \"Evidence\": [\"Evidence [EVIDENCE]\", \"[EVIDENCE END]\"],\n    \"Concluding Statement\": [\"Concluding Statement [CONCLUDING_STATEMENT]\", \"[CONCLUDING_STATEMENT END]\"]\n}\n\n\nDISCOURSE_END_TOKENS = [\n    \"[LEAD END]\",\n    \"[POSITION END]\",\n    \"[CLAIM END]\",\n    \"[COUNTER_CLAIM END]\",\n    \"[REBUTTAL END]\",\n    \"[EVIDENCE END]\",\n    \"[CONCLUDING_STATEMENT END]\",\n]\n\n\n\n\n\n\ndef relaxed_search(text, substring, min_length=2, fraction=0.99999):\n    \"\"\"\n    Returns substring's span from the given text with the certain precision.\n    \"\"\"\n\n    position = text.find(substring)\n    substring_length = len(substring)\n    if position == -1:\n        half_length = int(substring_length * fraction)\n        half_substring = substring[:half_length]\n        half_substring_length = len(half_substring)\n        if half_substring_length < min_length:\n            return [-1, 0]\n        else:\n            return relaxed_search(text=text,\n                                  substring=half_substring,\n                                  min_length=min_length,\n                                  fraction=fraction)\n\n    span = [position, position+substring_length]\n    return span\n\n\ndef build_span_map(discourse_list, essay_text):\n    reading_head = 0\n    to_return = dict()\n\n    for cur_discourse in discourse_list:\n        if cur_discourse not in to_return:\n            to_return[cur_discourse] = []\n\n        matches = re.finditer(re.escape(r'{}'.format(cur_discourse)), essay_text)\n        for match in matches:\n            span_start, span_end = match.span()\n            if span_end <= reading_head:\n                continue\n            to_return[cur_discourse].append(match.span())\n            reading_head = span_end\n            break\n\n    # post process\n    for cur_discourse in discourse_list:\n        if not to_return[cur_discourse]:\n            print(\"resorting to relaxed search...\")\n            to_return[cur_discourse] = [relaxed_search(essay_text, cur_discourse)]\n    return to_return\n\n\ndef get_substring_span(texts, mapping):\n    result = []\n    for text in texts:\n        ans = mapping[text].pop(0)\n        result.append(ans)\n    return result\n\n\ndef process_essay(essay_id, essay_text, prompt, anno_df):\n    \"\"\"insert newly added tokens in the essay text\n    \"\"\"\n    tmp_df = anno_df[anno_df[\"essay_id\"] == essay_id].copy()\n    tmp_df = tmp_df.sort_values(by=\"discourse_start\")\n    buffer = 0\n\n    for _, row in tmp_df.iterrows():\n        s, e, d_type = int(row.discourse_start) + buffer, int(row.discourse_end) + buffer, row.discourse_type\n        s_tok, e_tok = TOKEN_MAP[d_type]\n        essay_text = \" \".join([essay_text[:s], s_tok, essay_text[s:e], e_tok, essay_text[e:]])\n        buffer += len(s_tok) + len(e_tok) + 4\n\n    essay_text = \"[SOE]\" + \" [TOPIC] \" + prompt + \" [TOPIC END] \" +  essay_text + \"[EOE]\"\n    return essay_text\n\n\ndef process_input_df(anno_df, notes_df):\n    \"\"\"pre-process input dataframe\n\n    :param df: input dataframe\n    :type df: pd.DataFrame\n    :return: processed dataframe\n    :rtype: pd.DataFrame\n    \"\"\"\n    notes_df = deepcopy(notes_df)\n    anno_df = deepcopy(anno_df)\n\n    #------------------- Pre-Process Essay Text --------------------------#\n    anno_df[\"discourse_text\"] = anno_df[\"discourse_text\"].apply(lambda x: x.strip())  # pre-process\n    if \"discourse_effectiveness\" in anno_df.columns:\n        anno_df = anno_df[[\"discourse_id\", \"essay_id\", \"discourse_text\",\n                           \"discourse_type\", \"discourse_effectiveness\", \"uid\"]].copy()\n    else:\n        anno_df = anno_df[[\"discourse_id\", \"essay_id\", \"discourse_text\", \"discourse_type\", \"uid\"]].copy()\n\n    tmp_df = anno_df.groupby(\"essay_id\")[[\"discourse_id\", \"discourse_text\"]].agg(list).reset_index()\n    tmp_df = pd.merge(tmp_df, notes_df, on=\"essay_id\", how=\"left\")\n    tmp_df[\"span_map\"] = tmp_df[[\"discourse_text\", \"essay_text\"]].apply(\n        lambda x: build_span_map(x[0], x[1]), axis=1)\n    tmp_df[\"span\"] = tmp_df[[\"discourse_text\", \"span_map\"]].apply(\n        lambda x: get_substring_span(x[0], x[1]), axis=1)\n\n    all_discourse_ids = list(chain(*tmp_df[\"discourse_id\"].values))\n    all_discourse_spans = list(chain(*tmp_df[\"span\"].values))\n    span_df = pd.DataFrame()\n    span_df[\"discourse_id\"] = all_discourse_ids\n    span_df[\"span\"] = all_discourse_spans\n    span_df[\"discourse_start\"] = span_df[\"span\"].apply(lambda x: x[0])\n    span_df[\"discourse_end\"] = span_df[\"span\"].apply(lambda x: x[1])\n    span_df = span_df.drop(columns=\"span\")\n\n    anno_df = pd.merge(anno_df, span_df, on=\"discourse_id\", how=\"left\")\n    # anno_df = pd.merge(anno_df, notes_df, on=\"essay_id\", how=\"left\")\n\n    print(\"==\"*40)\n    print(\"processing essay text and inserting new tokens at span boundaries\")\n    notes_df[\"essay_text\"] = notes_df[[\"essay_id\", \"essay_text\", \"prompt\"]].apply(\n        lambda x: process_essay(x[0], x[1], x[2], anno_df), axis=1\n    )\n    print(\"==\"*40)\n\n    anno_df = anno_df.drop(columns=[\"discourse_start\", \"discourse_end\"])\n    notes_df = notes_df.drop_duplicates(subset=[\"essay_id\"])[[\"essay_id\", \"essay_text\"]].copy()\n\n    anno_df = pd.merge(anno_df, notes_df, on=\"essay_id\", how=\"left\")\n\n    if \"discourse_effectiveness\" in anno_df.columns:\n        grouped_df = anno_df.groupby(\"essay_id\")[[\"uid\", \"discourse_effectiveness\", \"discourse_type\"]].agg(list).reset_index()\n    else:\n        grouped_df = anno_df.groupby(\"essay_id\")[[\"uid\", \"discourse_type\"]].agg(list).reset_index()\n\n    grouped_df = pd.merge(grouped_df, notes_df, on=\"essay_id\", how=\"left\")\n    grouped_df = grouped_df.rename(columns={\"uid\": \"uids\"})\n\n    return grouped_df\n\n\n#--------------- Dataset ----------------------------------------------#\n\n\nclass AuxFeedbackDataset:\n    \"\"\"Dataset class for feedback prize effectiveness task\n    \"\"\"\n\n    def __init__(self, config):\n        self.config = config\n\n        self.label2id = {\n            \"Ineffective\": 0,\n            \"Adequate\": 1,\n            \"Effective\": 2,\n        }\n\n        self.discourse_type2id = {\n            \"Lead\": 1,\n            \"Position\": 2,\n            \"Claim\": 3,\n            \"Counterclaim\": 4,\n            \"Rebuttal\": 5,\n            \"Evidence\": 6,\n            \"Concluding Statement\": 7,\n        }\n\n        self.id2label = {v: k for k, v in self.label2id.items()}\n        self.load_tokenizer()\n\n    def load_tokenizer(self):\n        \"\"\"load tokenizer as per config \n        \"\"\"\n        self.tokenizer = get_tokenizer(self.config)\n        print(\"==\"*40)\n        print(\"token maps...\")\n        print(TOKEN_MAP)\n        print(\"==\"*40)\n\n        # print(\"adding new tokens...\")\n        # tokens_to_add = []\n        # for this_tok in NEW_TOKENS:\n        #     tokens_to_add.append(AddedToken(this_tok, lstrip=True, rstrip=False))\n        # self.tokenizer.add_tokens(tokens_to_add)\n        print(f\"tokenizer len: {len(self.tokenizer)}\")\n\n        self.discourse_token_ids = set(self.tokenizer.convert_tokens_to_ids(DISCOURSE_START_TOKENS))\n        self.discourse_end_ids = set(self.tokenizer.convert_tokens_to_ids(DISCOURSE_END_TOKENS))\n        self.global_tokens = self.discourse_token_ids.union(self.discourse_end_ids)\n\n    def tokenize_function(self, examples):\n        tz = self.tokenizer(\n            examples[\"essay_text\"],\n            padding=False,\n            truncation=False,  # no truncation at first\n            add_special_tokens=True,\n            return_offsets_mapping=True,\n        )\n        return tz\n\n    def process_spans(self, examples):\n\n        span_head_char_start_idxs, span_tail_char_end_idxs = [], []\n        span_head_idxs, span_tail_idxs = [], []\n\n        for example_input_ids, example_offset_mapping, example_uids in zip(examples[\"input_ids\"], examples[\"offset_mapping\"], examples[\"uids\"]):\n            example_span_head_idxs = [pos for pos, this_id in enumerate(\n                example_input_ids) if this_id in self.discourse_token_ids]\n            example_span_tail_idxs = [pos for pos, this_id in enumerate(\n                example_input_ids) if this_id in self.discourse_end_ids]\n\n            example_span_head_char_start_idxs = [example_offset_mapping[pos][0] for pos in example_span_head_idxs]\n            example_span_tail_char_end_idxs = [example_offset_mapping[pos][1] for pos in example_span_tail_idxs]\n\n            span_head_char_start_idxs.append(example_span_head_char_start_idxs)\n            span_tail_char_end_idxs.append(example_span_tail_char_end_idxs)\n\n            span_head_idxs.append(example_span_head_idxs)\n            span_tail_idxs.append(example_span_tail_idxs)\n\n        return {\n            \"span_head_idxs\": span_head_idxs,\n            \"span_tail_idxs\": span_tail_idxs,\n            \"span_head_char_start_idxs\": span_head_char_start_idxs,\n            \"span_tail_char_end_idxs\": span_tail_char_end_idxs,\n        }\n\n    def generate_labels(self, examples):\n        labels = []\n        for example_labels, example_uids in zip(examples[\"discourse_effectiveness\"], examples[\"uids\"]):\n            labels.append([self.label2id[l] for l in example_labels])\n        return {\"labels\": labels}\n\n    def generate_discourse_type_ids(self, examples):\n        discourse_type_ids = []\n        for example_discourse_types in examples[\"discourse_type\"]:\n            discourse_type_ids.append([self.discourse_type2id[dt] for dt in example_discourse_types])\n        return {\"discourse_type_ids\": discourse_type_ids}\n\n    def compute_input_length(self, examples):\n        return {\"input_length\": [len(x) for x in examples[\"input_ids\"]]}\n\n    def sanity_check_head_tail(self, examples):\n        for head_idxs, tail_idxs in zip(examples[\"span_head_idxs\"], examples[\"span_tail_idxs\"]):\n            assert len(head_idxs) == len(tail_idxs)\n            for head, tail in zip(head_idxs, tail_idxs):\n                assert tail > head + 1\n\n    def sanity_check_head_labels(self, examples):\n        for head_idxs, head_labels in zip(examples[\"span_head_idxs\"], examples[\"labels\"]):\n            assert len(head_idxs) == len(head_labels)\n\n    def get_dataset(self, df, essay_df, mode='train'):\n        \"\"\"main api for creating the Feedback dataset\n\n        :param df: input annotation dataframe\n        :type df: pd.DataFrame\n        :param essay_df: dataframe with essay texts\n        :type essay_df: pd.DataFrame\n        :param mode: check if required for train or infer, defaults to 'train'\n        :type mode: str, optional\n        :return: the created dataset\n        :rtype: Dataset\n        \"\"\"\n        df = process_input_df(df, essay_df)\n\n        # save a sample for sanity checks\n        sample_df = df.sample(min(16, len(df)))\n        sample_df.to_csv(os.path.join(self.config[\"model_dir\"], f\"{mode}_df_processed.csv\"), index=False)\n\n        task_dataset = Dataset.from_pandas(df)\n        task_dataset = task_dataset.map(self.tokenize_function, batched=True)\n        task_dataset = task_dataset.map(self.compute_input_length, batched=True)\n        task_dataset = task_dataset.map(self.process_spans, batched=True)\n        print(task_dataset)\n        # todo check edge cases\n        task_dataset = task_dataset.filter(lambda example: len(example['span_head_idxs']) == len(\n            example['span_tail_idxs']))  # no need to run on empty set\n        print(task_dataset)\n        task_dataset = task_dataset.map(self.generate_discourse_type_ids, batched=True)\n        task_dataset = task_dataset.map(self.sanity_check_head_tail, batched=True)\n\n        if mode != \"infer\":\n            task_dataset = task_dataset.map(self.generate_labels, batched=True)\n            task_dataset = task_dataset.map(self.sanity_check_head_labels, batched=True)\n\n        try:\n            task_dataset = task_dataset.remove_columns(column_names=[\"__index_level_0__\"])\n        except Exception as e:\n            pass\n        return df, task_dataset\n\n#--------------- dataset with truncation ---------------------------------------------#\n\n\ndef get_fast_dataset(config, df, essay_df, mode=\"train\"):\n    \"\"\"Function to get fast approach dataset with truncation & sliding window\n    \"\"\"\n    dataset_creator = AuxFeedbackDataset(config)\n    _, task_dataset = dataset_creator.get_dataset(df, essay_df, mode=mode)\n\n    original_dataset = deepcopy(task_dataset)\n    tokenizer = dataset_creator.tokenizer\n    START_IDS = dataset_creator.discourse_token_ids\n    END_IDS = dataset_creator.discourse_end_ids\n    GLOBAL_IDS = dataset_creator.global_tokens\n\n    def tokenize_with_truncation(examples):\n        tz = tokenizer(\n            examples[\"essay_text\"],\n            padding=False,\n            truncation=True,\n            add_special_tokens=True,\n            return_offsets_mapping=True,\n            max_length=config[\"max_length\"],\n            stride=config[\"stride\"],\n            return_overflowing_tokens=True,\n            return_token_type_ids=True,\n        )\n        return tz\n\n    def process_span(examples):\n        span_head_idxs, span_tail_idxs = [], []\n        span_head_char_start_idxs, span_tail_char_end_idxs = [], []\n\n        buffer = 25  # do not include a head if it is within buffer distance away from last token\n\n        for example_input_ids, example_offset_mapping in zip(examples[\"input_ids\"], examples[\"offset_mapping\"]):\n            # ------------------- Span Heads -----------------------------------------#\n            if len(example_input_ids) < config[\"max_length\"]:  # no truncation\n                head_candidate = [pos for pos, this_id in enumerate(example_input_ids) if this_id in START_IDS]\n            else:\n                head_candidate = [pos for pos, this_id in enumerate(example_input_ids) if (\n                    (this_id in START_IDS) & (pos <= config[\"max_length\"]-buffer))]\n\n            n_heads = len(head_candidate)\n\n            # ------------------- Span Tails -----------------------------------------#\n            tail_candidate = [pos for pos, this_id in enumerate(example_input_ids) if this_id in END_IDS]\n\n            # ------------------- Edge Cases -----------------------------------------#\n            # 1. A tail occurs before the first head in the sequence due to truncation\n            if (len(tail_candidate) > 0) & (len(head_candidate) > 0):\n                if tail_candidate[0] < head_candidate[0]:  # truncation effect\n                    # print(f\"check: heads: {head_candidate}, tails {tail_candidate}\")\n                    tail_candidate = tail_candidate[1:]  # shift by one\n\n            # 2. Tail got chopped off due to truncation but the corresponding head is still there\n            if len(tail_candidate) < n_heads:\n                assert len(tail_candidate) + 1 == n_heads\n                assert len(example_input_ids) == config[\"max_length\"]  # should only happen if input text is truncated\n                tail_candidate.append(config[\"max_length\"]-2)  # the token before [SEP] token\n\n            # 3. Additional tails remain in the buffer region\n            if len(tail_candidate) > len(head_candidate):\n                tail_candidate = tail_candidate[:len(head_candidate)]\n\n            # ------------------- Create the fields ------------------------------------#\n            example_span_head_char_start_idxs = [example_offset_mapping[pos][0] for pos in head_candidate]\n            example_span_tail_char_end_idxs = [example_offset_mapping[pos][1] for pos in tail_candidate]\n\n            span_head_idxs.append(head_candidate)\n            span_tail_idxs.append(tail_candidate)\n            span_head_char_start_idxs.append(example_span_head_char_start_idxs)\n            span_tail_char_end_idxs.append(example_span_tail_char_end_idxs)\n\n        return {\n            \"span_head_idxs\": span_head_idxs,\n            \"span_tail_idxs\": span_tail_idxs,\n            \"span_head_char_start_idxs\": span_head_char_start_idxs,\n            \"span_tail_char_end_idxs\": span_tail_char_end_idxs,\n        }\n\n    def get_global_attention_mask(examples):\n        global_attention_mask = []\n        for example_input_ids in examples[\"input_ids\"]:\n            global_attention_mask.append([1 if iid in GLOBAL_IDS else 0 for iid in example_input_ids])\n        return {\"global_attention_mask\": global_attention_mask}\n\n    def enforce_alignment(examples):\n        uids = []\n\n        for example_span_head_char_start_idxs, example_overflow_to_sample_mapping in zip(\n                examples[\"span_head_char_start_idxs\"], examples[\"overflow_to_sample_mapping\"]):\n\n            original_example = original_dataset[example_overflow_to_sample_mapping]\n            original_example_span_head_char_start_idxs = original_example[\"span_head_char_start_idxs\"]\n            original_example_uids = original_example[\"uids\"]\n            char2uid = {k: v for k, v in zip(original_example_span_head_char_start_idxs, original_example_uids)}\n            current_example_uids = [char2uid[char_idx] for char_idx in example_span_head_char_start_idxs]\n            uids.append(current_example_uids)\n        return {\"uids\": uids}\n\n    def recompute_labels(examples):\n        labels = []\n        for example_span_head_char_start_idxs, example_overflow_to_sample_mapping in zip(\n                examples[\"span_head_char_start_idxs\"], examples[\"overflow_to_sample_mapping\"]):\n\n            original_example = original_dataset[example_overflow_to_sample_mapping]\n            original_example_span_head_char_start_idxs = original_example[\"span_head_char_start_idxs\"]\n            original_example_labels = original_example[\"labels\"]\n            char2label = {k: v for k, v in zip(original_example_span_head_char_start_idxs, original_example_labels)}\n            current_example_labels = [char2label[char_idx] for char_idx in example_span_head_char_start_idxs]\n            labels.append(current_example_labels)\n        return {\"labels\": labels}\n\n    def recompute_discourse_type_ids(examples):\n        discourse_type_ids = []\n        for example_span_head_char_start_idxs, example_overflow_to_sample_mapping in zip(\n                examples[\"span_head_char_start_idxs\"], examples[\"overflow_to_sample_mapping\"]):\n\n            original_example = original_dataset[example_overflow_to_sample_mapping]\n            original_example_span_head_char_start_idxs = original_example[\"span_head_char_start_idxs\"]\n            original_example_discourse_type_ids = original_example[\"discourse_type_ids\"]\n            char2discourse_id = {k: v for k, v in zip(\n                original_example_span_head_char_start_idxs, original_example_discourse_type_ids)}\n            current_example_discourse_type_ids = [char2discourse_id[char_idx]\n                                                  for char_idx in example_span_head_char_start_idxs]\n            discourse_type_ids.append(current_example_discourse_type_ids)\n        return {\"discourse_type_ids\": discourse_type_ids}\n\n    def compute_input_length(examples):\n        return {\"input_length\": [len(x) for x in examples[\"input_ids\"]]}\n\n    def sanity_check_head_tail(examples):\n        for head_idxs, tail_idxs in zip(examples[\"span_head_idxs\"], examples[\"span_tail_idxs\"]):\n            assert len(head_idxs) == len(tail_idxs)\n            for head, tail in zip(head_idxs, tail_idxs):\n                assert tail > head + 1, f\"head idxs: {head_idxs}, tail idxs {tail_idxs}\"\n\n    task_dataset = task_dataset.map(\n        tokenize_with_truncation,\n        batched=True,\n        remove_columns=task_dataset.column_names,\n        batch_size=len(task_dataset)\n    )\n\n    task_dataset = task_dataset.map(process_span, batched=True)\n    task_dataset = task_dataset.map(enforce_alignment, batched=True)\n    task_dataset = task_dataset.map(recompute_discourse_type_ids, batched=True)\n    task_dataset = task_dataset.map(get_global_attention_mask, batched=True)\n\n    task_dataset = task_dataset.map(sanity_check_head_tail, batched=True)\n\n    # no need to run on empty set\n    task_dataset = task_dataset.filter(lambda example: len(example['span_head_idxs']) != 0)\n    task_dataset = task_dataset.map(compute_input_length, batched=True)\n\n    if mode != \"infer\":\n        task_dataset = task_dataset.map(recompute_labels, batched=True)\n\n    to_return = dict()\n    to_return[\"dataset\"] = task_dataset\n    to_return[\"original_dataset\"] = original_dataset\n    to_return[\"tokenizer\"] = tokenizer\n    return to_return","metadata":{"_uuid":"7b5102b9-cb77-4cbc-b71a-f986b57fb1e2","_cell_guid":"67add2e5-2142-4699-a61d-c59703f2bb26","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:16:20.394098Z","iopub.execute_input":"2022-08-23T12:16:20.394545Z","iopub.status.idle":"2022-08-23T12:16:20.521111Z","shell.execute_reply.started":"2022-08-23T12:16:20.394506Z","shell.execute_reply":"2022-08-23T12:16:20.520277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if use_exp212: \n    os.makedirs(config[\"model_dir\"], exist_ok=True)\n\n    print(\"creating the inference datasets...\")\n    infer_ds_dict = get_fast_dataset(config, test_df, essay_df, mode=\"infer\")\n    tokenizer = infer_ds_dict[\"tokenizer\"]\n    infer_dataset = infer_ds_dict[\"dataset\"]\n    print(infer_dataset)","metadata":{"_uuid":"7bdba8b1-1b1d-43c9-81b7-5b77a675a85d","_cell_guid":"dc046562-1469-40eb-b1b3-7450b4de9721","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:16:20.526277Z","iopub.execute_input":"2022-08-23T12:16:20.52834Z","iopub.status.idle":"2022-08-23T12:16:21.450122Z","shell.execute_reply.started":"2022-08-23T12:16:20.528302Z","shell.execute_reply":"2022-08-23T12:16:21.449226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if use_exp212: \n\n    config[\"len_tokenizer\"] = len(tokenizer)\n\n    infer_dataset = infer_dataset.sort(\"input_length\")\n\n    infer_dataset.set_format(\n        type=None,\n        columns=['input_ids', 'attention_mask', 'token_type_ids', 'span_head_idxs', 'global_attention_mask',\n                 'span_tail_idxs', 'discourse_type_ids', 'uids']\n    )","metadata":{"_uuid":"bf07c5b8-c5bb-4eeb-8a45-012c614196b6","_cell_guid":"2e13845b-f139-4ed1-aee3-c57c768a14be","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:16:21.452172Z","iopub.execute_input":"2022-08-23T12:16:21.45274Z","iopub.status.idle":"2022-08-23T12:16:21.465788Z","shell.execute_reply.started":"2022-08-23T12:16:21.452701Z","shell.execute_reply":"2022-08-23T12:16:21.464842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Loader","metadata":{"_uuid":"9035df93-677f-4fc9-a4c7-999604e6030f","_cell_guid":"0355029a-1af0-40ce-916a-61f77fb72308","trusted":true}},{"cell_type":"code","source":"from dataclasses import dataclass\n\nimport torch\nfrom transformers import DataCollatorWithPadding\n\n\n@dataclass\nclass CustomDataCollatorWithPadding(DataCollatorWithPadding):\n    \"\"\"\n    data collector for seq classification\n    \"\"\"\n\n    tokenizer = None\n    padding = True\n    max_length = None\n    pad_to_multiple_of = 512\n    return_tensors = \"pt\"\n\n    def __call__(self, features):\n        uids = [feature[\"uids\"] for feature in features]\n        discourse_type_ids = [feature[\"discourse_type_ids\"] for feature in features]\n        span_head_idxs = [feature[\"span_head_idxs\"] for feature in features]\n        span_tail_idxs = [feature[\"span_tail_idxs\"] for feature in features]\n        span_attention_mask = [[1]*len(feature[\"span_head_idxs\"]) for feature in features]\n        global_attention_mask = [feature[\"global_attention_mask\"] for feature in features]\n\n        labels = None\n        if \"labels\" in features[0].keys():\n            labels = [feature[\"labels\"] for feature in features]\n\n        batch = self.tokenizer.pad(\n            features,\n            padding=self.padding,\n            max_length=self.max_length,\n            pad_to_multiple_of=self.pad_to_multiple_of,\n            return_tensors=None,\n        )\n\n        b_max = max([len(l) for l in span_head_idxs])\n        max_len = len(batch[\"input_ids\"][0])\n\n        default_head_idx = max(max_len - 10, 1)  # for padding\n        default_tail_idx = max(max_len - 4, 1)  # for padding\n\n        batch[\"span_head_idxs\"] = [\n            ex_span_head_idxs + [default_head_idx] * (b_max - len(ex_span_head_idxs)) for ex_span_head_idxs in span_head_idxs\n        ]\n\n        batch[\"uids\"] = [ex_uids + [-1] * (b_max - len(ex_uids)) for ex_uids in uids]\n        batch[\"discourse_type_ids\"] = [ex_discourse_type_ids + [0] *\n                                       (b_max - len(ex_discourse_type_ids)) for ex_discourse_type_ids in discourse_type_ids]\n\n        batch[\"span_tail_idxs\"] = [\n            ex_span_tail_idxs + [default_tail_idx] * (b_max - len(ex_span_tail_idxs)) for ex_span_tail_idxs in span_tail_idxs\n        ]\n\n        batch[\"span_attention_mask\"] = [\n            ex_discourse_masks + [0] * (b_max - len(ex_discourse_masks)) for ex_discourse_masks in span_attention_mask\n        ]\n\n        batch[\"global_attention_mask\"] = [\n            ex_global_attention_mask + [0] * (max_len - len(ex_global_attention_mask)) for ex_global_attention_mask in global_attention_mask\n        ]\n\n        if labels is not None:\n            batch[\"labels\"] = [ex_labels + [-1] * (b_max - len(ex_labels)) for ex_labels in labels]\n\n        # multitask labels\n        def _get_additional_labels(label_id):\n            if label_id == 0:\n                vec = [0, 0]\n            elif label_id == 1:\n                vec = [1, 0]\n            elif label_id == 2:\n                vec = [1, 1]\n            elif label_id == -1:\n                vec = [-1, -1]\n            else:\n                raise\n            return vec\n\n        if labels is not None:\n            additional_labels = []\n            for ex_labels in batch[\"labels\"]:\n                ex_additional_labels = [_get_additional_labels(el) for el in ex_labels]\n                additional_labels.append(ex_additional_labels)\n            batch[\"multitask_labels\"] = additional_labels\n        # pdb.set_trace()\n\n        batch = {k: (torch.tensor(v, dtype=torch.int64) if k != \"multitask_labels\" else torch.tensor(\n            v, dtype=torch.float32)) for k, v in batch.items()}\n        return batch","metadata":{"_uuid":"415e6ac8-b0aa-46ae-b441-17075561e37f","_cell_guid":"a91dfead-1eaf-40f2-a426-6486cd4cb51e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:16:21.828637Z","iopub.execute_input":"2022-08-23T12:16:21.82922Z","iopub.status.idle":"2022-08-23T12:16:21.847728Z","shell.execute_reply.started":"2022-08-23T12:16:21.829158Z","shell.execute_reply":"2022-08-23T12:16:21.846837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if use_exp212: \n    data_collector = CustomDataCollatorWithPadding(tokenizer=tokenizer)\n\n    infer_dl = DataLoader(\n        infer_dataset,\n        batch_size=config[\"infer_bs\"],\n        shuffle=False,\n        collate_fn=data_collector\n    )","metadata":{"_uuid":"52a3fa2b-b7a0-49ca-9eb1-6cc53b5acac9","_cell_guid":"9be96009-9f4c-4395-9b48-d5221a7aed64","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:16:22.364376Z","iopub.execute_input":"2022-08-23T12:16:22.365031Z","iopub.status.idle":"2022-08-23T12:16:22.382003Z","shell.execute_reply.started":"2022-08-23T12:16:22.364994Z","shell.execute_reply":"2022-08-23T12:16:22.380831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{"_uuid":"8974f0c8-a78d-4adc-9218-eaa4ca55cd72","_cell_guid":"00d34159-dfe5-43ba-909a-f4e30f1b3656","trusted":true}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nfrom torch.nn import LayerNorm\nfrom transformers import AutoConfig, AutoModel, BertConfig\nfrom transformers.models.bert.modeling_bert import BertAttention\n\n\nclass FeedbackModel(nn.Module):\n    \"\"\"\n    The feedback prize effectiveness model for fast approach\n    \"\"\"\n\n    def __init__(self, config):\n        print(\"==\"*40)\n        print(\"initializing the feedback model...\")\n\n        super(FeedbackModel, self).__init__()\n        self.config = config\n\n        # base transformer\n        base_config = AutoConfig.from_pretrained(self.config[\"base_model_path\"])\n        self.base_model = AutoModel.from_pretrained(self.config[\"base_model_path\"], config=base_config)\n\n        # dropouts\n        self.dropout = nn.Dropout(self.config[\"dropout\"])\n\n        self.num_labels = self.config[\"num_labels\"]\n\n        # multi-head attention over span representations\n        attention_config = BertConfig()\n        attention_config.update(\n            {\n                \"num_attention_heads\": self.base_model.config.num_attention_heads,\n                \"hidden_size\": self.base_model.config.hidden_size,\n                \"attention_probs_dropout_prob\": self.base_model.config.attention_probs_dropout_prob,\n                \"is_decoder\": False,\n\n            }\n        )\n        self.fpe_span_attention = BertAttention(attention_config, position_embedding_type=\"relative_key\")\n\n        # classification\n        hidden_size = self.base_model.config.hidden_size\n        feature_size = hidden_size\n        self.layer_norm = LayerNorm(feature_size, self.base_model.config.layer_norm_eps)\n\n        # # LSTM Head\n        self.fpe_lstm_layer = nn.LSTM(\n            input_size=feature_size,\n            hidden_size=hidden_size//2,\n            num_layers=1,\n            batch_first=True,\n            bidirectional=True,\n        )\n\n        self.classifier = nn.Linear(feature_size, self.num_labels)\n\n\n    def forward(\n        self,\n        input_ids,\n        token_type_ids,\n        attention_mask,\n        span_head_idxs,\n        span_tail_idxs,\n        span_attention_mask,\n        global_attention_mask,\n        labels=None,\n        multitask_labels=None,\n        **kwargs\n    ):\n\n        bs = input_ids.shape[0]  # batch size\n\n        outputs = self.base_model(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            global_attention_mask=global_attention_mask,\n        )\n        encoder_layer = outputs[0]\n\n        self.fpe_lstm_layer.flatten_parameters()\n        encoder_layer = self.fpe_lstm_layer(encoder_layer)[0]  # LSTM layer outputs\n\n        mean_feature_vector = []\n\n        for i in range(bs):\n            span_vec_i = []\n\n            for head, tail in zip(span_head_idxs[i], span_tail_idxs[i]):\n                # span feature\n                tmp = torch.mean(encoder_layer[i, head+1:tail], dim=0)  # [h]\n                span_vec_i.append(tmp)\n            span_vec_i = torch.stack(span_vec_i)  # (num_disourse, h)\n            mean_feature_vector.append(span_vec_i)\n\n        mean_feature_vector = torch.stack(mean_feature_vector)  # (bs, num_disourse, h)\n        mean_feature_vector = self.layer_norm(mean_feature_vector)\n\n        # attention mechanism\n        extended_span_attention_mask = span_attention_mask[:, None, None, :]\n        # extended_span_attention_mask = extended_span_attention_mask.to(dtype=self.dtype)  # fp16 compatibility\n        extended_span_attention_mask = (1.0 - extended_span_attention_mask) * -10000.0\n        feature_vector = self.fpe_span_attention(mean_feature_vector, extended_span_attention_mask)[0]\n\n        feature_vector = self.dropout(feature_vector)\n        logits = self.classifier(feature_vector)\n        \n        return logits","metadata":{"_uuid":"e91a3d52-3a0e-4abc-b525-ac0c2a278dae","_cell_guid":"672be3b8-9cae-4541-8c8f-4408e8e4e01a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:16:23.112886Z","iopub.execute_input":"2022-08-23T12:16:23.11343Z","iopub.status.idle":"2022-08-23T12:16:23.129999Z","shell.execute_reply.started":"2022-08-23T12:16:23.113394Z","shell.execute_reply":"2022-08-23T12:16:23.128951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference","metadata":{"_uuid":"9d0aa45a-f8a5-40ad-a95a-9113c254aca7","_cell_guid":"8a119564-9f82-46b0-b7db-9dbf31f2fefc","trusted":true}},{"cell_type":"code","source":"checkpoints = [\n    \"../input/exp212-longformer-l-prompt-mlm50/fpe_model_fold_0_best.pth.tar\",\n    \"../input/exp212-longformer-l-prompt-mlm50/fpe_model_fold_1_best.pth.tar\",\n    \"../input/exp212-longformer-l-prompt-mlm50/fpe_model_fold_2_best.pth.tar\",\n    \"../input/exp212-longformer-l-prompt-mlm50/fpe_model_fold_3_best.pth.tar\",\n    \"../input/exp212-longformer-l-prompt-mlm50/fpe_model_fold_4_best.pth.tar\",\n    \"../input/exp212-longformer-l-prompt-mlm50/fpe_model_fold_5_best.pth.tar\",\n    \"../input/exp212-longformer-l-prompt-mlm50/fpe_model_fold_6_best.pth.tar\",\n    \"../input/exp212-longformer-l-prompt-mlm50/fpe_model_fold_7_best.pth.tar\",\n]","metadata":{"_uuid":"ddc089c2-0ba0-4222-b399-69897971f75b","_cell_guid":"b15e4c79-521f-4ed2-8404-8e01b45fa2e6","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:16:23.975449Z","iopub.execute_input":"2022-08-23T12:16:23.976042Z","iopub.status.idle":"2022-08-23T12:16:23.98077Z","shell.execute_reply.started":"2022-08-23T12:16:23.976005Z","shell.execute_reply":"2022-08-23T12:16:23.979886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def inference_fn(model, infer_dl, model_id):\n    all_preds = []\n    all_uids = []\n    accelerator = Accelerator()\n    model, infer_dl = accelerator.prepare(model, infer_dl)\n    \n    model.eval()\n    tk0 = tqdm(infer_dl, total=len(infer_dl))\n    \n    for batch in tk0:\n        with torch.no_grad():\n            logits = model(**batch) # (b, nd, 3)\n            batch_preds = F.softmax(logits, dim=-1)\n            batch_uids = batch[\"uids\"]\n        all_preds.append(batch_preds)\n        all_uids.append(batch_uids)\n    \n    all_preds = [p.to('cpu').detach().numpy().tolist() for p in all_preds]\n    all_preds = list(chain(*all_preds))\n    flat_preds = list(chain(*all_preds))\n    \n    all_uids = [p.to('cpu').detach().numpy().tolist() for p in all_uids]\n    all_uids = list(chain(*all_uids))\n    flat_uids = list(chain(*all_uids))    \n    \n    preds_df = pd.DataFrame(flat_preds)\n    preds_df.columns = [\"Ineffective\", \"Adequate\", \"Effective\"]\n    preds_df[\"span_uid\"] = flat_uids # SORTED_DISCOURSE_IDS\n    preds_df = preds_df[preds_df[\"span_uid\"]>=0].copy()\n    preds_df[\"discourse_id\"] = preds_df[\"span_uid\"].map(idx2discourse)\n    preds_df = preds_df[[\"discourse_id\", \"Ineffective\", \"Adequate\", \"Effective\"]].copy()\n    preds_df.to_csv(f\"exp212_longformer_model_preds_{model_id}.csv\", index=False)\n    \nif use_exp212: \n    for model_id, checkpoint in enumerate(checkpoints):\n        print(f\"infering from {checkpoint}\")\n        model = FeedbackModel(config)\n        ckpt = torch.load(checkpoint)\n        print(f\"model performance on validation set = {ckpt['loss']}\")\n        model.load_state_dict(ckpt['state_dict'])\n        inference_fn(model, infer_dl, model_id)\n\n    del model\n    del tokenizer, infer_dataset, infer_ds_dict, data_collector, infer_dl\n    gc.collect()\n    torch.cuda.empty_cache()","metadata":{"_uuid":"6e93c21b-3bfd-4299-8490-37c731aedce6","_cell_guid":"a56a687b-c6ae-4b48-bd62-77d2bb7527f0","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:16:24.396573Z","iopub.execute_input":"2022-08-23T12:16:24.397686Z","iopub.status.idle":"2022-08-23T12:21:58.55966Z","shell.execute_reply.started":"2022-08-23T12:16:24.397639Z","shell.execute_reply":"2022-08-23T12:21:58.558647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if use_exp212: \n    import glob\n    import pandas as pd\n\n    csvs = glob.glob(\"exp212_longformer_model_preds_*.csv\")\n\n    idx = []\n    preds = []\n\n    for csv_idx, csv in enumerate(csvs):\n\n        print(\"==\"*40)\n        print(f\"preds in {csv}\")\n        df = pd.read_csv(csv)\n        df = df.sort_values(by=[\"discourse_id\"])\n        print(df.head(10))\n        print(\"==\"*40)\n\n        temp_preds = df.drop([\"discourse_id\"], axis=1).values\n        if csv_idx == 0:\n            idx = list(df[\"discourse_id\"])\n            preds = temp_preds\n        else:\n            preds += temp_preds\n\n    preds = preds / len(csvs)\n\n    exp212_df = pd.DataFrame()\n    exp212_df[\"discourse_id\"]  = idx\n    exp212_df[\"Ineffective\"]   = preds[:, 0]\n    exp212_df[\"Adequate\"]      = preds[:, 1]\n    exp212_df[\"Effective\"]     = preds[:, 2]\n\n    exp212_df = exp212_df.groupby(\"discourse_id\")[[\"Ineffective\", \"Adequate\", \"Effective\"]].agg(np.mean).reset_index()","metadata":{"_uuid":"d335edd0-21b5-4ce0-92fd-c2e7c5f03931","_cell_guid":"a71ac2d3-41d9-4520-a6e3-7050fdd6e275","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:21:58.565079Z","iopub.execute_input":"2022-08-23T12:21:58.567714Z","iopub.status.idle":"2022-08-23T12:21:58.649059Z","shell.execute_reply.started":"2022-08-23T12:21:58.567672Z","shell.execute_reply":"2022-08-23T12:21:58.648144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if use_exp212: \n    display(exp212_df.head())","metadata":{"_uuid":"35530462-e1de-410b-8644-0311028f77b5","_cell_guid":"8b455457-0bde-4d33-89c1-900bb709863a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:21:58.653313Z","iopub.execute_input":"2022-08-23T12:21:58.654009Z","iopub.status.idle":"2022-08-23T12:21:58.672863Z","shell.execute_reply.started":"2022-08-23T12:21:58.653964Z","shell.execute_reply":"2022-08-23T12:21:58.671856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EXP3 & EXP4 - delv3 Fast Model - SPAN MLM 20%\n\nModel trained with span mlm 20% backbone","metadata":{"_uuid":"ec574016-01d7-482b-8d73-ae25e02d1cc1","_cell_guid":"437c1d3e-674d-476b-89d6-647a7c4a0287","trusted":true}},{"cell_type":"markdown","source":"## Config","metadata":{"_uuid":"c7030dd2-4a5a-4509-b138-18247f868e63","_cell_guid":"6363ba39-4f5c-4c6f-9f46-8c40c9f3d78d","trusted":true}},{"cell_type":"code","source":"config = \"\"\"{\n    \"debug\": false,\n\n    \"base_model_path\": \"../input/fpe-tapt-delv3-span-mlm-pos-1024\",\n    \"model_dir\": \"./outputs\",\n\n    \"max_length\": 1024,\n    \"stride\": 256,\n    \"num_labels\": 5,\n    \"dropout\": 0.1,\n    \"infer_bs\": 8\n}\n\"\"\"\nconfig = json.loads(config)","metadata":{"_uuid":"9b5fdebd-6059-45cc-82dc-763be89e9c35","_cell_guid":"ef346acb-4bbc-4ba0-8171-183e759ed60f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:21:58.678434Z","iopub.execute_input":"2022-08-23T12:21:58.678856Z","iopub.status.idle":"2022-08-23T12:21:58.688308Z","shell.execute_reply.started":"2022-08-23T12:21:58.678822Z","shell.execute_reply":"2022-08-23T12:21:58.687231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset","metadata":{"_uuid":"5460424d-5aae-4c67-a4ec-fca5961d62e9","_cell_guid":"926ab006-eec3-4b95-bb91-0f264b31bc48","trusted":true}},{"cell_type":"code","source":"import os\nimport re\nfrom copy import deepcopy\nfrom itertools import chain\n\nimport pandas as pd\nfrom datasets import Dataset\nfrom tokenizers import AddedToken\nfrom transformers import AutoTokenizer\n\n\n#--------------- Tokenizer ----------------------------------------------#\n\ndef get_tokenizer(config):\n    \"\"\"load the tokenizer\"\"\"\n\n    print(\"using auto tokenizer\")\n    tokenizer = AutoTokenizer.from_pretrained(config[\"base_model_path\"])\n\n    print(\"==\"*40)\n    print(f\"tokenizer len: {len(tokenizer)}\")\n    print(f\"tokenizer test: {tokenizer.tokenize('This [B-SPAN] is useful [E-SPAN]')}\")\n    print(\"==\"*40)\n    return tokenizer\n\n\n#--------------- Processing ---------------------------------------------#\n\nTOKEN_MAP = {\n    \"Lead\": [\"Lead [LEAD]\", \"[END]\"],\n    \"Position\": [\"Position [POSITION]\", \"[END]\"],\n    \"Claim\": [\"Claim [CLAIM]\", \"[END]\"],\n    \"Counterclaim\": [\"Counterclaim [COUNTER_CLAIM]\", \"[END]\"],\n    \"Rebuttal\": [\"Rebuttal [REBUTTAL]\", \"[END]\"],\n    \"Evidence\": [\"Evidence [EVIDENCE]\", \"[END]\"],\n    \"Concluding Statement\": [\"Concluding Statement [CONCLUDING_STATEMENT]\", \"[END]\"]\n}\n\n\nDISCOURSE_START_TOKENS = [\n    \"[LEAD]\",\n    \"[POSITION]\",\n    \"[CLAIM]\",\n    \"[COUNTER_CLAIM]\",\n    \"[REBUTTAL]\",\n    \"[EVIDENCE]\",\n    \"[CONCLUDING_STATEMENT]\"\n]\n\nDISCOURSE_END_TOKENS = [\n    \"[END]\",\n]\n\n#--------------- Span Detection ---------------------------------------------#\n\ndef relaxed_search(text, substring, min_length=2, fraction=0.99999):\n    \"\"\"\n    Returns substring's span from the given text with the certain precision.\n    \"\"\"\n\n    position = text.find(substring)\n    substring_length = len(substring)\n    if position == -1:\n        half_length = int(substring_length * fraction)\n        half_substring = substring[:half_length]\n        half_substring_length = len(half_substring)\n        if half_substring_length < min_length:\n            return [-1, 0]\n        else:\n            return relaxed_search(text=text,\n                                  substring=half_substring,\n                                  min_length=min_length,\n                                  fraction=fraction)\n\n    span = [position, position+substring_length]\n    return span\n\n\ndef build_span_map(discourse_list, essay_text):\n    reading_head = 0\n    to_return = dict()\n\n    for cur_discourse in discourse_list:\n        if cur_discourse not in to_return:\n            to_return[cur_discourse] = []\n\n        matches = re.finditer(re.escape(r'{}'.format(cur_discourse)), essay_text)\n        for match in matches:\n            span_start, span_end = match.span()\n            if span_end <= reading_head:\n                continue\n            to_return[cur_discourse].append(match.span())\n            reading_head = span_end\n            break\n\n    # post process\n    for cur_discourse in discourse_list:\n        if not to_return[cur_discourse]:\n            print(\"resorting to relaxed search...\")\n            to_return[cur_discourse] = [relaxed_search(essay_text, cur_discourse)]\n    return to_return\n\n\ndef get_substring_span(texts, mapping):\n    result = []\n    for text in texts:\n        ans = mapping[text].pop(0)\n        result.append(ans)\n    return result\n\n\ndef process_essay(essay_id, essay_text, anno_df):\n    \"\"\"insert newly added tokens in the essay text\n    \"\"\"\n    tmp_df = anno_df[anno_df[\"essay_id\"] == essay_id].copy()\n    tmp_df = tmp_df.sort_values(by=\"discourse_start\")\n    buffer = 0\n\n    for _, row in tmp_df.iterrows():\n        s, e, d_type = int(row.discourse_start) + buffer, int(row.discourse_end) + buffer, row.discourse_type\n        s_tok, e_tok = TOKEN_MAP[d_type]\n        essay_text = \" \".join([essay_text[:s], s_tok, essay_text[s:e], e_tok, essay_text[e:]])\n        buffer += len(s_tok) + len(e_tok) + 4\n\n    essay_text = \"[SOE]\" + essay_text + \"[EOE]\"\n    return essay_text\n\n\ndef process_input_df(anno_df, notes_df):\n    \"\"\"pre-process input dataframe\n\n    :param df: input dataframe\n    :type df: pd.DataFrame\n    :return: processed dataframe\n    :rtype: pd.DataFrame\n    \"\"\"\n    notes_df = deepcopy(notes_df)\n    anno_df = deepcopy(anno_df)\n\n    #------------------- Pre-Process Essay Text --------------------------#\n    anno_df[\"discourse_text\"] = anno_df[\"discourse_text\"].apply(lambda x: x.strip())  # pre-process\n    if \"discourse_effectiveness\" in anno_df.columns:\n        anno_df = anno_df[[\"discourse_id\", \"essay_id\", \"discourse_text\",\n                           \"discourse_type\", \"discourse_effectiveness\", \"uid\"]].copy()\n    else:\n        anno_df = anno_df[[\"discourse_id\", \"essay_id\", \"discourse_text\", \"discourse_type\", \"uid\"]].copy()\n\n\n    tmp_df = anno_df.groupby(\"essay_id\")[[\"discourse_id\", \"discourse_text\"]].agg(list).reset_index()\n    tmp_df = pd.merge(tmp_df, notes_df, on=\"essay_id\", how=\"left\")\n    tmp_df[\"span_map\"] = tmp_df[[\"discourse_text\", \"essay_text\"]].apply(\n        lambda x: build_span_map(x[0], x[1]), axis=1)\n    tmp_df[\"span\"] = tmp_df[[\"discourse_text\", \"span_map\"]].apply(\n        lambda x: get_substring_span(x[0], x[1]), axis=1)\n\n    all_discourse_ids = list(chain(*tmp_df[\"discourse_id\"].values))\n    all_discourse_spans = list(chain(*tmp_df[\"span\"].values))\n    span_df = pd.DataFrame()\n    span_df[\"discourse_id\"] = all_discourse_ids\n    span_df[\"span\"] = all_discourse_spans\n    span_df[\"discourse_start\"] = span_df[\"span\"].apply(lambda x: x[0])\n    span_df[\"discourse_end\"] = span_df[\"span\"].apply(lambda x: x[1])\n    span_df = span_df.drop(columns=\"span\")\n\n    anno_df = pd.merge(anno_df, span_df, on=\"discourse_id\", how=\"left\")\n    # anno_df = pd.merge(anno_df, notes_df, on=\"essay_id\", how=\"left\")\n\n    print(\"==\"*40)\n    print(\"processing essay text and inserting new tokens at span boundaries\")\n    notes_df[\"essay_text\"] = notes_df[[\"essay_id\", \"essay_text\"]].apply(\n        lambda x: process_essay(x[0], x[1], anno_df), axis=1\n    )\n    print(\"==\"*40)\n\n    anno_df = anno_df.drop(columns=[\"discourse_start\", \"discourse_end\"])\n    notes_df = notes_df.drop_duplicates(subset=[\"essay_id\"])[[\"essay_id\", \"essay_text\"]].copy()\n\n    anno_df = pd.merge(anno_df, notes_df, on=\"essay_id\", how=\"left\")\n\n    if \"discourse_effectiveness\" in anno_df.columns:\n        grouped_df = anno_df.groupby(\"essay_id\")[[\"uid\", \"discourse_effectiveness\", \"discourse_type\"]].agg(list).reset_index()\n    else:\n        grouped_df = anno_df.groupby(\"essay_id\")[[\"uid\", \"discourse_type\"]].agg(list).reset_index()\n\n    grouped_df = pd.merge(grouped_df, notes_df, on=\"essay_id\", how=\"left\")\n    grouped_df = grouped_df.rename(columns={\"uid\": \"uids\"})\n\n    return grouped_df\n\n\n#--------------- Dataset ----------------------------------------------#\n\n\nclass AuxFeedbackDataset:\n    \"\"\"Dataset class for feedback prize effectiveness task\n    \"\"\"\n\n    def __init__(self, config):\n        self.config = config\n\n        self.label2id = {\n            \"Ineffective\": 0,\n            \"Adequate\": 1,\n            \"Effective\": 2,\n        }\n\n        self.discourse_type2id = {\n            \"Lead\": 1,\n            \"Position\": 2,\n            \"Claim\": 3,\n            \"Counterclaim\": 4,\n            \"Rebuttal\": 5,\n            \"Evidence\": 6,\n            \"Concluding Statement\": 7,\n        }\n\n        self.id2label = {v: k for k, v in self.label2id.items()}\n        self.load_tokenizer()\n\n    def load_tokenizer(self):\n        \"\"\"load tokenizer as per config \n        \"\"\"\n        self.tokenizer = get_tokenizer(self.config)\n        self.discourse_token_ids = set(self.tokenizer.convert_tokens_to_ids(DISCOURSE_START_TOKENS))\n        self.discourse_end_ids = set(self.tokenizer.convert_tokens_to_ids(DISCOURSE_END_TOKENS))\n\n    def tokenize_function(self, examples):\n        tz = self.tokenizer(\n            examples[\"essay_text\"],\n            padding=False,\n            truncation=False,  # no truncation at first\n            add_special_tokens=True,\n            return_offsets_mapping=True,\n        )\n        return tz\n\n    def process_spans(self, examples):\n\n        span_head_char_start_idxs, span_tail_char_end_idxs = [], []\n        span_head_idxs, span_tail_idxs = [], []\n\n        for example_input_ids, example_offset_mapping, example_uids in zip(examples[\"input_ids\"], examples[\"offset_mapping\"], examples[\"uids\"]):\n            example_span_head_idxs = [pos for pos, this_id in enumerate(\n                example_input_ids) if this_id in self.discourse_token_ids]\n            example_span_tail_idxs = [pos for pos, this_id in enumerate(\n                example_input_ids) if this_id in self.discourse_end_ids]\n\n            example_span_head_char_start_idxs = [example_offset_mapping[pos][0] for pos in example_span_head_idxs]\n            example_span_tail_char_end_idxs = [example_offset_mapping[pos][1] for pos in example_span_tail_idxs]\n\n            span_head_char_start_idxs.append(example_span_head_char_start_idxs)\n            span_tail_char_end_idxs.append(example_span_tail_char_end_idxs)\n\n            span_head_idxs.append(example_span_head_idxs)\n            span_tail_idxs.append(example_span_tail_idxs)\n\n        return {\n            \"span_head_idxs\": span_head_idxs,\n            \"span_tail_idxs\": span_tail_idxs,\n            \"span_head_char_start_idxs\": span_head_char_start_idxs,\n            \"span_tail_char_end_idxs\": span_tail_char_end_idxs,\n        }\n\n    def generate_labels(self, examples):\n        labels = []\n        for example_labels, example_uids in zip(examples[\"discourse_effectiveness\"], examples[\"uids\"]):\n            labels.append([self.label2id[l] for l in example_labels])\n        return {\"labels\": labels}\n\n    def generate_discourse_type_ids(self, examples):\n        discourse_type_ids = []\n        for example_discourse_types in examples[\"discourse_type\"]:\n            discourse_type_ids.append([self.discourse_type2id[dt] for dt in example_discourse_types])\n        return {\"discourse_type_ids\": discourse_type_ids}\n\n    def compute_input_length(self, examples):\n        return {\"input_length\": [len(x) for x in examples[\"input_ids\"]]}\n\n    def sanity_check_head_tail(self, examples):\n        for head_idxs, tail_idxs in zip(examples[\"span_head_idxs\"], examples[\"span_tail_idxs\"]):\n            assert len(head_idxs) == len(tail_idxs)\n            for head, tail in zip(head_idxs, tail_idxs):\n                assert tail > head + 1\n\n    def sanity_check_head_labels(self, examples):\n        for head_idxs, head_labels in zip(examples[\"span_head_idxs\"], examples[\"labels\"]):\n            assert len(head_idxs) == len(head_labels)\n\n    def get_dataset(self, df, essay_df, mode='train'):\n        \"\"\"main api for creating the Feedback dataset\n\n        :param df: input annotation dataframe\n        :type df: pd.DataFrame\n        :param essay_df: dataframe with essay texts\n        :type essay_df: pd.DataFrame\n        :param mode: check if required for train or infer, defaults to 'train'\n        :type mode: str, optional\n        :return: the created dataset\n        :rtype: Dataset\n        \"\"\"\n        df = process_input_df(df, essay_df)\n\n        # save a sample for sanity checks\n        sample_df = df.sample(min(16, len(df)))\n        sample_df.to_csv(os.path.join(self.config[\"model_dir\"], f\"{mode}_df_processed.csv\"), index=False)\n\n        task_dataset = Dataset.from_pandas(df)\n        task_dataset = task_dataset.map(self.tokenize_function, batched=True)\n        task_dataset = task_dataset.map(self.compute_input_length, batched=True)\n        task_dataset = task_dataset.map(self.process_spans, batched=True)\n#         print(task_dataset)\n#         # todo check edge cases\n#         task_dataset = task_dataset.filter(lambda example: len(example['span_head_idxs']) == len(\n#             example['span_tail_idxs']))  # no need to run on empty set\n#         print(task_dataset)\n        task_dataset = task_dataset.map(self.generate_discourse_type_ids, batched=True)\n#         task_dataset = task_dataset.map(self.sanity_check_head_tail, batched=True)\n\n        if mode != \"infer\":\n            task_dataset = task_dataset.map(self.generate_labels, batched=True)\n            task_dataset = task_dataset.map(self.sanity_check_head_labels, batched=True)\n\n        try:\n            task_dataset = task_dataset.remove_columns(column_names=[\"__index_level_0__\"])\n        except Exception as e:\n            pass\n        return df, task_dataset\n\n#--------------- dataset with truncation ---------------------------------------------#\n\n\ndef get_fast_dataset(config, df, essay_df, mode=\"train\"):\n    \"\"\"Function to get fast approach dataset with truncation & sliding window\n    \"\"\"\n    dataset_creator = AuxFeedbackDataset(config)\n    _, task_dataset = dataset_creator.get_dataset(df, essay_df, mode=mode)\n\n    original_dataset = deepcopy(task_dataset)\n    tokenizer = dataset_creator.tokenizer\n    START_IDS = dataset_creator.discourse_token_ids\n    END_IDS = dataset_creator.discourse_end_ids\n\n    def tokenize_with_truncation(examples):\n        tz = tokenizer(\n            examples[\"essay_text\"],\n            padding=False,\n            truncation=True,\n            add_special_tokens=True,\n            return_offsets_mapping=True,\n            max_length=config[\"max_length\"],\n            stride=config[\"stride\"],\n            return_overflowing_tokens=True,\n        )\n        return tz\n\n    def process_span(examples):\n        span_head_idxs, span_tail_idxs = [], []\n        span_head_char_start_idxs, span_tail_char_end_idxs = [], []\n\n        buffer = 25  # do not include a head if it is within buffer distance away from last token\n\n        for example_input_ids, example_offset_mapping in zip(examples[\"input_ids\"], examples[\"offset_mapping\"]):\n            # ------------------- Span Heads -----------------------------------------#\n            if len(example_input_ids) < config[\"max_length\"]:  # no truncation\n                head_candidate = [pos for pos, this_id in enumerate(example_input_ids) if this_id in START_IDS]\n            else:\n                head_candidate = [pos for pos, this_id in enumerate(example_input_ids) if (\n                    (this_id in START_IDS) & (pos <= config[\"max_length\"]-buffer))]\n\n            n_heads = len(head_candidate)\n\n            # ------------------- Span Tails -----------------------------------------#\n            tail_candidate = [pos for pos, this_id in enumerate(example_input_ids) if this_id in END_IDS]\n\n            # ------------------- Edge Cases -----------------------------------------#\n            # 1. A tail occurs before the first head in the sequence due to truncation\n            if (len(tail_candidate) > 0) & (len(head_candidate) > 0):\n                if tail_candidate[0] < head_candidate[0]:  # truncation effect\n                    # print(f\"check: heads: {head_candidate}, tails {tail_candidate}\")\n                    tail_candidate = tail_candidate[1:]  # shift by one\n\n            # 2. Tail got chopped off due to truncation but the corresponding head is still there\n            if len(tail_candidate) < n_heads:\n                assert len(tail_candidate) + 1 == n_heads\n                assert len(example_input_ids) == config[\"max_length\"]  # should only happen if input text is truncated\n                tail_candidate.append(config[\"max_length\"]-2)  # the token before [SEP] token\n\n            # 3. Additional tails remain in the buffer region\n            if len(tail_candidate) > len(head_candidate):\n                tail_candidate = tail_candidate[:len(head_candidate)]\n\n            # ------------------- Create the fields ------------------------------------#\n            example_span_head_char_start_idxs = [example_offset_mapping[pos][0] for pos in head_candidate]\n            example_span_tail_char_end_idxs = [example_offset_mapping[pos][1] for pos in tail_candidate]\n\n            span_head_idxs.append(head_candidate)\n            span_tail_idxs.append(tail_candidate)\n            span_head_char_start_idxs.append(example_span_head_char_start_idxs)\n            span_tail_char_end_idxs.append(example_span_tail_char_end_idxs)\n\n        return {\n            \"span_head_idxs\": span_head_idxs,\n            \"span_tail_idxs\": span_tail_idxs,\n            \"span_head_char_start_idxs\": span_head_char_start_idxs,\n            \"span_tail_char_end_idxs\": span_tail_char_end_idxs,\n        }\n\n    def enforce_alignment(examples):\n        uids = []\n\n        for example_span_head_char_start_idxs, example_overflow_to_sample_mapping in zip(\n                examples[\"span_head_char_start_idxs\"], examples[\"overflow_to_sample_mapping\"]):\n\n            original_example = original_dataset[example_overflow_to_sample_mapping]\n            original_example_span_head_char_start_idxs = original_example[\"span_head_char_start_idxs\"]\n            original_example_uids = original_example[\"uids\"]\n            char2uid = {k: v for k, v in zip(original_example_span_head_char_start_idxs, original_example_uids)}\n            current_example_uids = [char2uid[char_idx] for char_idx in example_span_head_char_start_idxs]\n            uids.append(current_example_uids)\n        return {\"uids\": uids}\n\n    def recompute_labels(examples):\n        labels = []\n        for example_span_head_char_start_idxs, example_overflow_to_sample_mapping in zip(\n                examples[\"span_head_char_start_idxs\"], examples[\"overflow_to_sample_mapping\"]):\n\n            original_example = original_dataset[example_overflow_to_sample_mapping]\n            original_example_span_head_char_start_idxs = original_example[\"span_head_char_start_idxs\"]\n            original_example_labels = original_example[\"labels\"]\n            char2label = {k: v for k, v in zip(original_example_span_head_char_start_idxs, original_example_labels)}\n            current_example_labels = [char2label[char_idx] for char_idx in example_span_head_char_start_idxs]\n            labels.append(current_example_labels)\n        return {\"labels\": labels}\n\n    def recompute_discourse_type_ids(examples):\n        discourse_type_ids = []\n        for example_span_head_char_start_idxs, example_overflow_to_sample_mapping in zip(\n                examples[\"span_head_char_start_idxs\"], examples[\"overflow_to_sample_mapping\"]):\n\n            original_example = original_dataset[example_overflow_to_sample_mapping]\n            original_example_span_head_char_start_idxs = original_example[\"span_head_char_start_idxs\"]\n            original_example_discourse_type_ids = original_example[\"discourse_type_ids\"]\n            char2discourse_id = {k: v for k, v in zip(\n                original_example_span_head_char_start_idxs, original_example_discourse_type_ids)}\n            current_example_discourse_type_ids = [char2discourse_id[char_idx]\n                                                  for char_idx in example_span_head_char_start_idxs]\n            discourse_type_ids.append(current_example_discourse_type_ids)\n        return {\"discourse_type_ids\": discourse_type_ids}\n\n    def compute_input_length(examples):\n        return {\"input_length\": [len(x) for x in examples[\"input_ids\"]]}\n\n    def sanity_check_head_tail(examples):\n        for head_idxs, tail_idxs in zip(examples[\"span_head_idxs\"], examples[\"span_tail_idxs\"]):\n            assert len(head_idxs) == len(tail_idxs)\n            for head, tail in zip(head_idxs, tail_idxs):\n                assert tail > head + 1, f\"head idxs: {head_idxs}, tail idxs {tail_idxs}\"\n\n    task_dataset = task_dataset.map(\n        tokenize_with_truncation,\n        batched=True,\n        remove_columns=task_dataset.column_names,\n        batch_size=len(task_dataset)\n    )\n\n    task_dataset = task_dataset.map(process_span, batched=True)\n    task_dataset = task_dataset.map(enforce_alignment, batched=True)\n    task_dataset = task_dataset.map(recompute_discourse_type_ids, batched=True)\n    # task_dataset = task_dataset.map(sanity_check_head_tail, batched=True)\n\n    # no need to run on empty set\n    task_dataset = task_dataset.filter(lambda example: len(example['span_head_idxs']) != 0)\n    task_dataset = task_dataset.map(compute_input_length, batched=True)\n\n    to_return = dict()\n    to_return[\"dataset\"] = task_dataset\n    to_return[\"original_dataset\"] = original_dataset\n    to_return[\"tokenizer\"] = tokenizer\n    return to_return","metadata":{"_uuid":"c102231c-3122-4a2e-8fa0-4c1d1dd3895c","_cell_guid":"1db9acc5-9bff-448e-86f8-202c068d19e7","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:21:58.693675Z","iopub.execute_input":"2022-08-23T12:21:58.694221Z","iopub.status.idle":"2022-08-23T12:21:58.819801Z","shell.execute_reply.started":"2022-08-23T12:21:58.694165Z","shell.execute_reply":"2022-08-23T12:21:58.819027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if use_exp3 or use_exp4:\n    os.makedirs(config[\"model_dir\"], exist_ok=True)\n\n    print(\"creating the inference datasets...\")\n    infer_ds_dict = get_fast_dataset(config, test_df, essay_df, mode=\"infer\")\n    tokenizer = infer_ds_dict[\"tokenizer\"]\n    infer_dataset = infer_ds_dict[\"dataset\"]\n    print(infer_dataset)","metadata":{"_uuid":"80811a33-5291-427a-9526-3f6e0401d52b","_cell_guid":"7fc04660-828a-49e7-98ed-f59566630daf","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:21:58.823431Z","iopub.execute_input":"2022-08-23T12:21:58.823719Z","iopub.status.idle":"2022-08-23T12:21:58.833164Z","shell.execute_reply.started":"2022-08-23T12:21:58.823693Z","shell.execute_reply":"2022-08-23T12:21:58.832191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if use_exp3 or use_exp4:\n    config[\"len_tokenizer\"] = len(tokenizer)\n\n    infer_dataset = infer_dataset.sort(\"input_length\")\n\n    infer_dataset.set_format(\n        type=None,\n        columns=['input_ids', 'attention_mask', 'token_type_ids', 'span_head_idxs',\n                 'span_tail_idxs', 'discourse_type_ids', 'uids']\n    )","metadata":{"_uuid":"479dce1d-8030-4b82-83b4-0e09695956c2","_cell_guid":"013ee0e4-4d50-4513-8f32-cbedf0a8c1da","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:21:58.83505Z","iopub.execute_input":"2022-08-23T12:21:58.835913Z","iopub.status.idle":"2022-08-23T12:21:58.842429Z","shell.execute_reply.started":"2022-08-23T12:21:58.835696Z","shell.execute_reply":"2022-08-23T12:21:58.841437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## DataLoader","metadata":{"_uuid":"1d33fe83-0082-4ad2-8554-4cbbbe3f3e51","_cell_guid":"e59a5476-1715-4732-b224-7d4f5b17ed58","trusted":true}},{"cell_type":"code","source":"from dataclasses import dataclass\n\nimport torch\nfrom transformers import DataCollatorWithPadding\n\n\n@dataclass\nclass CustomDataCollatorWithPadding(DataCollatorWithPadding):\n    \"\"\"\n    data collector for seq classification\n    \"\"\"\n\n    tokenizer = None\n    padding = True\n    max_length = None\n    pad_to_multiple_of = None\n    return_tensors = \"pt\"\n\n    def __call__(self, features):\n        uids = [feature[\"uids\"] for feature in features]\n        discourse_type_ids = [feature[\"discourse_type_ids\"] for feature in features]\n        span_head_idxs = [feature[\"span_head_idxs\"] for feature in features]\n        span_tail_idxs = [feature[\"span_tail_idxs\"] for feature in features]\n        span_attention_mask = [[1]*len(feature[\"span_head_idxs\"]) for feature in features]\n\n        labels = None\n        if \"labels\" in features[0].keys():\n            labels = [feature[\"labels\"] for feature in features]\n\n        batch = self.tokenizer.pad(\n            features,\n            padding=self.padding,\n            max_length=self.max_length,\n            pad_to_multiple_of=self.pad_to_multiple_of,\n            return_tensors=None,\n        )\n\n        b_max = max([len(l) for l in span_head_idxs])\n        max_len = len(batch[\"input_ids\"][0])\n\n        default_head_idx = max(max_len - 10, 1)  # for padding\n        default_tail_idx = max(max_len - 4, 1)  # for padding\n\n        batch[\"span_head_idxs\"] = [\n            ex_span_head_idxs + [default_head_idx] * (b_max - len(ex_span_head_idxs)) for ex_span_head_idxs in span_head_idxs\n        ]\n\n        batch[\"uids\"] = [ex_uids + [-1] * (b_max - len(ex_uids)) for ex_uids in uids]\n        batch[\"discourse_type_ids\"] = [ex_discourse_type_ids + [0] *\n                                       (b_max - len(ex_discourse_type_ids)) for ex_discourse_type_ids in discourse_type_ids]\n\n        batch[\"span_tail_idxs\"] = [\n            ex_span_tail_idxs + [default_tail_idx] * (b_max - len(ex_span_tail_idxs)) for ex_span_tail_idxs in span_tail_idxs\n        ]\n\n        batch[\"span_attention_mask\"] = [\n            ex_discourse_masks + [0] * (b_max - len(ex_discourse_masks)) for ex_discourse_masks in span_attention_mask\n        ]\n\n        if labels is not None:\n            batch[\"labels\"] = [ex_labels + [-1] * (b_max - len(ex_labels)) for ex_labels in labels]\n\n        def _get_additional_labels(label_id):\n            if label_id == 0:\n                vec = [0, 0]\n            elif label_id == 1:\n                vec = [1, 0]\n            elif label_id == 2:\n                vec = [1, 1]\n            elif label_id == -1:\n                vec = [-1, -1]\n            else:\n                raise\n            return vec\n\n        if labels is not None:\n            additional_labels = []\n            for ex_labels in batch[\"labels\"]:\n                ex_additional_labels = [_get_additional_labels(el) for el in ex_labels]\n                additional_labels.append(ex_additional_labels)\n            batch[\"multitask_labels\"] = additional_labels\n\n        # batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n        batch = {k: (torch.tensor(v, dtype=torch.int64) if k != \"multitask_labels\" else torch.tensor(\n            v, dtype=torch.float32)) for k, v in batch.items()}\n        return batch","metadata":{"_uuid":"53d30d64-e0c8-46da-828e-1ab9a30b67b5","_cell_guid":"18af5d69-b259-4674-88e9-e0ed65d2fea0","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:21:58.844291Z","iopub.execute_input":"2022-08-23T12:21:58.845244Z","iopub.status.idle":"2022-08-23T12:21:58.872923Z","shell.execute_reply.started":"2022-08-23T12:21:58.845209Z","shell.execute_reply":"2022-08-23T12:21:58.872233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if use_exp3 or use_exp4:\n    data_collector = CustomDataCollatorWithPadding(tokenizer=tokenizer)\n\n    infer_dl = DataLoader(\n        infer_dataset,\n        batch_size=config[\"infer_bs\"],\n        shuffle=False,\n        collate_fn=data_collector\n    )","metadata":{"_uuid":"1de27134-5fb3-4bd2-92dd-1bc67232aef6","_cell_guid":"7be6b33f-c961-479e-a9a8-c21c2ae75bab","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:21:58.874746Z","iopub.execute_input":"2022-08-23T12:21:58.87564Z","iopub.status.idle":"2022-08-23T12:21:58.882779Z","shell.execute_reply.started":"2022-08-23T12:21:58.87549Z","shell.execute_reply":"2022-08-23T12:21:58.881708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{"_uuid":"09cf83ef-9746-4213-88c1-ad98638f630c","_cell_guid":"50c3a5a0-24b9-41a7-9ffc-cf23bcfe951e","trusted":true}},{"cell_type":"code","source":"import gc\nimport pdb\nfrom collections import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import LayerNorm\n\nimport torch.utils.checkpoint\nfrom transformers import AutoConfig, AutoModel\nfrom transformers.models.deberta_v2.modeling_deberta_v2 import ContextPooler, StableDropout, DebertaV2Attention\n\n\n#-------- Model ------------------------------------------------------------------#\nclass FeedbackModel(nn.Module):\n    \"\"\"The feedback prize effectiveness baseline model\n    \"\"\"\n\n    def __init__(self, config):\n        super(FeedbackModel, self).__init__()\n        self.config = config\n\n        # base transformer\n        base_config = AutoConfig.from_pretrained(self.config[\"base_model_path\"])\n        base_config.update({\"add_pooling_layer\": False, \"max_position_embeddings\": 1024})\n        self.base_model = AutoModel.from_pretrained(self.config[\"base_model_path\"], config=base_config)\n\n        # dropouts\n        self.dropout = StableDropout(self.config[\"dropout\"])\n        \n        # multi-head attention\n        attention_config = deepcopy(self.base_model.config)\n        attention_config.update({\"relative_attention\": False})\n        self.fpe_span_attention = DebertaV2Attention(attention_config)\n        \n        # classification\n        hidden_size = self.base_model.config.hidden_size\n        feature_size = hidden_size\n        self.layer_norm = LayerNorm(feature_size, self.base_model.config.layer_norm_eps)\n        \n        # # LSTM Head\n        self.fpe_lstm_layer = nn.LSTM(\n            input_size=feature_size,\n            hidden_size=hidden_size,\n            num_layers=1,\n            batch_first=True,\n            bidirectional=False,\n        )\n\n        self.num_labels = self.config[\"num_labels\"]\n        self.classifier = nn.Linear(feature_size, self.config[\"num_labels\"])\n\n    def forward(self, input_ids, attention_mask, span_head_idxs, span_tail_idxs, span_attention_mask, **kwargs):\n        \n        bs = input_ids.shape[0]  # batch size\n        outputs = self.base_model(input_ids, attention_mask=attention_mask)\n        encoder_layer = outputs[0]\n        \n        self.fpe_lstm_layer.flatten_parameters()\n        encoder_layer = self.fpe_lstm_layer(encoder_layer)[0]\n\n        mean_feature_vector = []\n        for i in range(bs):  # TODO: vectorize\n            span_vec_i = []\n\n            for head, tail in zip(span_head_idxs[i], span_tail_idxs[i]):\n                # span feature\n                tmp = torch.mean(encoder_layer[i, head+1:tail], dim=0)  # [h]\n                span_vec_i.append(tmp)\n            span_vec_i = torch.stack(span_vec_i)  # (num_disourse, h)\n            mean_feature_vector.append(span_vec_i)\n\n        mean_feature_vector = torch.stack(mean_feature_vector)  # (bs, num_disourse, h)\n        mean_feature_vector = self.layer_norm(mean_feature_vector)\n\n        # attend to other features\n        extended_span_attention_mask = span_attention_mask.unsqueeze(1).unsqueeze(2)\n        span_attention_mask = extended_span_attention_mask * extended_span_attention_mask.squeeze(-2).unsqueeze(-1)\n        span_attention_mask = span_attention_mask.byte()\n        feature_vector = self.fpe_span_attention(mean_feature_vector, span_attention_mask)\n\n        # feature_vector = mean_feature_vector\n        feature_vector = self.dropout(feature_vector)\n\n        logits = self.classifier(feature_vector)\n        logits = logits[:,:, :3] # main logits\n        return logits","metadata":{"_uuid":"70934f8f-73a2-4729-99ad-61b35b992f52","_cell_guid":"e5848f69-2f06-4728-a44e-7a0b0b6f3434","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:21:58.889649Z","iopub.execute_input":"2022-08-23T12:21:58.891738Z","iopub.status.idle":"2022-08-23T12:21:58.918629Z","shell.execute_reply.started":"2022-08-23T12:21:58.891704Z","shell.execute_reply":"2022-08-23T12:21:58.917743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference","metadata":{"_uuid":"fca0fbd5-b5fc-4c27-a2b6-c934498a2069","_cell_guid":"c43b6806-4dbc-480d-adf3-8f9318a8179c","trusted":true}},{"cell_type":"code","source":"\ncheckpoints = [\n    \"../input/a-delv3-prod-lstm-multihead-attention/lstm_multihead_v1_fpe_model_fold_0_best.pth.tar\",\n    \"../input/a-delv3-prod-lstm-multihead-attention/lstm_multihead_fpe_model_fold_1_best.pth.tar\",\n    \"../input/a-delv3-prod-lstm-multihead-attention/lstm_multihead_fpe_model_fold_2_best.pth.tar\",\n    \"../input/a-delv3-prod-lstm-multihead-attention/lstm_multihead_fpe_model_fold_3_best.pth.tar\",\n]","metadata":{"_uuid":"902a714c-765b-49a0-adf8-0b7d8af23404","_cell_guid":"c4637fe2-0cd0-4e66-b092-4a021fcb3c5d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:21:58.923913Z","iopub.execute_input":"2022-08-23T12:21:58.926928Z","iopub.status.idle":"2022-08-23T12:21:58.933856Z","shell.execute_reply.started":"2022-08-23T12:21:58.926893Z","shell.execute_reply":"2022-08-23T12:21:58.93283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def inference_fn(model, infer_dl, model_id):\n    all_preds = []\n    all_uids = []\n    accelerator = Accelerator()\n    model, infer_dl = accelerator.prepare(model, infer_dl)\n    \n    model.eval()\n    tk0 = tqdm(infer_dl, total=len(infer_dl))\n    \n    for batch in tk0:\n        with torch.no_grad():\n            logits = model(**batch) # (b, nd, 3)\n            batch_preds = F.softmax(logits, dim=-1)\n            batch_uids = batch[\"uids\"]\n        all_preds.append(batch_preds)\n        all_uids.append(batch_uids)\n    \n    all_preds = [p.to('cpu').detach().numpy().tolist() for p in all_preds]\n    all_preds = list(chain(*all_preds))\n    flat_preds = list(chain(*all_preds))\n    \n    all_uids = [p.to('cpu').detach().numpy().tolist() for p in all_uids]\n    all_uids = list(chain(*all_uids))\n    flat_uids = list(chain(*all_uids))    \n    \n    preds_df = pd.DataFrame(flat_preds)\n    preds_df.columns = [\"Ineffective\", \"Adequate\", \"Effective\"]\n    preds_df[\"span_uid\"] = flat_uids # SORTED_DISCOURSE_IDS\n    preds_df = preds_df[preds_df[\"span_uid\"]>=0].copy()\n    preds_df[\"discourse_id\"] = preds_df[\"span_uid\"].map(idx2discourse)\n    preds_df = preds_df[[\"discourse_id\", \"Ineffective\", \"Adequate\", \"Effective\"]].copy()\n    preds_df.to_csv(f\"exp03_delv3_mlm20_model_preds_{model_id}.csv\", index=False)\n    \nif use_exp3:\n    for model_id, checkpoint in enumerate(checkpoints):\n        print(f\"infering from {checkpoint}\")\n        model = FeedbackModel(config)\n        ckpt = torch.load(checkpoint)\n        print(f\"model performance on validation set = {ckpt['loss']}\")\n        model.load_state_dict(ckpt['state_dict'])\n        inference_fn(model, infer_dl, model_id)\n\n    del model\n    del tokenizer, infer_dataset, infer_ds_dict, data_collector, infer_dl\n    gc.collect()\n    torch.cuda.empty_cache()","metadata":{"_uuid":"c8f223c1-c47a-4244-84e9-b1095e5c2acb","_cell_guid":"64882816-8351-4297-918e-80171941e445","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:21:58.93835Z","iopub.execute_input":"2022-08-23T12:21:58.941806Z","iopub.status.idle":"2022-08-23T12:21:58.96247Z","shell.execute_reply.started":"2022-08-23T12:21:58.941702Z","shell.execute_reply":"2022-08-23T12:21:58.96153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoints = [\n    \"../input/a-delv3-prod-lstm-multihead-attention/resolved_fpe_model_fold_0_best.pth.tar\",\n    \"../input/a-delv3-prod-lstm-multihead-attention/resolved_fpe_model_fold_1_best.pth.tar\",\n    \"../input/a-delv3-prod-lstm-multihead-attention/resolved_v1_fpe_model_fold_2_best.pth.tar\",\n    \"../input/a-delv3-prod-lstm-multihead-attention/resolved_fpe_model_fold_3_best.pth.tar\",\n]","metadata":{"_uuid":"ce2f64f4-33d2-4d16-a7f0-d5e9dae9018d","_cell_guid":"e2b9ff9d-84c4-4bcc-be34-1f720ee78a20","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:21:58.967799Z","iopub.execute_input":"2022-08-23T12:21:58.971218Z","iopub.status.idle":"2022-08-23T12:21:58.978073Z","shell.execute_reply.started":"2022-08-23T12:21:58.971171Z","shell.execute_reply":"2022-08-23T12:21:58.977379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def inference_fn(model, infer_dl, model_id):\n    all_preds = []\n    all_uids = []\n    accelerator = Accelerator()\n    model, infer_dl = accelerator.prepare(model, infer_dl)\n    \n    model.eval()\n    tk0 = tqdm(infer_dl, total=len(infer_dl))\n    \n    for batch in tk0:\n        with torch.no_grad():\n            logits = model(**batch) # (b, nd, 3)\n            batch_preds = F.softmax(logits, dim=-1)\n            batch_uids = batch[\"uids\"]\n        all_preds.append(batch_preds)\n        all_uids.append(batch_uids)\n    \n    all_preds = [p.to('cpu').detach().numpy().tolist() for p in all_preds]\n    all_preds = list(chain(*all_preds))\n    flat_preds = list(chain(*all_preds))\n    \n    all_uids = [p.to('cpu').detach().numpy().tolist() for p in all_uids]\n    all_uids = list(chain(*all_uids))\n    flat_uids = list(chain(*all_uids))    \n    \n    preds_df = pd.DataFrame(flat_preds)\n    preds_df.columns = [\"Ineffective\", \"Adequate\", \"Effective\"]\n    preds_df[\"span_uid\"] = flat_uids # SORTED_DISCOURSE_IDS\n    preds_df = preds_df[preds_df[\"span_uid\"]>=0].copy()\n    preds_df[\"discourse_id\"] = preds_df[\"span_uid\"].map(idx2discourse)\n    preds_df = preds_df[[\"discourse_id\", \"Ineffective\", \"Adequate\", \"Effective\"]].copy()\n    preds_df.to_csv(f\"exp04_delv3_mlm20_resolved_model_preds_{model_id}.csv\", index=False)\n    \nif use_exp4:\n    for model_id, checkpoint in enumerate(checkpoints):\n        print(f\"infering from {checkpoint}\")\n        model = FeedbackModel(config)\n        ckpt = torch.load(checkpoint)\n        print(f\"model performance on validation set = {ckpt['loss']}\")\n        model.load_state_dict(ckpt['state_dict'])\n        inference_fn(model, infer_dl, model_id)\n\n    del model\n    del tokenizer, infer_dataset, infer_ds_dict, data_collector, infer_dl\n\n    gc.collect()\n    torch.cuda.empty_cache()","metadata":{"_uuid":"64b6b78e-280e-410b-bc44-c897286bca3a","_cell_guid":"d3a9a1ee-4403-4cbf-97af-222f7a10b9b4","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:21:58.979968Z","iopub.execute_input":"2022-08-23T12:21:58.980319Z","iopub.status.idle":"2022-08-23T12:21:59.008676Z","shell.execute_reply.started":"2022-08-23T12:21:58.980286Z","shell.execute_reply":"2022-08-23T12:21:59.007952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if use_exp3:\n    import glob\n    import pandas as pd\n\n    csvs = glob.glob(\"exp03_delv3_mlm20_model_preds_*.csv\")\n\n    idx = []\n    preds = []\n\n    for csv_idx, csv in enumerate(csvs):\n\n        print(\"==\"*40)\n        print(f\"preds in {csv}\")\n        df = pd.read_csv(csv)\n        df = df.sort_values(by=[\"discourse_id\"])\n        print(df.head(10))\n        print(\"==\"*40)\n\n        temp_preds = df.drop([\"discourse_id\"], axis=1).values\n        if csv_idx == 0:\n            idx = list(df[\"discourse_id\"])\n            preds = temp_preds\n        else:\n            preds += temp_preds\n\n    preds = preds / len(csvs)\n\n    exp03_df = pd.DataFrame()\n    exp03_df[\"discourse_id\"]  = idx\n    exp03_df[\"Ineffective\"]   = preds[:, 0]\n    exp03_df[\"Adequate\"]      = preds[:, 1]\n    exp03_df[\"Effective\"]     = preds[:, 2]\n\n    exp03_df = exp03_df.groupby(\"discourse_id\")[[\"Ineffective\", \"Adequate\", \"Effective\"]].agg(np.mean).reset_index()","metadata":{"_uuid":"6f192529-a0f4-404d-b944-e9309420d25b","_cell_guid":"cf88d31b-6e41-40f1-ae5c-accf05f59434","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:21:59.010883Z","iopub.execute_input":"2022-08-23T12:21:59.011174Z","iopub.status.idle":"2022-08-23T12:21:59.027067Z","shell.execute_reply.started":"2022-08-23T12:21:59.011146Z","shell.execute_reply":"2022-08-23T12:21:59.026171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if use_exp3:\n    display(exp03_df.head())","metadata":{"_uuid":"909ebe1a-9f04-4b7d-952f-6a4f87b36f3b","_cell_guid":"0f4bde2e-ccb2-48bb-ac06-589688b7b5a2","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:21:59.028119Z","iopub.execute_input":"2022-08-23T12:21:59.028484Z","iopub.status.idle":"2022-08-23T12:21:59.034728Z","shell.execute_reply.started":"2022-08-23T12:21:59.028448Z","shell.execute_reply":"2022-08-23T12:21:59.033655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if use_exp4:\n    import glob\n    import pandas as pd\n\n    csvs = glob.glob(\"exp04_delv3_mlm20_resolved_model_preds_*.csv\")\n\n    idx = []\n    preds = []\n\n    for csv_idx, csv in enumerate(csvs):\n\n        print(\"==\"*40)\n        print(f\"preds in {csv}\")\n        df = pd.read_csv(csv)\n        df = df.sort_values(by=[\"discourse_id\"])\n        print(df.head(10))\n        print(\"==\"*40)\n\n        temp_preds = df.drop([\"discourse_id\"], axis=1).values\n        if csv_idx == 0:\n            idx = list(df[\"discourse_id\"])\n            preds = temp_preds\n        else:\n            preds += temp_preds\n\n    preds = preds / len(csvs)\n\n    exp04_df = pd.DataFrame()\n    exp04_df[\"discourse_id\"]  = idx\n    exp04_df[\"Ineffective\"]   = preds[:, 0]\n    exp04_df[\"Adequate\"]      = preds[:, 1]\n    exp04_df[\"Effective\"]     = preds[:, 2]\n\n    exp04_df = exp04_df.groupby(\"discourse_id\")[[\"Ineffective\", \"Adequate\", \"Effective\"]].agg(np.mean).reset_index()","metadata":{"_uuid":"b8e91c0a-55c5-47bb-b539-824a67834be1","_cell_guid":"294f33b2-e6d8-4a47-b8e9-369c9e6ccc43","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:21:59.036242Z","iopub.execute_input":"2022-08-23T12:21:59.037219Z","iopub.status.idle":"2022-08-23T12:21:59.047832Z","shell.execute_reply.started":"2022-08-23T12:21:59.037167Z","shell.execute_reply":"2022-08-23T12:21:59.047122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if use_exp4:\n    display(exp04_df.head())","metadata":{"_uuid":"31e9dbf4-f62d-4be9-9c6d-b37658a192e3","_cell_guid":"735c262d-4cba-4b23-bf06-b9a319cd13d3","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:21:59.049361Z","iopub.execute_input":"2022-08-23T12:21:59.050137Z","iopub.status.idle":"2022-08-23T12:21:59.057096Z","shell.execute_reply.started":"2022-08-23T12:21:59.05004Z","shell.execute_reply":"2022-08-23T12:21:59.056255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EXP1 - delv3  8 fold Fast Model - SPAN MLM 40%","metadata":{"_uuid":"3ed8b281-5d3a-47fb-b95c-46f3e67fadb2","_cell_guid":"6c6be5c3-bae2-4e3b-bdb9-d438ee259163","execution":{"iopub.status.busy":"2022-07-17T18:52:10.311607Z","iopub.execute_input":"2022-07-17T18:52:10.311971Z","iopub.status.idle":"2022-07-17T18:52:10.323544Z","shell.execute_reply.started":"2022-07-17T18:52:10.311935Z","shell.execute_reply":"2022-07-17T18:52:10.32276Z"},"trusted":true}},{"cell_type":"markdown","source":"## Config","metadata":{"_uuid":"edd674d5-14d1-4383-9234-b1a5e31ed6dd","_cell_guid":"fd4dce71-4ec4-4500-bf9b-eb7f7320b266","trusted":true}},{"cell_type":"code","source":"config = \"\"\"{\n    \"debug\": false,\n\n    \"base_model_path\": \"../input/tapt-fpe-delv3-span-mlm-04\",\n    \"model_dir\": \"./outputs\",\n\n    \"max_length\": 1024,\n    \"stride\": 256,\n    \"num_labels\": 5,\n    \"dropout\": 0.1,\n    \"infer_bs\": 8\n}\n\"\"\"\nconfig = json.loads(config)","metadata":{"_uuid":"46508739-344a-4e14-a7cf-098975a2b265","_cell_guid":"c82c8476-d05c-4243-bc20-3bc75f5fc2db","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:21:59.058623Z","iopub.execute_input":"2022-08-23T12:21:59.059007Z","iopub.status.idle":"2022-08-23T12:21:59.066651Z","shell.execute_reply.started":"2022-08-23T12:21:59.058972Z","shell.execute_reply":"2022-08-23T12:21:59.065845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset","metadata":{"_uuid":"6b54ef85-98e6-4c08-a752-4becf174ed8a","_cell_guid":"f0f1bad7-b2c4-40ed-bf81-e210c749d436","trusted":true}},{"cell_type":"code","source":"import os\nimport re\nfrom copy import deepcopy\nfrom itertools import chain\n\nimport pandas as pd\nfrom datasets import Dataset\nfrom tokenizers import AddedToken\nfrom transformers import AutoTokenizer\n\n\n#--------------- Tokenizer ---------------------------------------------#\ndef get_tokenizer(config):\n    \"\"\"load the tokenizer\"\"\"\n\n    print(\"using auto tokenizer\")\n    tokenizer = AutoTokenizer.from_pretrained(config[\"base_model_path\"])\n\n    print(\"==\"*40)\n    print(f\"tokenizer len: {len(tokenizer)}\")\n    print(\n        f\"tokenizer test: {tokenizer.tokenize('Starts: [SOE] [LEAD] [CLAIM] [POSITION]')}\")\n    print(\n        f\"tokenizer test: {tokenizer.tokenize('Starts: [EOE] [LEAD_END] [POSITION_END]')}\")\n\n    print(\"==\"*40)\n    return tokenizer\n\n\n#--------------- Processing ---------------------------------------------#\nTOKEN_MAP = {\n    \"Lead\": [\"Lead [LEAD]\", \"[LEAD_END]\"],\n    \"Position\": [\"Position [POSITION]\", \"[POSITION_END]\"],\n    \"Claim\": [\"Claim [CLAIM]\", \"[CLAIM_END]\"],\n    \"Counterclaim\": [\"Counterclaim [COUNTER_CLAIM]\", \"[COUNTER_CLAIM_END]\"],\n    \"Rebuttal\": [\"Rebuttal [REBUTTAL]\", \"[REBUTTAL_END]\"],\n    \"Evidence\": [\"Evidence [EVIDENCE]\", \"[EVIDENCE_END]\"],\n    \"Concluding Statement\": [\"Concluding Statement [CONCLUDING_STATEMENT]\", \"[CONCLUDING_STATEMENT_END]\"]\n}\n\nDISCOURSE_START_TOKENS = [\n    \"[LEAD]\",\n    \"[POSITION]\",\n    \"[CLAIM]\",\n    \"[COUNTER_CLAIM]\",\n    \"[REBUTTAL]\",\n    \"[EVIDENCE]\",\n    \"[CONCLUDING_STATEMENT]\"\n]\n\nDISCOURSE_END_TOKENS = [\n    \"[LEAD_END]\",\n    \"[POSITION_END]\",\n    \"[CLAIM_END]\",\n    \"[COUNTER_CLAIM_END]\",\n    \"[REBUTTAL_END]\",\n    \"[EVIDENCE_END]\",\n    \"[CONCLUDING_STATEMENT_END]\"\n]\n\n\ndef relaxed_search(text, substring, min_length=2, fraction=0.99999):\n    \"\"\"\n    Returns substring's span from the given text with the certain precision.\n    \"\"\"\n\n    position = text.find(substring)\n    substring_length = len(substring)\n    if position == -1:\n        half_length = int(substring_length * fraction)\n        half_substring = substring[:half_length]\n        half_substring_length = len(half_substring)\n        if half_substring_length < min_length:\n            return [-1, 0]\n        else:\n            return relaxed_search(text=text,\n                                  substring=half_substring,\n                                  min_length=min_length,\n                                  fraction=fraction)\n\n    span = [position, position+substring_length]\n    return span\n\n\ndef build_span_map(discourse_list, essay_text):\n    reading_head = 0\n    to_return = dict()\n\n    for cur_discourse in discourse_list:\n        if cur_discourse not in to_return:\n            to_return[cur_discourse] = []\n\n        matches = re.finditer(re.escape(r'{}'.format(cur_discourse)), essay_text)\n        for match in matches:\n            span_start, span_end = match.span()\n            if span_end <= reading_head:\n                continue\n            to_return[cur_discourse].append(match.span())\n            reading_head = span_end\n            break\n\n    # post process\n    for cur_discourse in discourse_list:\n        if not to_return[cur_discourse]:\n            print(\"resorting to relaxed search...\")\n            to_return[cur_discourse] = [relaxed_search(essay_text, cur_discourse)]\n    return to_return\n\n\ndef get_substring_span(texts, mapping):\n    result = []\n    for text in texts:\n        ans = mapping[text].pop(0)\n        result.append(ans)\n    return result\n\n\ndef process_essay(essay_id, essay_text, anno_df):\n    \"\"\"insert newly added tokens in the essay text\n    \"\"\"\n    tmp_df = anno_df[anno_df[\"essay_id\"] == essay_id].copy()\n    tmp_df = tmp_df.sort_values(by=\"discourse_start\")\n    buffer = 0\n\n    for _, row in tmp_df.iterrows():\n        s, e, d_type = int(row.discourse_start) + buffer, int(row.discourse_end) + buffer, row.discourse_type\n        s_tok, e_tok = TOKEN_MAP[d_type]\n        essay_text = \" \".join([essay_text[:s], s_tok, essay_text[s:e], e_tok, essay_text[e:]])\n        buffer += len(s_tok) + len(e_tok) + 4\n\n    essay_text = \"[SOE]\" + essay_text + \"[EOE]\"\n    return essay_text\n\n\ndef process_input_df(anno_df, notes_df):\n    \"\"\"pre-process input dataframe\n\n    :param df: input dataframe\n    :type df: pd.DataFrame\n    :return: processed dataframe\n    :rtype: pd.DataFrame\n    \"\"\"\n    notes_df = deepcopy(notes_df)\n    anno_df = deepcopy(anno_df)\n\n    #------------------- Pre-Process Essay Text --------------------------#\n    anno_df[\"discourse_text\"] = anno_df[\"discourse_text\"].apply(lambda x: x.strip())  # pre-process\n    if \"discourse_effectiveness\" in anno_df.columns:\n        anno_df = anno_df[[\"discourse_id\", \"essay_id\", \"discourse_text\",\n                           \"discourse_type\", \"discourse_effectiveness\", \"uid\"]].copy()\n    else:\n        anno_df = anno_df[[\"discourse_id\", \"essay_id\", \"discourse_text\", \"discourse_type\", \"uid\"]].copy()\n\n    # anno_df[\"discourse_span\"] = anno_df[[\"essay_id\", \"discourse_text\"]].apply(\n    #     lambda x: get_substring_span(\n    #         notes_df[notes_df[\"essay_id\"] == x[0]].iloc[0].essay_text,\n    #         x[1]\n    #     ), axis=1\n    # )\n\n    # anno_df[\"discourse_start\"] = anno_df[\"discourse_span\"].apply(lambda x: x[0])\n    # anno_df[\"discourse_end\"] = anno_df[\"discourse_span\"].apply(lambda x: x[1])\n\n    tmp_df = anno_df.groupby(\"essay_id\")[[\"discourse_id\", \"discourse_text\"]].agg(list).reset_index()\n    tmp_df = pd.merge(tmp_df, notes_df, on=\"essay_id\", how=\"left\")\n#     set_trace()\n    tmp_df[\"span_map\"] = tmp_df[[\"discourse_text\", \"essay_text\"]].apply(\n        lambda x: build_span_map(x[0], x[1]), axis=1)\n    tmp_df[\"span\"] = tmp_df[[\"discourse_text\", \"span_map\"]].apply(\n        lambda x: get_substring_span(x[0], x[1]), axis=1)\n\n    all_discourse_ids = list(chain(*tmp_df[\"discourse_id\"].values))\n    all_discourse_spans = list(chain(*tmp_df[\"span\"].values))\n    span_df = pd.DataFrame()\n    span_df[\"discourse_id\"] = all_discourse_ids\n    span_df[\"span\"] = all_discourse_spans\n    span_df[\"discourse_start\"] = span_df[\"span\"].apply(lambda x: x[0])\n    span_df[\"discourse_end\"] = span_df[\"span\"].apply(lambda x: x[1])\n    span_df = span_df.drop(columns=\"span\")\n\n    anno_df = pd.merge(anno_df, span_df, on=\"discourse_id\", how=\"left\")\n    # anno_df = pd.merge(anno_df, notes_df, on=\"essay_id\", how=\"left\")\n\n    print(\"==\"*40)\n    print(\"processing essay text and inserting new tokens at span boundaries\")\n    notes_df[\"essay_text\"] = notes_df[[\"essay_id\", \"essay_text\"]].apply(\n        lambda x: process_essay(x[0], x[1], anno_df), axis=1\n    )\n    print(\"==\"*40)\n\n    anno_df = anno_df.drop(columns=[\"discourse_start\", \"discourse_end\"])\n    notes_df = notes_df.drop_duplicates(subset=[\"essay_id\"])[[\"essay_id\", \"essay_text\"]].copy()\n\n    anno_df = pd.merge(anno_df, notes_df, on=\"essay_id\", how=\"left\")\n\n    if \"discourse_effectiveness\" in anno_df.columns:\n        grouped_df = anno_df.groupby(\"essay_id\")[[\"uid\", \"discourse_effectiveness\", \"discourse_type\"]].agg(list).reset_index()\n    else:\n        grouped_df = anno_df.groupby(\"essay_id\")[[\"uid\", \"discourse_type\"]].agg(list).reset_index()\n\n    grouped_df = pd.merge(grouped_df, notes_df, on=\"essay_id\", how=\"left\")\n    grouped_df = grouped_df.rename(columns={\"uid\": \"uids\"})\n\n    return grouped_df\n\n\n#--------------- Dataset ----------------------------------------------#\n\n\nclass AuxFeedbackDataset:\n    \"\"\"Dataset class for feedback prize effectiveness task\n    \"\"\"\n\n    def __init__(self, config):\n        self.config = config\n\n        self.label2id = {\n            \"Ineffective\": 0,\n            \"Adequate\": 1,\n            \"Effective\": 2,\n        }\n\n        self.discourse_type2id = {\n            \"Lead\": 1,\n            \"Position\": 2,\n            \"Claim\": 3,\n            \"Counterclaim\": 4,\n            \"Rebuttal\": 5,\n            \"Evidence\": 6,\n            \"Concluding Statement\": 7,\n        }\n\n        self.id2label = {v: k for k, v in self.label2id.items()}\n        self.load_tokenizer()\n\n    def load_tokenizer(self):\n        \"\"\"load tokenizer as per config \n        \"\"\"\n        self.tokenizer = get_tokenizer(self.config)\n        print(\"==\"*40)\n        print(\"token maps...\")\n        print(TOKEN_MAP)\n        print(\"==\"*40)\n\n        self.discourse_token_ids = set(self.tokenizer.convert_tokens_to_ids(DISCOURSE_START_TOKENS))\n        self.discourse_end_ids = set(self.tokenizer.convert_tokens_to_ids(DISCOURSE_END_TOKENS))\n\n    def tokenize_function(self, examples):\n        tz = self.tokenizer(\n            examples[\"essay_text\"],\n            padding=False,\n            truncation=False,  # no truncation at first\n            add_special_tokens=True,\n            return_offsets_mapping=True,\n        )\n        return tz\n\n    def process_spans(self, examples):\n\n        span_head_char_start_idxs, span_tail_char_end_idxs = [], []\n        span_head_idxs, span_tail_idxs = [], []\n\n        for example_input_ids, example_offset_mapping, example_uids in zip(examples[\"input_ids\"], examples[\"offset_mapping\"], examples[\"uids\"]):\n            example_span_head_idxs = [pos for pos, this_id in enumerate(\n                example_input_ids) if this_id in self.discourse_token_ids]\n            example_span_tail_idxs = [pos for pos, this_id in enumerate(\n                example_input_ids) if this_id in self.discourse_end_ids]\n\n            example_span_head_char_start_idxs = [example_offset_mapping[pos][0] for pos in example_span_head_idxs]\n            example_span_tail_char_end_idxs = [example_offset_mapping[pos][1] for pos in example_span_tail_idxs]\n\n            span_head_char_start_idxs.append(example_span_head_char_start_idxs)\n            span_tail_char_end_idxs.append(example_span_tail_char_end_idxs)\n\n            span_head_idxs.append(example_span_head_idxs)\n            span_tail_idxs.append(example_span_tail_idxs)\n\n        return {\n            \"span_head_idxs\": span_head_idxs,\n            \"span_tail_idxs\": span_tail_idxs,\n            \"span_head_char_start_idxs\": span_head_char_start_idxs,\n            \"span_tail_char_end_idxs\": span_tail_char_end_idxs,\n        }\n\n    def generate_labels(self, examples):\n        labels = []\n        for example_labels, example_uids in zip(examples[\"discourse_effectiveness\"], examples[\"uids\"]):\n            labels.append([self.label2id[l] for l in example_labels])\n        return {\"labels\": labels}\n\n    def generate_discourse_type_ids(self, examples):\n        discourse_type_ids = []\n        for example_discourse_types in examples[\"discourse_type\"]:\n            discourse_type_ids.append([self.discourse_type2id[dt] for dt in example_discourse_types])\n        return {\"discourse_type_ids\": discourse_type_ids}\n\n    def compute_input_length(self, examples):\n        return {\"input_length\": [len(x) for x in examples[\"input_ids\"]]}\n\n    def sanity_check_head_tail(self, examples):\n        for head_idxs, tail_idxs in zip(examples[\"span_head_idxs\"], examples[\"span_tail_idxs\"]):\n            assert len(head_idxs) == len(tail_idxs)\n            for head, tail in zip(head_idxs, tail_idxs):\n                assert tail > head + 1\n\n    def sanity_check_head_labels(self, examples):\n        for head_idxs, head_labels in zip(examples[\"span_head_idxs\"], examples[\"labels\"]):\n            assert len(head_idxs) == len(head_labels)\n\n    def get_dataset(self, df, essay_df, mode='train'):\n        \"\"\"main api for creating the Feedback dataset\n\n        :param df: input annotation dataframe\n        :type df: pd.DataFrame\n        :param essay_df: dataframe with essay texts\n        :type essay_df: pd.DataFrame\n        :param mode: check if required for train or infer, defaults to 'train'\n        :type mode: str, optional\n        :return: the created dataset\n        :rtype: Dataset\n        \"\"\"\n        df = process_input_df(df, essay_df)\n\n        # save a sample for sanity checks\n        sample_df = df.sample(min(16, len(df)))\n        sample_df.to_csv(os.path.join(self.config[\"model_dir\"], f\"{mode}_df_processed.csv\"), index=False)\n\n        task_dataset = Dataset.from_pandas(df)\n        task_dataset = task_dataset.map(self.tokenize_function, batched=True)\n        task_dataset = task_dataset.map(self.compute_input_length, batched=True)\n        task_dataset = task_dataset.map(self.process_spans, batched=True)\n        print(task_dataset)\n        # todo check edge cases\n        task_dataset = task_dataset.filter(lambda example: len(example['span_head_idxs']) == len(\n            example['span_tail_idxs']))  # no need to run on empty set\n        print(task_dataset)\n        task_dataset = task_dataset.map(self.generate_discourse_type_ids, batched=True)\n        task_dataset = task_dataset.map(self.sanity_check_head_tail, batched=True)\n\n        if mode != \"infer\":\n            task_dataset = task_dataset.map(self.generate_labels, batched=True)\n            task_dataset = task_dataset.map(self.sanity_check_head_labels, batched=True)\n\n        try:\n            task_dataset = task_dataset.remove_columns(column_names=[\"__index_level_0__\"])\n        except Exception as e:\n            pass\n        return df, task_dataset\n\n#--------------- dataset with truncation ---------------------------------------------#\n\n\ndef get_fast_dataset(config, df, essay_df, mode=\"train\"):\n    \"\"\"Function to get fast approach dataset with truncation & sliding window\n    \"\"\"\n    dataset_creator = AuxFeedbackDataset(config)\n    _, task_dataset = dataset_creator.get_dataset(df, essay_df, mode=mode)\n\n    original_dataset = deepcopy(task_dataset)\n    tokenizer = dataset_creator.tokenizer\n    START_IDS = dataset_creator.discourse_token_ids\n    END_IDS = dataset_creator.discourse_end_ids\n\n    def tokenize_with_truncation(examples):\n        tz = tokenizer(\n            examples[\"essay_text\"],\n            padding=False,\n            truncation=True,\n            add_special_tokens=True,\n            return_offsets_mapping=True,\n            max_length=config[\"max_length\"],\n            stride=config[\"stride\"],\n            return_overflowing_tokens=True,\n        )\n        return tz\n\n    def process_span(examples):\n        span_head_idxs, span_tail_idxs = [], []\n        span_head_char_start_idxs, span_tail_char_end_idxs = [], []\n\n        buffer = 25  # do not include a head if it is within buffer distance away from last token\n\n        for example_input_ids, example_offset_mapping in zip(examples[\"input_ids\"], examples[\"offset_mapping\"]):\n            # ------------------- Span Heads -----------------------------------------#\n            if len(example_input_ids) < config[\"max_length\"]:  # no truncation\n                head_candidate = [pos for pos, this_id in enumerate(example_input_ids) if this_id in START_IDS]\n            else:\n                head_candidate = [pos for pos, this_id in enumerate(example_input_ids) if (\n                    (this_id in START_IDS) & (pos <= config[\"max_length\"]-buffer))]\n\n            n_heads = len(head_candidate)\n\n            # ------------------- Span Tails -----------------------------------------#\n            tail_candidate = [pos for pos, this_id in enumerate(example_input_ids) if this_id in END_IDS]\n\n            # ------------------- Edge Cases -----------------------------------------#\n            # 1. A tail occurs before the first head in the sequence due to truncation\n            if (len(tail_candidate) > 0) & (len(head_candidate) > 0):\n                if tail_candidate[0] < head_candidate[0]:  # truncation effect\n                    # print(f\"check: heads: {head_candidate}, tails {tail_candidate}\")\n                    tail_candidate = tail_candidate[1:]  # shift by one\n\n            # 2. Tail got chopped off due to truncation but the corresponding head is still there\n            if len(tail_candidate) < n_heads:\n                assert len(tail_candidate) + 1 == n_heads\n                assert len(example_input_ids) == config[\"max_length\"]  # should only happen if input text is truncated\n                tail_candidate.append(config[\"max_length\"]-2)  # the token before [SEP] token\n\n            # 3. Additional tails remain in the buffer region\n            if len(tail_candidate) > len(head_candidate):\n                tail_candidate = tail_candidate[:len(head_candidate)]\n\n            # ------------------- Create the fields ------------------------------------#\n            example_span_head_char_start_idxs = [example_offset_mapping[pos][0] for pos in head_candidate]\n            example_span_tail_char_end_idxs = [example_offset_mapping[pos][1] for pos in tail_candidate]\n\n            span_head_idxs.append(head_candidate)\n            span_tail_idxs.append(tail_candidate)\n            span_head_char_start_idxs.append(example_span_head_char_start_idxs)\n            span_tail_char_end_idxs.append(example_span_tail_char_end_idxs)\n\n        return {\n            \"span_head_idxs\": span_head_idxs,\n            \"span_tail_idxs\": span_tail_idxs,\n            \"span_head_char_start_idxs\": span_head_char_start_idxs,\n            \"span_tail_char_end_idxs\": span_tail_char_end_idxs,\n        }\n\n    def enforce_alignment(examples):\n        uids = []\n\n        for example_span_head_char_start_idxs, example_overflow_to_sample_mapping in zip(\n                examples[\"span_head_char_start_idxs\"], examples[\"overflow_to_sample_mapping\"]):\n\n            original_example = original_dataset[example_overflow_to_sample_mapping]\n            original_example_span_head_char_start_idxs = original_example[\"span_head_char_start_idxs\"]\n            original_example_uids = original_example[\"uids\"]\n            char2uid = {k: v for k, v in zip(original_example_span_head_char_start_idxs, original_example_uids)}\n            current_example_uids = [char2uid[char_idx] for char_idx in example_span_head_char_start_idxs]\n            uids.append(current_example_uids)\n        return {\"uids\": uids}\n\n    def recompute_labels(examples):\n        labels = []\n        for example_span_head_char_start_idxs, example_overflow_to_sample_mapping in zip(\n                examples[\"span_head_char_start_idxs\"], examples[\"overflow_to_sample_mapping\"]):\n\n            original_example = original_dataset[example_overflow_to_sample_mapping]\n            original_example_span_head_char_start_idxs = original_example[\"span_head_char_start_idxs\"]\n            original_example_labels = original_example[\"labels\"]\n            char2label = {k: v for k, v in zip(original_example_span_head_char_start_idxs, original_example_labels)}\n            current_example_labels = [char2label[char_idx] for char_idx in example_span_head_char_start_idxs]\n            labels.append(current_example_labels)\n        return {\"labels\": labels}\n\n    def recompute_discourse_type_ids(examples):\n        discourse_type_ids = []\n        for example_span_head_char_start_idxs, example_overflow_to_sample_mapping in zip(\n                examples[\"span_head_char_start_idxs\"], examples[\"overflow_to_sample_mapping\"]):\n\n            original_example = original_dataset[example_overflow_to_sample_mapping]\n            original_example_span_head_char_start_idxs = original_example[\"span_head_char_start_idxs\"]\n            original_example_discourse_type_ids = original_example[\"discourse_type_ids\"]\n            char2discourse_id = {k: v for k, v in zip(\n                original_example_span_head_char_start_idxs, original_example_discourse_type_ids)}\n            current_example_discourse_type_ids = [char2discourse_id[char_idx]\n                                                  for char_idx in example_span_head_char_start_idxs]\n            discourse_type_ids.append(current_example_discourse_type_ids)\n        return {\"discourse_type_ids\": discourse_type_ids}\n\n    def compute_input_length(examples):\n        return {\"input_length\": [len(x) for x in examples[\"input_ids\"]]}\n\n    def sanity_check_head_tail(examples):\n        for head_idxs, tail_idxs in zip(examples[\"span_head_idxs\"], examples[\"span_tail_idxs\"]):\n            assert len(head_idxs) == len(tail_idxs)\n            for head, tail in zip(head_idxs, tail_idxs):\n                assert tail > head + 1, f\"head idxs: {head_idxs}, tail idxs {tail_idxs}\"\n\n    task_dataset = task_dataset.map(\n        tokenize_with_truncation,\n        batched=True,\n        remove_columns=task_dataset.column_names,\n        batch_size=len(task_dataset)\n    )\n\n    task_dataset = task_dataset.map(process_span, batched=True)\n    task_dataset = task_dataset.map(enforce_alignment, batched=True)\n    task_dataset = task_dataset.map(recompute_discourse_type_ids, batched=True)\n    task_dataset = task_dataset.map(sanity_check_head_tail, batched=True)\n\n    # no need to run on empty set\n    task_dataset = task_dataset.filter(lambda example: len(example['span_head_idxs']) != 0)\n    task_dataset = task_dataset.map(compute_input_length, batched=True)\n\n    if mode != \"infer\":\n        task_dataset = task_dataset.map(recompute_labels, batched=True)\n\n    to_return = dict()\n    to_return[\"dataset\"] = task_dataset\n    to_return[\"original_dataset\"] = original_dataset\n    to_return[\"tokenizer\"] = tokenizer\n    return to_return","metadata":{"_uuid":"a2227c78-1659-47b0-8b72-9af8489aafaa","_cell_guid":"e28658a5-3343-4f34-8027-8354bc1308a0","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:21:59.068949Z","iopub.execute_input":"2022-08-23T12:21:59.069409Z","iopub.status.idle":"2022-08-23T12:21:59.143427Z","shell.execute_reply.started":"2022-08-23T12:21:59.069372Z","shell.execute_reply":"2022-08-23T12:21:59.142358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if use_exp1 or use_exp10 or use_exp11 or use_exp16:\n\n    os.makedirs(config[\"model_dir\"], exist_ok=True)\n\n    print(\"creating the inference datasets...\")\n    infer_ds_dict = get_fast_dataset(config, test_df, essay_df, mode=\"infer\")\n    tokenizer = infer_ds_dict[\"tokenizer\"]\n    infer_dataset = infer_ds_dict[\"dataset\"]\n    print(infer_dataset)","metadata":{"_uuid":"65295d4b-c0f9-4c50-8bb4-ee49299e18ca","_cell_guid":"f4ae6e21-dc69-44d9-8171-97c4ad491d88","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:21:59.145312Z","iopub.execute_input":"2022-08-23T12:21:59.145717Z","iopub.status.idle":"2022-08-23T12:22:00.255959Z","shell.execute_reply.started":"2022-08-23T12:21:59.145681Z","shell.execute_reply":"2022-08-23T12:22:00.254975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nif use_exp1 or use_exp10 or use_exp11:\n    config[\"len_tokenizer\"] = len(tokenizer)\n    infer_dataset = infer_dataset.sort(\"input_length\")\n    infer_dataset.set_format(\n        type=None,\n        columns=['input_ids', 'attention_mask', 'token_type_ids', 'span_head_idxs',\n                 'span_tail_idxs', 'discourse_type_ids', 'uids']\n    )","metadata":{"_uuid":"3845f98a-f931-4378-bd42-de635e7341b4","_cell_guid":"b5f86631-d14b-40e6-8603-34c92cd783a3","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:22:00.257606Z","iopub.execute_input":"2022-08-23T12:22:00.258155Z","iopub.status.idle":"2022-08-23T12:22:00.272555Z","shell.execute_reply.started":"2022-08-23T12:22:00.258113Z","shell.execute_reply":"2022-08-23T12:22:00.271466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Loader","metadata":{"_uuid":"5ce0d79f-18b2-44d3-bd34-652b284049c8","_cell_guid":"e91203b7-4835-4cbe-95a3-e3588fd7cdb7","trusted":true}},{"cell_type":"code","source":"from copy import deepcopy\nfrom dataclasses import dataclass\n\nimport torch\nfrom transformers import DataCollatorWithPadding\n\n\n@dataclass\nclass CustomDataCollatorWithPadding(DataCollatorWithPadding):\n    \"\"\"\n    data collector for seq classification\n    \"\"\"\n\n    tokenizer = None\n    padding = True\n    max_length = None\n    pad_to_multiple_of = None\n    return_tensors = \"pt\"\n\n    def __call__(self, features):\n        uids = [feature[\"uids\"] for feature in features]\n        discourse_type_ids = [feature[\"discourse_type_ids\"] for feature in features]\n        span_head_idxs = [feature[\"span_head_idxs\"] for feature in features]\n        span_tail_idxs = [feature[\"span_tail_idxs\"] for feature in features]\n        span_attention_mask = [[1]*len(feature[\"span_head_idxs\"]) for feature in features]\n\n        labels = None\n        if \"labels\" in features[0].keys():\n            labels = [feature[\"labels\"] for feature in features]\n\n        batch = self.tokenizer.pad(\n            features,\n            padding=self.padding,\n            max_length=self.max_length,\n            pad_to_multiple_of=self.pad_to_multiple_of,\n            return_tensors=None,\n        )\n\n        b_max = max([len(l) for l in span_head_idxs])\n        max_len = len(batch[\"input_ids\"][0])\n\n        default_head_idx = max(max_len - 10, 1)  # for padding\n        default_tail_idx = max(max_len - 4, 1)  # for padding\n\n        batch[\"span_head_idxs\"] = [\n            ex_span_head_idxs + [default_head_idx] * (b_max - len(ex_span_head_idxs)) for ex_span_head_idxs in span_head_idxs\n        ]\n\n        batch[\"uids\"] = [ex_uids + [-1] * (b_max - len(ex_uids)) for ex_uids in uids]\n        batch[\"discourse_type_ids\"] = [ex_discourse_type_ids + [0] *\n                                       (b_max - len(ex_discourse_type_ids)) for ex_discourse_type_ids in discourse_type_ids]\n\n        batch[\"span_tail_idxs\"] = [\n            ex_span_tail_idxs + [default_tail_idx] * (b_max - len(ex_span_tail_idxs)) for ex_span_tail_idxs in span_tail_idxs\n        ]\n\n        batch[\"span_attention_mask\"] = [\n            ex_discourse_masks + [0] * (b_max - len(ex_discourse_masks)) for ex_discourse_masks in span_attention_mask\n        ]\n\n        if labels is not None:\n            batch[\"labels\"] = [ex_labels + [-1] * (b_max - len(ex_labels)) for ex_labels in labels]\n\n        def _get_additional_labels(label_id):\n            if label_id == 0:\n                vec = [0, 0]\n            elif label_id == 1:\n                vec = [1, 0]\n            elif label_id == 2:\n                vec = [1, 1]\n            elif label_id == -1:\n                vec = [-1, -1]\n            else:\n                raise\n            return vec\n\n        if labels is not None:\n            additional_labels = []\n            for ex_labels in batch[\"labels\"]:\n                ex_additional_labels = [_get_additional_labels(el) for el in ex_labels]\n                additional_labels.append(ex_additional_labels)\n            batch[\"multitask_labels\"] = additional_labels\n\n        # batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n        batch = {k: (torch.tensor(v, dtype=torch.int64) if k != \"multitask_labels\" else torch.tensor(\n            v, dtype=torch.float32)) for k, v in batch.items()}\n        return batch","metadata":{"_uuid":"3de69aaa-fe19-4766-a2ce-05ed6b5a563e","_cell_guid":"da2764df-ea5c-4ec7-922e-03a9ad964206","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:22:00.274178Z","iopub.execute_input":"2022-08-23T12:22:00.274637Z","iopub.status.idle":"2022-08-23T12:22:00.292981Z","shell.execute_reply.started":"2022-08-23T12:22:00.274573Z","shell.execute_reply":"2022-08-23T12:22:00.291822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if use_exp1 or use_exp10 or use_exp11:\n    data_collector = CustomDataCollatorWithPadding(tokenizer=tokenizer)\n\n    infer_dl = DataLoader(\n        infer_dataset,\n        batch_size=config[\"infer_bs\"],\n        shuffle=False,\n        collate_fn=data_collector\n    )","metadata":{"_uuid":"f1f1c73e-9693-4876-a0f9-955b86a017f2","_cell_guid":"1b0cd052-c7fd-4e04-a889-b66f4cc5615b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:22:00.294787Z","iopub.execute_input":"2022-08-23T12:22:00.295953Z","iopub.status.idle":"2022-08-23T12:22:00.305319Z","shell.execute_reply.started":"2022-08-23T12:22:00.295913Z","shell.execute_reply":"2022-08-23T12:22:00.304091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{"_uuid":"fd8d49be-551a-41e9-86ab-8b2d44f63f84","_cell_guid":"66c4a662-9a32-41a6-8f32-ecaad5e97277","trusted":true}},{"cell_type":"code","source":"import gc\nimport pdb\nfrom collections import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import LayerNorm\n\nimport torch.utils.checkpoint\nfrom transformers import AutoConfig, AutoModel\nfrom transformers.models.deberta_v2.modeling_deberta_v2 import ContextPooler, StableDropout, DebertaV2Attention\n\n\n#-------- Model ------------------------------------------------------------------#\nclass FeedbackModel(nn.Module):\n    \"\"\"The feedback prize effectiveness baseline model\n    \"\"\"\n\n    def __init__(self, config):\n        super(FeedbackModel, self).__init__()\n        self.config = config\n\n        # base transformer\n        base_config = AutoConfig.from_pretrained(self.config[\"base_model_path\"])\n        base_config.update({\"add_pooling_layer\": False, \"max_position_embeddings\": 1024})\n        self.base_model = AutoModel.from_pretrained(self.config[\"base_model_path\"], config=base_config)\n\n        # dropouts\n        self.dropout = StableDropout(self.config[\"dropout\"])\n        \n        # multi-head attention\n        attention_config = deepcopy(self.base_model.config)\n        attention_config.update({\"relative_attention\": False})\n        self.fpe_span_attention = DebertaV2Attention(attention_config)\n        \n        # classification\n        hidden_size = self.base_model.config.hidden_size\n        feature_size = hidden_size\n        self.layer_norm = LayerNorm(feature_size, self.base_model.config.layer_norm_eps)\n        \n        # # LSTM Head\n        self.fpe_lstm_layer = nn.LSTM(\n            input_size=feature_size,\n            hidden_size=hidden_size,\n            num_layers=1,\n            batch_first=True,\n            bidirectional=False,\n        )\n\n        self.num_labels = self.config[\"num_labels\"] # 5\n        self.classifier = nn.Linear(feature_size, self.config[\"num_labels\"])\n\n    def forward(self, input_ids, attention_mask, span_head_idxs, span_tail_idxs, span_attention_mask, **kwargs):\n        \n        bs = input_ids.shape[0]  # batch size\n        outputs = self.base_model(input_ids, attention_mask=attention_mask)\n        encoder_layer = outputs[0]\n        \n        self.fpe_lstm_layer.flatten_parameters()\n        encoder_layer = self.fpe_lstm_layer(encoder_layer)[0]\n\n        mean_feature_vector = []\n        for i in range(bs):  # TODO: vectorize\n            span_vec_i = []\n\n            for head, tail in zip(span_head_idxs[i], span_tail_idxs[i]):\n                # span feature\n                tmp = torch.mean(encoder_layer[i, head+1:tail], dim=0)  # [h]\n                span_vec_i.append(tmp)\n            span_vec_i = torch.stack(span_vec_i)  # (num_disourse, h)\n            mean_feature_vector.append(span_vec_i)\n\n        mean_feature_vector = torch.stack(mean_feature_vector)  # (bs, num_disourse, h)\n        mean_feature_vector = self.layer_norm(mean_feature_vector)\n\n        # attend to other features\n        extended_span_attention_mask = span_attention_mask.unsqueeze(1).unsqueeze(2)\n        span_attention_mask = extended_span_attention_mask * extended_span_attention_mask.squeeze(-2).unsqueeze(-1)\n        span_attention_mask = span_attention_mask.byte()\n        feature_vector = self.fpe_span_attention(mean_feature_vector, span_attention_mask)\n\n        # feature_vector = mean_feature_vector\n        feature_vector = self.dropout(feature_vector)\n\n        logits = self.classifier(feature_vector)\n        logits = logits[:,:, :3] # main logits\n        return logits","metadata":{"_uuid":"eb2a3a23-59dd-4ad3-bac0-efeabd37cf5c","_cell_guid":"8a36ea7f-f447-47e2-a189-718ab7251b63","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:22:00.306948Z","iopub.execute_input":"2022-08-23T12:22:00.307446Z","iopub.status.idle":"2022-08-23T12:22:00.324359Z","shell.execute_reply.started":"2022-08-23T12:22:00.307411Z","shell.execute_reply":"2022-08-23T12:22:00.32324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_swa_checkpoint(checkpoint_path):\n    \"\"\"\n    helper function to process swa checkpoints\n    \"\"\"\n    ckpt = torch.load(checkpoint_path)\n\n    print(\"processing ckpt...\")\n    print(\"removing module from keys...\")\n    state_dict = ckpt['state_dict']\n    new_state_dict = OrderedDict()\n\n    for k, v in state_dict.items():\n        if k == \"n_averaged\":\n            continue\n        name = k[7:]  # remove 'module.'\n        new_state_dict[name] = v\n    processed_state = {\"state_dict\": new_state_dict}\n\n    # delete old state\n    del state_dict\n    gc.collect()\n\n    return processed_state","metadata":{"_uuid":"84b9442e-17b5-4021-b6d3-9f5cc44d2ca1","_cell_guid":"4bddd93a-f83e-46fe-9390-8e26e4312a2f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:22:00.325636Z","iopub.execute_input":"2022-08-23T12:22:00.326406Z","iopub.status.idle":"2022-08-23T12:22:00.337283Z","shell.execute_reply.started":"2022-08-23T12:22:00.326372Z","shell.execute_reply":"2022-08-23T12:22:00.336272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoints = [\n    \"../input/a-delv3-prod-8-folds/fpe_model_fold_0_best.pth.tar\",\n    \"../input/a-delv3-prod-8-folds/fpe_model_fold_1_best.pth.tar\",\n    \"../input/a-delv3-prod-8-folds/fpe_model_fold_2_best.pth.tar\",\n    \"../input/a-delv3-prod-8-folds/fpe_model_fold_3_best.pth.tar\",\n    \"../input/a-delv3-prod-8-folds/fpe_model_fold_4_best.pth.tar\",\n    \"../input/a-delv3-prod-8-folds/fpe_model_fold_5_best.pth.tar\",\n    \"../input/a-delv3-prod-8-folds/fpe_model_fold_6_best.pth.tar\",\n    \"../input/a-delv3-prod-8-folds/fpe_model_fold_7_best.pth.tar\",\n]","metadata":{"_uuid":"6cdad44c-0f96-4356-8128-5f9237ebc562","_cell_guid":"05d58172-42b2-4268-9cad-331a5d22add4","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:22:00.340684Z","iopub.execute_input":"2022-08-23T12:22:00.341175Z","iopub.status.idle":"2022-08-23T12:22:00.347035Z","shell.execute_reply.started":"2022-08-23T12:22:00.341099Z","shell.execute_reply":"2022-08-23T12:22:00.34579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def inference_fn(model, infer_dl, model_id):\n    all_preds = []\n    all_uids = []\n    accelerator = Accelerator()\n    model, infer_dl = accelerator.prepare(model, infer_dl)\n    \n    model.eval()\n    tk0 = tqdm(infer_dl, total=len(infer_dl))\n    \n    for batch in tk0:\n        with torch.no_grad():\n            logits = model(**batch) # (b, nd, 3)\n            batch_preds = F.softmax(logits, dim=-1)\n            batch_uids = batch[\"uids\"]\n        all_preds.append(batch_preds)\n        all_uids.append(batch_uids)\n    \n    all_preds = [p.to('cpu').detach().numpy().tolist() for p in all_preds]\n    all_preds = list(chain(*all_preds))\n    flat_preds = list(chain(*all_preds))\n    \n    all_uids = [p.to('cpu').detach().numpy().tolist() for p in all_uids]\n    all_uids = list(chain(*all_uids))\n    flat_uids = list(chain(*all_uids))    \n    \n    preds_df = pd.DataFrame(flat_preds)\n    preds_df.columns = [\"Ineffective\", \"Adequate\", \"Effective\"]\n    preds_df[\"span_uid\"] = flat_uids # SORTED_DISCOURSE_IDS\n    preds_df = preds_df[preds_df[\"span_uid\"]>=0].copy()\n    preds_df[\"discourse_id\"] = preds_df[\"span_uid\"].map(idx2discourse)\n    preds_df = preds_df[[\"discourse_id\", \"Ineffective\", \"Adequate\", \"Effective\"]].copy()\n    preds_df.to_csv(f\"exp01_delv3_8folds_model_preds_{model_id}.csv\", index=False)\n    \nif use_exp1:\n\n    for model_id, checkpoint in enumerate(checkpoints):\n        print(f\"infering from {checkpoint}\")\n        model = FeedbackModel(config)\n        if \"swa\" in checkpoint:\n            ckpt = process_swa_checkpoint(checkpoint)\n        else:\n            ckpt = torch.load(checkpoint)\n            print(f\"validation score for fold {model_id} = {ckpt['loss']}\")\n        model.load_state_dict(ckpt['state_dict'])\n        inference_fn(model, infer_dl, model_id)\n\n    del model\n    # del tokenizer, infer_dataset, infer_ds_dict, data_collector, infer_dl\n    gc.collect()","metadata":{"_uuid":"a98da7ee-5d7f-4dfa-bfcd-5182b466f5ff","_cell_guid":"c87e6ab7-e13e-434a-a2b8-03ecbcc48e6d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:22:00.353776Z","iopub.execute_input":"2022-08-23T12:22:00.35449Z","iopub.status.idle":"2022-08-23T12:27:28.357969Z","shell.execute_reply.started":"2022-08-23T12:22:00.354464Z","shell.execute_reply":"2022-08-23T12:27:28.357141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if use_exp1:\n    import glob\n    import pandas as pd\n\n    csvs = glob.glob(\"exp01_delv3_8folds_model_preds_*.csv\")\n\n    idx = []\n    preds = []\n\n\n    for csv_idx, csv in enumerate(csvs):\n\n        print(\"==\"*40)\n        print(f\"preds in {csv}\")\n        df = pd.read_csv(csv)\n        df = df.sort_values(by=[\"discourse_id\"])\n        print(df.head(10))\n        print(\"==\"*40)\n\n        temp_preds = df.drop([\"discourse_id\"], axis=1).values\n        if csv_idx == 0:\n            idx = list(df[\"discourse_id\"])\n            preds = temp_preds\n        else:\n            preds += temp_preds\n\n    preds = preds / len(csvs)\n\n    exp01_df = pd.DataFrame()\n    exp01_df[\"discourse_id\"] = idx\n    exp01_df[\"Ineffective\"]  = preds[:, 0]\n    exp01_df[\"Adequate\"]     = preds[:, 1]\n    exp01_df[\"Effective\"]    = preds[:, 2]\n\n    exp01_df = exp01_df.groupby(\"discourse_id\")[[\"Ineffective\", \"Adequate\", \"Effective\"]].agg(np.mean).reset_index()","metadata":{"_uuid":"50e32248-b541-4162-ad36-2d7ff40ef9bd","_cell_guid":"9a78737d-fc5f-44e3-9f1b-c73c1a0c2b49","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:27:28.360396Z","iopub.execute_input":"2022-08-23T12:27:28.360681Z","iopub.status.idle":"2022-08-23T12:27:28.415243Z","shell.execute_reply.started":"2022-08-23T12:27:28.360654Z","shell.execute_reply":"2022-08-23T12:27:28.414422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if use_exp1:\n    exp01_df.head()","metadata":{"_uuid":"030294a0-13dd-4680-93f6-0a6c677a6158","_cell_guid":"0bc795ce-10ae-49a1-9ada-9d3bce4e1442","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:27:28.416837Z","iopub.execute_input":"2022-08-23T12:27:28.417547Z","iopub.status.idle":"2022-08-23T12:27:28.42239Z","shell.execute_reply.started":"2022-08-23T12:27:28.417505Z","shell.execute_reply":"2022-08-23T12:27:28.421472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exp 10: Revisited DELV3 8 Folds","metadata":{"_uuid":"95291403-f42f-4088-acb1-876c2901b189","_cell_guid":"85979dcc-8f21-4554-842d-a5aefc9c9d70","trusted":true}},{"cell_type":"code","source":"config = \"\"\"{\n    \"debug\": false,\n\n    \"base_model_path\": \"../input/tapt-fpe-delv3-span-mlm-04\",\n    \"model_dir\": \"./outputs\",\n\n    \"max_length\": 1024,\n    \"stride\": 256,\n    \"num_labels\": 5,\n    \"dropout\": 0.1,\n    \"infer_bs\": 8\n}\n\"\"\"\nconfig = json.loads(config)\n\nimport gc\nimport pdb\nfrom collections import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import LayerNorm\n\nimport torch.utils.checkpoint\nfrom transformers import AutoConfig, AutoModel\nfrom transformers.models.deberta_v2.modeling_deberta_v2 import ContextPooler, StableDropout, DebertaV2Attention\n\n\n#-------- Model ------------------------------------------------------------------#\nclass FeedbackModel(nn.Module):\n    \"\"\"The feedback prize effectiveness baseline model\n    \"\"\"\n\n    def __init__(self, config):\n        super(FeedbackModel, self).__init__()\n        self.config = config\n\n        # base transformer\n        base_config = AutoConfig.from_pretrained(self.config[\"base_model_path\"])\n        base_config.update({\"add_pooling_layer\": False, \"max_position_embeddings\": 1024})\n        self.base_model = AutoModel.from_pretrained(self.config[\"base_model_path\"], config=base_config)\n\n        # dropouts\n        self.dropout = StableDropout(self.config[\"dropout\"])\n        \n        # multi-head attention\n        attention_config = deepcopy(self.base_model.config)\n        attention_config.update({\"relative_attention\": False})\n        self.fpe_span_attention = DebertaV2Attention(attention_config)\n        \n        # classification\n        hidden_size = self.base_model.config.hidden_size\n        feature_size = hidden_size\n        self.layer_norm = LayerNorm(feature_size, self.base_model.config.layer_norm_eps)\n        \n        # # LSTM Head\n        self.fpe_lstm_layer = nn.LSTM(\n            input_size=feature_size,\n            hidden_size=hidden_size//2,\n            num_layers=1,\n            batch_first=True,\n            bidirectional=True,\n        )\n\n        self.num_labels = self.config[\"num_labels\"]\n        self.classifier = nn.Linear(feature_size, self.config[\"num_labels\"])\n\n    def forward(self, input_ids, attention_mask, span_head_idxs, span_tail_idxs, span_attention_mask, **kwargs):\n        \n        bs = input_ids.shape[0]  # batch size\n        outputs = self.base_model(input_ids, attention_mask=attention_mask)\n        encoder_layer = outputs[0]\n        \n        self.fpe_lstm_layer.flatten_parameters()\n        encoder_layer = self.fpe_lstm_layer(encoder_layer)[0]\n\n        mean_feature_vector = []\n        for i in range(bs):  # TODO: vectorize\n            span_vec_i = []\n\n            for head, tail in zip(span_head_idxs[i], span_tail_idxs[i]):\n                # span feature\n                tmp = torch.mean(encoder_layer[i, head+1:tail], dim=0)  # [h]\n                span_vec_i.append(tmp)\n            span_vec_i = torch.stack(span_vec_i)  # (num_disourse, h)\n            mean_feature_vector.append(span_vec_i)\n\n        mean_feature_vector = torch.stack(mean_feature_vector)  # (bs, num_disourse, h)\n        mean_feature_vector = self.layer_norm(mean_feature_vector)\n\n        # attend to other features\n        extended_span_attention_mask = span_attention_mask.unsqueeze(1).unsqueeze(2)\n        span_attention_mask = extended_span_attention_mask * extended_span_attention_mask.squeeze(-2).unsqueeze(-1)\n        span_attention_mask = span_attention_mask.byte()\n        feature_vector = self.fpe_span_attention(mean_feature_vector, span_attention_mask)\n\n        # feature_vector = mean_feature_vector\n        feature_vector = self.dropout(feature_vector)\n\n        logits = self.classifier(feature_vector)\n        logits = logits[:,:, :3] # main logits\n        return logits","metadata":{"_uuid":"308935b0-10ef-4a86-b6ec-1c52327baf8c","_cell_guid":"34ec38c3-7e09-489a-8cba-22a83750603a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:27:28.423855Z","iopub.execute_input":"2022-08-23T12:27:28.424226Z","iopub.status.idle":"2022-08-23T12:27:28.441731Z","shell.execute_reply.started":"2022-08-23T12:27:28.424172Z","shell.execute_reply":"2022-08-23T12:27:28.440869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoints = [\n    \"../input/exp-10-delv3-revisited-8-folds/fpe_model_fold_0_best.pth.tar\",\n    \"../input/exp-10-delv3-revisited-8-folds/fpe_model_fold_1_best.pth.tar\",\n    \"../input/exp-10-delv3-revisited-8-folds/fpe_model_fold_2_best.pth.tar\",\n    \"../input/exp-10-delv3-revisited-8-folds/fpe_model_fold_3_best.pth.tar\",\n    \"../input/exp-10-delv3-revisited-8-folds/fpe_model_fold_4_best.pth.tar\",\n    \"../input/exp-10-delv3-revisited-8-folds/fpe_model_fold_5_best.pth.tar\",\n    \"../input/exp-10-delv3-revisited-8-folds/fpe_model_fold_6_best.pth.tar\",\n    \"../input/exp-10-delv3-revisited-8-folds/fpe_model_fold_7_best.pth.tar\",\n]","metadata":{"_uuid":"e317b477-2355-4c1d-a8a4-67c2d1ee91a2","_cell_guid":"5ca45a64-87ee-42bb-a528-aaad73c41b2c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:27:28.443083Z","iopub.execute_input":"2022-08-23T12:27:28.443494Z","iopub.status.idle":"2022-08-23T12:27:28.454695Z","shell.execute_reply.started":"2022-08-23T12:27:28.443461Z","shell.execute_reply":"2022-08-23T12:27:28.453947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def inference_fn(model, infer_dl, model_id):\n    all_preds = []\n    all_uids = []\n    accelerator = Accelerator()\n    model, infer_dl = accelerator.prepare(model, infer_dl)\n    \n    model.eval()\n    tk0 = tqdm(infer_dl, total=len(infer_dl))\n    \n    for batch in tk0:\n        with torch.no_grad():\n            logits = model(**batch) # (b, nd, 3)\n            batch_preds = F.softmax(logits, dim=-1)\n            batch_uids = batch[\"uids\"]\n        all_preds.append(batch_preds)\n        all_uids.append(batch_uids)\n    \n    all_preds = [p.to('cpu').detach().numpy().tolist() for p in all_preds]\n    all_preds = list(chain(*all_preds))\n    flat_preds = list(chain(*all_preds))\n    \n    all_uids = [p.to('cpu').detach().numpy().tolist() for p in all_uids]\n    all_uids = list(chain(*all_uids))\n    flat_uids = list(chain(*all_uids))    \n    \n    preds_df = pd.DataFrame(flat_preds)\n    preds_df.columns = [\"Ineffective\", \"Adequate\", \"Effective\"]\n    preds_df[\"span_uid\"] = flat_uids # SORTED_DISCOURSE_IDS\n    preds_df = preds_df[preds_df[\"span_uid\"]>=0].copy()\n    preds_df[\"discourse_id\"] = preds_df[\"span_uid\"].map(idx2discourse)\n    preds_df = preds_df[[\"discourse_id\", \"Ineffective\", \"Adequate\", \"Effective\"]].copy()\n    preds_df.to_csv(f\"exp10_delv3_8folds_revisited_preds_{model_id}.csv\", index=False)\n    \n\nif use_exp10:\n    for model_id, checkpoint in enumerate(checkpoints):\n        print(f\"infering from {checkpoint}\")\n        model = FeedbackModel(config)\n        if \"swa\" in checkpoint:\n            ckpt = process_swa_checkpoint(checkpoint)\n        else:\n            ckpt = torch.load(checkpoint)\n            print(f\"validation score for fold {model_id} = {ckpt['loss']}\")\n        model.load_state_dict(ckpt['state_dict'])\n        inference_fn(model, infer_dl, model_id)\n\n    del model\n    #del tokenizer, infer_dataset, infer_ds_dict, data_collector, infer_dl\n    torch.cuda.empty_cache()\n    gc.collect()","metadata":{"_uuid":"d433334b-e40a-420a-a60a-d0b24d851cce","_cell_guid":"f10b8aa3-c05e-4559-a4dd-63fe25094162","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:27:28.456285Z","iopub.execute_input":"2022-08-23T12:27:28.456697Z","iopub.status.idle":"2022-08-23T12:27:28.469943Z","shell.execute_reply.started":"2022-08-23T12:27:28.456663Z","shell.execute_reply":"2022-08-23T12:27:28.468974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if use_exp10:\n    import glob\n    import pandas as pd\n\n    csvs = glob.glob(\"exp10_delv3_8folds_revisited_preds_*.csv\")\n\n    idx = []\n    preds = []\n\n\n    for csv_idx, csv in enumerate(csvs):\n\n        print(\"==\"*40)\n        print(f\"preds in {csv}\")\n        df = pd.read_csv(csv)\n        df = df.sort_values(by=[\"discourse_id\"])\n        print(df.head(10))\n        print(\"==\"*40)\n\n        temp_preds = df.drop([\"discourse_id\"], axis=1).values\n        if csv_idx == 0:\n            idx = list(df[\"discourse_id\"])\n            preds = temp_preds\n        else:\n            preds += temp_preds\n\n    preds = preds / len(csvs)\n\n    exp10_df = pd.DataFrame()\n    exp10_df[\"discourse_id\"] = idx\n    exp10_df[\"Ineffective\"]  = preds[:, 0]\n    exp10_df[\"Adequate\"]     = preds[:, 1]\n    exp10_df[\"Effective\"]    = preds[:, 2]\n\n    exp10_df = exp10_df.groupby(\"discourse_id\")[[\"Ineffective\", \"Adequate\", \"Effective\"]].agg(np.mean).reset_index()","metadata":{"_uuid":"23cd8a32-225b-4ffc-ac99-e47fbd9f09e4","_cell_guid":"f6646942-d79e-43ce-a439-d4f37d32830f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:27:28.471061Z","iopub.execute_input":"2022-08-23T12:27:28.471998Z","iopub.status.idle":"2022-08-23T12:27:28.48272Z","shell.execute_reply.started":"2022-08-23T12:27:28.471964Z","shell.execute_reply":"2022-08-23T12:27:28.481887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if use_exp10:\n    exp10_df.head()","metadata":{"_uuid":"b4c5ac96-cab0-4a69-9934-8c754fa7e3cb","_cell_guid":"738c894b-b660-4d5b-b2f1-add7baddd036","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:27:28.484055Z","iopub.execute_input":"2022-08-23T12:27:28.48462Z","iopub.status.idle":"2022-08-23T12:27:28.49542Z","shell.execute_reply.started":"2022-08-23T12:27:28.484585Z","shell.execute_reply":"2022-08-23T12:27:28.494681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exp 11 - UDA","metadata":{"_uuid":"dd2f5d93-c2ad-4d82-a12d-6332817d78ea","_cell_guid":"ce07e016-e149-416b-a024-0f54e859f98b","trusted":true}},{"cell_type":"code","source":"config = \"\"\"{\n    \"debug\": false,\n\n    \"base_model_path\": \"../input/tapt-fpe-delv3-span-mlm-04\",\n    \"model_dir\": \"./outputs\",\n\n    \"max_length\": 1024,\n    \"stride\": 256,\n    \"num_labels\": 3,\n    \"dropout\": 0.1,\n    \"infer_bs\": 8\n}\n\"\"\"\nconfig = json.loads(config)","metadata":{"_uuid":"cd6123c5-ebc8-4564-9167-b94b86148575","_cell_guid":"4bbcb210-8f23-4338-a5ee-cabb50cb680c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:27:28.496582Z","iopub.execute_input":"2022-08-23T12:27:28.497003Z","iopub.status.idle":"2022-08-23T12:27:28.50413Z","shell.execute_reply.started":"2022-08-23T12:27:28.496968Z","shell.execute_reply":"2022-08-23T12:27:28.503238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\nimport pdb\nfrom collections import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import LayerNorm\n\nimport torch.utils.checkpoint\nfrom transformers import AutoConfig, AutoModel\nfrom transformers.models.deberta_v2.modeling_deberta_v2 import ContextPooler, StableDropout, DebertaV2Attention\n\n\n#-------- Model ------------------------------------------------------------------#\nclass FeedbackModel(nn.Module):\n    \"\"\"The feedback prize effectiveness baseline model\n    \"\"\"\n\n    def __init__(self, config):\n        super(FeedbackModel, self).__init__()\n        self.config = config\n\n        # base transformer\n        base_config = AutoConfig.from_pretrained(self.config[\"base_model_path\"])\n        base_config.update({\"add_pooling_layer\": False, \"max_position_embeddings\": 1024})\n        self.base_model = AutoModel.from_pretrained(self.config[\"base_model_path\"], config=base_config)\n\n        # dropouts\n        self.dropout = StableDropout(self.config[\"dropout\"])\n        \n        # multi-head attention\n        attention_config = deepcopy(self.base_model.config)\n        attention_config.update({\"relative_attention\": False})\n        self.fpe_span_attention = DebertaV2Attention(attention_config)\n        \n        # classification\n        hidden_size = self.base_model.config.hidden_size\n        feature_size = hidden_size\n        self.layer_norm = LayerNorm(feature_size, self.base_model.config.layer_norm_eps)\n        \n        # # LSTM Head\n        self.fpe_lstm_layer = nn.LSTM(\n            input_size=feature_size,\n            hidden_size=hidden_size//2,\n            num_layers=1,\n            batch_first=True,\n            bidirectional=True,\n        )\n\n        self.num_labels = self.config[\"num_labels\"]\n        self.classifier = nn.Linear(feature_size, self.config[\"num_labels\"])\n\n    def forward(self, input_ids, attention_mask, span_head_idxs, span_tail_idxs, span_attention_mask, **kwargs):\n        \n        bs = input_ids.shape[0]  # batch size\n        outputs = self.base_model(input_ids, attention_mask=attention_mask)\n        encoder_layer = outputs[0]\n        \n        self.fpe_lstm_layer.flatten_parameters()\n        encoder_layer = self.fpe_lstm_layer(encoder_layer)[0]\n\n        mean_feature_vector = []\n        for i in range(bs):  # TODO: vectorize\n            span_vec_i = []\n\n            for head, tail in zip(span_head_idxs[i], span_tail_idxs[i]):\n                # span feature\n                tmp = torch.mean(encoder_layer[i, head+1:tail], dim=0)  # [h]\n                span_vec_i.append(tmp)\n            span_vec_i = torch.stack(span_vec_i)  # (num_disourse, h)\n            mean_feature_vector.append(span_vec_i)\n\n        mean_feature_vector = torch.stack(mean_feature_vector)  # (bs, num_disourse, h)\n        mean_feature_vector = self.layer_norm(mean_feature_vector)\n\n        # attend to other features\n        extended_span_attention_mask = span_attention_mask.unsqueeze(1).unsqueeze(2)\n        span_attention_mask = extended_span_attention_mask * extended_span_attention_mask.squeeze(-2).unsqueeze(-1)\n        span_attention_mask = span_attention_mask.byte()\n        feature_vector = self.fpe_span_attention(mean_feature_vector, span_attention_mask)\n\n        # feature_vector = mean_feature_vector\n        feature_vector = self.dropout(feature_vector)\n\n        logits = self.classifier(feature_vector)\n        logits = logits[:,:, :3] # main logits\n        return logits","metadata":{"_uuid":"70939144-2a45-4ace-b943-a71a0d6a0554","_cell_guid":"a7ad8403-a2ee-4d46-a376-d565bfe33e42","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:27:28.5073Z","iopub.execute_input":"2022-08-23T12:27:28.507754Z","iopub.status.idle":"2022-08-23T12:27:28.523131Z","shell.execute_reply.started":"2022-08-23T12:27:28.507729Z","shell.execute_reply":"2022-08-23T12:27:28.52229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_swa_checkpoint(checkpoint_path):\n        ckpt = torch.load(checkpoint_path)\n\n        print(\"processing ckpt...\")\n        print(\"removing module from keys...\")\n        state_dict = ckpt['state_dict']\n        new_state_dict = OrderedDict()\n\n        for k, v in state_dict.items():\n            if k == \"n_averaged\":\n                print(f\"# of snapshots in {checkpoint_path} = {v}\")\n                continue\n            name = k[7:]  # remove 'module.'\n            new_state_dict[name] = v\n        processed_state = {\"state_dict\": new_state_dict}\n        \n        # delete old state\n        del state_dict\n        gc.collect()\n        \n        return processed_state","metadata":{"_uuid":"73d4b535-4643-4a51-8bf6-2fe9015da3f9","_cell_guid":"402da0bd-1e49-4257-9a0e-414ffd59fa2d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:27:28.52432Z","iopub.execute_input":"2022-08-23T12:27:28.524828Z","iopub.status.idle":"2022-08-23T12:27:28.535602Z","shell.execute_reply.started":"2022-08-23T12:27:28.524794Z","shell.execute_reply":"2022-08-23T12:27:28.534709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoints = [\n    \"../input/exp11-delv3-uda-4-folds/exp-11-delv3-uda-4-folds/fpe_model_fold_0_best.pth.tar\",\n    \"../input/exp11-delv3-uda-4-folds/exp-11-delv3-uda-4-folds/fpe_model_fold_1_best.pth.tar\",\n    \"../input/exp11-delv3-uda-4-folds/exp-11-delv3-uda-4-folds/fpe_model_fold_2_best.pth.tar\",\n    \"../input/exp11-delv3-uda-4-folds/exp-11-delv3-uda-4-folds/fpe_model_fold_3_best.pth.tar\",\n]\n\ndef inference_fn(model, infer_dl, model_id):\n    all_preds = []\n    all_uids = []\n    accelerator = Accelerator()\n    model, infer_dl = accelerator.prepare(model, infer_dl)\n    \n    model.eval()\n    tk0 = tqdm(infer_dl, total=len(infer_dl))\n    \n    for batch in tk0:\n        with torch.no_grad():\n            logits = model(**batch) # (b, nd, 3)\n            batch_preds = F.softmax(logits, dim=-1)\n            batch_uids = batch[\"uids\"]\n        all_preds.append(batch_preds)\n        all_uids.append(batch_uids)\n    \n    all_preds = [p.to('cpu').detach().numpy().tolist() for p in all_preds]\n    all_preds = list(chain(*all_preds))\n    flat_preds = list(chain(*all_preds))\n    \n    all_uids = [p.to('cpu').detach().numpy().tolist() for p in all_uids]\n    all_uids = list(chain(*all_uids))\n    flat_uids = list(chain(*all_uids))    \n    \n    preds_df = pd.DataFrame(flat_preds)\n    preds_df.columns = [\"Ineffective\", \"Adequate\", \"Effective\"]\n    preds_df[\"span_uid\"] = flat_uids # SORTED_DISCOURSE_IDS\n    preds_df = preds_df[preds_df[\"span_uid\"]>=0].copy()\n    preds_df[\"discourse_id\"] = preds_df[\"span_uid\"].map(idx2discourse)\n    preds_df = preds_df[[\"discourse_id\", \"Ineffective\", \"Adequate\", \"Effective\"]].copy()\n    preds_df.to_csv(f\"exp11_preds_{model_id}.csv\", index=False)\n    \nif use_exp11:\n    for model_id, checkpoint in enumerate(checkpoints):\n        print(f\"infering from {checkpoint}\")\n        model = FeedbackModel(config)\n        if \"swa\" in checkpoint:\n            ckpt = process_swa_checkpoint(checkpoint)\n        else:\n            ckpt = torch.load(checkpoint)\n            print(f\"validation score for fold {model_id} = {ckpt['loss']}\")\n        model.load_state_dict(ckpt['state_dict'])\n        inference_fn(model, infer_dl, model_id)\n\n    del model\n    #del tokenizer, infer_dataset, infer_ds_dict, data_collector, infer_dl\n    torch.cuda.empty_cache()\n    gc.collect()","metadata":{"_uuid":"238a5adf-9cc4-4c09-9da8-ed2677014f95","_cell_guid":"305a1e90-be58-439e-b306-73f79fd06557","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:27:28.536804Z","iopub.execute_input":"2022-08-23T12:27:28.53721Z","iopub.status.idle":"2022-08-23T12:27:28.551756Z","shell.execute_reply.started":"2022-08-23T12:27:28.537156Z","shell.execute_reply":"2022-08-23T12:27:28.550924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if use_exp11:\n    import glob\n    import pandas as pd\n\n    csvs = glob.glob(\"exp11_preds_*.csv\")\n\n    idx = []\n    preds = []\n\n\n    for csv_idx, csv in enumerate(csvs):\n\n        print(\"==\"*40)\n        print(f\"preds in {csv}\")\n        df = pd.read_csv(csv)\n        df = df.sort_values(by=[\"discourse_id\"])\n        print(df.head(10))\n        print(\"==\"*40)\n\n        temp_preds = df.drop([\"discourse_id\"], axis=1).values\n        if csv_idx == 0:\n            idx = list(df[\"discourse_id\"])\n            preds = temp_preds\n        else:\n            preds += temp_preds\n\n    preds = preds / len(csvs)\n\n    exp11_df = pd.DataFrame()\n    exp11_df[\"discourse_id\"] = idx\n    exp11_df[\"Ineffective\"] = preds[:, 0]\n    exp11_df[\"Adequate\"] = preds[:, 1]\n    exp11_df[\"Effective\"] = preds[:, 2]\n\n    exp11_df = exp11_df.groupby(\"discourse_id\")[[\"Ineffective\", \"Adequate\", \"Effective\"]].agg(np.mean).reset_index()\n    exp11_df.to_csv(\"submission.csv\", index=False)","metadata":{"_uuid":"2e3fb9f4-bdb9-4c14-8787-50eba551dc36","_cell_guid":"476166f4-ffef-4aed-8578-f6170d2bef9b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:27:28.553277Z","iopub.execute_input":"2022-08-23T12:27:28.553852Z","iopub.status.idle":"2022-08-23T12:27:28.564885Z","shell.execute_reply.started":"2022-08-23T12:27:28.553818Z","shell.execute_reply":"2022-08-23T12:27:28.564036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if use_exp11:\n    exp11_df.head()","metadata":{"_uuid":"37bae50a-b531-4ff4-9ab8-aa1cd18913fd","_cell_guid":"b2c228a1-f77f-41b1-ae43-4a4f515d7bc9","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:27:28.566324Z","iopub.execute_input":"2022-08-23T12:27:28.566664Z","iopub.status.idle":"2022-08-23T12:27:28.573003Z","shell.execute_reply.started":"2022-08-23T12:27:28.566631Z","shell.execute_reply":"2022-08-23T12:27:28.572328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\nimport pdb\nfrom collections import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import LayerNorm\n\nimport torch.utils.checkpoint\nfrom transformers import AutoConfig, AutoModel\nfrom transformers.models.deberta_v2.modeling_deberta_v2 import ContextPooler, StableDropout, DebertaV2Attention\n\n\n#-------- Model ------------------------------------------------------------------#\nclass FeedbackModel(nn.Module):\n    \"\"\"The feedback prize effectiveness baseline model\n    \"\"\"\n\n    def __init__(self, config):\n        super(FeedbackModel, self).__init__()\n        self.config = config\n\n        # base transformer\n        base_config = AutoConfig.from_pretrained(self.config[\"base_model_path\"])\n        base_config.update({\"add_pooling_layer\": False, \"max_position_embeddings\": 1024})\n        self.base_model = AutoModel.from_pretrained(self.config[\"base_model_path\"], config=base_config)\n\n        # dropouts\n        self.dropout = StableDropout(self.config[\"dropout\"])\n        \n        # multi-head attention\n        attention_config = deepcopy(self.base_model.config)\n        attention_config.update({\"relative_attention\": False})\n        self.fpe_span_attention = DebertaV2Attention(attention_config)\n        \n        # classification\n        hidden_size = self.base_model.config.hidden_size\n        feature_size = hidden_size\n        self.layer_norm = LayerNorm(feature_size, self.base_model.config.layer_norm_eps)\n        \n        # # LSTM Head\n        self.fpe_lstm_layer = nn.LSTM(\n            input_size=feature_size,\n            hidden_size=hidden_size//2,\n            num_layers=1,\n            batch_first=True,\n            bidirectional=True,\n        )\n\n        self.num_labels = self.config[\"num_labels\"]\n        self.classifier = nn.Linear(feature_size, self.config[\"num_labels\"])\n\n    def forward(self, input_ids, attention_mask, span_head_idxs, span_tail_idxs, span_attention_mask, **kwargs):\n        \n        bs = input_ids.shape[0]  # batch size\n        outputs = self.base_model(input_ids, attention_mask=attention_mask)\n        encoder_layer = outputs[0]\n        \n        self.fpe_lstm_layer.flatten_parameters()\n        encoder_layer = self.fpe_lstm_layer(encoder_layer)[0]\n\n        mean_feature_vector = []\n        for i in range(bs):  # TODO: vectorize\n            span_vec_i = []\n\n            for head, tail in zip(span_head_idxs[i], span_tail_idxs[i]):\n                # span feature\n                tmp = torch.mean(encoder_layer[i, head+1:tail], dim=0)  # [h]\n                span_vec_i.append(tmp)\n            span_vec_i = torch.stack(span_vec_i)  # (num_disourse, h)\n            mean_feature_vector.append(span_vec_i)\n\n        mean_feature_vector = torch.stack(mean_feature_vector)  # (bs, num_disourse, h)\n        mean_feature_vector = self.layer_norm(mean_feature_vector)\n\n        # attend to other features\n        extended_span_attention_mask = span_attention_mask.unsqueeze(1).unsqueeze(2)\n        span_attention_mask = extended_span_attention_mask * extended_span_attention_mask.squeeze(-2).unsqueeze(-1)\n        span_attention_mask = span_attention_mask.byte()\n        feature_vector = self.fpe_span_attention(mean_feature_vector, span_attention_mask)\n\n        # feature_vector = mean_feature_vector\n        feature_vector = self.dropout(feature_vector)\n\n        logits = self.classifier(feature_vector)\n        logits = logits[:,:, :3] # main logits\n        return logits","metadata":{"execution":{"iopub.status.busy":"2022-08-23T12:27:28.574339Z","iopub.execute_input":"2022-08-23T12:27:28.57488Z","iopub.status.idle":"2022-08-23T12:27:28.591365Z","shell.execute_reply.started":"2022-08-23T12:27:28.574844Z","shell.execute_reply":"2022-08-23T12:27:28.590484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoints = [\n    \"../input/exp-16-part-1/fpe_model_fold_0_best.pth.tar\",\n    \"../input/exp-16-part-1/fpe_model_fold_1_best.pth.tar\",\n    \"../input/exp-16-part-1/fpe_model_fold_2_best.pth.tar\",\n    \"../input/exp-16-part-1/fpe_model_fold_3_best.pth.tar\",\n    \"../input/exp-16-part-1/fpe_model_fold_4_best.pth.tar\",\n    \"../input/exp-16-part-2/fpe_model_fold_5_best.pth.tar\",\n    \"../input/exp-16-part-2/fpe_model_fold_6_best.pth.tar\",\n    \"../input/exp-16-part-2/fpe_model_fold_7_best.pth.tar\",\n    \"../input/exp-16-part-2/fpe_model_fold_8_best.pth.tar\",\n    \"../input/exp-16-part-2/fpe_model_fold_9_best.pth.tar\",\n]\n\ndef inference_fn(model, infer_dl, model_id):\n    all_preds = []\n    all_uids = []\n    accelerator = Accelerator()\n    model, infer_dl = accelerator.prepare(model, infer_dl)\n    \n    model.eval()\n    tk0 = tqdm(infer_dl, total=len(infer_dl))\n    \n    for batch in tk0:\n        with torch.no_grad():\n            logits = model(**batch) # (b, nd, 3)\n            batch_preds = F.softmax(logits, dim=-1)\n            batch_uids = batch[\"uids\"]\n        all_preds.append(batch_preds)\n        all_uids.append(batch_uids)\n    \n    all_preds = [p.to('cpu').detach().numpy().tolist() for p in all_preds]\n    all_preds = list(chain(*all_preds))\n    flat_preds = list(chain(*all_preds))\n    \n    all_uids = [p.to('cpu').detach().numpy().tolist() for p in all_uids]\n    all_uids = list(chain(*all_uids))\n    flat_uids = list(chain(*all_uids))    \n    \n    preds_df = pd.DataFrame(flat_preds)\n    preds_df.columns = [\"Ineffective\", \"Adequate\", \"Effective\"]\n    preds_df[\"span_uid\"] = flat_uids # SORTED_DISCOURSE_IDS\n    preds_df = preds_df[preds_df[\"span_uid\"]>=0].copy()\n    preds_df[\"discourse_id\"] = preds_df[\"span_uid\"].map(idx2discourse)\n    preds_df = preds_df[[\"discourse_id\", \"Ineffective\", \"Adequate\", \"Effective\"]].copy()\n    preds_df.to_csv(f\"exp16_preds_{model_id}.csv\", index=False)\n    \n\nfor model_id, checkpoint in enumerate(checkpoints):\n    print(f\"infering from {checkpoint}\")\n    model = FeedbackModel(config)\n    if \"swa\" in checkpoint:\n        ckpt = process_swa_checkpoint(checkpoint)\n    else:\n        ckpt = torch.load(checkpoint)\n        print(f\"validation score for fold {model_id} = {ckpt['loss']}\")\n    model.load_state_dict(ckpt['state_dict'])\n    inference_fn(model, infer_dl, model_id)\n\ndel model\n# del tokenizer, infer_dataset, infer_ds_dict, data_collector, infer_dl\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-08-23T12:27:28.592916Z","iopub.execute_input":"2022-08-23T12:27:28.593355Z","iopub.status.idle":"2022-08-23T12:34:05.517472Z","shell.execute_reply.started":"2022-08-23T12:27:28.593321Z","shell.execute_reply":"2022-08-23T12:34:05.516611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import glob\nimport pandas as pd\n\ncsvs = glob.glob(\"exp16_preds_*.csv\")\n\nidx = []\npreds = []\n\n\nfor csv_idx, csv in enumerate(csvs):\n    \n    print(\"==\"*40)\n    print(f\"preds in {csv}\")\n    df = pd.read_csv(csv)\n    df = df.sort_values(by=[\"discourse_id\"])\n    print(df.head(10))\n    print(\"==\"*40)\n    \n    temp_preds = df.drop([\"discourse_id\"], axis=1).values\n    if csv_idx == 0:\n        idx = list(df[\"discourse_id\"])\n        preds = temp_preds\n    else:\n        preds += temp_preds\n\npreds = preds / len(csvs)\n\nexp16_df = pd.DataFrame()\nexp16_df[\"discourse_id\"] = idx\nexp16_df[\"Ineffective\"] = preds[:, 0]\nexp16_df[\"Adequate\"] = preds[:, 1]\nexp16_df[\"Effective\"] = preds[:, 2]\n\nexp16_df = exp16_df.groupby(\"discourse_id\")[[\"Ineffective\", \"Adequate\", \"Effective\"]].agg(np.mean).reset_index()\n","metadata":{"execution":{"iopub.status.busy":"2022-08-23T12:34:05.519891Z","iopub.execute_input":"2022-08-23T12:34:05.520335Z","iopub.status.idle":"2022-08-23T12:34:05.581466Z","shell.execute_reply.started":"2022-08-23T12:34:05.520307Z","shell.execute_reply":"2022-08-23T12:34:05.58068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"exp16_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-23T12:34:05.582734Z","iopub.execute_input":"2022-08-23T12:34:05.583264Z","iopub.status.idle":"2022-08-23T12:34:05.595033Z","shell.execute_reply.started":"2022-08-23T12:34:05.583228Z","shell.execute_reply":"2022-08-23T12:34:05.594111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ALL Data Trained Models","metadata":{}},{"cell_type":"code","source":"import gc\nimport pdb\nfrom collections import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import LayerNorm\n\nimport torch.utils.checkpoint\nfrom transformers import AutoConfig, AutoModel\nfrom transformers.models.deberta_v2.modeling_deberta_v2 import StableDropout, DebertaV2Attention\n\n\n#-------- Model ------------------------------------------------------------------#\nclass FeedbackModel(nn.Module):\n    \"\"\"The feedback prize effectiveness baseline model\n    \"\"\"\n\n    def __init__(self, config):\n        super(FeedbackModel, self).__init__()\n        self.config = config\n\n        # base transformer\n        base_config = AutoConfig.from_pretrained(self.config[\"base_model_path\"])\n        base_config.update({\"add_pooling_layer\": False, \"max_position_embeddings\": 1024})\n        self.base_model = AutoModel.from_pretrained(self.config[\"base_model_path\"], config=base_config)\n\n        # dropouts\n        self.dropout = StableDropout(self.config[\"dropout\"])\n\n        # multi-head attention\n        attention_config = deepcopy(self.base_model.config)\n        attention_config.update({\"relative_attention\": False})\n        self.fpe_span_attention = DebertaV2Attention(attention_config)\n\n        # classification\n        hidden_size = self.base_model.config.hidden_size\n        feature_size = hidden_size\n        self.layer_norm = LayerNorm(feature_size, self.base_model.config.layer_norm_eps)\n\n        # # LSTM Head\n        self.fpe_lstm_layer = nn.LSTM(\n            input_size=feature_size,\n            hidden_size=hidden_size//2,\n            num_layers=1,\n            batch_first=True,\n            bidirectional=True,\n        )\n\n        self.num_labels = self.config[\"num_labels\"]\n        self.classifier = nn.Linear(feature_size, self.config[\"num_labels\"])\n\n    def forward(self, input_ids, attention_mask, span_head_idxs, span_tail_idxs, span_attention_mask, **kwargs):\n\n        bs = input_ids.shape[0]  # batch size\n        outputs = self.base_model(input_ids, attention_mask=attention_mask)\n        encoder_layer = outputs[0]\n\n        self.fpe_lstm_layer.flatten_parameters()\n        encoder_layer = self.fpe_lstm_layer(encoder_layer)[0]\n\n        mean_feature_vector = []\n        for i in range(bs):  # TODO: vectorize\n            span_vec_i = []\n\n            for head, tail in zip(span_head_idxs[i], span_tail_idxs[i]):\n                # span feature\n                tmp = torch.mean(encoder_layer[i, head+1:tail], dim=0)  # [h]\n                span_vec_i.append(tmp)\n            span_vec_i = torch.stack(span_vec_i)  # (num_disourse, h)\n            mean_feature_vector.append(span_vec_i)\n\n        mean_feature_vector = torch.stack(mean_feature_vector)  # (bs, num_disourse, h)\n        mean_feature_vector = self.layer_norm(mean_feature_vector)\n\n        # attend to other features\n        extended_span_attention_mask = span_attention_mask.unsqueeze(1).unsqueeze(2)\n        span_attention_mask = extended_span_attention_mask * extended_span_attention_mask.squeeze(-2).unsqueeze(-1)\n        span_attention_mask = span_attention_mask.byte()\n        feature_vector = self.fpe_span_attention(mean_feature_vector, span_attention_mask)\n\n        # feature_vector = mean_feature_vector\n        feature_vector = self.dropout(feature_vector)\n\n        logits = self.classifier(feature_vector)\n        logits = logits[:,:, :3] # main logits\n        return logits\n    \n\n#########################\nfrom copy import deepcopy\n\ncheckpoints = [\n    \"../input/delv3-all-folds/swa_fpe_all_exp_10a_mask_aug.pth.tar\",\n    \"../input/delv3-all-folds/fpe_all_exp_16a_mixout_high_gamma_high_mask_aug.pth.tar\",\n]\n\ndef inference_fn(model, infer_dl, model_id):\n    all_preds = []\n    all_uids = []\n    accelerator = Accelerator()\n    model, infer_dl = accelerator.prepare(model, infer_dl)\n\n    model.eval()\n    tk0 = tqdm(infer_dl, total=len(infer_dl))\n\n    for batch in tk0:\n        with torch.no_grad():\n            logits = model(**batch) # (b, nd, 3)\n            batch_preds = F.softmax(logits, dim=-1)\n            batch_uids = batch[\"uids\"]\n        all_preds.append(batch_preds)\n        all_uids.append(batch_uids)\n\n    all_preds = [p.to('cpu').detach().numpy().tolist() for p in all_preds]\n    all_preds = list(chain(*all_preds))\n    flat_preds = list(chain(*all_preds))\n\n    all_uids = [p.to('cpu').detach().numpy().tolist() for p in all_uids]\n    all_uids = list(chain(*all_uids))\n    flat_uids = list(chain(*all_uids))    \n\n    preds_df = pd.DataFrame(flat_preds)\n    preds_df.columns = [\"Ineffective\", \"Adequate\", \"Effective\"]\n    preds_df[\"span_uid\"] = flat_uids # SORTED_DISCOURSE_IDS\n    preds_df = preds_df[preds_df[\"span_uid\"]>=0].copy()\n    preds_df[\"discourse_id\"] = preds_df[\"span_uid\"].map(idx2discourse)\n    preds_df = preds_df[[\"discourse_id\", \"Ineffective\", \"Adequate\", \"Effective\"]].copy()\n    preds_df.to_csv(f\"exp_rb_full_data_preds_{model_id}.csv\", index=False)\n\n\nfor model_id, checkpoint in enumerate(checkpoints):\n    print(f\"infering from {checkpoint}\")\n    new_config = deepcopy(config)\n    new_config[\"num_labels\"] = 5\n\n    model = FeedbackModel(new_config)\n    if \"swa\" in checkpoint:\n        ckpt = process_swa_checkpoint(checkpoint)\n    else:\n        ckpt = torch.load(checkpoint)\n        print(f\"validation score for fold {model_id} = {ckpt['loss']}\")\n    model.load_state_dict(ckpt['state_dict'])\n    inference_fn(model, infer_dl, model_id)\n\ndel model\n# del tokenizer, infer_dataset, infer_ds_dict, data_collector, infer_dl\ntorch.cuda.empty_cache()\ngc.collect()\n\n\n\nimport glob\nimport pandas as pd\n\ncsvs = glob.glob(\"exp_rb_full_data_preds_*.csv\")\n\nidx = []\npreds = []\n\n\nfor csv_idx, csv in enumerate(csvs):\n\n    print(\"==\"*40)\n    print(f\"preds in {csv}\")\n    df = pd.read_csv(csv)\n    df = df.sort_values(by=[\"discourse_id\"])\n    print(df.head(10))\n    print(\"==\"*40)\n\n    temp_preds = df.drop([\"discourse_id\"], axis=1).values\n    if csv_idx == 0:\n        idx = list(df[\"discourse_id\"])\n        preds = temp_preds\n    else:\n        preds += temp_preds\n\npreds = preds / len(csvs)\n\nexp99_rb_all_df = pd.DataFrame()\nexp99_rb_all_df[\"discourse_id\"] = idx\nexp99_rb_all_df[\"Ineffective\"] = preds[:, 0]\nexp99_rb_all_df[\"Adequate\"] = preds[:, 1]\nexp99_rb_all_df[\"Effective\"] = preds[:, 2]\n\nexp99_rb_all_df = exp99_rb_all_df.groupby(\"discourse_id\")[[\"Ineffective\", \"Adequate\", \"Effective\"]].agg(np.mean).reset_index()\nexp99_rb_all_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-23T12:34:05.598331Z","iopub.execute_input":"2022-08-23T12:34:05.598588Z","iopub.status.idle":"2022-08-23T12:35:25.415572Z","shell.execute_reply.started":"2022-08-23T12:34:05.598564Z","shell.execute_reply":"2022-08-23T12:35:25.414744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### UDA","metadata":{}},{"cell_type":"code","source":"# # UDA Model\n# config = \"\"\"{\n#     \"debug\": false,\n\n#     \"base_model_path\": \"../input/tapt-fpe-delv3-span-mlm-04\",\n#     \"model_dir\": \"./outputs\",\n\n#     \"max_length\": 1024,\n#     \"stride\": 256,\n#     \"num_labels\": 3,\n#     \"dropout\": 0.1,\n#     \"infer_bs\": 12\n# }\n# \"\"\"\n# config = json.loads(config)\n# config[\"len_tokenizer\"] = len(tokenizer)\n\n# import gc\n# import pdb\n# from collections import OrderedDict\n\n# import torch\n# import torch.nn as nn\n# import torch.nn.functional as F\n# from torch.nn import LayerNorm\n\n# import torch.utils.checkpoint\n# from transformers import AutoConfig, AutoModel\n# from transformers.models.deberta_v2.modeling_deberta_v2 import ContextPooler, StableDropout, DebertaV2Attention\n\n\n# #-------- Model ------------------------------------------------------------------#\n# class FeedbackModel(nn.Module):\n#     \"\"\"The feedback prize effectiveness baseline model\n#     \"\"\"\n\n#     def __init__(self, config):\n#         super(FeedbackModel, self).__init__()\n#         self.config = config\n\n#         # base transformer\n#         base_config = AutoConfig.from_pretrained(self.config[\"base_model_path\"])\n#         base_config.update({\"add_pooling_layer\": False, \"max_position_embeddings\": 1024})\n#         self.base_model = AutoModel.from_pretrained(self.config[\"base_model_path\"], config=base_config)\n\n#         # dropouts\n#         self.dropout = StableDropout(self.config[\"dropout\"])\n        \n#         # multi-head attention\n#         attention_config = deepcopy(self.base_model.config)\n#         attention_config.update({\"relative_attention\": False})\n#         self.fpe_span_attention = DebertaV2Attention(attention_config)\n        \n#         # classification\n#         hidden_size = self.base_model.config.hidden_size\n#         feature_size = hidden_size\n#         self.layer_norm = LayerNorm(feature_size, self.base_model.config.layer_norm_eps)\n        \n#         # # LSTM Head\n#         self.fpe_lstm_layer = nn.LSTM(\n#             input_size=feature_size,\n#             hidden_size=hidden_size//2,\n#             num_layers=1,\n#             batch_first=True,\n#             bidirectional=True,\n#         )\n\n#         self.num_labels = self.config[\"num_labels\"]\n#         self.classifier = nn.Linear(feature_size, self.config[\"num_labels\"])\n\n#     def forward(self, input_ids, attention_mask, span_head_idxs, span_tail_idxs, span_attention_mask, **kwargs):\n        \n#         bs = input_ids.shape[0]  # batch size\n#         outputs = self.base_model(input_ids, attention_mask=attention_mask)\n#         encoder_layer = outputs[0]\n        \n#         self.fpe_lstm_layer.flatten_parameters()\n#         encoder_layer = self.fpe_lstm_layer(encoder_layer)[0]\n\n#         mean_feature_vector = []\n#         for i in range(bs):  # TODO: vectorize\n#             span_vec_i = []\n\n#             for head, tail in zip(span_head_idxs[i], span_tail_idxs[i]):\n#                 # span feature\n#                 tmp = torch.mean(encoder_layer[i, head+1:tail], dim=0)  # [h]\n#                 span_vec_i.append(tmp)\n#             span_vec_i = torch.stack(span_vec_i)  # (num_disourse, h)\n#             mean_feature_vector.append(span_vec_i)\n\n#         mean_feature_vector = torch.stack(mean_feature_vector)  # (bs, num_disourse, h)\n#         mean_feature_vector = self.layer_norm(mean_feature_vector)\n\n#         # attend to other features\n#         extended_span_attention_mask = span_attention_mask.unsqueeze(1).unsqueeze(2)\n#         span_attention_mask = extended_span_attention_mask * extended_span_attention_mask.squeeze(-2).unsqueeze(-1)\n#         span_attention_mask = span_attention_mask.byte()\n#         feature_vector = self.fpe_span_attention(mean_feature_vector, span_attention_mask)\n\n#         # feature_vector = mean_feature_vector\n#         feature_vector = self.dropout(feature_vector)\n\n#         logits = self.classifier(feature_vector)\n#         logits = logits[:,:, :3] # main logits\n#         return logits\n    \n# ########################\n# checkpoints = [\n#     \"../input/exp21-uda-all-data/fpe_model_kd_seed_1.pth.tar\",\n#     \"../input/exp21-uda-all-data/fpe_model_kd_seed_2.pth.tar\",\n    \n# ]\n\n# def inference_fn(model, infer_dl, model_id):\n#     all_preds = []\n#     all_uids = []\n#     accelerator = Accelerator()\n#     model, infer_dl = accelerator.prepare(model, infer_dl)\n    \n#     model.eval()\n#     tk0 = tqdm(infer_dl, total=len(infer_dl))\n    \n#     for batch in tk0:\n#         with torch.no_grad():\n#             logits = model(**batch) # (b, nd, 3)\n#             batch_preds = F.softmax(logits, dim=-1)\n#             batch_uids = batch[\"uids\"]\n#         all_preds.append(batch_preds)\n#         all_uids.append(batch_uids)\n    \n#     all_preds = [p.to('cpu').detach().numpy().tolist() for p in all_preds]\n#     all_preds = list(chain(*all_preds))\n#     flat_preds = list(chain(*all_preds))\n    \n#     all_uids = [p.to('cpu').detach().numpy().tolist() for p in all_uids]\n#     all_uids = list(chain(*all_uids))\n#     flat_uids = list(chain(*all_uids))    \n    \n#     preds_df = pd.DataFrame(flat_preds)\n#     preds_df.columns = [\"Ineffective\", \"Adequate\", \"Effective\"]\n#     preds_df[\"span_uid\"] = flat_uids # SORTED_DISCOURSE_IDS\n#     preds_df = preds_df[preds_df[\"span_uid\"]>=0].copy()\n#     preds_df[\"discourse_id\"] = preds_df[\"span_uid\"].map(idx2discourse)\n#     preds_df = preds_df[[\"discourse_id\", \"Ineffective\", \"Adequate\", \"Effective\"]].copy()\n#     preds_df.to_csv(f\"exp21f_uda_preds_{model_id}.csv\", index=False)\n    \n# from copy import deepcopy\n# for model_id, checkpoint in enumerate(checkpoints):\n#     print(f\"infering from {checkpoint}\")\n#     new_config = deepcopy(config)\n#     if \"10a\" in checkpoint:\n#         new_config[\"num_labels\"] = 5\n        \n#     model = FeedbackModel(new_config)\n#     if \"swa\" in checkpoint:\n#         ckpt = process_swa_checkpoint(checkpoint)\n#     else:\n#         ckpt = torch.load(checkpoint)\n#         print(f\"validation score for fold {model_id} = {ckpt['loss']}\")\n#     model.load_state_dict(ckpt['state_dict'])\n#     inference_fn(model, infer_dl, model_id)\n    \n# del model\n# gc.collect()\n# torch.cuda.empty_cache()\n\n# #####\n# import glob\n# import pandas as pd\n\n# csvs = glob.glob(\"exp21f_uda_preds_*.csv\")\n\n# idx = []\n# preds = []\n\n\n# for csv_idx, csv in enumerate(csvs):\n    \n#     print(\"==\"*40)\n#     print(f\"preds in {csv}\")\n#     df = pd.read_csv(csv)\n#     df = df.sort_values(by=[\"discourse_id\"])\n#     print(df.head(10))\n#     print(\"==\"*40)\n    \n#     temp_preds = df.drop([\"discourse_id\"], axis=1).values\n#     if csv_idx == 0:\n#         idx = list(df[\"discourse_id\"])\n#         preds = temp_preds\n#     else:\n#         preds += temp_preds\n\n# preds = preds / len(csvs)\n\n# exp21f_df = pd.DataFrame()\n# exp21f_df[\"discourse_id\"] = idx\n# exp21f_df[\"Ineffective\"] = preds[:, 0]\n# exp21f_df[\"Adequate\"] = preds[:, 1]\n# exp21f_df[\"Effective\"] = preds[:, 2]\n\n# exp21f_df = exp21f_df.groupby(\"discourse_id\")[[\"Ineffective\", \"Adequate\", \"Effective\"]].agg(np.mean).reset_index()\n# exp21f_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-22T22:00:24.478436Z","iopub.execute_input":"2022-08-22T22:00:24.480542Z","iopub.status.idle":"2022-08-22T22:01:44.849576Z","shell.execute_reply.started":"2022-08-22T22:00:24.480503Z","shell.execute_reply":"2022-08-22T22:01:44.848544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# del model\n# torch.cuda.empty_cache()\n# gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-08-22T22:01:44.851855Z","iopub.execute_input":"2022-08-22T22:01:44.852207Z","iopub.status.idle":"2022-08-22T22:01:44.85594Z","shell.execute_reply.started":"2022-08-22T22:01:44.852172Z","shell.execute_reply":"2022-08-22T22:01:44.854954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    del tokenizer, infer_dataset, infer_ds_dict, data_collector, infer_dl\n    gc.collect()\nexcept Exception as e:\n    print(e)","metadata":{"execution":{"iopub.status.busy":"2022-08-23T12:35:25.417667Z","iopub.execute_input":"2022-08-23T12:35:25.419451Z","iopub.status.idle":"2022-08-23T12:35:25.839974Z","shell.execute_reply.started":"2022-08-23T12:35:25.419413Z","shell.execute_reply":"2022-08-23T12:35:25.839049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    del model\n    gc.collect()\n    torch.cuda.empty_cache()\nexcept Exception as e:\n    print(e)","metadata":{"execution":{"iopub.status.busy":"2022-08-23T12:35:25.842236Z","iopub.execute_input":"2022-08-23T12:35:25.842636Z","iopub.status.idle":"2022-08-23T12:35:25.850993Z","shell.execute_reply.started":"2022-08-23T12:35:25.842599Z","shell.execute_reply":"2022-08-23T12:35:25.850202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exp 102 Fast Model - SPAN MLM 40% + MSD","metadata":{"_uuid":"fae93993-d3e9-4f65-859f-9992fb129d05","_cell_guid":"beab67c4-2d18-4aa5-a662-d045429e5e8c","trusted":true}},{"cell_type":"code","source":"config = \"\"\"{\n    \"debug\": false,\n\n    \"base_model_path\": \"../input/tapt-fpe-delv3-span-mlm-04\",\n    \"model_dir\": \"./outputs\",\n\n    \"max_length\": 1024,\n    \"stride\": 256,\n    \"num_labels\": 3,\n    \"dropout\": 0.1,\n    \"infer_bs\": 8\n}\n\"\"\"\nconfig = json.loads(config)","metadata":{"_uuid":"49d9350b-e672-4a11-aac8-23b094fb1fc8","_cell_guid":"1f8c5b45-75b5-4a2b-a135-4aa8ec1dcddb","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:35:25.852472Z","iopub.execute_input":"2022-08-23T12:35:25.853121Z","iopub.status.idle":"2022-08-23T12:35:25.860838Z","shell.execute_reply.started":"2022-08-23T12:35:25.853008Z","shell.execute_reply":"2022-08-23T12:35:25.860064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\nimport pdb\nfrom collections import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import LayerNorm\n\nimport torch.utils.checkpoint\nfrom transformers import AutoConfig, AutoModel\nfrom transformers.models.deberta_v2.modeling_deberta_v2 import ContextPooler, StableDropout, DebertaV2Attention\n\n\n#-------- Model ------------------------------------------------------------------#\nclass FeedbackModel(nn.Module):\n    \"\"\"The feedback prize effectiveness baseline model\n    \"\"\"\n\n    def __init__(self, config):\n        super(FeedbackModel, self).__init__()\n        self.config = config\n\n        # base transformer\n        base_config = AutoConfig.from_pretrained(self.config[\"base_model_path\"])\n        base_config.update({\"add_pooling_layer\": False, \"max_position_embeddings\": 1024})\n        self.base_model = AutoModel.from_pretrained(self.config[\"base_model_path\"], config=base_config)\n\n        # dropouts\n        self.dropout = StableDropout(self.config[\"dropout\"])\n        self.dropout1 = StableDropout(self.config[\"dropout\"]+0.1)\n        self.dropout2 = StableDropout(self.config[\"dropout\"]+0.2)\n        self.dropout3 = StableDropout(self.config[\"dropout\"]+0.3)\n        self.dropout4 = StableDropout(self.config[\"dropout\"]+0.4)\n        \n        # multi-head attention\n        attention_config = deepcopy(self.base_model.config)\n        attention_config.update({\"relative_attention\": False})\n        self.fpe_span_attention = DebertaV2Attention(attention_config)\n        \n        # classification\n        hidden_size = self.base_model.config.hidden_size\n        feature_size = hidden_size\n        self.layer_norm = LayerNorm(feature_size, self.base_model.config.layer_norm_eps)\n        \n        # # LSTM Head\n        self.fpe_lstm_layer = nn.LSTM(\n            input_size=feature_size,\n            hidden_size=hidden_size,\n            num_layers=1,\n            batch_first=True,\n            bidirectional=False,\n        )\n\n        self.num_labels = self.config[\"num_labels\"]\n        self.classifier = nn.Linear(feature_size, self.config[\"num_labels\"])\n\n    def forward(self, input_ids, attention_mask, span_head_idxs, span_tail_idxs, span_attention_mask, **kwargs):\n        \n        bs = input_ids.shape[0]  # batch size\n        outputs = self.base_model(input_ids, attention_mask=attention_mask)\n        encoder_layer = outputs[0]\n        \n        self.fpe_lstm_layer.flatten_parameters()\n        encoder_layer = self.fpe_lstm_layer(encoder_layer)[0]\n\n        mean_feature_vector = []\n        for i in range(bs):  # TODO: vectorize\n            span_vec_i = []\n\n            for head, tail in zip(span_head_idxs[i], span_tail_idxs[i]):\n                # span feature\n                tmp = torch.mean(encoder_layer[i, head+1:tail], dim=0)  # [h]\n                span_vec_i.append(tmp)\n            span_vec_i = torch.stack(span_vec_i)  # (num_disourse, h)\n            mean_feature_vector.append(span_vec_i)\n\n        mean_feature_vector = torch.stack(mean_feature_vector)  # (bs, num_disourse, h)\n        mean_feature_vector = self.layer_norm(mean_feature_vector)\n\n        # attend to other features\n        extended_span_attention_mask = span_attention_mask.unsqueeze(1).unsqueeze(2)\n        span_attention_mask = extended_span_attention_mask * extended_span_attention_mask.squeeze(-2).unsqueeze(-1)\n        span_attention_mask = span_attention_mask.byte()\n        feature_vector = self.fpe_span_attention(mean_feature_vector, span_attention_mask)\n\n        # feature_vector = mean_feature_vector\n        feature_vector1 = self.dropout(feature_vector)\n        feature_vector2 = self.dropout1(feature_vector)\n        feature_vector3 = self.dropout2(feature_vector)\n        feature_vector4 = self.dropout3(feature_vector)\n        feature_vector5 = self.dropout4(feature_vector)\n\n        logits1 = self.classifier(feature_vector1)\n        logits2 = self.classifier(feature_vector2)\n        logits3 = self.classifier(feature_vector3)\n        logits4 = self.classifier(feature_vector4)\n        logits5 = self.classifier(feature_vector5)\n        \n        logits = (logits1 + logits2 + logits3 + logits4 + logits5)/5\n#         logits = logits[:,:, :3] # main logits\n        return logits","metadata":{"_uuid":"f039ea60-905f-46b9-8e92-e807813e06b0","_cell_guid":"104d53db-e8b8-45a1-a9f3-98e073b15a7b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:35:25.862433Z","iopub.execute_input":"2022-08-23T12:35:25.862921Z","iopub.status.idle":"2022-08-23T12:35:25.883091Z","shell.execute_reply.started":"2022-08-23T12:35:25.862885Z","shell.execute_reply":"2022-08-23T12:35:25.882237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoints = [\n    \"../input/v3l-msd-fast-approach/fpe_model_fold_0_best.pth.tar\",\n    \"../input/v3l-msd-fast-approach/fpe_model_fold_1_best.pth.tar\",\n    \"../input/v3l-msd-fast-approach/fpe_model_fold_2_best.pth.tar\",\n    \"../input/v3l-msd-fast-approach/fpe_model_fold_3_best.pth.tar\",\n    \"../input/v3l-msd-fast-approach/fpe_model_fold_4_best.pth.tar\"]","metadata":{"_uuid":"0a36dad9-3380-4a90-865f-ef6f2591edcc","_cell_guid":"5523fdf0-c6d4-4bae-a5f3-94c4212d2d23","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:35:25.884442Z","iopub.execute_input":"2022-08-23T12:35:25.884821Z","iopub.status.idle":"2022-08-23T12:35:25.895162Z","shell.execute_reply.started":"2022-08-23T12:35:25.884786Z","shell.execute_reply":"2022-08-23T12:35:25.894372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def inference_fn(model, infer_dl, model_id):\n    all_preds = []\n    all_uids = []\n    accelerator = Accelerator()\n    model, infer_dl = accelerator.prepare(model, infer_dl)\n    \n    model.eval()\n    tk0 = tqdm(infer_dl, total=len(infer_dl))\n    \n    for batch in tk0:\n        with torch.no_grad():\n            logits = model(**batch) # (b, nd, 3)\n            batch_preds = F.softmax(logits, dim=-1)\n            batch_uids = batch[\"uids\"]\n        all_preds.append(batch_preds)\n        all_uids.append(batch_uids)\n    \n    all_preds = [p.to('cpu').detach().numpy().tolist() for p in all_preds]\n    all_preds = list(chain(*all_preds))\n    flat_preds = list(chain(*all_preds))\n    \n    all_uids = [p.to('cpu').detach().numpy().tolist() for p in all_uids]\n    all_uids = list(chain(*all_uids))\n    flat_uids = list(chain(*all_uids))    \n    \n    preds_df = pd.DataFrame(flat_preds)\n    preds_df.columns = [\"Ineffective\", \"Adequate\", \"Effective\"]\n    preds_df[\"span_uid\"] = flat_uids # SORTED_DISCOURSE_IDS\n    preds_df = preds_df[preds_df[\"span_uid\"]>=0].copy()\n    preds_df[\"discourse_id\"] = preds_df[\"span_uid\"].map(idx2discourse)\n    preds_df = preds_df[[\"discourse_id\", \"Ineffective\", \"Adequate\", \"Effective\"]].copy()\n    preds_df.to_csv(f\"exp102_delv3_msd_5folds_model_preds_{model_id}.csv\", index=False)\n    \nif use_exp102:\n    for model_id, checkpoint in enumerate(checkpoints):\n        print(f\"infering from {checkpoint}\")\n        model = FeedbackModel(config)\n        if \"swa\" in checkpoint:\n            ckpt = process_swa_checkpoint(checkpoint)\n        else:\n            ckpt = torch.load(checkpoint)\n            print(f\"validation score for fold {model_id} = {ckpt['loss']}\")\n        model.load_state_dict(ckpt['state_dict'])\n        inference_fn(model, infer_dl, model_id)\n\n    del model\n    del tokenizer, infer_dataset, infer_ds_dict, data_collector, infer_dl\n    gc.collect()\n    torch.cuda.empty_cache()","metadata":{"_uuid":"3056072d-7d27-4abe-a60c-e72d61ef07bc","_cell_guid":"b3529ae2-50e4-4b34-b56d-f4e254284827","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:35:25.89708Z","iopub.execute_input":"2022-08-23T12:35:25.897798Z","iopub.status.idle":"2022-08-23T12:35:25.910934Z","shell.execute_reply.started":"2022-08-23T12:35:25.897664Z","shell.execute_reply":"2022-08-23T12:35:25.910009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if use_exp102:\n    import glob\n    import pandas as pd\n\n    csvs = glob.glob(\"exp102_delv3_msd_5folds_model_preds_*.csv\")\n\n    idx = []\n    preds = []\n\n\n    for csv_idx, csv in enumerate(csvs):\n\n        print(\"==\"*40)\n        print(f\"preds in {csv}\")\n        df = pd.read_csv(csv)\n        df = df.sort_values(by=[\"discourse_id\"])\n        print(df.head(10))\n        print(\"==\"*40)\n\n        temp_preds = df.drop([\"discourse_id\"], axis=1).values\n        if csv_idx == 0:\n            idx = list(df[\"discourse_id\"])\n            preds = temp_preds\n        else:\n            preds += temp_preds\n\n    preds = preds / len(csvs)\n\n    exp102_df = pd.DataFrame()\n    exp102_df[\"discourse_id\"] = idx\n    exp102_df[\"Ineffective\"]  = preds[:, 0]\n    exp102_df[\"Adequate\"]     = preds[:, 1]\n    exp102_df[\"Effective\"]    = preds[:, 2]\n\n    exp102_df = exp102_df.groupby(\"discourse_id\")[[\"Ineffective\", \"Adequate\", \"Effective\"]].agg(np.mean).reset_index()","metadata":{"_uuid":"4a243645-9815-4599-baed-64bef569394a","_cell_guid":"970970cd-06a8-44c1-8c06-058823c8232e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:35:25.912346Z","iopub.execute_input":"2022-08-23T12:35:25.912757Z","iopub.status.idle":"2022-08-23T12:35:25.92339Z","shell.execute_reply.started":"2022-08-23T12:35:25.912725Z","shell.execute_reply":"2022-08-23T12:35:25.922637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"d159a471-5eb8-4618-804d-684406a87152","_cell_guid":"86d49d57-9110-4e70-b9f0-bb6974ddc8b5","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EXP 213 - Deberta-large 10 fold LB 0.565","metadata":{"_uuid":"02972e1d-152c-4d0a-ad8f-ef433000e206","_cell_guid":"d4d81a20-8516-489c-bea5-1d5efef0932e","trusted":true}},{"cell_type":"markdown","source":"## Config","metadata":{"_uuid":"3a5e4c3b-a499-47f8-b2cc-3f5da713f473","_cell_guid":"f6b54386-c90a-4922-a5ef-ca9f8d88aa44","trusted":true}},{"cell_type":"code","source":"config = \"\"\"{\n    \"debug\": false,\n\n    \"base_model_path\": \"../input/deberta-large-prompt-mlm40/\",\n    \"model_dir\": \"./outputs\",\n\n    \"max_length\": 1024,\n    \"stride\": 256,\n    \"num_labels\": 3,\n    \"dropout\": 0.1,\n    \"infer_bs\": 8\n}\n\"\"\"\nconfig = json.loads(config)","metadata":{"_uuid":"0917e7f8-8a30-4615-8c46-64c062ad352b","_cell_guid":"a702bef9-0a76-4348-ab2e-e35bc7332598","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:35:25.924286Z","iopub.execute_input":"2022-08-23T12:35:25.924567Z","iopub.status.idle":"2022-08-23T12:35:25.93703Z","shell.execute_reply.started":"2022-08-23T12:35:25.924544Z","shell.execute_reply":"2022-08-23T12:35:25.936277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset","metadata":{"_uuid":"253e8234-bbbd-4f68-8bd8-bf8c19ceda89","_cell_guid":"eb0a725d-33f2-4659-88e6-d188ddea04bc","trusted":true}},{"cell_type":"code","source":"import os\nimport re\nfrom copy import deepcopy\nfrom itertools import chain\n\nimport pandas as pd\nfrom datasets import Dataset\nfrom tokenizers import AddedToken\nfrom transformers import AutoTokenizer\n\n\n#--------------- Tokenizer ---------------------------------------------#\ndef get_tokenizer(config):\n    \"\"\"load the tokenizer\"\"\"\n\n    print(\"using auto tokenizer\")\n    tokenizer = AutoTokenizer.from_pretrained(config[\"base_model_path\"])\n\n    print(\"==\"*40)\n    print(f\"tokenizer len: {len(tokenizer)}\")\n    print(\n        f\"tokenizer test: {tokenizer.tokenize('Starts: [SOE] [LEAD] [CLAIM] [POSITION] [COUNTER_CLAIM]')}\")\n    print(\n        f\"tokenizer test: {tokenizer.tokenize('Starts: [EOE] [LEAD_END] [POSITION_END] [CLAIM_END]')}\")\n\n    print(\"==\"*40)\n    return tokenizer\n\n\n#--------------- Processing ---------------------------------------------#\n\n\nDISCOURSE_START_TOKENS = [\n    \"[LEAD]\",\n    \"[POSITION]\",\n    \"[CLAIM]\",\n    \"[COUNTER_CLAIM]\",\n    \"[REBUTTAL]\",\n    \"[EVIDENCE]\",\n    \"[CONCLUDING_STATEMENT]\"\n]\n\nTOKEN_MAP = {\n    \"topic\": [\"Topic [TOPIC]\", \"[TOPIC END]\"],\n    \"Lead\": [\"Lead [LEAD]\", \"[LEAD END]\"],\n    \"Position\": [\"Position [POSITION]\", \"[POSITION END]\"],\n    \"Claim\": [\"Claim [CLAIM]\", \"[CLAIM END]\"],\n    \"Counterclaim\": [\"Counterclaim [COUNTER_CLAIM]\", \"[COUNTER_CLAIM END]\"],\n    \"Rebuttal\": [\"Rebuttal [REBUTTAL]\", \"[REBUTTAL END]\"],\n    \"Evidence\": [\"Evidence [EVIDENCE]\", \"[EVIDENCE END]\"],\n    \"Concluding Statement\": [\"Concluding Statement [CONCLUDING_STATEMENT]\", \"[CONCLUDING_STATEMENT END]\"]\n}\n\n\nDISCOURSE_END_TOKENS = [\n    \"[LEAD END]\",\n    \"[POSITION END]\",\n    \"[CLAIM END]\",\n    \"[COUNTER_CLAIM END]\",\n    \"[REBUTTAL END]\",\n    \"[EVIDENCE END]\",\n    \"[CONCLUDING_STATEMENT END]\",\n]\n\n\n\ndef relaxed_search(text, substring, min_length=2, fraction=0.99999):\n    \"\"\"\n    Returns substring's span from the given text with the certain precision.\n    \"\"\"\n\n    position = text.find(substring)\n    substring_length = len(substring)\n    if position == -1:\n        half_length = int(substring_length * fraction)\n        half_substring = substring[:half_length]\n        half_substring_length = len(half_substring)\n        if half_substring_length < min_length:\n            return [-1, 0]\n        else:\n            return relaxed_search(text=text,\n                                  substring=half_substring,\n                                  min_length=min_length,\n                                  fraction=fraction)\n\n    span = [position, position+substring_length]\n    return span\n\n\ndef build_span_map(discourse_list, essay_text):\n    reading_head = 0\n    to_return = dict()\n\n    for cur_discourse in discourse_list:\n        if cur_discourse not in to_return:\n            to_return[cur_discourse] = []\n\n        matches = re.finditer(re.escape(r'{}'.format(cur_discourse)), essay_text)\n        for match in matches:\n            span_start, span_end = match.span()\n            if span_end <= reading_head:\n                continue\n            to_return[cur_discourse].append(match.span())\n            reading_head = span_end\n            break\n\n    # post process\n    for cur_discourse in discourse_list:\n        if not to_return[cur_discourse]:\n            print(\"resorting to relaxed search...\")\n            to_return[cur_discourse] = [relaxed_search(essay_text, cur_discourse)]\n    return to_return\n\n\ndef get_substring_span(texts, mapping):\n    result = []\n    for text in texts:\n        ans = mapping[text].pop(0)\n        result.append(ans)\n    return result\n\n\ndef process_essay(essay_id, essay_text, topic, prompt, anno_df):\n    \"\"\"insert newly added tokens in the essay text\n    \"\"\"\n    tmp_df = anno_df[anno_df[\"essay_id\"] == essay_id].copy()\n    tmp_df = tmp_df.sort_values(by=\"discourse_start\")\n    buffer = 0\n\n    for _, row in tmp_df.iterrows():\n        s, e, d_type = int(row.discourse_start) + buffer, int(row.discourse_end) + buffer, row.discourse_type\n        s_tok, e_tok = TOKEN_MAP[d_type]\n        essay_text = \" \".join([essay_text[:s], s_tok, essay_text[s:e], e_tok, essay_text[e:]])\n        buffer += len(s_tok) + len(e_tok) + 4\n\n    #essay_text = \"[SOE]\" + \" [TOPIC] \" + topic + \" [TOPIC END] \" +  \"[PROMPT] \" + prompt + \" [PROMPT END] \" + essay_text + \"[EOE]\"\n    essay_text = \"[SOE]\" + \" [TOPIC] \" + prompt + \" [TOPIC END] \" +  essay_text + \"[EOE]\"\n\n    return essay_text\n\n\ndef process_input_df(anno_df, notes_df):\n    \"\"\"pre-process input dataframe\n\n    :param df: input dataframe\n    :type df: pd.DataFrame\n    :return: processed dataframe\n    :rtype: pd.DataFrame\n    \"\"\"\n    notes_df = deepcopy(notes_df)\n    anno_df = deepcopy(anno_df)\n\n    #------------------- Pre-Process Essay Text --------------------------#\n    anno_df[\"discourse_text\"] = anno_df[\"discourse_text\"].apply(lambda x: x.strip())  # pre-process\n    if \"discourse_effectiveness\" in anno_df.columns:\n        anno_df = anno_df[[\"discourse_id\", \"essay_id\", \"discourse_text\",\n                           \"discourse_type\", \"discourse_effectiveness\", \"uid\"]].copy()\n    else:\n        anno_df = anno_df[[\"discourse_id\", \"essay_id\", \"discourse_text\", \"discourse_type\", \"uid\"]].copy()\n\n    tmp_df = anno_df.groupby(\"essay_id\")[[\"discourse_id\", \"discourse_text\"]].agg(list).reset_index()\n    tmp_df = pd.merge(tmp_df, notes_df, on=\"essay_id\", how=\"left\")\n    tmp_df[\"span_map\"] = tmp_df[[\"discourse_text\", \"essay_text\"]].apply(\n        lambda x: build_span_map(x[0], x[1]), axis=1)\n    tmp_df[\"span\"] = tmp_df[[\"discourse_text\", \"span_map\"]].apply(\n        lambda x: get_substring_span(x[0], x[1]), axis=1)\n\n    all_discourse_ids = list(chain(*tmp_df[\"discourse_id\"].values))\n    all_discourse_spans = list(chain(*tmp_df[\"span\"].values))\n    span_df = pd.DataFrame()\n    span_df[\"discourse_id\"] = all_discourse_ids\n    span_df[\"span\"] = all_discourse_spans\n    span_df[\"discourse_start\"] = span_df[\"span\"].apply(lambda x: x[0])\n    span_df[\"discourse_end\"] = span_df[\"span\"].apply(lambda x: x[1])\n    span_df = span_df.drop(columns=\"span\")\n\n    anno_df = pd.merge(anno_df, span_df, on=\"discourse_id\", how=\"left\")\n    # anno_df = pd.merge(anno_df, notes_df, on=\"essay_id\", how=\"left\")\n\n    print(\"==\"*40)\n    print(\"processing essay text and inserting new tokens at span boundaries\")\n    notes_df[\"essay_text\"] = notes_df[[\"essay_id\", \"essay_text\", \"topic\", \"prompt\"]].apply(\n        lambda x: process_essay(x[0], x[1], x[2], x[3], anno_df), axis=1\n    )\n    print(\"==\"*40)\n\n    anno_df = anno_df.drop(columns=[\"discourse_start\", \"discourse_end\"])\n    notes_df = notes_df.drop_duplicates(subset=[\"essay_id\"])[[\"essay_id\", \"essay_text\"]].copy()\n\n    anno_df = pd.merge(anno_df, notes_df, on=\"essay_id\", how=\"left\")\n\n    if \"discourse_effectiveness\" in anno_df.columns:\n        grouped_df = anno_df.groupby(\"essay_id\")[[\"uid\", \"discourse_effectiveness\", \"discourse_type\"]].agg(list).reset_index()\n    else:\n        grouped_df = anno_df.groupby(\"essay_id\")[[\"uid\", \"discourse_type\"]].agg(list).reset_index()\n\n    grouped_df = pd.merge(grouped_df, notes_df, on=\"essay_id\", how=\"left\")\n    grouped_df = grouped_df.rename(columns={\"uid\": \"uids\"})\n\n    return grouped_df\n\n\n#--------------- Dataset ----------------------------------------------#\n\n\nclass AuxFeedbackDataset:\n    \"\"\"Dataset class for feedback prize effectiveness task\n    \"\"\"\n\n    def __init__(self, config):\n        self.config = config\n\n        self.label2id = {\n            \"Ineffective\": 0,\n            \"Adequate\": 1,\n            \"Effective\": 2,\n        }\n\n        self.discourse_type2id = {\n            \"Lead\": 1,\n            \"Position\": 2,\n            \"Claim\": 3,\n            \"Counterclaim\": 4,\n            \"Rebuttal\": 5,\n            \"Evidence\": 6,\n            \"Concluding Statement\": 7,\n        }\n\n        self.id2label = {v: k for k, v in self.label2id.items()}\n        self.load_tokenizer()\n\n    def load_tokenizer(self):\n        \"\"\"load tokenizer as per config \n        \"\"\"\n        self.tokenizer = get_tokenizer(self.config)\n        print(\"==\"*40)\n        print(\"token maps...\")\n        print(TOKEN_MAP)\n        print(\"==\"*40)\n\n        # print(\"adding new tokens...\")\n        # tokens_to_add = []\n        # for this_tok in NEW_TOKENS:\n        #     tokens_to_add.append(AddedToken(this_tok, lstrip=True, rstrip=False))\n        # self.tokenizer.add_tokens(tokens_to_add)\n        print(f\"tokenizer len: {len(self.tokenizer)}\")\n\n        self.discourse_token_ids = set(self.tokenizer.convert_tokens_to_ids(DISCOURSE_START_TOKENS))\n        self.discourse_end_ids = set(self.tokenizer.convert_tokens_to_ids(DISCOURSE_END_TOKENS))\n        self.global_tokens = self.discourse_token_ids.union(self.discourse_end_ids)\n\n    def tokenize_function(self, examples):\n        tz = self.tokenizer(\n            examples[\"essay_text\"],\n            padding=False,\n            truncation=False,  # no truncation at first\n            add_special_tokens=True,\n            return_offsets_mapping=True,\n        )\n        return tz\n\n    def process_spans(self, examples):\n\n        span_head_char_start_idxs, span_tail_char_end_idxs = [], []\n        span_head_idxs, span_tail_idxs = [], []\n\n        for example_input_ids, example_offset_mapping, example_uids in zip(examples[\"input_ids\"], examples[\"offset_mapping\"], examples[\"uids\"]):\n            example_span_head_idxs = [pos for pos, this_id in enumerate(\n                example_input_ids) if this_id in self.discourse_token_ids]\n            example_span_tail_idxs = [pos for pos, this_id in enumerate(\n                example_input_ids) if this_id in self.discourse_end_ids]\n\n            example_span_head_char_start_idxs = [example_offset_mapping[pos][0] for pos in example_span_head_idxs]\n            example_span_tail_char_end_idxs = [example_offset_mapping[pos][1] for pos in example_span_tail_idxs]\n\n            span_head_char_start_idxs.append(example_span_head_char_start_idxs)\n            span_tail_char_end_idxs.append(example_span_tail_char_end_idxs)\n\n            span_head_idxs.append(example_span_head_idxs)\n            span_tail_idxs.append(example_span_tail_idxs)\n\n        return {\n            \"span_head_idxs\": span_head_idxs,\n            \"span_tail_idxs\": span_tail_idxs,\n            \"span_head_char_start_idxs\": span_head_char_start_idxs,\n            \"span_tail_char_end_idxs\": span_tail_char_end_idxs,\n        }\n\n    def generate_labels(self, examples):\n        labels = []\n        for example_labels, example_uids in zip(examples[\"discourse_effectiveness\"], examples[\"uids\"]):\n            labels.append([self.label2id[l] for l in example_labels])\n        return {\"labels\": labels}\n\n    def generate_discourse_type_ids(self, examples):\n        discourse_type_ids = []\n        for example_discourse_types in examples[\"discourse_type\"]:\n            discourse_type_ids.append([self.discourse_type2id[dt] for dt in example_discourse_types])\n        return {\"discourse_type_ids\": discourse_type_ids}\n\n    def compute_input_length(self, examples):\n        return {\"input_length\": [len(x) for x in examples[\"input_ids\"]]}\n\n    def sanity_check_head_tail(self, examples):\n        for head_idxs, tail_idxs in zip(examples[\"span_head_idxs\"], examples[\"span_tail_idxs\"]):\n            assert len(head_idxs) == len(tail_idxs)\n            for head, tail in zip(head_idxs, tail_idxs):\n                assert tail > head + 1\n\n    def sanity_check_head_labels(self, examples):\n        for head_idxs, head_labels in zip(examples[\"span_head_idxs\"], examples[\"labels\"]):\n            assert len(head_idxs) == len(head_labels)\n\n    def get_dataset(self, df, essay_df, mode='train'):\n        \"\"\"main api for creating the Feedback dataset\n\n        :param df: input annotation dataframe\n        :type df: pd.DataFrame\n        :param essay_df: dataframe with essay texts\n        :type essay_df: pd.DataFrame\n        :param mode: check if required for train or infer, defaults to 'train'\n        :type mode: str, optional\n        :return: the created dataset\n        :rtype: Dataset\n        \"\"\"\n        df = process_input_df(df, essay_df)\n\n        # save a sample for sanity checks\n        sample_df = df.sample(min(16, len(df)))\n        sample_df.to_csv(os.path.join(self.config[\"model_dir\"], f\"{mode}_df_processed.csv\"), index=False)\n\n        task_dataset = Dataset.from_pandas(df)\n        task_dataset = task_dataset.map(self.tokenize_function, batched=True)\n        task_dataset = task_dataset.map(self.compute_input_length, batched=True)\n        task_dataset = task_dataset.map(self.process_spans, batched=True)\n        print(task_dataset)\n        # todo check edge cases\n        task_dataset = task_dataset.filter(lambda example: len(example['span_head_idxs']) == len(\n            example['span_tail_idxs']))  # no need to run on empty set\n        print(task_dataset)\n        task_dataset = task_dataset.map(self.generate_discourse_type_ids, batched=True)\n        task_dataset = task_dataset.map(self.sanity_check_head_tail, batched=True)\n\n        if mode != \"infer\":\n            task_dataset = task_dataset.map(self.generate_labels, batched=True)\n            task_dataset = task_dataset.map(self.sanity_check_head_labels, batched=True)\n\n        try:\n            task_dataset = task_dataset.remove_columns(column_names=[\"__index_level_0__\"])\n        except Exception as e:\n            pass\n        return df, task_dataset\n\n#--------------- dataset with truncation ---------------------------------------------#\n\n\ndef get_fast_dataset(config, df, essay_df, mode=\"train\"):\n    \"\"\"Function to get fast approach dataset with truncation & sliding window\n    \"\"\"\n    dataset_creator = AuxFeedbackDataset(config)\n    _, task_dataset = dataset_creator.get_dataset(df, essay_df, mode=mode)\n\n    original_dataset = deepcopy(task_dataset)\n    tokenizer = dataset_creator.tokenizer\n    START_IDS = dataset_creator.discourse_token_ids\n    END_IDS = dataset_creator.discourse_end_ids\n    GLOBAL_IDS = dataset_creator.global_tokens\n\n    def tokenize_with_truncation(examples):\n        tz = tokenizer(\n            examples[\"essay_text\"],\n            padding=False,\n            truncation=True,\n            add_special_tokens=True,\n            return_offsets_mapping=True,\n            max_length=config[\"max_length\"],\n            stride=config[\"stride\"],\n            return_overflowing_tokens=True,\n            return_token_type_ids=True,\n        )\n        return tz\n\n    def process_span(examples):\n        span_head_idxs, span_tail_idxs = [], []\n        span_head_char_start_idxs, span_tail_char_end_idxs = [], []\n\n        buffer = 25  # do not include a head if it is within buffer distance away from last token\n\n        for example_input_ids, example_offset_mapping in zip(examples[\"input_ids\"], examples[\"offset_mapping\"]):\n            # ------------------- Span Heads -----------------------------------------#\n            if len(example_input_ids) < config[\"max_length\"]:  # no truncation\n                head_candidate = [pos for pos, this_id in enumerate(example_input_ids) if this_id in START_IDS]\n            else:\n                head_candidate = [pos for pos, this_id in enumerate(example_input_ids) if (\n                    (this_id in START_IDS) & (pos <= config[\"max_length\"]-buffer))]\n\n            n_heads = len(head_candidate)\n\n            # ------------------- Span Tails -----------------------------------------#\n            tail_candidate = [pos for pos, this_id in enumerate(example_input_ids) if this_id in END_IDS]\n\n            # ------------------- Edge Cases -----------------------------------------#\n            # 1. A tail occurs before the first head in the sequence due to truncation\n            if (len(tail_candidate) > 0) & (len(head_candidate) > 0):\n                if tail_candidate[0] < head_candidate[0]:  # truncation effect\n                    # print(f\"check: heads: {head_candidate}, tails {tail_candidate}\")\n                    tail_candidate = tail_candidate[1:]  # shift by one\n\n            # 2. Tail got chopped off due to truncation but the corresponding head is still there\n            if len(tail_candidate) < n_heads:\n                assert len(tail_candidate) + 1 == n_heads\n                assert len(example_input_ids) == config[\"max_length\"]  # should only happen if input text is truncated\n                tail_candidate.append(config[\"max_length\"]-2)  # the token before [SEP] token\n\n            # 3. Additional tails remain in the buffer region\n            if len(tail_candidate) > len(head_candidate):\n                tail_candidate = tail_candidate[:len(head_candidate)]\n\n            # ------------------- Create the fields ------------------------------------#\n            example_span_head_char_start_idxs = [example_offset_mapping[pos][0] for pos in head_candidate]\n            example_span_tail_char_end_idxs = [example_offset_mapping[pos][1] for pos in tail_candidate]\n\n            span_head_idxs.append(head_candidate)\n            span_tail_idxs.append(tail_candidate)\n            span_head_char_start_idxs.append(example_span_head_char_start_idxs)\n            span_tail_char_end_idxs.append(example_span_tail_char_end_idxs)\n\n        return {\n            \"span_head_idxs\": span_head_idxs,\n            \"span_tail_idxs\": span_tail_idxs,\n            \"span_head_char_start_idxs\": span_head_char_start_idxs,\n            \"span_tail_char_end_idxs\": span_tail_char_end_idxs,\n        }\n\n    def enforce_alignment(examples):\n        uids = []\n\n        for example_span_head_char_start_idxs, example_overflow_to_sample_mapping in zip(\n                examples[\"span_head_char_start_idxs\"], examples[\"overflow_to_sample_mapping\"]):\n\n            original_example = original_dataset[example_overflow_to_sample_mapping]\n            original_example_span_head_char_start_idxs = original_example[\"span_head_char_start_idxs\"]\n            original_example_uids = original_example[\"uids\"]\n            char2uid = {k: v for k, v in zip(original_example_span_head_char_start_idxs, original_example_uids)}\n            current_example_uids = [char2uid[char_idx] for char_idx in example_span_head_char_start_idxs]\n            uids.append(current_example_uids)\n        return {\"uids\": uids}\n\n    def recompute_labels(examples):\n        labels = []\n        for example_span_head_char_start_idxs, example_overflow_to_sample_mapping in zip(\n                examples[\"span_head_char_start_idxs\"], examples[\"overflow_to_sample_mapping\"]):\n\n            original_example = original_dataset[example_overflow_to_sample_mapping]\n            original_example_span_head_char_start_idxs = original_example[\"span_head_char_start_idxs\"]\n            original_example_labels = original_example[\"labels\"]\n            char2label = {k: v for k, v in zip(original_example_span_head_char_start_idxs, original_example_labels)}\n            current_example_labels = [char2label[char_idx] for char_idx in example_span_head_char_start_idxs]\n            labels.append(current_example_labels)\n        return {\"labels\": labels}\n\n    def recompute_discourse_type_ids(examples):\n        discourse_type_ids = []\n        for example_span_head_char_start_idxs, example_overflow_to_sample_mapping in zip(\n                examples[\"span_head_char_start_idxs\"], examples[\"overflow_to_sample_mapping\"]):\n\n            original_example = original_dataset[example_overflow_to_sample_mapping]\n            original_example_span_head_char_start_idxs = original_example[\"span_head_char_start_idxs\"]\n            original_example_discourse_type_ids = original_example[\"discourse_type_ids\"]\n            char2discourse_id = {k: v for k, v in zip(\n                original_example_span_head_char_start_idxs, original_example_discourse_type_ids)}\n            current_example_discourse_type_ids = [char2discourse_id[char_idx]\n                                                  for char_idx in example_span_head_char_start_idxs]\n            discourse_type_ids.append(current_example_discourse_type_ids)\n        return {\"discourse_type_ids\": discourse_type_ids}\n\n    def compute_input_length(examples):\n        return {\"input_length\": [len(x) for x in examples[\"input_ids\"]]}\n\n    def sanity_check_head_tail(examples):\n        for head_idxs, tail_idxs in zip(examples[\"span_head_idxs\"], examples[\"span_tail_idxs\"]):\n            assert len(head_idxs) == len(tail_idxs)\n            for head, tail in zip(head_idxs, tail_idxs):\n                assert tail > head + 1, f\"head idxs: {head_idxs}, tail idxs {tail_idxs}\"\n\n    task_dataset = task_dataset.map(\n        tokenize_with_truncation,\n        batched=True,\n        remove_columns=task_dataset.column_names,\n        batch_size=len(task_dataset)\n    )\n\n    task_dataset = task_dataset.map(process_span, batched=True)\n    task_dataset = task_dataset.map(enforce_alignment, batched=True)\n    task_dataset = task_dataset.map(recompute_discourse_type_ids, batched=True)\n    task_dataset = task_dataset.map(sanity_check_head_tail, batched=True)\n\n    # no need to run on empty set\n    task_dataset = task_dataset.filter(lambda example: len(example['span_head_idxs']) != 0)\n    task_dataset = task_dataset.map(compute_input_length, batched=True)\n\n    if mode != \"infer\":\n        task_dataset = task_dataset.map(recompute_labels, batched=True)\n\n    to_return = dict()\n    to_return[\"dataset\"] = task_dataset\n    to_return[\"original_dataset\"] = original_dataset\n    to_return[\"tokenizer\"] = tokenizer\n    return to_return","metadata":{"_uuid":"a9511e14-ea9b-40df-af2c-1d0d331a863e","_cell_guid":"68e36456-d85d-4a39-9dc4-0545b83fbc73","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:35:25.938884Z","iopub.execute_input":"2022-08-23T12:35:25.939257Z","iopub.status.idle":"2022-08-23T12:35:26.011058Z","shell.execute_reply.started":"2022-08-23T12:35:25.939222Z","shell.execute_reply":"2022-08-23T12:35:26.010047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.makedirs(config[\"model_dir\"], exist_ok=True)\n\nprint(\"creating the inference datasets...\")\ninfer_ds_dict = get_fast_dataset(config, test_df, essay_df, mode=\"infer\")\ntokenizer = infer_ds_dict[\"tokenizer\"]\ninfer_dataset = infer_ds_dict[\"dataset\"]\nprint(infer_dataset)","metadata":{"_uuid":"8116b355-9d4b-4833-a59b-5bc37d575da1","_cell_guid":"926321e7-086c-476b-a0ac-c1e292d17aab","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:35:26.01323Z","iopub.execute_input":"2022-08-23T12:35:26.014294Z","iopub.status.idle":"2022-08-23T12:35:26.7974Z","shell.execute_reply.started":"2022-08-23T12:35:26.014247Z","shell.execute_reply":"2022-08-23T12:35:26.796484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config[\"len_tokenizer\"] = len(tokenizer)\n\ninfer_dataset = infer_dataset.sort(\"input_length\")\n\ninfer_dataset.set_format(\n    type=None,\n    columns=['input_ids', 'attention_mask', 'token_type_ids', 'span_head_idxs',\n             'span_tail_idxs', 'discourse_type_ids', 'uids']\n)","metadata":{"_uuid":"4a19ed92-7ff4-449f-8e6d-98fe1eabec2c","_cell_guid":"62b026b5-dca1-44fb-9d70-9b7e57bab364","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:35:26.798698Z","iopub.execute_input":"2022-08-23T12:35:26.79914Z","iopub.status.idle":"2022-08-23T12:35:26.813261Z","shell.execute_reply.started":"2022-08-23T12:35:26.799104Z","shell.execute_reply":"2022-08-23T12:35:26.812424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Loader","metadata":{"_uuid":"ba8cb633-1c43-497f-8eb3-9223b42af619","_cell_guid":"1f4bc6c1-7b17-4b21-ada1-4e48e490e971","trusted":true}},{"cell_type":"code","source":"from copy import deepcopy\nfrom dataclasses import dataclass\n\nimport torch\nfrom transformers import DataCollatorWithPadding\n\n\n@dataclass\nclass CustomDataCollatorWithPadding(DataCollatorWithPadding):\n    \"\"\"\n    data collector for seq classification\n    \"\"\"\n\n    tokenizer = None\n    padding = True\n    max_length = None\n    pad_to_multiple_of = 512\n    return_tensors = \"pt\"\n\n    def __call__(self, features):\n        uids = [feature[\"uids\"] for feature in features]\n        discourse_type_ids = [feature[\"discourse_type_ids\"] for feature in features]\n        span_head_idxs = [feature[\"span_head_idxs\"] for feature in features]\n        span_tail_idxs = [feature[\"span_tail_idxs\"] for feature in features]\n        span_attention_mask = [[1]*len(feature[\"span_head_idxs\"]) for feature in features]\n\n        labels = None\n        if \"labels\" in features[0].keys():\n            labels = [feature[\"labels\"] for feature in features]\n\n        batch = self.tokenizer.pad(\n            features,\n            padding=self.padding,\n            max_length=self.max_length,\n            pad_to_multiple_of=self.pad_to_multiple_of,\n            return_tensors=None,\n        )\n\n        b_max = max([len(l) for l in span_head_idxs])\n        max_len = len(batch[\"input_ids\"][0])\n\n        default_head_idx = max(max_len - 10, 1)  # for padding\n        default_tail_idx = max(max_len - 4, 1)  # for padding\n\n        batch[\"span_head_idxs\"] = [\n            ex_span_head_idxs + [default_head_idx] * (b_max - len(ex_span_head_idxs)) for ex_span_head_idxs in span_head_idxs\n        ]\n\n        batch[\"uids\"] = [ex_uids + [-1] * (b_max - len(ex_uids)) for ex_uids in uids]\n        batch[\"discourse_type_ids\"] = [ex_discourse_type_ids + [0] *\n                                       (b_max - len(ex_discourse_type_ids)) for ex_discourse_type_ids in discourse_type_ids]\n\n        batch[\"span_tail_idxs\"] = [\n            ex_span_tail_idxs + [default_tail_idx] * (b_max - len(ex_span_tail_idxs)) for ex_span_tail_idxs in span_tail_idxs\n        ]\n\n        batch[\"span_attention_mask\"] = [\n            ex_discourse_masks + [0] * (b_max - len(ex_discourse_masks)) for ex_discourse_masks in span_attention_mask\n        ]\n\n        if labels is not None:\n            batch[\"labels\"] = [ex_labels + [-1] * (b_max - len(ex_labels)) for ex_labels in labels]\n\n        # multitask labels\n        def _get_additional_labels(label_id):\n            if label_id == 0:\n                vec = [0, 0]\n            elif label_id == 1:\n                vec = [1, 0]\n            elif label_id == 2:\n                vec = [1, 1]\n            elif label_id == -1:\n                vec = [-1, -1]\n            else:\n                raise\n            return vec\n\n        if labels is not None:\n            additional_labels = []\n            for ex_labels in batch[\"labels\"]:\n                ex_additional_labels = [_get_additional_labels(el) for el in ex_labels]\n                additional_labels.append(ex_additional_labels)\n            batch[\"multitask_labels\"] = additional_labels\n        # pdb.set_trace()\n\n        batch = {k: (torch.tensor(v, dtype=torch.int64) if k != \"multitask_labels\" else torch.tensor(\n            v, dtype=torch.float32)) for k, v in batch.items()}\n        return batch","metadata":{"_uuid":"58713047-e37c-49b0-8f28-766e5b7c50eb","_cell_guid":"9235e428-9df4-48dd-b94f-78284561025c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:35:26.814955Z","iopub.execute_input":"2022-08-23T12:35:26.815451Z","iopub.status.idle":"2022-08-23T12:35:26.832391Z","shell.execute_reply.started":"2022-08-23T12:35:26.815416Z","shell.execute_reply":"2022-08-23T12:35:26.83146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_collector = CustomDataCollatorWithPadding(tokenizer=tokenizer)\n\ninfer_dl = DataLoader(\n    infer_dataset,\n    batch_size=config[\"infer_bs\"],\n    shuffle=False,\n    collate_fn=data_collector\n)","metadata":{"_uuid":"719806d7-ec2b-48c1-8136-e79d0ac1df85","_cell_guid":"566ef6f6-2e9f-4258-9299-09e0146e85ba","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:35:26.835477Z","iopub.execute_input":"2022-08-23T12:35:26.835769Z","iopub.status.idle":"2022-08-23T12:35:26.845846Z","shell.execute_reply.started":"2022-08-23T12:35:26.835744Z","shell.execute_reply":"2022-08-23T12:35:26.845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{"_uuid":"49e23661-bd56-44b5-8bd4-25b44b3e5b77","_cell_guid":"ab07deea-639e-4705-b9d6-c0142edc736a","trusted":true}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nfrom torch.nn import LayerNorm\nfrom transformers import AutoConfig, AutoModel, BertConfig\nfrom transformers.models.bert.modeling_bert import BertAttention\n\n\n#-------- Model ------------------------------------------------------------------#\nclass FeedbackModel(nn.Module):\n    \"\"\"The feedback prize effectiveness baseline model\n    \"\"\"\n\n    def __init__(self, config):\n        super(FeedbackModel, self).__init__()\n        self.config = config\n\n        # base transformer\n        base_config = AutoConfig.from_pretrained(self.config[\"base_model_path\"])\n        self.base_model = AutoModel.from_pretrained(self.config[\"base_model_path\"], config=base_config)\n\n        # dropouts\n        self.dropout = nn.Dropout(self.config[\"dropout\"])\n        \n        # multi-head attention over span representations\n        attention_config = BertConfig()\n        attention_config.update(\n            {\n                \"num_attention_heads\": self.base_model.config.num_attention_heads,\n                \"hidden_size\": self.base_model.config.hidden_size,\n                \"attention_probs_dropout_prob\": self.base_model.config.attention_probs_dropout_prob,\n                \"is_decoder\": False,\n\n            }\n        )\n        self.fpe_span_attention = BertAttention(attention_config, position_embedding_type=\"relative_key\")\n        \n        # classification\n        hidden_size = self.base_model.config.hidden_size\n        feature_size = hidden_size\n        self.layer_norm = LayerNorm(feature_size, self.base_model.config.layer_norm_eps)\n        \n        # # LSTM Head\n        self.fpe_lstm_layer = nn.LSTM(\n            input_size=feature_size,\n            hidden_size=hidden_size//2,\n            num_layers=1,\n            batch_first=True,\n            bidirectional=True,\n        )\n\n\n        self.num_labels = self.config[\"num_labels\"]\n        self.classifier = nn.Linear(feature_size, self.config[\"num_labels\"])\n\n    def forward(self, input_ids, token_type_ids, attention_mask, span_head_idxs, span_tail_idxs, span_attention_mask, **kwargs):\n        bs = input_ids.shape[0]  # batch size\n\n        outputs = self.base_model(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n        )\n        encoder_layer = outputs[0]\n\n        self.fpe_lstm_layer.flatten_parameters()\n        encoder_layer = self.fpe_lstm_layer(encoder_layer)[0]  # LSTM layer outputs\n\n        mean_feature_vector = []\n\n        for i in range(bs):\n            span_vec_i = []\n\n            for head, tail in zip(span_head_idxs[i], span_tail_idxs[i]):\n                # span feature\n                tmp = torch.mean(encoder_layer[i, head+1:tail], dim=0)  # [h]\n                span_vec_i.append(tmp)\n            span_vec_i = torch.stack(span_vec_i)  # (num_disourse, h)\n            mean_feature_vector.append(span_vec_i)\n\n        mean_feature_vector = torch.stack(mean_feature_vector)  # (bs, num_disourse, h)\n        mean_feature_vector = self.layer_norm(mean_feature_vector)\n\n        # attention mechanism\n        extended_span_attention_mask = span_attention_mask[:, None, None, :]\n        # extended_span_attention_mask = extended_span_attention_mask.to(dtype=self.dtype)  # fp16 compatibility\n        extended_span_attention_mask = (1.0 - extended_span_attention_mask) * -10000.0\n        feature_vector = self.fpe_span_attention(mean_feature_vector, extended_span_attention_mask)[0]\n\n        feature_vector = self.dropout(feature_vector)\n        logits = self.classifier(feature_vector)\n        \n        ######\n        \n        #logits = logits[:,:, :3] # main logits\n        return logits","metadata":{"_uuid":"de8cece1-363f-4268-8052-ce52f0287a9a","_cell_guid":"3156e21b-7c46-4ee6-8261-dd996c3008a6","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:35:26.847257Z","iopub.execute_input":"2022-08-23T12:35:26.847841Z","iopub.status.idle":"2022-08-23T12:35:26.86428Z","shell.execute_reply.started":"2022-08-23T12:35:26.847806Z","shell.execute_reply":"2022-08-23T12:35:26.863429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference","metadata":{"_uuid":"d87c94dd-9ffc-44e4-9087-87343fdad264","_cell_guid":"d17bac00-82d9-40e5-98ef-0d47ba4cd1b2","trusted":true}},{"cell_type":"code","source":"checkpoints = [\n    \"../input/tk-fpe-models-v5/exp213-deb-l-prompt-mlm50/fpe_model_fold_0_best.pth.tar\",\n    \"../input/tk-fpe-models-v5/exp213-deb-l-prompt-mlm50/fpe_model_fold_1_best.pth.tar\",\n    \"../input/tk-fpe-models-v5/exp213-deb-l-prompt-mlm50/fpe_model_fold_2_best.pth.tar\",\n    \"../input/tk-fpe-models-v5/exp213-deb-l-prompt-mlm50/fpe_model_fold_3_best.pth.tar\",\n    \"../input/tk-fpe-models-v5/exp213-deb-l-prompt-mlm50/fpe_model_fold_4_best.pth.tar\",\n    \"../input/tk-fpe-models-v5/exp213-deb-l-prompt-mlm50/fpe_model_fold_5_best.pth.tar\",\n    \"../input/tk-fpe-models-v5/exp213-deb-l-prompt-mlm50/fpe_model_fold_6_best.pth.tar\",\n    \"../input/tk-fpe-models-v5/exp213-deb-l-prompt-mlm50/fpe_model_fold_7_best.pth.tar\",\n    \"../input/tk-fpe-models-v5/exp213-deb-l-prompt-mlm50/fpe_model_fold_8_best.pth.tar\",\n    \"../input/tk-fpe-models-v5/exp213-deb-l-prompt-mlm50/fpe_model_fold_9_best.pth.tar\",\n\n]","metadata":{"_uuid":"46bfd580-eb10-4f82-a2ea-bb128c527e42","_cell_guid":"35f49ce2-6545-45e0-a708-a0fb4e932d46","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:35:26.865504Z","iopub.execute_input":"2022-08-23T12:35:26.866031Z","iopub.status.idle":"2022-08-23T12:35:26.877145Z","shell.execute_reply.started":"2022-08-23T12:35:26.865987Z","shell.execute_reply":"2022-08-23T12:35:26.876316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def inference_fn(model, infer_dl, model_id):\n    all_preds = []\n    all_uids = []\n    accelerator = Accelerator()\n    model, infer_dl = accelerator.prepare(model, infer_dl)\n    \n    model.eval()\n    tk0 = tqdm(infer_dl, total=len(infer_dl))\n    \n    for batch in tk0:\n        with torch.no_grad():\n            logits = model(**batch) # (b, nd, 3)\n            batch_preds = F.softmax(logits, dim=-1)\n            batch_uids = batch[\"uids\"]\n        all_preds.append(batch_preds)\n        all_uids.append(batch_uids)\n    \n    all_preds = [p.to('cpu').detach().numpy().tolist() for p in all_preds]\n    all_preds = list(chain(*all_preds))\n    flat_preds = list(chain(*all_preds))\n    \n    all_uids = [p.to('cpu').detach().numpy().tolist() for p in all_uids]\n    all_uids = list(chain(*all_uids))\n    flat_uids = list(chain(*all_uids))    \n    \n    preds_df = pd.DataFrame(flat_preds)\n    preds_df.columns = [\"Ineffective\", \"Adequate\", \"Effective\"]\n    preds_df[\"span_uid\"] = flat_uids # SORTED_DISCOURSE_IDS\n    preds_df = preds_df[preds_df[\"span_uid\"]>=0].copy()\n    preds_df[\"discourse_id\"] = preds_df[\"span_uid\"].map(idx2discourse)\n    preds_df = preds_df[[\"discourse_id\", \"Ineffective\", \"Adequate\", \"Effective\"]].copy()\n    preds_df.to_csv(f\"exp213_dl_model_preds_{model_id}.csv\", index=False)\n    \n\nfor model_id, checkpoint in enumerate(checkpoints):\n    print(f\"infering from {checkpoint}\")\n    model = FeedbackModel(config)\n    ckpt = torch.load(checkpoint)\n    print(f\"validation score for fold {model_id} = {ckpt['loss']}\")\n    model.load_state_dict(ckpt['state_dict'])\n    inference_fn(model, infer_dl, model_id)\n    \ndel model\n#del tokenizer, infer_dataset, infer_ds_dict, data_collector, infer_dl\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"_uuid":"ae6c2453-3a23-410e-91b1-6207c9ba7ca4","_cell_guid":"a7ecbec7-50b6-44f0-85ae-f437e2735830","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:35:26.878677Z","iopub.execute_input":"2022-08-23T12:35:26.879543Z","iopub.status.idle":"2022-08-23T12:41:09.476548Z","shell.execute_reply.started":"2022-08-23T12:35:26.879508Z","shell.execute_reply":"2022-08-23T12:41:09.475498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exp213a","metadata":{}},{"cell_type":"code","source":"checkpoints = [\n    \"../input/exp213a-deb-l-prompt/fpe_model_fold_0_best.pth.tar\",\n    \"../input/exp213a-deb-l-prompt/fpe_model_fold_1_best.pth.tar\",\n    \"../input/exp213a-deb-l-prompt/fpe_model_fold_2_best.pth.tar\",\n    \"../input/exp213a-deb-l-prompt/fpe_model_fold_3_best.pth.tar\",\n    \"../input/exp213a-deb-l-prompt/fpe_model_fold_4_best.pth.tar\",\n    \"../input/exp213a-deb-l-prompt/fpe_model_fold_5_best.pth.tar\",\n    \"../input/exp213a-deb-l-prompt/fpe_model_fold_6_best.pth.tar\",\n    \"../input/exp213a-deb-l-prompt/fpe_model_fold_7_best.pth.tar\",\n    \"../input/exp213a-deb-l-prompt/fpe_model_fold_8_best.pth.tar\",\n    \"../input/exp213a-deb-l-prompt/fpe_model_fold_9_best.pth.tar\",\n\n]","metadata":{"execution":{"iopub.status.busy":"2022-08-23T12:41:09.478836Z","iopub.execute_input":"2022-08-23T12:41:09.48046Z","iopub.status.idle":"2022-08-23T12:41:09.485216Z","shell.execute_reply.started":"2022-08-23T12:41:09.48042Z","shell.execute_reply":"2022-08-23T12:41:09.484383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def inference_fn(model, infer_dl, model_id):\n    all_preds = []\n    all_uids = []\n    accelerator = Accelerator()\n    model, infer_dl = accelerator.prepare(model, infer_dl)\n    \n    model.eval()\n    tk0 = tqdm(infer_dl, total=len(infer_dl))\n    \n    for batch in tk0:\n        with torch.no_grad():\n            logits = model(**batch) # (b, nd, 3)\n            batch_preds = F.softmax(logits, dim=-1)\n            batch_uids = batch[\"uids\"]\n        all_preds.append(batch_preds)\n        all_uids.append(batch_uids)\n    \n    all_preds = [p.to('cpu').detach().numpy().tolist() for p in all_preds]\n    all_preds = list(chain(*all_preds))\n    flat_preds = list(chain(*all_preds))\n    \n    all_uids = [p.to('cpu').detach().numpy().tolist() for p in all_uids]\n    all_uids = list(chain(*all_uids))\n    flat_uids = list(chain(*all_uids))    \n    \n    preds_df = pd.DataFrame(flat_preds)\n    preds_df.columns = [\"Ineffective\", \"Adequate\", \"Effective\"]\n    preds_df[\"span_uid\"] = flat_uids # SORTED_DISCOURSE_IDS\n    preds_df = preds_df[preds_df[\"span_uid\"]>=0].copy()\n    preds_df[\"discourse_id\"] = preds_df[\"span_uid\"].map(idx2discourse)\n    preds_df = preds_df[[\"discourse_id\", \"Ineffective\", \"Adequate\", \"Effective\"]].copy()\n    preds_df.to_csv(f\"exp213a_dl_model_preds_{model_id}.csv\", index=False)\n    \n\nfor model_id, checkpoint in enumerate(checkpoints):\n    print(f\"infering from {checkpoint}\")\n    model = FeedbackModel(config)\n    ckpt = torch.load(checkpoint)\n    print(f\"validation score for fold {model_id} = {ckpt['loss']}\")\n    model.load_state_dict(ckpt['state_dict'])\n    inference_fn(model, infer_dl, model_id)\n    \ndel model\n# del tokenizer, infer_dataset, infer_ds_dict, data_collector, infer_dl\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2022-08-23T12:41:09.486424Z","iopub.execute_input":"2022-08-23T12:41:09.486846Z","iopub.status.idle":"2022-08-23T12:46:54.870276Z","shell.execute_reply.started":"2022-08-23T12:41:09.486805Z","shell.execute_reply":"2022-08-23T12:46:54.869314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import glob\nimport pandas as pd\n\ncsvs = glob.glob(\"exp213_dl_model_preds_*.csv\")\n\nidx = []\npreds = []\n\n\nfor csv_idx, csv in enumerate(csvs):\n    \n    print(\"==\"*40)\n    print(f\"preds in {csv}\")\n    df = pd.read_csv(csv)\n    df = df.sort_values(by=[\"discourse_id\"])\n    print(df.head(10))\n    print(\"==\"*40)\n    \n    temp_preds = df.drop([\"discourse_id\"], axis=1).values\n    if csv_idx == 0:\n        idx = list(df[\"discourse_id\"])\n        preds = temp_preds\n    else:\n        preds += temp_preds\n\npreds = preds / len(csvs)\n\nexp213_df = pd.DataFrame()\nexp213_df[\"discourse_id\"] = idx\nexp213_df[\"Ineffective\"]  = preds[:, 0]\nexp213_df[\"Adequate\"]     = preds[:, 1]\nexp213_df[\"Effective\"]    = preds[:, 2]\n\nexp213_df = exp213_df.groupby(\"discourse_id\")[[\"Ineffective\", \"Adequate\", \"Effective\"]].agg(np.mean).reset_index()","metadata":{"_uuid":"c70edf55-b86b-4d4a-a094-72803484b5f2","_cell_guid":"9037b356-1451-4a90-824d-ded36be1a594","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:46:54.872785Z","iopub.execute_input":"2022-08-23T12:46:54.87333Z","iopub.status.idle":"2022-08-23T12:46:54.940015Z","shell.execute_reply.started":"2022-08-23T12:46:54.873292Z","shell.execute_reply":"2022-08-23T12:46:54.939247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"exp213_df.head()","metadata":{"_uuid":"e0bb4801-3c3c-484b-a45f-b778763190f3","_cell_guid":"942034be-d069-4dd1-b108-5ab0b0fde463","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:46:54.941326Z","iopub.execute_input":"2022-08-23T12:46:54.941831Z","iopub.status.idle":"2022-08-23T12:46:54.953473Z","shell.execute_reply.started":"2022-08-23T12:46:54.941795Z","shell.execute_reply":"2022-08-23T12:46:54.952382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import glob\nimport pandas as pd\n\ncsvs = glob.glob(\"exp213a_dl_model_preds_*.csv\")\n\nidx = []\npreds = []\n\n\nfor csv_idx, csv in enumerate(csvs):\n    \n    print(\"==\"*40)\n    print(f\"preds in {csv}\")\n    df = pd.read_csv(csv)\n    df = df.sort_values(by=[\"discourse_id\"])\n    print(df.head(10))\n    print(\"==\"*40)\n    \n    temp_preds = df.drop([\"discourse_id\"], axis=1).values\n    if csv_idx == 0:\n        idx = list(df[\"discourse_id\"])\n        preds = temp_preds\n    else:\n        preds += temp_preds\n\npreds = preds / len(csvs)\n\nexp213a_df = pd.DataFrame()\nexp213a_df[\"discourse_id\"] = idx\nexp213a_df[\"Ineffective\"]  = preds[:, 0]\nexp213a_df[\"Adequate\"]     = preds[:, 1]\nexp213a_df[\"Effective\"]    = preds[:, 2]\n\nexp213a_df = exp213a_df.groupby(\"discourse_id\")[[\"Ineffective\", \"Adequate\", \"Effective\"]].agg(np.mean).reset_index()","metadata":{"execution":{"iopub.status.busy":"2022-08-23T12:46:54.955673Z","iopub.execute_input":"2022-08-23T12:46:54.956367Z","iopub.status.idle":"2022-08-23T12:46:55.016713Z","shell.execute_reply.started":"2022-08-23T12:46:54.956324Z","shell.execute_reply":"2022-08-23T12:46:55.015176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"exp213a_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-23T12:46:55.017973Z","iopub.execute_input":"2022-08-23T12:46:55.018331Z","iopub.status.idle":"2022-08-23T12:46:55.028803Z","shell.execute_reply.started":"2022-08-23T12:46:55.018304Z","shell.execute_reply":"2022-08-23T12:46:55.027839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Full data models","metadata":{}},{"cell_type":"code","source":"if use_full_data_models:\n\n    checkpoints = [\n        \"../input/exp213f-deb-l-prompt-all/fpe_model_fold_0_best.pth.tar\",\n        \"../input/exp213f-deb-l-prompt-all/fpe_model_fold_1_best.pth.tar\",\n    ]\n\n    def inference_fn(model, infer_dl, model_id):\n        all_preds = []\n        all_uids = []\n        accelerator = Accelerator()\n        model, infer_dl = accelerator.prepare(model, infer_dl)\n\n        model.eval()\n        tk0 = tqdm(infer_dl, total=len(infer_dl))\n\n        for batch in tk0:\n            with torch.no_grad():\n                logits = model(**batch) # (b, nd, 3)\n                batch_preds = F.softmax(logits, dim=-1)\n                batch_uids = batch[\"uids\"]\n            all_preds.append(batch_preds)\n            all_uids.append(batch_uids)\n\n        all_preds = [p.to('cpu').detach().numpy().tolist() for p in all_preds]\n        all_preds = list(chain(*all_preds))\n        flat_preds = list(chain(*all_preds))\n\n        all_uids = [p.to('cpu').detach().numpy().tolist() for p in all_uids]\n        all_uids = list(chain(*all_uids))\n        flat_uids = list(chain(*all_uids))    \n\n        preds_df = pd.DataFrame(flat_preds)\n        preds_df.columns = [\"Ineffective\", \"Adequate\", \"Effective\"]\n        preds_df[\"span_uid\"] = flat_uids # SORTED_DISCOURSE_IDS\n        preds_df = preds_df[preds_df[\"span_uid\"]>=0].copy()\n        preds_df[\"discourse_id\"] = preds_df[\"span_uid\"].map(idx2discourse)\n        preds_df = preds_df[[\"discourse_id\", \"Ineffective\", \"Adequate\", \"Effective\"]].copy()\n        preds_df.to_csv(f\"exp213f_preds_{model_id}.csv\", index=False)\n\n    from copy import deepcopy\n    for model_id, checkpoint in enumerate(checkpoints):\n        #print(f\"infering from {checkpoint}\")\n        new_config = deepcopy(config)\n\n        model = FeedbackModel(new_config)\n        #model.half()\n        if \"swa\" in checkpoint:\n            ckpt = process_swa_checkpoint(checkpoint)\n        else:\n            ckpt = torch.load(checkpoint)\n            #print(f\"validation score for fold {model_id} = {ckpt['loss']}\")\n        model.load_state_dict(ckpt['state_dict'])\n        inference_fn(model, infer_dl, model_id)","metadata":{"execution":{"iopub.status.busy":"2022-08-23T12:46:55.030521Z","iopub.execute_input":"2022-08-23T12:46:55.030893Z","iopub.status.idle":"2022-08-23T12:48:04.68323Z","shell.execute_reply.started":"2022-08-23T12:46:55.030858Z","shell.execute_reply":"2022-08-23T12:48:04.682263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if use_full_data_models:\n\n    import glob\n    import pandas as pd\n\n    csvs = glob.glob(\"exp213f_preds_*.csv\")\n\n    idx = []\n    preds = []\n\n\n    for csv_idx, csv in enumerate(csvs):\n\n        #print(\"==\"*40)\n        #print(f\"preds in {csv}\")\n        df = pd.read_csv(csv)\n        df = df.sort_values(by=[\"discourse_id\"])\n        #print(df.head(10))\n        #print(\"==\"*40)\n\n        temp_preds = df.drop([\"discourse_id\"], axis=1).values\n        if csv_idx == 0:\n            idx = list(df[\"discourse_id\"])\n            preds = temp_preds\n        else:\n            preds += temp_preds\n\n    preds = preds / len(csvs)\n\n    exp213f_df = pd.DataFrame()\n    exp213f_df[\"discourse_id\"] = idx\n    exp213f_df[\"Ineffective\"] = preds[:, 0]\n    exp213f_df[\"Adequate\"] = preds[:, 1]\n    exp213f_df[\"Effective\"] = preds[:, 2]\n\n    exp213f_df = exp213f_df.groupby(\"discourse_id\")[[\"Ineffective\", \"Adequate\", \"Effective\"]].agg(np.mean).reset_index()\n    exp213f_df.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-08-23T12:48:04.685643Z","iopub.execute_input":"2022-08-23T12:48:04.686125Z","iopub.status.idle":"2022-08-23T12:48:04.712681Z","shell.execute_reply.started":"2022-08-23T12:48:04.686081Z","shell.execute_reply":"2022-08-23T12:48:04.711922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if use_full_data_models:\n    print(exp213f_df.head())","metadata":{"execution":{"iopub.status.busy":"2022-08-23T12:48:04.715679Z","iopub.execute_input":"2022-08-23T12:48:04.716024Z","iopub.status.idle":"2022-08-23T12:48:04.724925Z","shell.execute_reply.started":"2022-08-23T12:48:04.715992Z","shell.execute_reply":"2022-08-23T12:48:04.724042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    del tokenizer, infer_dataset, infer_ds_dict, data_collector, infer_dl\n    gc.collect()\nexcept Exception as e:\n    print(e)","metadata":{"execution":{"iopub.status.busy":"2022-08-23T12:48:04.726477Z","iopub.execute_input":"2022-08-23T12:48:04.726886Z","iopub.status.idle":"2022-08-23T12:48:05.088732Z","shell.execute_reply.started":"2022-08-23T12:48:04.726851Z","shell.execute_reply":"2022-08-23T12:48:05.087786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    del model\n    gc.collect()\n    torch.cuda.empty_cache()\nexcept Exception as e:\n    print(e)","metadata":{"execution":{"iopub.status.busy":"2022-08-23T12:48:05.09128Z","iopub.execute_input":"2022-08-23T12:48:05.091722Z","iopub.status.idle":"2022-08-23T12:48:05.480447Z","shell.execute_reply.started":"2022-08-23T12:48:05.091682Z","shell.execute_reply":"2022-08-23T12:48:05.479456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EXP 205 - Deberta-v3-large - MSD + prompt","metadata":{"_uuid":"429414b6-5222-4723-9ee5-eb8d00efc93b","_cell_guid":"d1489c83-f3e2-488f-8637-06974f142dcf","trusted":true}},{"cell_type":"markdown","source":"## Config","metadata":{"_uuid":"ae92f3fd-fe1e-4c1b-9437-cfda3ad3ec04","_cell_guid":"c02108f0-f24a-48fa-820c-16df91e3b93f","trusted":true}},{"cell_type":"code","source":"config = \"\"\"{\n    \"debug\": false,\n\n    \"base_model_path\": \"../input/tk-fpe-models-v2/exp205-debv3-l-prompt/mlm_model/\",\n    \"model_dir\": \"./outputs\",\n\n    \"max_length\": 1024,\n    \"stride\": 256,\n    \"num_labels\": 3,\n    \"dropout\": 0.1,\n    \"infer_bs\": 8\n}\n\"\"\"\nconfig = json.loads(config)","metadata":{"_uuid":"a43cc35b-0921-4c04-912c-09343e78c037","_cell_guid":"6b2ca6b4-d0c3-4b18-8251-44d4b085c8ff","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:48:05.48189Z","iopub.execute_input":"2022-08-23T12:48:05.48264Z","iopub.status.idle":"2022-08-23T12:48:05.490464Z","shell.execute_reply.started":"2022-08-23T12:48:05.482608Z","shell.execute_reply":"2022-08-23T12:48:05.489605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset","metadata":{"_uuid":"c09ea61a-2cd6-4c71-97d6-3d620b4348cc","_cell_guid":"534aa692-95c9-44f2-b96b-d407b5800c8a","trusted":true}},{"cell_type":"code","source":"import os\nimport re\nfrom copy import deepcopy\nfrom itertools import chain\n\nimport pandas as pd\nfrom datasets import Dataset\nfrom tokenizers import AddedToken\nfrom transformers import AutoTokenizer\n\n\n#--------------- Tokenizer ---------------------------------------------#\ndef get_tokenizer(config):\n    \"\"\"load the tokenizer\"\"\"\n\n    print(\"using auto tokenizer\")\n    tokenizer = AutoTokenizer.from_pretrained(config[\"base_model_path\"])\n\n    print(\"==\"*40)\n    print(f\"tokenizer len: {len(tokenizer)}\")\n    print(\n        f\"tokenizer test: {tokenizer.tokenize('Starts: [SOE] [LEAD] [CLAIM] [POSITION] [COUNTER_CLAIM]')}\")\n    print(\n        f\"tokenizer test: {tokenizer.tokenize('Starts: [EOE] [LEAD_END] [POSITION_END] [CLAIM_END]')}\")\n\n    print(\"==\"*40)\n    return tokenizer\n\n\n#--------------- Processing ---------------------------------------------#\n\n\nDISCOURSE_START_TOKENS = [\n    \"[LEAD]\",\n    \"[POSITION]\",\n    \"[CLAIM]\",\n    \"[COUNTER_CLAIM]\",\n    \"[REBUTTAL]\",\n    \"[EVIDENCE]\",\n    \"[CONCLUDING_STATEMENT]\"\n]\n\nTOKEN_MAP = {\n    \"topic\": [\"Topic [TOPIC]\", \"[TOPIC END]\"],\n    \"Lead\": [\"Lead [LEAD]\", \"[LEAD END]\"],\n    \"Position\": [\"Position [POSITION]\", \"[POSITION END]\"],\n    \"Claim\": [\"Claim [CLAIM]\", \"[CLAIM END]\"],\n    \"Counterclaim\": [\"Counterclaim [COUNTER_CLAIM]\", \"[COUNTER_CLAIM END]\"],\n    \"Rebuttal\": [\"Rebuttal [REBUTTAL]\", \"[REBUTTAL END]\"],\n    \"Evidence\": [\"Evidence [EVIDENCE]\", \"[EVIDENCE END]\"],\n    \"Concluding Statement\": [\"Concluding Statement [CONCLUDING_STATEMENT]\", \"[CONCLUDING_STATEMENT END]\"]\n}\n\n\nDISCOURSE_END_TOKENS = [\n    \"[LEAD END]\",\n    \"[POSITION END]\",\n    \"[CLAIM END]\",\n    \"[COUNTER_CLAIM END]\",\n    \"[REBUTTAL END]\",\n    \"[EVIDENCE END]\",\n    \"[CONCLUDING_STATEMENT END]\",\n]\n\n\n\ndef relaxed_search(text, substring, min_length=2, fraction=0.99999):\n    \"\"\"\n    Returns substring's span from the given text with the certain precision.\n    \"\"\"\n\n    position = text.find(substring)\n    substring_length = len(substring)\n    if position == -1:\n        half_length = int(substring_length * fraction)\n        half_substring = substring[:half_length]\n        half_substring_length = len(half_substring)\n        if half_substring_length < min_length:\n            return [-1, 0]\n        else:\n            return relaxed_search(text=text,\n                                  substring=half_substring,\n                                  min_length=min_length,\n                                  fraction=fraction)\n\n    span = [position, position+substring_length]\n    return span\n\n\ndef build_span_map(discourse_list, essay_text):\n    reading_head = 0\n    to_return = dict()\n\n    for cur_discourse in discourse_list:\n        if cur_discourse not in to_return:\n            to_return[cur_discourse] = []\n\n        matches = re.finditer(re.escape(r'{}'.format(cur_discourse)), essay_text)\n        for match in matches:\n            span_start, span_end = match.span()\n            if span_end <= reading_head:\n                continue\n            to_return[cur_discourse].append(match.span())\n            reading_head = span_end\n            break\n\n    # post process\n    for cur_discourse in discourse_list:\n        if not to_return[cur_discourse]:\n            print(\"resorting to relaxed search...\")\n            to_return[cur_discourse] = [relaxed_search(essay_text, cur_discourse)]\n    return to_return\n\n\ndef get_substring_span(texts, mapping):\n    result = []\n    for text in texts:\n        ans = mapping[text].pop(0)\n        result.append(ans)\n    return result\n\n\ndef process_essay(essay_id, essay_text, topic, prompt, anno_df):\n    \"\"\"insert newly added tokens in the essay text\n    \"\"\"\n    tmp_df = anno_df[anno_df[\"essay_id\"] == essay_id].copy()\n    tmp_df = tmp_df.sort_values(by=\"discourse_start\")\n    buffer = 0\n\n    for _, row in tmp_df.iterrows():\n        s, e, d_type = int(row.discourse_start) + buffer, int(row.discourse_end) + buffer, row.discourse_type\n        s_tok, e_tok = TOKEN_MAP[d_type]\n        essay_text = \" \".join([essay_text[:s], s_tok, essay_text[s:e], e_tok, essay_text[e:]])\n        buffer += len(s_tok) + len(e_tok) + 4\n\n    essay_text = \"[SOE]\" + \" [TOPIC] \" + prompt + \" [TOPIC END] \" +  essay_text + \"[EOE]\"\n    return essay_text\n\n\ndef process_input_df(anno_df, notes_df):\n    \"\"\"pre-process input dataframe\n\n    :param df: input dataframe\n    :type df: pd.DataFrame\n    :return: processed dataframe\n    :rtype: pd.DataFrame\n    \"\"\"\n    notes_df = deepcopy(notes_df)\n    anno_df = deepcopy(anno_df)\n\n    #------------------- Pre-Process Essay Text --------------------------#\n    anno_df[\"discourse_text\"] = anno_df[\"discourse_text\"].apply(lambda x: x.strip())  # pre-process\n    if \"discourse_effectiveness\" in anno_df.columns:\n        anno_df = anno_df[[\"discourse_id\", \"essay_id\", \"discourse_text\",\n                           \"discourse_type\", \"discourse_effectiveness\", \"uid\"]].copy()\n    else:\n        anno_df = anno_df[[\"discourse_id\", \"essay_id\", \"discourse_text\", \"discourse_type\", \"uid\"]].copy()\n\n    tmp_df = anno_df.groupby(\"essay_id\")[[\"discourse_id\", \"discourse_text\"]].agg(list).reset_index()\n    tmp_df = pd.merge(tmp_df, notes_df, on=\"essay_id\", how=\"left\")\n    tmp_df[\"span_map\"] = tmp_df[[\"discourse_text\", \"essay_text\"]].apply(\n        lambda x: build_span_map(x[0], x[1]), axis=1)\n    tmp_df[\"span\"] = tmp_df[[\"discourse_text\", \"span_map\"]].apply(\n        lambda x: get_substring_span(x[0], x[1]), axis=1)\n\n    all_discourse_ids = list(chain(*tmp_df[\"discourse_id\"].values))\n    all_discourse_spans = list(chain(*tmp_df[\"span\"].values))\n    span_df = pd.DataFrame()\n    span_df[\"discourse_id\"] = all_discourse_ids\n    span_df[\"span\"] = all_discourse_spans\n    span_df[\"discourse_start\"] = span_df[\"span\"].apply(lambda x: x[0])\n    span_df[\"discourse_end\"] = span_df[\"span\"].apply(lambda x: x[1])\n    span_df = span_df.drop(columns=\"span\")\n\n    anno_df = pd.merge(anno_df, span_df, on=\"discourse_id\", how=\"left\")\n    # anno_df = pd.merge(anno_df, notes_df, on=\"essay_id\", how=\"left\")\n\n    print(\"==\"*40)\n    print(\"processing essay text and inserting new tokens at span boundaries\")\n    notes_df[\"essay_text\"] = notes_df[[\"essay_id\", \"essay_text\", \"topic\", \"prompt\"]].apply(\n        lambda x: process_essay(x[0], x[1], x[2], x[3], anno_df), axis=1\n    )\n    print(\"==\"*40)\n\n    anno_df = anno_df.drop(columns=[\"discourse_start\", \"discourse_end\"])\n    notes_df = notes_df.drop_duplicates(subset=[\"essay_id\"])[[\"essay_id\", \"essay_text\"]].copy()\n\n    anno_df = pd.merge(anno_df, notes_df, on=\"essay_id\", how=\"left\")\n\n    if \"discourse_effectiveness\" in anno_df.columns:\n        grouped_df = anno_df.groupby(\"essay_id\")[[\"uid\", \"discourse_effectiveness\", \"discourse_type\"]].agg(list).reset_index()\n    else:\n        grouped_df = anno_df.groupby(\"essay_id\")[[\"uid\", \"discourse_type\"]].agg(list).reset_index()\n\n    grouped_df = pd.merge(grouped_df, notes_df, on=\"essay_id\", how=\"left\")\n    grouped_df = grouped_df.rename(columns={\"uid\": \"uids\"})\n\n    return grouped_df\n\n\n#--------------- Dataset ----------------------------------------------#\n\n\nclass AuxFeedbackDataset:\n    \"\"\"Dataset class for feedback prize effectiveness task\n    \"\"\"\n\n    def __init__(self, config):\n        self.config = config\n\n        self.label2id = {\n            \"Ineffective\": 0,\n            \"Adequate\": 1,\n            \"Effective\": 2,\n        }\n\n        self.discourse_type2id = {\n            \"Lead\": 1,\n            \"Position\": 2,\n            \"Claim\": 3,\n            \"Counterclaim\": 4,\n            \"Rebuttal\": 5,\n            \"Evidence\": 6,\n            \"Concluding Statement\": 7,\n        }\n\n        self.id2label = {v: k for k, v in self.label2id.items()}\n        self.load_tokenizer()\n\n    def load_tokenizer(self):\n        \"\"\"load tokenizer as per config \n        \"\"\"\n        self.tokenizer = get_tokenizer(self.config)\n        print(\"==\"*40)\n        print(\"token maps...\")\n        print(TOKEN_MAP)\n        print(\"==\"*40)\n\n        # print(\"adding new tokens...\")\n        # tokens_to_add = []\n        # for this_tok in NEW_TOKENS:\n        #     tokens_to_add.append(AddedToken(this_tok, lstrip=True, rstrip=False))\n        # self.tokenizer.add_tokens(tokens_to_add)\n        print(f\"tokenizer len: {len(self.tokenizer)}\")\n\n        self.discourse_token_ids = set(self.tokenizer.convert_tokens_to_ids(DISCOURSE_START_TOKENS))\n        self.discourse_end_ids = set(self.tokenizer.convert_tokens_to_ids(DISCOURSE_END_TOKENS))\n        self.global_tokens = self.discourse_token_ids.union(self.discourse_end_ids)\n\n    def tokenize_function(self, examples):\n        tz = self.tokenizer(\n            examples[\"essay_text\"],\n            padding=False,\n            truncation=False,  # no truncation at first\n            add_special_tokens=True,\n            return_offsets_mapping=True,\n        )\n        return tz\n\n    def process_spans(self, examples):\n\n        span_head_char_start_idxs, span_tail_char_end_idxs = [], []\n        span_head_idxs, span_tail_idxs = [], []\n\n        for example_input_ids, example_offset_mapping, example_uids in zip(examples[\"input_ids\"], examples[\"offset_mapping\"], examples[\"uids\"]):\n            example_span_head_idxs = [pos for pos, this_id in enumerate(\n                example_input_ids) if this_id in self.discourse_token_ids]\n            example_span_tail_idxs = [pos for pos, this_id in enumerate(\n                example_input_ids) if this_id in self.discourse_end_ids]\n\n            example_span_head_char_start_idxs = [example_offset_mapping[pos][0] for pos in example_span_head_idxs]\n            example_span_tail_char_end_idxs = [example_offset_mapping[pos][1] for pos in example_span_tail_idxs]\n\n            span_head_char_start_idxs.append(example_span_head_char_start_idxs)\n            span_tail_char_end_idxs.append(example_span_tail_char_end_idxs)\n\n            span_head_idxs.append(example_span_head_idxs)\n            span_tail_idxs.append(example_span_tail_idxs)\n\n        return {\n            \"span_head_idxs\": span_head_idxs,\n            \"span_tail_idxs\": span_tail_idxs,\n            \"span_head_char_start_idxs\": span_head_char_start_idxs,\n            \"span_tail_char_end_idxs\": span_tail_char_end_idxs,\n        }\n\n    def generate_labels(self, examples):\n        labels = []\n        for example_labels, example_uids in zip(examples[\"discourse_effectiveness\"], examples[\"uids\"]):\n            labels.append([self.label2id[l] for l in example_labels])\n        return {\"labels\": labels}\n\n    def generate_discourse_type_ids(self, examples):\n        discourse_type_ids = []\n        for example_discourse_types in examples[\"discourse_type\"]:\n            discourse_type_ids.append([self.discourse_type2id[dt] for dt in example_discourse_types])\n        return {\"discourse_type_ids\": discourse_type_ids}\n\n    def compute_input_length(self, examples):\n        return {\"input_length\": [len(x) for x in examples[\"input_ids\"]]}\n\n    def sanity_check_head_tail(self, examples):\n        for head_idxs, tail_idxs in zip(examples[\"span_head_idxs\"], examples[\"span_tail_idxs\"]):\n            assert len(head_idxs) == len(tail_idxs)\n            for head, tail in zip(head_idxs, tail_idxs):\n                assert tail > head + 1\n\n    def sanity_check_head_labels(self, examples):\n        for head_idxs, head_labels in zip(examples[\"span_head_idxs\"], examples[\"labels\"]):\n            assert len(head_idxs) == len(head_labels)\n\n    def get_dataset(self, df, essay_df, mode='train'):\n        \"\"\"main api for creating the Feedback dataset\n\n        :param df: input annotation dataframe\n        :type df: pd.DataFrame\n        :param essay_df: dataframe with essay texts\n        :type essay_df: pd.DataFrame\n        :param mode: check if required for train or infer, defaults to 'train'\n        :type mode: str, optional\n        :return: the created dataset\n        :rtype: Dataset\n        \"\"\"\n        df = process_input_df(df, essay_df)\n\n        # save a sample for sanity checks\n        sample_df = df.sample(min(16, len(df)))\n        sample_df.to_csv(os.path.join(self.config[\"model_dir\"], f\"{mode}_df_processed.csv\"), index=False)\n\n        task_dataset = Dataset.from_pandas(df)\n        task_dataset = task_dataset.map(self.tokenize_function, batched=True)\n        task_dataset = task_dataset.map(self.compute_input_length, batched=True)\n        task_dataset = task_dataset.map(self.process_spans, batched=True)\n        print(task_dataset)\n        # todo check edge cases\n        task_dataset = task_dataset.filter(lambda example: len(example['span_head_idxs']) == len(\n            example['span_tail_idxs']))  # no need to run on empty set\n        print(task_dataset)\n        task_dataset = task_dataset.map(self.generate_discourse_type_ids, batched=True)\n        task_dataset = task_dataset.map(self.sanity_check_head_tail, batched=True)\n\n        if mode != \"infer\":\n            task_dataset = task_dataset.map(self.generate_labels, batched=True)\n            task_dataset = task_dataset.map(self.sanity_check_head_labels, batched=True)\n\n        try:\n            task_dataset = task_dataset.remove_columns(column_names=[\"__index_level_0__\"])\n        except Exception as e:\n            pass\n        return df, task_dataset\n\n#--------------- dataset with truncation ---------------------------------------------#\n\n\ndef get_fast_dataset(config, df, essay_df, mode=\"train\"):\n    \"\"\"Function to get fast approach dataset with truncation & sliding window\n    \"\"\"\n    dataset_creator = AuxFeedbackDataset(config)\n    _, task_dataset = dataset_creator.get_dataset(df, essay_df, mode=mode)\n\n    original_dataset = deepcopy(task_dataset)\n    tokenizer = dataset_creator.tokenizer\n    START_IDS = dataset_creator.discourse_token_ids\n    END_IDS = dataset_creator.discourse_end_ids\n    GLOBAL_IDS = dataset_creator.global_tokens\n\n    def tokenize_with_truncation(examples):\n        tz = tokenizer(\n            examples[\"essay_text\"],\n            padding=False,\n            truncation=True,\n            add_special_tokens=True,\n            return_offsets_mapping=True,\n            max_length=config[\"max_length\"],\n            stride=config[\"stride\"],\n            return_overflowing_tokens=True,\n            return_token_type_ids=True,\n        )\n        return tz\n\n    def process_span(examples):\n        span_head_idxs, span_tail_idxs = [], []\n        span_head_char_start_idxs, span_tail_char_end_idxs = [], []\n\n        buffer = 25  # do not include a head if it is within buffer distance away from last token\n\n        for example_input_ids, example_offset_mapping in zip(examples[\"input_ids\"], examples[\"offset_mapping\"]):\n            # ------------------- Span Heads -----------------------------------------#\n            if len(example_input_ids) < config[\"max_length\"]:  # no truncation\n                head_candidate = [pos for pos, this_id in enumerate(example_input_ids) if this_id in START_IDS]\n            else:\n                head_candidate = [pos for pos, this_id in enumerate(example_input_ids) if (\n                    (this_id in START_IDS) & (pos <= config[\"max_length\"]-buffer))]\n\n            n_heads = len(head_candidate)\n\n            # ------------------- Span Tails -----------------------------------------#\n            tail_candidate = [pos for pos, this_id in enumerate(example_input_ids) if this_id in END_IDS]\n\n            # ------------------- Edge Cases -----------------------------------------#\n            # 1. A tail occurs before the first head in the sequence due to truncation\n            if (len(tail_candidate) > 0) & (len(head_candidate) > 0):\n                if tail_candidate[0] < head_candidate[0]:  # truncation effect\n                    # print(f\"check: heads: {head_candidate}, tails {tail_candidate}\")\n                    tail_candidate = tail_candidate[1:]  # shift by one\n\n            # 2. Tail got chopped off due to truncation but the corresponding head is still there\n            if len(tail_candidate) < n_heads:\n                assert len(tail_candidate) + 1 == n_heads\n                assert len(example_input_ids) == config[\"max_length\"]  # should only happen if input text is truncated\n                tail_candidate.append(config[\"max_length\"]-2)  # the token before [SEP] token\n\n            # 3. Additional tails remain in the buffer region\n            if len(tail_candidate) > len(head_candidate):\n                tail_candidate = tail_candidate[:len(head_candidate)]\n\n            # ------------------- Create the fields ------------------------------------#\n            example_span_head_char_start_idxs = [example_offset_mapping[pos][0] for pos in head_candidate]\n            example_span_tail_char_end_idxs = [example_offset_mapping[pos][1] for pos in tail_candidate]\n\n            span_head_idxs.append(head_candidate)\n            span_tail_idxs.append(tail_candidate)\n            span_head_char_start_idxs.append(example_span_head_char_start_idxs)\n            span_tail_char_end_idxs.append(example_span_tail_char_end_idxs)\n\n        return {\n            \"span_head_idxs\": span_head_idxs,\n            \"span_tail_idxs\": span_tail_idxs,\n            \"span_head_char_start_idxs\": span_head_char_start_idxs,\n            \"span_tail_char_end_idxs\": span_tail_char_end_idxs,\n        }\n\n    def enforce_alignment(examples):\n        uids = []\n\n        for example_span_head_char_start_idxs, example_overflow_to_sample_mapping in zip(\n                examples[\"span_head_char_start_idxs\"], examples[\"overflow_to_sample_mapping\"]):\n\n            original_example = original_dataset[example_overflow_to_sample_mapping]\n            original_example_span_head_char_start_idxs = original_example[\"span_head_char_start_idxs\"]\n            original_example_uids = original_example[\"uids\"]\n            char2uid = {k: v for k, v in zip(original_example_span_head_char_start_idxs, original_example_uids)}\n            current_example_uids = [char2uid[char_idx] for char_idx in example_span_head_char_start_idxs]\n            uids.append(current_example_uids)\n        return {\"uids\": uids}\n\n    def recompute_labels(examples):\n        labels = []\n        for example_span_head_char_start_idxs, example_overflow_to_sample_mapping in zip(\n                examples[\"span_head_char_start_idxs\"], examples[\"overflow_to_sample_mapping\"]):\n\n            original_example = original_dataset[example_overflow_to_sample_mapping]\n            original_example_span_head_char_start_idxs = original_example[\"span_head_char_start_idxs\"]\n            original_example_labels = original_example[\"labels\"]\n            char2label = {k: v for k, v in zip(original_example_span_head_char_start_idxs, original_example_labels)}\n            current_example_labels = [char2label[char_idx] for char_idx in example_span_head_char_start_idxs]\n            labels.append(current_example_labels)\n        return {\"labels\": labels}\n\n    def recompute_discourse_type_ids(examples):\n        discourse_type_ids = []\n        for example_span_head_char_start_idxs, example_overflow_to_sample_mapping in zip(\n                examples[\"span_head_char_start_idxs\"], examples[\"overflow_to_sample_mapping\"]):\n\n            original_example = original_dataset[example_overflow_to_sample_mapping]\n            original_example_span_head_char_start_idxs = original_example[\"span_head_char_start_idxs\"]\n            original_example_discourse_type_ids = original_example[\"discourse_type_ids\"]\n            char2discourse_id = {k: v for k, v in zip(\n                original_example_span_head_char_start_idxs, original_example_discourse_type_ids)}\n            current_example_discourse_type_ids = [char2discourse_id[char_idx]\n                                                  for char_idx in example_span_head_char_start_idxs]\n            discourse_type_ids.append(current_example_discourse_type_ids)\n        return {\"discourse_type_ids\": discourse_type_ids}\n\n    def compute_input_length(examples):\n        return {\"input_length\": [len(x) for x in examples[\"input_ids\"]]}\n\n    def sanity_check_head_tail(examples):\n        for head_idxs, tail_idxs in zip(examples[\"span_head_idxs\"], examples[\"span_tail_idxs\"]):\n            assert len(head_idxs) == len(tail_idxs)\n            for head, tail in zip(head_idxs, tail_idxs):\n                assert tail > head + 1, f\"head idxs: {head_idxs}, tail idxs {tail_idxs}\"\n\n    task_dataset = task_dataset.map(\n        tokenize_with_truncation,\n        batched=True,\n        remove_columns=task_dataset.column_names,\n        batch_size=len(task_dataset)\n    )\n\n    task_dataset = task_dataset.map(process_span, batched=True)\n    task_dataset = task_dataset.map(enforce_alignment, batched=True)\n    task_dataset = task_dataset.map(recompute_discourse_type_ids, batched=True)\n    task_dataset = task_dataset.map(sanity_check_head_tail, batched=True)\n\n    # no need to run on empty set\n    task_dataset = task_dataset.filter(lambda example: len(example['span_head_idxs']) != 0)\n    task_dataset = task_dataset.map(compute_input_length, batched=True)\n\n    if mode != \"infer\":\n        task_dataset = task_dataset.map(recompute_labels, batched=True)\n\n    to_return = dict()\n    to_return[\"dataset\"] = task_dataset\n    to_return[\"original_dataset\"] = original_dataset\n    to_return[\"tokenizer\"] = tokenizer\n    return to_return","metadata":{"_uuid":"0db231b0-3d33-494b-9076-6b33f938bf1d","_cell_guid":"c4016129-ac73-40cd-ba29-5aa49eb74407","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:48:05.492365Z","iopub.execute_input":"2022-08-23T12:48:05.492814Z","iopub.status.idle":"2022-08-23T12:48:05.561106Z","shell.execute_reply.started":"2022-08-23T12:48:05.492776Z","shell.execute_reply":"2022-08-23T12:48:05.560223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if use_exp205 or use_exp209:\n    os.makedirs(config[\"model_dir\"], exist_ok=True)\n\n    print(\"creating the inference datasets...\")\n    infer_ds_dict = get_fast_dataset(config, test_df, essay_df, mode=\"infer\")\n    tokenizer = infer_ds_dict[\"tokenizer\"]\n    infer_dataset = infer_ds_dict[\"dataset\"]\n    print(infer_dataset)","metadata":{"_uuid":"936a54dc-badf-48e0-923c-bd534d84128b","_cell_guid":"1798b479-3053-4621-a60a-b0d5f273fdf3","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:48:05.562314Z","iopub.execute_input":"2022-08-23T12:48:05.563103Z","iopub.status.idle":"2022-08-23T12:48:06.555169Z","shell.execute_reply.started":"2022-08-23T12:48:05.563065Z","shell.execute_reply":"2022-08-23T12:48:06.554248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if use_exp205 or use_exp209:\n    config[\"len_tokenizer\"] = len(tokenizer)\n\n    infer_dataset = infer_dataset.sort(\"input_length\")\n\n    infer_dataset.set_format(\n        type=None,\n        columns=['input_ids', 'attention_mask', 'token_type_ids', 'span_head_idxs',\n                 'span_tail_idxs', 'discourse_type_ids', 'uids']\n    )","metadata":{"_uuid":"0b1b45db-10a5-4c42-a0da-129f2151f962","_cell_guid":"ef40822c-40e8-435f-ac5c-74f88f711a63","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:48:06.55675Z","iopub.execute_input":"2022-08-23T12:48:06.55713Z","iopub.status.idle":"2022-08-23T12:48:06.573126Z","shell.execute_reply.started":"2022-08-23T12:48:06.557094Z","shell.execute_reply":"2022-08-23T12:48:06.572336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Loader","metadata":{"_uuid":"a1943357-b34f-4daa-b27c-926437a1c0ed","_cell_guid":"9fcf9948-d6ba-4fca-a8ff-b1ada4cf8b67","trusted":true}},{"cell_type":"code","source":"from copy import deepcopy\nfrom dataclasses import dataclass\n\nimport torch\nfrom transformers import DataCollatorWithPadding\n\n\n@dataclass\nclass CustomDataCollatorWithPadding(DataCollatorWithPadding):\n    \"\"\"\n    data collector for seq classification\n    \"\"\"\n\n    tokenizer = None\n    padding = True\n    max_length = None\n    pad_to_multiple_of = 512\n    return_tensors = \"pt\"\n\n    def __call__(self, features):\n        uids = [feature[\"uids\"] for feature in features]\n        discourse_type_ids = [feature[\"discourse_type_ids\"] for feature in features]\n        span_head_idxs = [feature[\"span_head_idxs\"] for feature in features]\n        span_tail_idxs = [feature[\"span_tail_idxs\"] for feature in features]\n        span_attention_mask = [[1]*len(feature[\"span_head_idxs\"]) for feature in features]\n\n        labels = None\n        if \"labels\" in features[0].keys():\n            labels = [feature[\"labels\"] for feature in features]\n\n        batch = self.tokenizer.pad(\n            features,\n            padding=self.padding,\n            max_length=self.max_length,\n            pad_to_multiple_of=self.pad_to_multiple_of,\n            return_tensors=None,\n        )\n\n        b_max = max([len(l) for l in span_head_idxs])\n        max_len = len(batch[\"input_ids\"][0])\n\n        default_head_idx = max(max_len - 10, 1)  # for padding\n        default_tail_idx = max(max_len - 4, 1)  # for padding\n\n        batch[\"span_head_idxs\"] = [\n            ex_span_head_idxs + [default_head_idx] * (b_max - len(ex_span_head_idxs)) for ex_span_head_idxs in span_head_idxs\n        ]\n\n        batch[\"uids\"] = [ex_uids + [-1] * (b_max - len(ex_uids)) for ex_uids in uids]\n        batch[\"discourse_type_ids\"] = [ex_discourse_type_ids + [0] *\n                                       (b_max - len(ex_discourse_type_ids)) for ex_discourse_type_ids in discourse_type_ids]\n\n        batch[\"span_tail_idxs\"] = [\n            ex_span_tail_idxs + [default_tail_idx] * (b_max - len(ex_span_tail_idxs)) for ex_span_tail_idxs in span_tail_idxs\n        ]\n\n        batch[\"span_attention_mask\"] = [\n            ex_discourse_masks + [0] * (b_max - len(ex_discourse_masks)) for ex_discourse_masks in span_attention_mask\n        ]\n\n        if labels is not None:\n            batch[\"labels\"] = [ex_labels + [-1] * (b_max - len(ex_labels)) for ex_labels in labels]\n\n        # multitask labels\n        def _get_additional_labels(label_id):\n            if label_id == 0:\n                vec = [0, 0]\n            elif label_id == 1:\n                vec = [1, 0]\n            elif label_id == 2:\n                vec = [1, 1]\n            elif label_id == -1:\n                vec = [-1, -1]\n            else:\n                raise\n            return vec\n\n        if labels is not None:\n            additional_labels = []\n            for ex_labels in batch[\"labels\"]:\n                ex_additional_labels = [_get_additional_labels(el) for el in ex_labels]\n                additional_labels.append(ex_additional_labels)\n            batch[\"multitask_labels\"] = additional_labels\n        # pdb.set_trace()\n\n        batch = {k: (torch.tensor(v, dtype=torch.int64) if k != \"multitask_labels\" else torch.tensor(\n            v, dtype=torch.float32)) for k, v in batch.items()}\n        return batch","metadata":{"_uuid":"c729f0de-9bf1-458e-bea5-1b621f9f8592","_cell_guid":"7a82e541-93de-45db-9418-e7b48728e2d9","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:48:06.575739Z","iopub.execute_input":"2022-08-23T12:48:06.576284Z","iopub.status.idle":"2022-08-23T12:48:06.593019Z","shell.execute_reply.started":"2022-08-23T12:48:06.576247Z","shell.execute_reply":"2022-08-23T12:48:06.592166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if use_exp205 or use_exp209:\n    data_collector = CustomDataCollatorWithPadding(tokenizer=tokenizer)\n\n    infer_dl = DataLoader(\n        infer_dataset,\n        batch_size=config[\"infer_bs\"],\n        shuffle=False,\n        collate_fn=data_collector\n    )","metadata":{"_uuid":"2ef549d0-ea15-41fb-be3a-2df709411c28","_cell_guid":"ab0b0ee8-48b9-43e1-a77b-320d60d6f6b5","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:48:06.594399Z","iopub.execute_input":"2022-08-23T12:48:06.594811Z","iopub.status.idle":"2022-08-23T12:48:06.607083Z","shell.execute_reply.started":"2022-08-23T12:48:06.594776Z","shell.execute_reply":"2022-08-23T12:48:06.606228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{"_uuid":"31c95aa4-819c-49eb-bfc1-3c52d8aa1896","_cell_guid":"42bfe046-3404-4707-ab15-afb64bd0bcbd","trusted":true}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nfrom torch.nn import LayerNorm\nfrom transformers import AutoConfig, AutoModel, BertConfig\nfrom transformers.models.bert.modeling_bert import BertAttention\n\nfrom transformers.models.deberta_v2.modeling_deberta_v2 import ContextPooler, StableDropout, DebertaV2Attention\n\n\n#-------- Model ------------------------------------------------------------------#\nclass FeedbackModel(nn.Module):\n    \"\"\"The feedback prize effectiveness baseline model\n    \"\"\"\n\n    def __init__(self, config):\n        super(FeedbackModel, self).__init__()\n        self.config = config\n\n        # base transformer\n        base_config = AutoConfig.from_pretrained(self.config[\"base_model_path\"])\n        base_config.update({\"add_pooling_layer\": False, \"max_position_embeddings\": 1024})\n        self.base_model = AutoModel.from_pretrained(self.config[\"base_model_path\"], config=base_config)\n\n        # dropouts\n        self.dropout = StableDropout(self.config[\"dropout\"])\n        \n        # multi-head attention\n        attention_config = deepcopy(self.base_model.config)\n        attention_config.update({\"relative_attention\": False})\n        self.fpe_span_attention = DebertaV2Attention(attention_config)\n        \n        # classification\n        hidden_size = self.base_model.config.hidden_size\n        feature_size = hidden_size\n        self.layer_norm = LayerNorm(feature_size, self.base_model.config.layer_norm_eps)\n        \n        # # LSTM Head\n        self.fpe_lstm_layer = nn.LSTM(\n            input_size=feature_size,\n            hidden_size=hidden_size,\n            num_layers=1,\n            batch_first=True,\n            bidirectional=False,\n        )\n\n        self.num_labels = self.config[\"num_labels\"]\n        self.classifier = nn.Linear(feature_size, self.config[\"num_labels\"])\n\n    def forward(self, input_ids, attention_mask, span_head_idxs, span_tail_idxs, span_attention_mask, **kwargs):\n        \n        bs = input_ids.shape[0]  # batch size\n        outputs = self.base_model(input_ids, attention_mask=attention_mask)\n        encoder_layer = outputs[0]\n        \n        self.fpe_lstm_layer.flatten_parameters()\n        encoder_layer = self.fpe_lstm_layer(encoder_layer)[0]\n\n        mean_feature_vector = []\n        for i in range(bs):  # TODO: vectorize\n            span_vec_i = []\n\n            for head, tail in zip(span_head_idxs[i], span_tail_idxs[i]):\n                # span feature\n                tmp = torch.mean(encoder_layer[i, head+1:tail], dim=0)  # [h]\n                span_vec_i.append(tmp)\n            span_vec_i = torch.stack(span_vec_i)  # (num_disourse, h)\n            mean_feature_vector.append(span_vec_i)\n\n        mean_feature_vector = torch.stack(mean_feature_vector)  # (bs, num_disourse, h)\n        mean_feature_vector = self.layer_norm(mean_feature_vector)\n\n        # attend to other features\n        extended_span_attention_mask = span_attention_mask.unsqueeze(1).unsqueeze(2)\n        span_attention_mask = extended_span_attention_mask * extended_span_attention_mask.squeeze(-2).unsqueeze(-1)\n        span_attention_mask = span_attention_mask.byte()\n        feature_vector = self.fpe_span_attention(mean_feature_vector, span_attention_mask)\n\n        feature_vector = self.dropout(feature_vector)\n\n        logits = self.classifier(feature_vector)\n\n        return logits","metadata":{"_uuid":"43aab2fc-bf98-47a3-8d5d-bc5e8fb39102","_cell_guid":"5629924d-e2ed-40af-a084-1358fe3de63d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:48:06.608705Z","iopub.execute_input":"2022-08-23T12:48:06.609041Z","iopub.status.idle":"2022-08-23T12:48:06.625724Z","shell.execute_reply.started":"2022-08-23T12:48:06.609014Z","shell.execute_reply":"2022-08-23T12:48:06.624832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference","metadata":{"_uuid":"62c4e849-38d0-4188-a93d-6c6e3f9b9ee3","_cell_guid":"0bbbc3f6-5223-4fb9-b194-2a391181bc59","trusted":true}},{"cell_type":"code","source":"checkpoints = [\n    \"../input/exp205-debv3-l-prompt/fpe_model_fold_0_best.pth.tar\",\n    \"../input/exp205-debv3-l-prompt/fpe_model_fold_1_best.pth.tar\",\n    \"../input/exp205-debv3-l-prompt/fpe_model_fold_2_best.pth.tar\",\n    \"../input/exp205-debv3-l-prompt/fpe_model_fold_3_best.pth.tar\",\n    \"../input/exp205-debv3-l-prompt/fpe_model_fold_4_best.pth.tar\",\n    \"../input/exp205-debv3-l-prompt/fpe_model_fold_5_best.pth.tar\",\n    \"../input/exp205-debv3-l-prompt/fpe_model_fold_6_best.pth.tar\",\n    \"../input/exp205-debv3-l-prompt/fpe_model_fold_7_best.pth.tar\",\n]","metadata":{"_uuid":"88344524-a86a-4f92-945a-b03bcce827f0","_cell_guid":"a0ef46dc-5139-4189-bfa8-4513f2ccdaeb","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:48:06.627051Z","iopub.execute_input":"2022-08-23T12:48:06.627438Z","iopub.status.idle":"2022-08-23T12:48:06.639161Z","shell.execute_reply.started":"2022-08-23T12:48:06.627404Z","shell.execute_reply":"2022-08-23T12:48:06.638344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def inference_fn(model, infer_dl, model_id):\n    all_preds = []\n    all_uids = []\n    accelerator = Accelerator()\n    model, infer_dl = accelerator.prepare(model, infer_dl)\n    \n    model.eval()\n    tk0 = tqdm(infer_dl, total=len(infer_dl))\n    \n    for batch in tk0:\n        with torch.no_grad():\n            logits = model(**batch) # (b, nd, 3)\n            batch_preds = F.softmax(logits, dim=-1)\n            batch_uids = batch[\"uids\"]\n        all_preds.append(batch_preds)\n        all_uids.append(batch_uids)\n    \n    all_preds = [p.to('cpu').detach().numpy().tolist() for p in all_preds]\n    all_preds = list(chain(*all_preds))\n    flat_preds = list(chain(*all_preds))\n    \n    all_uids = [p.to('cpu').detach().numpy().tolist() for p in all_uids]\n    all_uids = list(chain(*all_uids))\n    flat_uids = list(chain(*all_uids))    \n    \n    preds_df = pd.DataFrame(flat_preds)\n    preds_df.columns = [\"Ineffective\", \"Adequate\", \"Effective\"]\n    preds_df[\"span_uid\"] = flat_uids # SORTED_DISCOURSE_IDS\n    preds_df = preds_df[preds_df[\"span_uid\"]>=0].copy()\n    preds_df[\"discourse_id\"] = preds_df[\"span_uid\"].map(idx2discourse)\n    preds_df = preds_df[[\"discourse_id\", \"Ineffective\", \"Adequate\", \"Effective\"]].copy()\n    preds_df.to_csv(f\"exp205_model_preds_{model_id}.csv\", index=False)\n    \nif use_exp205:\n\n    for model_id, checkpoint in enumerate(checkpoints):\n        print(f\"infering from {checkpoint}\")\n        model = FeedbackModel(config)\n        ckpt = torch.load(checkpoint)\n        print(f\"validation score for fold {model_id} = {ckpt['loss']}\")\n        model.load_state_dict(ckpt['state_dict'])\n        inference_fn(model, infer_dl, model_id)\n\n    del model\n    gc.collect()\n    torch.cuda.empty_cache()","metadata":{"_uuid":"aadfa46b-52d7-42db-aa7c-11d5a75ec49a","_cell_guid":"effee015-2c0e-478c-b8c0-07086e9fa034","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:48:06.647485Z","iopub.execute_input":"2022-08-23T12:48:06.647747Z","iopub.status.idle":"2022-08-23T12:48:06.659341Z","shell.execute_reply.started":"2022-08-23T12:48:06.647722Z","shell.execute_reply":"2022-08-23T12:48:06.658327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if use_exp205:\n    import glob\n    import pandas as pd\n\n    csvs = glob.glob(\"exp205_model_preds_*.csv\")\n\n    idx = []\n    preds = []\n\n\n    for csv_idx, csv in enumerate(csvs):\n\n        print(\"==\"*40)\n        print(f\"preds in {csv}\")\n        df = pd.read_csv(csv)\n        df = df.sort_values(by=[\"discourse_id\"])\n        print(df.head(10))\n        print(\"==\"*40)\n\n        temp_preds = df.drop([\"discourse_id\"], axis=1).values\n        if csv_idx == 0:\n            idx = list(df[\"discourse_id\"])\n            preds = temp_preds\n        else:\n            preds += temp_preds\n\n    preds = preds / len(csvs)\n\n    exp205_df = pd.DataFrame()\n    exp205_df[\"discourse_id\"] = idx\n    exp205_df[\"Ineffective\"]  = preds[:, 0]\n    exp205_df[\"Adequate\"]     = preds[:, 1]\n    exp205_df[\"Effective\"]    = preds[:, 2]\n\n    exp205_df = exp205_df.groupby(\"discourse_id\")[[\"Ineffective\", \"Adequate\", \"Effective\"]].agg(np.mean).reset_index()","metadata":{"_uuid":"39e4cf27-0688-44cb-9992-172c45fb7b20","_cell_guid":"13620f4b-0c5e-41e1-961e-6957e6e80883","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:48:06.66066Z","iopub.execute_input":"2022-08-23T12:48:06.661027Z","iopub.status.idle":"2022-08-23T12:48:06.671975Z","shell.execute_reply.started":"2022-08-23T12:48:06.660993Z","shell.execute_reply":"2022-08-23T12:48:06.671095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if use_exp205:\n    exp205_df.head()","metadata":{"_uuid":"c8537aaf-674b-49d9-8a07-86dbda95ae80","_cell_guid":"4bbfca82-8c04-49c8-b1f7-d320bdf4c749","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T12:48:06.67298Z","iopub.execute_input":"2022-08-23T12:48:06.675963Z","iopub.status.idle":"2022-08-23T12:48:06.685014Z","shell.execute_reply.started":"2022-08-23T12:48:06.675936Z","shell.execute_reply":"2022-08-23T12:48:06.684171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exp209 - 10 fold debv3-l","metadata":{}},{"cell_type":"code","source":"checkpoints = [\n    \"../input/tk-fpe-models-v6/exp209-debv3-l-prompt/fpe_model_fold_0_best.pth.tar\",\n    \"../input/tk-fpe-models-v6/exp209-debv3-l-prompt/fpe_model_fold_1_best.pth.tar\",\n    \"../input/tk-fpe-models-v6/exp209-debv3-l-prompt/fpe_model_fold_2_best.pth.tar\",\n    \"../input/tk-fpe-models-v6/exp209-debv3-l-prompt/fpe_model_fold_3_best.pth.tar\",\n    \"../input/tk-fpe-models-v6/exp209-debv3-l-prompt/fpe_model_fold_4_best.pth.tar\",\n    \"../input/tk-fpe-models-v6/exp209-debv3-l-prompt/fpe_model_fold_5_best.pth.tar\",\n    \"../input/tk-fpe-models-v6/exp209-debv3-l-prompt/fpe_model_fold_6_best.pth.tar\",\n    \"../input/tk-fpe-models-v6/exp209-debv3-l-prompt/fpe_model_fold_7_best.pth.tar\",\n    \"../input/tk-fpe-models-v6/exp209-debv3-l-prompt/fpe_model_fold_8_best.pth.tar\",\n    \"../input/tk-fpe-models-v6/exp209-debv3-l-prompt/fpe_model_fold_9_best.pth.tar\",\n]","metadata":{"execution":{"iopub.status.busy":"2022-08-23T12:48:06.685816Z","iopub.execute_input":"2022-08-23T12:48:06.686104Z","iopub.status.idle":"2022-08-23T12:48:06.694701Z","shell.execute_reply.started":"2022-08-23T12:48:06.68608Z","shell.execute_reply":"2022-08-23T12:48:06.693822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nfrom torch.nn import LayerNorm\nfrom transformers import AutoConfig, AutoModel, BertConfig\nfrom transformers.models.bert.modeling_bert import BertAttention\n\nfrom transformers.models.deberta_v2.modeling_deberta_v2 import ContextPooler, StableDropout, DebertaV2Attention\n\n\n#-------- Model ------------------------------------------------------------------#\nclass FeedbackModel(nn.Module):\n    \"\"\"The feedback prize effectiveness baseline model\n    \"\"\"\n\n    def __init__(self, config):\n        super(FeedbackModel, self).__init__()\n        self.config = config\n\n        # base transformer\n        base_config = AutoConfig.from_pretrained(self.config[\"base_model_path\"])\n        base_config.update({\"add_pooling_layer\": False, \"max_position_embeddings\": 1024})\n        self.base_model = AutoModel.from_pretrained(self.config[\"base_model_path\"], config=base_config)\n\n        # dropouts\n        self.dropout = StableDropout(self.config[\"dropout\"])\n        \n        # multi-head attention\n        attention_config = deepcopy(self.base_model.config)\n        attention_config.update({\"relative_attention\": False})\n        self.fpe_span_attention = DebertaV2Attention(attention_config)\n        \n        # classification\n        hidden_size = self.base_model.config.hidden_size\n        feature_size = hidden_size\n        self.layer_norm = LayerNorm(feature_size, self.base_model.config.layer_norm_eps)\n        \n        # # LSTM Head\n        self.fpe_lstm_layer = nn.LSTM(\n            input_size=feature_size,\n            hidden_size=hidden_size//2,\n            num_layers=1,\n            batch_first=True,\n            bidirectional=True,\n        )\n\n        self.num_labels = self.config[\"num_labels\"]\n        self.classifier = nn.Linear(feature_size, self.config[\"num_labels\"])\n\n    def forward(self, input_ids, attention_mask, span_head_idxs, span_tail_idxs, span_attention_mask, **kwargs):\n        \n        bs = input_ids.shape[0]  # batch size\n        outputs = self.base_model(input_ids, attention_mask=attention_mask)\n        encoder_layer = outputs[0]\n        \n        self.fpe_lstm_layer.flatten_parameters()\n        encoder_layer = self.fpe_lstm_layer(encoder_layer)[0]\n\n        mean_feature_vector = []\n        for i in range(bs):  # TODO: vectorize\n            span_vec_i = []\n\n            for head, tail in zip(span_head_idxs[i], span_tail_idxs[i]):\n                # span feature\n                tmp = torch.mean(encoder_layer[i, head+1:tail], dim=0)  # [h]\n                span_vec_i.append(tmp)\n            span_vec_i = torch.stack(span_vec_i)  # (num_disourse, h)\n            mean_feature_vector.append(span_vec_i)\n\n        mean_feature_vector = torch.stack(mean_feature_vector)  # (bs, num_disourse, h)\n        mean_feature_vector = self.layer_norm(mean_feature_vector)\n\n        # attend to other features\n        extended_span_attention_mask = span_attention_mask.unsqueeze(1).unsqueeze(2)\n        span_attention_mask = extended_span_attention_mask * extended_span_attention_mask.squeeze(-2).unsqueeze(-1)\n        span_attention_mask = span_attention_mask.byte()\n        feature_vector = self.fpe_span_attention(mean_feature_vector, span_attention_mask)\n\n        feature_vector = self.dropout(feature_vector)\n\n        logits = self.classifier(feature_vector)\n\n        return logits","metadata":{"execution":{"iopub.status.busy":"2022-08-23T12:48:06.696005Z","iopub.execute_input":"2022-08-23T12:48:06.696688Z","iopub.status.idle":"2022-08-23T12:48:06.713261Z","shell.execute_reply.started":"2022-08-23T12:48:06.696654Z","shell.execute_reply":"2022-08-23T12:48:06.711918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def inference_fn(model, infer_dl, model_id):\n    all_preds = []\n    all_uids = []\n    accelerator = Accelerator()\n    model, infer_dl = accelerator.prepare(model, infer_dl)\n    \n    model.eval()\n    tk0 = tqdm(infer_dl, total=len(infer_dl))\n    \n    for batch in tk0:\n        with torch.no_grad():\n            logits = model(**batch) # (b, nd, 3)\n            batch_preds = F.softmax(logits, dim=-1)\n            batch_uids = batch[\"uids\"]\n        all_preds.append(batch_preds)\n        all_uids.append(batch_uids)\n    \n    all_preds = [p.to('cpu').detach().numpy().tolist() for p in all_preds]\n    all_preds = list(chain(*all_preds))\n    flat_preds = list(chain(*all_preds))\n    \n    all_uids = [p.to('cpu').detach().numpy().tolist() for p in all_uids]\n    all_uids = list(chain(*all_uids))\n    flat_uids = list(chain(*all_uids))    \n    \n    preds_df = pd.DataFrame(flat_preds)\n    preds_df.columns = [\"Ineffective\", \"Adequate\", \"Effective\"]\n    preds_df[\"span_uid\"] = flat_uids # SORTED_DISCOURSE_IDS\n    preds_df = preds_df[preds_df[\"span_uid\"]>=0].copy()\n    preds_df[\"discourse_id\"] = preds_df[\"span_uid\"].map(idx2discourse)\n    preds_df = preds_df[[\"discourse_id\", \"Ineffective\", \"Adequate\", \"Effective\"]].copy()\n    preds_df.to_csv(f\"exp209_model_preds_{model_id}.csv\", index=False)\n    \n\nfor model_id, checkpoint in enumerate(checkpoints):\n    print(f\"infering from {checkpoint}\")\n    model = FeedbackModel(config)\n    ckpt = torch.load(checkpoint)\n    print(f\"validation score for fold {model_id} = {ckpt['loss']}\")\n    model.load_state_dict(ckpt['state_dict'])\n    inference_fn(model, infer_dl, model_id)\n    \ndel model\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2022-08-23T12:48:06.714743Z","iopub.execute_input":"2022-08-23T12:48:06.715347Z","iopub.status.idle":"2022-08-23T12:54:24.793898Z","shell.execute_reply.started":"2022-08-23T12:48:06.71531Z","shell.execute_reply":"2022-08-23T12:54:24.791949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import glob\nimport pandas as pd\n\ncsvs = glob.glob(\"exp209_model_preds_*.csv\")\n\nidx = []\npreds = []\n\n\nfor csv_idx, csv in enumerate(csvs):\n    \n    print(\"==\"*40)\n    print(f\"preds in {csv}\")\n    df = pd.read_csv(csv)\n    df = df.sort_values(by=[\"discourse_id\"])\n    print(df.head(10))\n    print(\"==\"*40)\n    \n    temp_preds = df.drop([\"discourse_id\"], axis=1).values\n    if csv_idx == 0:\n        idx = list(df[\"discourse_id\"])\n        preds = temp_preds\n    else:\n        preds += temp_preds\n\npreds = preds / len(csvs)\n\nexp209_df = pd.DataFrame()\nexp209_df[\"discourse_id\"] = idx\nexp209_df[\"Ineffective\"]  = preds[:, 0]\nexp209_df[\"Adequate\"]     = preds[:, 1]\nexp209_df[\"Effective\"]    = preds[:, 2]\n\nexp209_df = exp209_df.groupby(\"discourse_id\")[[\"Ineffective\", \"Adequate\", \"Effective\"]].agg(np.mean).reset_index()","metadata":{"execution":{"iopub.status.busy":"2022-08-23T12:54:24.797144Z","iopub.execute_input":"2022-08-23T12:54:24.798625Z","iopub.status.idle":"2022-08-23T12:54:24.86721Z","shell.execute_reply.started":"2022-08-23T12:54:24.798543Z","shell.execute_reply":"2022-08-23T12:54:24.865259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"exp209_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-23T12:54:24.868869Z","iopub.execute_input":"2022-08-23T12:54:24.869279Z","iopub.status.idle":"2022-08-23T12:54:24.881069Z","shell.execute_reply.started":"2022-08-23T12:54:24.869241Z","shell.execute_reply":"2022-08-23T12:54:24.880111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### All data model","metadata":{}},{"cell_type":"code","source":"if use_full_data_models:\n\n    checkpoints = [\n        \"../input/exp209a-debv3-l-prompt-all/fpe_model_fold_0_best.pth.tar\",\n    ]\n\n    def inference_fn(model, infer_dl, model_id):\n        all_preds = []\n        all_uids = []\n        accelerator = Accelerator()\n        model, infer_dl = accelerator.prepare(model, infer_dl)\n\n        model.eval()\n        tk0 = tqdm(infer_dl, total=len(infer_dl))\n\n        for batch in tk0:\n            with torch.no_grad():\n                logits = model(**batch) # (b, nd, 3)\n                batch_preds = F.softmax(logits, dim=-1)\n                batch_uids = batch[\"uids\"]\n            all_preds.append(batch_preds)\n            all_uids.append(batch_uids)\n\n        all_preds = [p.to('cpu').detach().numpy().tolist() for p in all_preds]\n        all_preds = list(chain(*all_preds))\n        flat_preds = list(chain(*all_preds))\n\n        all_uids = [p.to('cpu').detach().numpy().tolist() for p in all_uids]\n        all_uids = list(chain(*all_uids))\n        flat_uids = list(chain(*all_uids))    \n\n        preds_df = pd.DataFrame(flat_preds)\n        preds_df.columns = [\"Ineffective\", \"Adequate\", \"Effective\"]\n        preds_df[\"span_uid\"] = flat_uids # SORTED_DISCOURSE_IDS\n        preds_df = preds_df[preds_df[\"span_uid\"]>=0].copy()\n        preds_df[\"discourse_id\"] = preds_df[\"span_uid\"].map(idx2discourse)\n        preds_df = preds_df[[\"discourse_id\", \"Ineffective\", \"Adequate\", \"Effective\"]].copy()\n        preds_df.to_csv(f\"exp209a_preds_{model_id}.csv\", index=False)\n\n    from copy import deepcopy\n    for model_id, checkpoint in enumerate(checkpoints):\n        print(f\"infering from {checkpoint}\")\n        new_config = deepcopy(config)\n        if \"10a\" in checkpoint:\n            new_config[\"num_labels\"] = 5\n\n        model = FeedbackModel(new_config)\n        model.half()\n        if \"swa\" in checkpoint:\n            ckpt = process_swa_checkpoint(checkpoint)\n        else:\n            ckpt = torch.load(checkpoint)\n            print(f\"validation score for fold {model_id} = {ckpt['loss']}\")\n        model.load_state_dict(ckpt['state_dict'])\n        inference_fn(model, infer_dl, model_id)","metadata":{"execution":{"iopub.status.busy":"2022-08-23T12:54:24.882647Z","iopub.execute_input":"2022-08-23T12:54:24.883427Z","iopub.status.idle":"2022-08-23T12:55:04.977367Z","shell.execute_reply.started":"2022-08-23T12:54:24.883389Z","shell.execute_reply":"2022-08-23T12:55:04.976258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if use_full_data_models:\n\n    import glob\n    import pandas as pd\n\n    csvs = glob.glob(\"exp209a_preds_*.csv\")\n\n    idx = []\n    preds = []\n\n\n    for csv_idx, csv in enumerate(csvs):\n\n        print(\"==\"*40)\n        print(f\"preds in {csv}\")\n        df = pd.read_csv(csv)\n        df = df.sort_values(by=[\"discourse_id\"])\n        print(df.head(10))\n        print(\"==\"*40)\n\n        temp_preds = df.drop([\"discourse_id\"], axis=1).values\n        if csv_idx == 0:\n            idx = list(df[\"discourse_id\"])\n            preds = temp_preds\n        else:\n            preds += temp_preds\n\n    preds = preds / len(csvs)\n\n    exp209a_df = pd.DataFrame()\n    exp209a_df[\"discourse_id\"] = idx\n    exp209a_df[\"Ineffective\"] = preds[:, 0]\n    exp209a_df[\"Adequate\"] = preds[:, 1]\n    exp209a_df[\"Effective\"] = preds[:, 2]\n\n    exp209a_df = exp209a_df.groupby(\"discourse_id\")[[\"Ineffective\", \"Adequate\", \"Effective\"]].agg(np.mean).reset_index()","metadata":{"execution":{"iopub.status.busy":"2022-08-23T12:55:04.97899Z","iopub.execute_input":"2022-08-23T12:55:04.979382Z","iopub.status.idle":"2022-08-23T12:55:05.001963Z","shell.execute_reply.started":"2022-08-23T12:55:04.979346Z","shell.execute_reply":"2022-08-23T12:55:05.001231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if use_full_data_models:\n    print(exp209a_df.head())","metadata":{"execution":{"iopub.status.busy":"2022-08-23T12:55:05.003153Z","iopub.execute_input":"2022-08-23T12:55:05.003597Z","iopub.status.idle":"2022-08-23T12:55:05.015169Z","shell.execute_reply.started":"2022-08-23T12:55:05.003562Z","shell.execute_reply":"2022-08-23T12:55:05.014227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    del model\n    gc.collect()\n    torch.cuda.empty_cache()\nexcept Exception as e:\n    print(e)","metadata":{"execution":{"iopub.status.busy":"2022-08-23T12:55:05.016663Z","iopub.execute_input":"2022-08-23T12:55:05.017708Z","iopub.status.idle":"2022-08-23T12:55:05.416189Z","shell.execute_reply.started":"2022-08-23T12:55:05.017535Z","shell.execute_reply":"2022-08-23T12:55:05.415053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LUKE","metadata":{}},{"cell_type":"code","source":"config = \"\"\"{\n    \"debug\": false,\n\n    \"base_model_path\": \"../input/luke-span-mlm\",\n    \"model_dir\": \"./outputs\",\n\n    \"max_length\": 512,\n    \"max_position_embeddings\": 512,\n    \"stride\": 128,\n    \"max_mention_length\": 400,\n    \"max_entity_length\": 24,\n    \"num_labels\": 3,\n    \"dropout\": 0.1,\n    \"infer_bs\": 16\n}\n\"\"\"\nconfig = json.loads(config)","metadata":{"execution":{"iopub.status.busy":"2022-08-23T12:55:05.417758Z","iopub.execute_input":"2022-08-23T12:55:05.418216Z","iopub.status.idle":"2022-08-23T12:55:05.426481Z","shell.execute_reply.started":"2022-08-23T12:55:05.418154Z","shell.execute_reply":"2022-08-23T12:55:05.425588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pdb\nimport re\nfrom copy import deepcopy\nfrom itertools import chain\n\nimport pandas as pd\nfrom datasets import Dataset\nfrom tokenizers import AddedToken\nfrom transformers import AutoTokenizer, LukeTokenizer\n\n\n#--------------- Tokenizer ---------------------------------------------#\ndef tokenizer_test(tokenizer):\n    print(\"==\"*40)\n    print(f\"tokenizer len: {len(tokenizer)}\")\n    print(\n        f\"tokenizer test: {tokenizer.tokenize('Starts: [==SOE==] [==SPAN==] [==END==]')}\")\n    print(\n        f\"tokenizer test: {tokenizer.tokenize('Starts: [==EOE==] [==SPAN==] [==END==]')}\")\n\n    print(\"==\"*40)\n\n\ndef get_tokenizer(config):\n    \"\"\"load the tokenizer\"\"\"\n\n    print(\"using auto tokenizer\")\n    tokenizer = AutoTokenizer.from_pretrained(config[\"base_model_path\"])\n\n    return tokenizer\n\n\n#--------------- Additional Tokens ---------------------------------------------#\n\nTOKEN_MAP = {\n    \"Lead\":                     [\"[==SPAN==]\", \"[==END==]\"],\n    \"Position\":                 [\"[==SPAN==]\", \"[==END==]\"],\n    \"Claim\":                    [\"[==SPAN==]\", \"[==END==]\"],\n    \"Counterclaim\":             [\"[==SPAN==]\", \"[==END==]\"],\n    \"Rebuttal\":                 [\"[==SPAN==]\", \"[==END==]\"],\n    \"Evidence\":                 [\"[==SPAN==]\", \"[==END==]\"],\n    \"Concluding Statement\":     [\"[==SPAN==]\", \"[==END==]\"]\n}\n\nDISCOURSE_START_TOKENS = [\n    \"[==SPAN==]\",\n]\n\nDISCOURSE_END_TOKENS = [\n    \"[==END==]\",\n]\n\nNEW_TOKENS = [\n    \"[==SPAN==]\",\n    \"[==END==]\",\n    \"[==SOE==]\",\n    \"[==EOE==]\",\n]\n\nADD_NEW_TOKENS_IN_LUKE = False\n#--------------- Data Processing ---------------------------------------------#\n\n\ndef relaxed_search(text, substring, min_length=2, fraction=0.99999):\n    \"\"\"\n    Returns substring's span from the given text with the certain precision.\n    \"\"\"\n\n    position = text.find(substring)\n    substring_length = len(substring)\n    if position == -1:\n        half_length = int(substring_length * fraction)\n        half_substring = substring[:half_length]\n        half_substring_length = len(half_substring)\n        if half_substring_length < min_length:\n            return [-1, 0]\n        else:\n            return relaxed_search(text=text,\n                                  substring=half_substring,\n                                  min_length=min_length,\n                                  fraction=fraction)\n\n    span = [position, position+substring_length]\n    return span\n\n\ndef build_span_map(discourse_list, essay_text):\n    reading_head = 0\n    to_return = dict()\n\n    try:\n        for cur_discourse in discourse_list:\n            if cur_discourse not in to_return:\n                to_return[cur_discourse] = []\n\n            matches = re.finditer(re.escape(r'{}'.format(cur_discourse)), essay_text)\n            for match in matches:\n                span_start, span_end = match.span()\n                if span_end <= reading_head:\n                    continue\n                to_return[cur_discourse].append(match.span())\n                reading_head = span_end\n                break\n\n        # post process\n        for cur_discourse in discourse_list:\n            if not to_return[cur_discourse]:\n                print(\"resorting to relaxed search...\")\n                to_return[cur_discourse] = [relaxed_search(essay_text, cur_discourse)]\n    except Exception as e:\n        pdb.set_trace()\n    return to_return\n\n\ndef get_substring_span(texts, mapping):\n    result = []\n    for text in texts:\n        ans = mapping[text].pop(0)\n        result.append(ans)\n    return result\n\n\ndef process_essay(essay_id, essay_text, anno_df):\n    \"\"\"insert newly added tokens in the essay text\n    \"\"\"\n    tmp_df = anno_df[anno_df[\"essay_id\"] == essay_id].copy()\n    tmp_df = tmp_df.sort_values(by=\"discourse_start\")\n    buffer = 0\n\n    for _, row in tmp_df.iterrows():\n        s, e, d_type = int(row.discourse_start) + buffer, int(row.discourse_end) + buffer, row.discourse_type\n        s_tok, e_tok = TOKEN_MAP[d_type]\n        essay_text = \" \".join([essay_text[:s], s_tok, essay_text[s:e], e_tok, essay_text[e:]])\n        buffer += len(s_tok) + len(e_tok) + 4\n\n    essay_text = \"[==SOE==]\" + essay_text + \"[==EOE==]\"\n    return essay_text\n\n\ndef process_input_df(anno_df, notes_df):\n    \"\"\"pre-process input dataframe\n\n    :param df: input dataframe\n    :type df: pd.DataFrame\n    :return: processed dataframe\n    :rtype: pd.DataFrame\n    \"\"\"\n    notes_df = deepcopy(notes_df)\n    anno_df = deepcopy(anno_df)\n    # pdb.set_trace()\n    # set_trace()\n\n    #------------------- Pre-Process Essay Text --------------------------#\n    anno_df[\"discourse_text\"] = anno_df[\"discourse_text\"].apply(lambda x: x.strip())  # pre-process\n    if \"discourse_effectiveness\" in anno_df.columns:\n        anno_df = anno_df[[\"discourse_id\", \"essay_id\", \"discourse_text\",\n                           \"discourse_type\", \"discourse_effectiveness\", \"uid\"]].copy()\n    else:\n        anno_df = anno_df[[\"discourse_id\", \"essay_id\", \"discourse_text\", \"discourse_type\", \"uid\"]].copy()\n\n    tmp_df = anno_df.groupby(\"essay_id\")[[\"discourse_id\", \"discourse_text\"]].agg(list).reset_index()\n    tmp_df = pd.merge(tmp_df, notes_df, on=\"essay_id\", how=\"left\")\n\n    print(\"--\"*40)\n    print(\"Warning! the following essay_ids are removed during processing...\")\n    remove_essay_ids = tmp_df[tmp_df[\"essay_text\"].isna()].essay_id.unique()\n    print(remove_essay_ids)\n    tmp_df = tmp_df[~tmp_df[\"essay_id\"].isin(remove_essay_ids)].copy()\n    anno_df = anno_df[~anno_df[\"essay_id\"].isin(remove_essay_ids)].copy()\n    notes_df = notes_df[~notes_df[\"essay_id\"].isin(remove_essay_ids)].copy()\n    print(\"--\"*40)\n\n    tmp_df[\"span_map\"] = tmp_df[[\"discourse_text\", \"essay_text\"]].apply(\n        lambda x: build_span_map(x[0], x[1]), axis=1)\n    tmp_df[\"span\"] = tmp_df[[\"discourse_text\", \"span_map\"]].apply(\n        lambda x: get_substring_span(x[0], x[1]), axis=1)\n\n    all_discourse_ids = list(chain(*tmp_df[\"discourse_id\"].values))\n    all_discourse_spans = list(chain(*tmp_df[\"span\"].values))\n    span_df = pd.DataFrame()\n    span_df[\"discourse_id\"] = all_discourse_ids\n    span_df[\"span\"] = all_discourse_spans\n    span_df[\"discourse_start\"] = span_df[\"span\"].apply(lambda x: x[0])\n    span_df[\"discourse_end\"] = span_df[\"span\"].apply(lambda x: x[1])\n    span_df = span_df.drop(columns=\"span\")\n\n    anno_df = pd.merge(anno_df, span_df, on=\"discourse_id\", how=\"left\")\n    # anno_df = pd.merge(anno_df, notes_df, on=\"essay_id\", how=\"left\")\n\n    print(\"==\"*40)\n    print(\"processing essay text and inserting new tokens at span boundaries\")\n    notes_df[\"essay_text\"] = notes_df[[\"essay_id\", \"essay_text\"]].apply(\n        lambda x: process_essay(x[0], x[1], anno_df), axis=1\n    )\n    print(\"==\"*40)\n\n    anno_df = anno_df.drop(columns=[\"discourse_start\", \"discourse_end\"])\n    notes_df = notes_df.drop_duplicates(subset=[\"essay_id\"])[[\"essay_id\", \"essay_text\"]].copy()\n\n    anno_df = pd.merge(anno_df, notes_df, on=\"essay_id\", how=\"left\")\n\n    if \"discourse_effectiveness\" in anno_df.columns:\n        grouped_df = anno_df.groupby(\"essay_id\")[[\"uid\", \"discourse_effectiveness\", \"discourse_type\"]].agg(list).reset_index()\n    else:\n        grouped_df = anno_df.groupby(\"essay_id\")[[\"uid\", \"discourse_type\"]].agg(list).reset_index()\n\n    grouped_df = pd.merge(grouped_df, notes_df, on=\"essay_id\", how=\"left\")\n    grouped_df = grouped_df.rename(columns={\"uid\": \"uids\"})\n\n    return grouped_df\n\n\n#--------------- Dataset w/o Truncation ----------------------------------------------#\n\n\nclass AuxFeedbackDataset:\n    \"\"\"Dataset class for feedback prize effectiveness task\n    \"\"\"\n\n    def __init__(self, config):\n        self.config = config\n\n        self.label2id = {\n            \"Ineffective\": 0,\n            \"Adequate\": 1,\n            \"Effective\": 2,\n            \"Mask\": -1,\n        }\n\n        self.discourse_type2id = {\n            \"Lead\": 0,\n            \"Position\": 1,\n            \"Claim\": 2,\n            \"Counterclaim\": 3,\n            \"Rebuttal\": 4,\n            \"Evidence\": 5,\n            \"Concluding Statement\": 6,\n        }\n\n        self.id2label = {v: k for k, v in self.label2id.items()}\n        self.load_tokenizer()\n\n    def load_tokenizer(self):\n        \"\"\"load tokenizer as per config \n        \"\"\"\n        self.tokenizer = get_tokenizer(self.config)\n        print(\"==\"*40)\n        print(\"token maps...\")\n        print(TOKEN_MAP)\n        print(\"==\"*40)\n\n        print(\"adding new tokens...\")\n        tokens_to_add = []\n\n        for this_tok in NEW_TOKENS:\n            tokens_to_add.append(AddedToken(this_tok, lstrip=False, rstrip=False))\n        self.tokenizer.add_tokens(tokens_to_add)\n        print(f\"tokenizer len: {len(self.tokenizer)}\")\n\n        self.discourse_token_ids = set(self.tokenizer.convert_tokens_to_ids(DISCOURSE_START_TOKENS))\n        self.discourse_end_ids = set(self.tokenizer.convert_tokens_to_ids(DISCOURSE_END_TOKENS))\n\n        tokenizer_test(self.tokenizer)\n\n    def tokenize_function(self, examples):\n        tz = self.tokenizer(\n            examples[\"essay_text\"],\n            padding=False,\n            truncation=False,  # no truncation at first\n            add_special_tokens=False,\n            return_offsets_mapping=True,\n        )\n        return tz\n\n    def process_spans(self, examples):\n\n        span_head_char_start_idxs, span_tail_char_end_idxs = [], []\n        span_head_idxs, span_tail_idxs = [], []\n\n        for example_input_ids, example_offset_mapping, example_uids in zip(examples[\"input_ids\"], examples[\"offset_mapping\"], examples[\"uids\"]):\n            example_span_head_idxs = [pos for pos, this_id in enumerate(\n                example_input_ids) if this_id in self.discourse_token_ids]\n            example_span_tail_idxs = [pos for pos, this_id in enumerate(\n                example_input_ids) if this_id in self.discourse_end_ids]\n\n            example_span_head_char_start_idxs = [example_offset_mapping[pos][0] for pos in example_span_head_idxs]\n            example_span_tail_char_end_idxs = [example_offset_mapping[pos][1] for pos in example_span_tail_idxs]\n\n            span_head_char_start_idxs.append(example_span_head_char_start_idxs)\n            span_tail_char_end_idxs.append(example_span_tail_char_end_idxs)\n\n            span_head_idxs.append(example_span_head_idxs)\n            span_tail_idxs.append(example_span_tail_idxs)\n\n        return {\n            \"span_head_idxs\": span_head_idxs,\n            \"span_tail_idxs\": span_tail_idxs,\n            \"span_head_char_start_idxs\": span_head_char_start_idxs,\n            \"span_tail_char_end_idxs\": span_tail_char_end_idxs,\n        }\n\n    def generate_labels(self, examples):\n        labels = []\n        for example_labels, example_uids in zip(examples[\"discourse_effectiveness\"], examples[\"uids\"]):\n            labels.append([self.label2id[l] for l in example_labels])\n        return {\"labels\": labels}\n\n    def generate_discourse_type_ids(self, examples):\n        discourse_type_ids = []\n        for example_discourse_types in examples[\"discourse_type\"]:\n            discourse_type_ids.append([self.discourse_type2id[dt] for dt in example_discourse_types])\n        return {\"discourse_type_ids\": discourse_type_ids}\n\n    def compute_input_length(self, examples):\n        return {\"input_length\": [len(x) for x in examples[\"input_ids\"]]}\n\n    def sanity_check_head_tail(self, examples):\n        for head_idxs, tail_idxs in zip(examples[\"span_head_idxs\"], examples[\"span_tail_idxs\"]):\n            assert len(head_idxs) == len(tail_idxs)\n            for head, tail in zip(head_idxs, tail_idxs):\n                assert tail > head + 1\n\n    def sanity_check_head_labels(self, examples):\n        for head_idxs, head_labels in zip(examples[\"span_head_idxs\"], examples[\"labels\"]):\n            assert len(head_idxs) == len(head_labels)\n\n    def get_dataset(self, df, essay_df, mode='train'):\n        \"\"\"main api for creating the Feedback dataset\n\n        :param df: input annotation dataframe\n        :type df: pd.DataFrame\n        :param essay_df: dataframe with essay texts\n        :type essay_df: pd.DataFrame\n        :param mode: check if required for train or infer, defaults to 'train'\n        :type mode: str, optional\n        :return: the created dataset\n        :rtype: Dataset\n        \"\"\"\n        df = process_input_df(df, essay_df)\n\n        # save a sample for sanity checks\n        sample_df = df.sample(min(16, len(df)))\n        sample_df.to_csv(os.path.join(self.config[\"model_dir\"], f\"{mode}_df_processed.csv\"), index=False)\n\n        task_dataset = Dataset.from_pandas(df)\n        task_dataset = task_dataset.map(self.tokenize_function, batched=True)\n        task_dataset = task_dataset.map(self.compute_input_length, batched=True)\n        task_dataset = task_dataset.map(self.process_spans, batched=True)\n        print(task_dataset)\n        # todo check edge cases\n        task_dataset = task_dataset.filter(lambda example: len(example['span_head_idxs']) == len(\n            example['span_tail_idxs']))  # no need to run on empty set\n        print(task_dataset)\n        task_dataset = task_dataset.map(self.generate_discourse_type_ids, batched=True)\n        task_dataset = task_dataset.map(self.sanity_check_head_tail, batched=True)\n\n        if mode != \"infer\":\n            task_dataset = task_dataset.map(self.generate_labels, batched=True)\n            task_dataset = task_dataset.map(self.sanity_check_head_labels, batched=True)\n\n        try:\n            task_dataset = task_dataset.remove_columns(column_names=[\"__index_level_0__\"])\n        except Exception as e:\n            pass\n        return df, task_dataset\n\n#--------------- Dataset w truncation ---------------------------------------------#\n\n\ndef get_fast_dataset(config, df, essay_df, mode=\"train\"):\n    \"\"\"Function to get fast approach dataset with truncation & sliding window\n    \"\"\"\n    dataset_creator = AuxFeedbackDataset(config)\n    _, task_dataset = dataset_creator.get_dataset(df, essay_df, mode=mode)\n\n    original_dataset = deepcopy(task_dataset)\n    tokenizer = dataset_creator.tokenizer\n    START_IDS = dataset_creator.discourse_token_ids\n    END_IDS = dataset_creator.discourse_end_ids\n\n    def tokenize_with_truncation(examples):\n        tz = tokenizer(\n            examples[\"essay_text\"],\n            padding=False,\n            truncation=True,\n            add_special_tokens=False,\n            return_offsets_mapping=True,\n            max_length=config[\"max_length\"],\n            stride=config[\"stride\"],\n            return_overflowing_tokens=True,\n        )\n        return tz\n\n    def process_span(examples):\n        span_head_idxs, span_tail_idxs = [], []\n        span_head_char_start_idxs, span_tail_char_end_idxs = [], []\n\n        buffer = 25  # do not include a head if it is within buffer distance away from last token\n\n        for example_input_ids, example_offset_mapping in zip(examples[\"input_ids\"], examples[\"offset_mapping\"]):\n            # ------------------- Span Heads -----------------------------------------#\n            if len(example_input_ids) < config[\"max_length\"]:  # no truncation\n                head_candidate = [pos for pos, this_id in enumerate(example_input_ids) if this_id in START_IDS]\n            else:\n                head_candidate = [pos for pos, this_id in enumerate(example_input_ids) if (\n                    (this_id in START_IDS) & (pos <= config[\"max_length\"]-buffer))]\n\n            n_heads = len(head_candidate)\n\n            # ------------------- Span Tails -----------------------------------------#\n            tail_candidate = [pos for pos, this_id in enumerate(example_input_ids) if this_id in END_IDS]\n\n            # ------------------- Edge Cases -----------------------------------------#\n            # 1. A tail occurs before the first head in the sequence due to truncation\n            if (len(tail_candidate) > 0) & (len(head_candidate) > 0):\n                if tail_candidate[0] < head_candidate[0]:  # truncation effect\n                    # print(f\"check: heads: {head_candidate}, tails {tail_candidate}\")\n                    tail_candidate = tail_candidate[1:]  # shift by one\n\n            # 2. Tail got chopped off due to truncation but the corresponding head is still there\n            if len(tail_candidate) < n_heads:\n                assert len(tail_candidate) + 1 == n_heads\n                assert len(example_input_ids) == config[\"max_length\"]  # should only happen if input text is truncated\n                tail_candidate.append(config[\"max_length\"]-2)  # the token before [SEP] token\n\n            # 3. Additional tails remain in the buffer region\n            if len(tail_candidate) > len(head_candidate):\n                tail_candidate = tail_candidate[:len(head_candidate)]\n\n            # ------------------- Create the fields ------------------------------------#\n            example_span_head_char_start_idxs = [example_offset_mapping[pos][0] for pos in head_candidate]\n            example_span_tail_char_end_idxs = [example_offset_mapping[pos][1] for pos in tail_candidate]\n\n            span_head_idxs.append(head_candidate)\n            span_tail_idxs.append(tail_candidate)\n            span_head_char_start_idxs.append(example_span_head_char_start_idxs)\n            span_tail_char_end_idxs.append(example_span_tail_char_end_idxs)\n\n        return {\n            \"span_head_idxs\": span_head_idxs,\n            \"span_tail_idxs\": span_tail_idxs,\n            \"span_head_char_start_idxs\": span_head_char_start_idxs,\n            \"span_tail_char_end_idxs\": span_tail_char_end_idxs,\n        }\n\n    def restore_essay_text(examples):\n        essay_text = []\n\n        for example_overflow_to_sample_mapping in examples[\"overflow_to_sample_mapping\"]:\n\n            original_example = original_dataset[example_overflow_to_sample_mapping]\n            original_example_essay_text = original_example[\"essay_text\"]\n            essay_text.append(original_example_essay_text)\n        return {\"essay_text\": essay_text}\n\n    def enforce_alignment(examples):\n        uids = []\n\n        for example_span_head_char_start_idxs, example_overflow_to_sample_mapping in zip(\n                examples[\"span_head_char_start_idxs\"], examples[\"overflow_to_sample_mapping\"]):\n\n            original_example = original_dataset[example_overflow_to_sample_mapping]\n            original_example_span_head_char_start_idxs = original_example[\"span_head_char_start_idxs\"]\n            original_example_uids = original_example[\"uids\"]\n            char2uid = {k: v for k, v in zip(original_example_span_head_char_start_idxs, original_example_uids)}\n            current_example_uids = [char2uid[char_idx] for char_idx in example_span_head_char_start_idxs]\n            uids.append(current_example_uids)\n        return {\"uids\": uids}\n\n    def recompute_labels(examples):\n        labels = []\n        for example_span_head_char_start_idxs, example_overflow_to_sample_mapping in zip(\n                examples[\"span_head_char_start_idxs\"], examples[\"overflow_to_sample_mapping\"]):\n\n            original_example = original_dataset[example_overflow_to_sample_mapping]\n            original_example_span_head_char_start_idxs = original_example[\"span_head_char_start_idxs\"]\n            original_example_labels = original_example[\"labels\"]\n            char2label = {k: v for k, v in zip(original_example_span_head_char_start_idxs, original_example_labels)}\n            current_example_labels = [char2label[char_idx] for char_idx in example_span_head_char_start_idxs]\n            labels.append(current_example_labels)\n        return {\"labels\": labels}\n\n    def recompute_discourse_type_ids(examples):\n        discourse_type_ids = []\n        for example_span_head_char_start_idxs, example_overflow_to_sample_mapping in zip(\n                examples[\"span_head_char_start_idxs\"], examples[\"overflow_to_sample_mapping\"]):\n\n            original_example = original_dataset[example_overflow_to_sample_mapping]\n            original_example_span_head_char_start_idxs = original_example[\"span_head_char_start_idxs\"]\n            original_example_discourse_type_ids = original_example[\"discourse_type_ids\"]\n            char2discourse_id = {k: v for k, v in zip(\n                original_example_span_head_char_start_idxs, original_example_discourse_type_ids)}\n            current_example_discourse_type_ids = [char2discourse_id[char_idx]\n                                                  for char_idx in example_span_head_char_start_idxs]\n            discourse_type_ids.append(current_example_discourse_type_ids)\n        return {\"discourse_type_ids\": discourse_type_ids}\n\n    def update_head_tail_char_idx(examples):\n        span_head_char_start_idxs, span_tail_char_end_idxs = [], []\n\n        new_texts = []\n\n        for example_span_head_char_start_idxs, example_span_tail_char_end_idxs, example_offset_mapping, example_essay_text in zip(\n                examples[\"span_head_char_start_idxs\"], examples[\"span_tail_char_end_idxs\"], examples[\"offset_mapping\"], examples[\"essay_text\"]):\n\n            offset_start = example_offset_mapping[0][0]\n            offset_end = example_offset_mapping[-1][1]\n\n            example_essay_text = example_essay_text[offset_start:offset_end]\n            new_texts.append(example_essay_text)\n\n            example_span_head_char_start_idxs = [pos - offset_start for pos in example_span_head_char_start_idxs]\n            example_span_tail_char_end_idxs = [pos - offset_start for pos in example_span_tail_char_end_idxs]\n            span_head_char_start_idxs.append(example_span_head_char_start_idxs)\n            span_tail_char_end_idxs.append(example_span_tail_char_end_idxs)\n        return {\"span_head_char_start_idxs\": span_head_char_start_idxs, \"span_tail_char_end_idxs\": span_tail_char_end_idxs, \"essay_text\": new_texts}\n\n    def compute_input_length(examples):\n        return {\"input_length\": [len(x) for x in examples[\"input_ids\"]]}\n\n    def sanity_check_head_tail(examples):\n        for head_idxs, tail_idxs in zip(examples[\"span_head_idxs\"], examples[\"span_tail_idxs\"]):\n            assert len(head_idxs) == len(tail_idxs)\n            for head, tail in zip(head_idxs, tail_idxs):\n                assert tail > head + 1, f\"head idxs: {head_idxs}, tail idxs {tail_idxs}\"\n\n    task_dataset = task_dataset.map(\n        tokenize_with_truncation,\n        batched=True,\n        remove_columns=task_dataset.column_names,\n        batch_size=len(task_dataset)\n    )\n\n    task_dataset = task_dataset.map(process_span, batched=True)\n    task_dataset = task_dataset.map(enforce_alignment, batched=True)\n    task_dataset = task_dataset.map(recompute_discourse_type_ids, batched=True)\n    task_dataset = task_dataset.map(sanity_check_head_tail, batched=True)\n\n    task_dataset = task_dataset.map(restore_essay_text, batched=True)\n\n    # no need to run on empty set\n    task_dataset = task_dataset.filter(lambda example: len(example['span_head_idxs']) != 0)\n    task_dataset = task_dataset.map(compute_input_length, batched=True)\n\n    if mode != \"infer\":\n        task_dataset = task_dataset.map(recompute_labels, batched=True)\n\n    task_dataset = task_dataset.map(update_head_tail_char_idx, batched=True)\n\n    to_return = dict()\n    to_return[\"dataset\"] = task_dataset\n    to_return[\"original_dataset\"] = original_dataset\n    to_return[\"tokenizer\"] = tokenizer\n    return to_return\n\n\ndef get_luke_dataset(config, df, essay_df, mode=\"train\"):\n    stage_one_config = deepcopy(config)\n    stage_one_config[\"base_model_path\"] = \"../input/roberta-base\"  # Fast Tokenizer\n    buffer = 2\n    stage_one_config[\"max_length\"] = config[\"max_length\"] - buffer  # - config[\"max_entity_length\"]\n    dataset_dict = get_fast_dataset(stage_one_config, df, essay_df, mode)\n\n    task_dataset = dataset_dict[\"dataset\"]\n\n    def get_entity_spans(examples):\n        entity_spans = []\n        for ex_starts, ex_ends in zip(examples[\"span_head_char_start_idxs\"], examples[\"span_tail_char_end_idxs\"]):\n            ex_entity_spans = [tuple([a, b]) for a, b in zip(ex_starts, ex_ends)]\n            entity_spans.append(ex_entity_spans)\n        return {\"entity_spans\": entity_spans}\n\n    # prepare luke specific inputs\n    task_dataset = task_dataset.map(get_entity_spans, batched=True)\n\n    tokenizer = LukeTokenizer.from_pretrained(\n        config[\"base_model_path\"], task=\"entity_span_classification\", max_mention_length=config[\"max_mention_length\"])\n\n    # add new tokens\n    if ADD_NEW_TOKENS_IN_LUKE:\n        print(\"adding new tokens...\")\n        tokens_to_add = []\n        for this_tok in NEW_TOKENS:\n            tokens_to_add.append(AddedToken(this_tok, lstrip=False, rstrip=False))\n        tokenizer.add_tokens(tokens_to_add)\n\n    tokenizer_test(tokenizer)\n\n    def tokenize_with_entity_spans(example):\n        tz = tokenizer(\n            example[\"essay_text\"],\n            entity_spans=[tuple(t) for t in example[\"entity_spans\"]],\n            max_entity_length=config[\"max_entity_length\"],\n            padding=False,\n            truncation=False,\n            add_special_tokens=True,\n        )\n        return tz\n    task_dataset = task_dataset.map(tokenize_with_entity_spans, batched=False)\n\n    return_dict = {\n        \"dataset\": task_dataset,\n        \"tokenizer\": tokenizer\n    }\n\n    return return_dict","metadata":{"execution":{"iopub.status.busy":"2022-08-23T12:55:05.429974Z","iopub.execute_input":"2022-08-23T12:55:05.430295Z","iopub.status.idle":"2022-08-23T12:55:05.51474Z","shell.execute_reply.started":"2022-08-23T12:55:05.430264Z","shell.execute_reply":"2022-08-23T12:55:05.513739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nos.makedirs(config[\"model_dir\"], exist_ok=True)\n\nprint(\"creating the inference datasets...\")\ninfer_ds_dict = get_luke_dataset(config, test_df, essay_df, mode=\"infer\")\ntokenizer = infer_ds_dict[\"tokenizer\"]\ninfer_dataset = infer_ds_dict[\"dataset\"]\nprint(infer_dataset)","metadata":{"execution":{"iopub.status.busy":"2022-08-23T12:55:05.51664Z","iopub.execute_input":"2022-08-23T12:55:05.517028Z","iopub.status.idle":"2022-08-23T12:55:06.621419Z","shell.execute_reply.started":"2022-08-23T12:55:05.516982Z","shell.execute_reply":"2022-08-23T12:55:06.620471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config[\"len_tokenizer\"] = len(tokenizer)\n\ninfer_dataset = infer_dataset.sort(\"input_length\")\n\ninfer_dataset.set_format(\n    type=None,\n    columns=[\"input_ids\", \"attention_mask\", \"entity_ids\", \"entity_position_ids\", \"discourse_type_ids\",\n             \"entity_attention_mask\", \"entity_start_positions\", \"entity_end_positions\", \"uids\"]\n)\n#","metadata":{"execution":{"iopub.status.busy":"2022-08-23T12:55:06.62299Z","iopub.execute_input":"2022-08-23T12:55:06.623411Z","iopub.status.idle":"2022-08-23T12:55:06.639034Z","shell.execute_reply.started":"2022-08-23T12:55:06.623374Z","shell.execute_reply":"2022-08-23T12:55:06.638297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from copy import deepcopy\nfrom dataclasses import dataclass\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom transformers import DataCollatorWithPadding\n\n\n@dataclass\nclass CustomDataCollatorWithPadding(DataCollatorWithPadding):\n    \"\"\"\n    data collector for seq classification\n    \"\"\"\n\n    tokenizer = None\n    padding = True\n    max_length = None\n    pad_to_multiple_of = None\n    return_tensors = \"pt\"\n\n    def __call__(self, features):\n        uids = [feature[\"uids\"] for feature in features]\n        discourse_type_ids = [feature[\"discourse_type_ids\"] for feature in features]\n\n        labels = None\n        if \"labels\" in features[0].keys():\n            labels = [feature[\"labels\"] for feature in features]\n\n        batch = self.tokenizer.pad(\n            features,\n            padding=self.padding,\n            max_length=self.max_length,\n            pad_to_multiple_of=self.pad_to_multiple_of,\n            return_tensors=None,\n        )\n\n        b_max = max([len(l) for l in uids])\n\n        batch[\"uids\"] = [ex_uids + [-1] * (b_max - len(ex_uids)) for ex_uids in uids]\n        batch[\"discourse_type_ids\"] = [ex_dts + [-1] * (b_max - len(ex_dts)) for ex_dts in discourse_type_ids]\n\n        if labels is not None:\n            batch[\"labels\"] = [ex_labels + [-1] * (b_max - len(ex_labels)) for ex_labels in labels]\n\n        batch = {k: (torch.tensor(v, dtype=torch.int64)) for k, v in batch.items()}\n        return batch","metadata":{"execution":{"iopub.status.busy":"2022-08-23T12:55:06.640519Z","iopub.execute_input":"2022-08-23T12:55:06.641116Z","iopub.status.idle":"2022-08-23T12:55:06.651871Z","shell.execute_reply.started":"2022-08-23T12:55:06.641073Z","shell.execute_reply":"2022-08-23T12:55:06.651049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_collector = CustomDataCollatorWithPadding(tokenizer=tokenizer)\n\ninfer_dl = DataLoader(\n    infer_dataset,\n    batch_size=config[\"infer_bs\"],\n    shuffle=False,\n    collate_fn=data_collector\n)","metadata":{"execution":{"iopub.status.busy":"2022-08-23T12:55:06.652884Z","iopub.execute_input":"2022-08-23T12:55:06.655154Z","iopub.status.idle":"2022-08-23T12:55:06.741212Z","shell.execute_reply.started":"2022-08-23T12:55:06.655112Z","shell.execute_reply":"2022-08-23T12:55:06.740248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pdb\nfrom copy import deepcopy\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nfrom torch.nn import LayerNorm\nfrom transformers import AutoConfig, AutoModel, BertConfig\nfrom transformers.models.bert.modeling_bert import BertAttention, BertEncoder\n\n\n#-------- Model ------------------------------------------------------------------#\n\nclass FeedbackModel(nn.Module):\n    \"\"\"\n    The feedback prize effectiveness model for fast approach\n    \"\"\"\n\n    def __init__(self, config):\n        print(\"==\"*40)\n        print(\"initializing the feedback model...\")\n\n        super(FeedbackModel, self).__init__()\n        self.config = config\n\n        # base transformer\n        base_config = AutoConfig.from_pretrained(self.config[\"base_model_path\"])\n        base_config.update(\n            {\"max_position_embeddings\": config[\"max_position_embeddings\"]+2}\n        )\n        self.base_model = AutoModel.from_pretrained(self.config[\"base_model_path\"], config=base_config)\n\n        # resize model embeddings\n        print(\"resizing model embeddings...\")\n        print(f\"tokenizer length = {config['len_tokenizer']}\")\n        self.base_model.resize_token_embeddings(config[\"len_tokenizer\"])\n\n        self.num_labels = self.config[\"num_labels\"]\n\n        # LSTM Head\n        hidden_size = self.base_model.config.hidden_size\n\n        self.fpe_lstm_layer = nn.LSTM(\n            input_size=hidden_size,\n            hidden_size=hidden_size//2,\n            num_layers=1,\n            batch_first=True,\n            bidirectional=True,\n        )\n        # classification\n        feature_size = hidden_size * 3\n        self.classifier = nn.Linear(feature_size, self.num_labels)\n        self.discourse_classifier = nn.Linear(feature_size, 7)  # 7 discourse elements\n\n        # dropout family\n        self.dropout = nn.Dropout(self.config[\"dropout\"])\n        self.dropout1 = nn.Dropout(0.1)\n        self.dropout2 = nn.Dropout(0.2)\n        self.dropout3 = nn.Dropout(0.3)\n        self.dropout4 = nn.Dropout(0.4)\n        self.dropout5 = nn.Dropout(0.5)\n\n        self.layer_norm = LayerNorm(feature_size, self.base_model.config.layer_norm_eps)\n\n\n    def forward(\n        self,\n        input_ids,\n        attention_mask,\n        entity_ids,\n        entity_attention_mask,\n        entity_position_ids,\n        entity_start_positions,\n        entity_end_positions,\n        discourse_type_ids,\n        **kwargs\n    ):\n\n        # get contextual token representations from base transformer\n        outputs = self.base_model(\n            input_ids,\n            attention_mask=attention_mask,\n            entity_ids=entity_ids,\n            entity_attention_mask=entity_attention_mask,\n            entity_position_ids=entity_position_ids,\n        )\n\n        # run contextual information through lstm\n        encoder_layer = outputs.last_hidden_state\n        encoder_layer_entity = outputs.entity_last_hidden_state\n\n        self.fpe_lstm_layer.flatten_parameters()\n        encoder_layer = self.fpe_lstm_layer(encoder_layer)[0]\n\n        hidden_size = outputs.last_hidden_state.size(-1)\n\n        entity_start_positions = entity_start_positions.unsqueeze(-1).expand(-1, -1, hidden_size)\n        start_states = torch.gather(encoder_layer, -2, entity_start_positions)\n        entity_end_positions = entity_end_positions.unsqueeze(-1).expand(-1, -1, hidden_size)\n        end_states = torch.gather(encoder_layer, -2, entity_end_positions)\n        feature_vector = torch.cat([start_states, end_states, encoder_layer_entity],\n                                   dim=2)  # check if should use lstm\n\n        feature_vector1 = self.dropout1(feature_vector)\n        feature_vector2 = self.dropout2(feature_vector)\n        feature_vector3 = self.dropout3(feature_vector)\n        feature_vector4 = self.dropout4(feature_vector)\n        feature_vector5 = self.dropout5(feature_vector)\n\n        # logits = self.classifier(feature_vector)\n        logits1 = self.classifier(feature_vector1)\n        logits2 = self.classifier(feature_vector2)\n        logits3 = self.classifier(feature_vector3)\n        logits4 = self.classifier(feature_vector4)\n        logits5 = self.classifier(feature_vector5)\n        logits = (logits1 + logits2 + logits3 + logits4 + logits5)/5\n\n        return logits","metadata":{"execution":{"iopub.status.busy":"2022-08-23T12:55:06.74534Z","iopub.execute_input":"2022-08-23T12:55:06.746462Z","iopub.status.idle":"2022-08-23T12:55:06.764782Z","shell.execute_reply.started":"2022-08-23T12:55:06.746424Z","shell.execute_reply":"2022-08-23T12:55:06.763999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoints = [\n    \"../input/exp17-luke-dataset-part-1/exp-17-luke-8folds-part-1/fpe_model_fold_0_best.pth.tar\",\n    \"../input/exp17-luke-dataset-part-1/exp-17-luke-8folds-part-1/fpe_model_fold_1_best.pth.tar\",\n    \"../input/exp17-luke-dataset-part-1/exp-17-luke-8folds-part-1/fpe_model_fold_2_best.pth.tar\",\n    \"../input/exp17-luke-dataset-part-1/exp-17-luke-8folds-part-1/fpe_model_fold_3_best.pth.tar\",\n    \"../input/exp17-luke-dataset-part-2/exp-17-luke-8folds-part-2/fpe_model_fold_4_best.pth.tar\",\n    \"../input/exp17-luke-dataset-part-2/exp-17-luke-8folds-part-2/fpe_model_fold_5_best.pth.tar\",\n    \"../input/exp17-luke-dataset-part-2/exp-17-luke-8folds-part-2/fpe_model_fold_6_best.pth.tar\",\n    \"../input/exp17-luke-dataset-part-2/exp-17-luke-8folds-part-2/fpe_model_fold_7_best.pth.tar\",\n]\n\ndef inference_fn(model, infer_dl, model_id):\n    all_preds = []\n    all_uids = []\n    accelerator = Accelerator()\n    model, infer_dl = accelerator.prepare(model, infer_dl)\n    \n    model.eval()\n    tk0 = tqdm(infer_dl, total=len(infer_dl))\n    \n    for batch in tk0:\n        with torch.no_grad():\n            logits = model(**batch) # (b, nd, 3)\n            batch_preds = F.softmax(logits, dim=-1)\n            batch_uids = batch[\"uids\"]\n        all_preds.append(batch_preds)\n        all_uids.append(batch_uids)\n    \n    all_preds = [p.to('cpu').detach().numpy().tolist() for p in all_preds]\n    all_preds = list(chain(*all_preds))\n    flat_preds = list(chain(*all_preds))\n    \n    all_uids = [p.to('cpu').detach().numpy().tolist() for p in all_uids]\n    all_uids = list(chain(*all_uids))\n    flat_uids = list(chain(*all_uids))    \n    \n    preds_df = pd.DataFrame(flat_preds)\n    preds_df.columns = [\"Ineffective\", \"Adequate\", \"Effective\"]\n    preds_df[\"span_uid\"] = flat_uids # SORTED_DISCOURSE_IDS\n    preds_df = preds_df[preds_df[\"span_uid\"]>=0].copy()\n    preds_df[\"discourse_id\"] = preds_df[\"span_uid\"].map(idx2discourse)\n    preds_df = preds_df[[\"discourse_id\", \"Ineffective\", \"Adequate\", \"Effective\"]].copy()\n    preds_df.to_csv(f\"exp17_model_preds_{model_id}.csv\", index=False)\n    \n\nfor model_id, checkpoint in enumerate(checkpoints):\n    print(f\"infering from {checkpoint}\")\n    model = FeedbackModel(config)\n    ckpt = torch.load(checkpoint)\n    print(f\"validation score for fold {model_id} = {ckpt['loss']}\")\n    model.load_state_dict(ckpt['state_dict'])\n    inference_fn(model, infer_dl, model_id)\n    \n    \ndel model\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2022-08-23T12:55:06.766463Z","iopub.execute_input":"2022-08-23T12:55:06.766974Z","iopub.status.idle":"2022-08-23T12:59:26.355951Z","shell.execute_reply.started":"2022-08-23T12:55:06.766934Z","shell.execute_reply":"2022-08-23T12:59:26.354815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import glob\nimport pandas as pd\n\ncsvs = glob.glob(\"exp17_model_preds_*.csv\")\n\nidx = []\npreds = []\n\n\nfor csv_idx, csv in enumerate(csvs):\n    \n    print(\"==\"*40)\n    print(f\"preds in {csv}\")\n    df = pd.read_csv(csv)\n    df = df.sort_values(by=[\"discourse_id\"])\n    print(df.head(10))\n    print(\"==\"*40)\n    \n    temp_preds = df.drop([\"discourse_id\"], axis=1).values\n    if csv_idx == 0:\n        idx = list(df[\"discourse_id\"])\n        preds = temp_preds\n    else:\n        preds += temp_preds\n\npreds = preds / len(csvs)\n\nexp17_df = pd.DataFrame()\nexp17_df[\"discourse_id\"] = idx\nexp17_df[\"Ineffective\"] = preds[:, 0]\nexp17_df[\"Adequate\"] = preds[:, 1]\nexp17_df[\"Effective\"] = preds[:, 2]\n\nexp17_df = exp17_df.groupby(\"discourse_id\")[[\"Ineffective\", \"Adequate\", \"Effective\"]].agg(np.mean).reset_index()\n# exp17_df.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-08-23T12:59:26.358075Z","iopub.execute_input":"2022-08-23T12:59:26.358455Z","iopub.status.idle":"2022-08-23T12:59:26.411388Z","shell.execute_reply.started":"2022-08-23T12:59:26.358419Z","shell.execute_reply":"2022-08-23T12:59:26.410241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"exp17_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-23T12:59:26.412531Z","iopub.execute_input":"2022-08-23T12:59:26.413477Z","iopub.status.idle":"2022-08-23T12:59:26.423286Z","shell.execute_reply.started":"2022-08-23T12:59:26.413438Z","shell.execute_reply":"2022-08-23T12:59:26.422426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### LUKE - All data trained","metadata":{}},{"cell_type":"code","source":"## \ncheckpoints = [\n    \"../input/exp17f-luke-all-data-models/fpe_model_fold_0.pth.tar\",\n    \"../input/exp17f-luke-all-data-models/fpe_model_fold_1.pth.tar\",\n]\n\ndef inference_fn(model, infer_dl, model_id):\n    all_preds = []\n    all_uids = []\n    accelerator = Accelerator()\n    model, infer_dl = accelerator.prepare(model, infer_dl)\n    \n    model.eval()\n    tk0 = tqdm(infer_dl, total=len(infer_dl))\n    \n    for batch in tk0:\n        with torch.no_grad():\n            logits = model(**batch) # (b, nd, 3)\n            batch_preds = F.softmax(logits, dim=-1)\n            batch_uids = batch[\"uids\"]\n        all_preds.append(batch_preds)\n        all_uids.append(batch_uids)\n    \n    all_preds = [p.to('cpu').detach().numpy().tolist() for p in all_preds]\n    all_preds = list(chain(*all_preds))\n    flat_preds = list(chain(*all_preds))\n    \n    all_uids = [p.to('cpu').detach().numpy().tolist() for p in all_uids]\n    all_uids = list(chain(*all_uids))\n    flat_uids = list(chain(*all_uids))    \n    \n    preds_df = pd.DataFrame(flat_preds)\n    preds_df.columns = [\"Ineffective\", \"Adequate\", \"Effective\"]\n    preds_df[\"span_uid\"] = flat_uids # SORTED_DISCOURSE_IDS\n    preds_df = preds_df[preds_df[\"span_uid\"]>=0].copy()\n    preds_df[\"discourse_id\"] = preds_df[\"span_uid\"].map(idx2discourse)\n    preds_df = preds_df[[\"discourse_id\", \"Ineffective\", \"Adequate\", \"Effective\"]].copy()\n    preds_df.to_csv(f\"exp17f_luke_all_preds_{model_id}.csv\", index=False)\n    \n\nfor model_id, checkpoint in enumerate(checkpoints):\n    print(f\"infering from {checkpoint}\")\n    model = FeedbackModel(config)\n    ckpt = torch.load(checkpoint)\n    print(f\"validation score for fold {model_id} = {ckpt['loss']}\")\n    model.load_state_dict(ckpt['state_dict'])\n    inference_fn(model, infer_dl, model_id)\n    \n####\nimport glob\nimport pandas as pd\n\ncsvs = glob.glob(\"exp17f_luke_all_preds_*.csv\")\n\nidx = []\npreds = []\n\n\nfor csv_idx, csv in enumerate(csvs):\n    \n    print(\"==\"*40)\n    print(f\"preds in {csv}\")\n    df = pd.read_csv(csv)\n    df = df.sort_values(by=[\"discourse_id\"])\n    print(df.head(10))\n    print(\"==\"*40)\n    \n    temp_preds = df.drop([\"discourse_id\"], axis=1).values\n    if csv_idx == 0:\n        idx = list(df[\"discourse_id\"])\n        preds = temp_preds\n    else:\n        preds += temp_preds\n\npreds = preds / len(csvs)\n\nexp17f_df = pd.DataFrame()\nexp17f_df[\"discourse_id\"] = idx\nexp17f_df[\"Ineffective\"] = preds[:, 0]\nexp17f_df[\"Adequate\"] = preds[:, 1]\nexp17f_df[\"Effective\"] = preds[:, 2]\n\nexp17f_df = exp17f_df.groupby(\"discourse_id\")[[\"Ineffective\", \"Adequate\", \"Effective\"]].agg(np.mean).reset_index()\nexp17f_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-23T12:59:26.424861Z","iopub.execute_input":"2022-08-23T12:59:26.425474Z","iopub.status.idle":"2022-08-23T13:00:25.246482Z","shell.execute_reply.started":"2022-08-23T12:59:26.425438Z","shell.execute_reply":"2022-08-23T13:00:25.245417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del model\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2022-08-23T13:00:25.247907Z","iopub.execute_input":"2022-08-23T13:00:25.248301Z","iopub.status.idle":"2022-08-23T13:00:25.650034Z","shell.execute_reply.started":"2022-08-23T13:00:25.248266Z","shell.execute_reply":"2022-08-23T13:00:25.648646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ensemble","metadata":{"_uuid":"67b68093-7eb4-45b6-8ee8-3d4acc4f7514","_cell_guid":"7e29367d-337e-41c8-84cd-534949563ab0","trusted":true}},{"cell_type":"code","source":"if use_exp1:\n    exp01_df = exp01_df.sort_values(by=\"discourse_id\")\n\n    \nif use_exp3:\n    exp03_df = exp03_df.sort_values(by=\"discourse_id\")\n    \nif use_exp4:\n    exp04_df = exp04_df.sort_values(by=\"discourse_id\")\n    \nif use_exp6:\n    exp06_df = exp06_df.sort_values(by=\"discourse_id\")\n\nexp19_df = exp19_df.sort_values(by=\"discourse_id\")\n\nif use_exp8:\n    exp08_df = exp_08_df.sort_values(by=\"discourse_id\") # todo: fix naming as per convention\n    \nif use_exp10:\n    exp10_df = exp10_df.sort_values(by=\"discourse_id\") \n    \nif use_exp11:\n    exp11_df = exp11_df.sort_values(by=\"discourse_id\") \n\nif use_exp102:\n    exp102_df = exp102_df.sort_values(by=\"discourse_id\")\n\nexp16_df = exp16_df.sort_values(by=\"discourse_id\")\n\nexp213_df = exp213_df.sort_values(by=\"discourse_id\")\nexp213a_df = exp213a_df.sort_values(by=\"discourse_id\")\n\nif use_exp205:\n    exp205_df = exp205_df.sort_values(by=\"discourse_id\")\n\nexp209_df = exp209_df.sort_values(by=\"discourse_id\")\n\nif use_exp212:\n    exp212_df = exp212_df.sort_values(by=\"discourse_id\")\n\nif use_exp214:\n    exp214_df = exp214_df.sort_values(by=\"discourse_id\")\n\nexp17_df = exp17_df.sort_values(by=\"discourse_id\")\n\n\n# full data models\nexp17f_df = exp17f_df.sort_values(by=\"discourse_id\") # luke\nexp19f_df = exp19f_df.sort_values(by=\"discourse_id\")\nexp20f_df = exp20f_df.sort_values(by=\"discourse_id\")\n# exp21f_df = exp21f_df.sort_values(by=\"discourse_id\")\n\nexp99_rb_all_df = exp99_rb_all_df.sort_values(by=\"discourse_id\")\nexp209a_df = exp209a_df.sort_values(by=\"discourse_id\")\nexp213f_df = exp213f_df.sort_values(by=\"discourse_id\")","metadata":{"_uuid":"b5a4788b-a887-4a99-9c9a-7019403a365e","_cell_guid":"09512f64-7c11-4447-86fc-6cb121039e4d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T13:00:48.848313Z","iopub.execute_input":"2022-08-23T13:00:48.849116Z","iopub.status.idle":"2022-08-23T13:00:48.866003Z","shell.execute_reply.started":"2022-08-23T13:00:48.849078Z","shell.execute_reply":"2022-08-23T13:00:48.865165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"exp17f_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-23T13:00:50.240295Z","iopub.execute_input":"2022-08-23T13:00:50.241139Z","iopub.status.idle":"2022-08-23T13:00:50.254027Z","shell.execute_reply.started":"2022-08-23T13:00:50.2411Z","shell.execute_reply":"2022-08-23T13:00:50.252937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"exp20f_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-23T13:00:51.208294Z","iopub.execute_input":"2022-08-23T13:00:51.208772Z","iopub.status.idle":"2022-08-23T13:00:51.227323Z","shell.execute_reply.started":"2022-08-23T13:00:51.208729Z","shell.execute_reply":"2022-08-23T13:00:51.22641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if use_full_data_models:\n    print(exp99_rb_all_df.head())","metadata":{"execution":{"iopub.status.busy":"2022-08-23T13:01:21.522716Z","iopub.execute_input":"2022-08-23T13:01:21.523083Z","iopub.status.idle":"2022-08-23T13:01:21.532626Z","shell.execute_reply.started":"2022-08-23T13:01:21.523051Z","shell.execute_reply":"2022-08-23T13:01:21.531485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if use_full_data_models:\n    print(exp209a_df.head())","metadata":{"execution":{"iopub.status.busy":"2022-08-23T13:01:22.376912Z","iopub.execute_input":"2022-08-23T13:01:22.377325Z","iopub.status.idle":"2022-08-23T13:01:22.386349Z","shell.execute_reply.started":"2022-08-23T13:01:22.377289Z","shell.execute_reply":"2022-08-23T13:01:22.384254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if use_full_data_models:\n    print(exp213f_df.head())","metadata":{"execution":{"iopub.status.busy":"2022-08-23T13:01:23.03192Z","iopub.execute_input":"2022-08-23T13:01:23.03421Z","iopub.status.idle":"2022-08-23T13:01:23.043573Z","shell.execute_reply.started":"2022-08-23T13:01:23.034146Z","shell.execute_reply":"2022-08-23T13:01:23.042761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# delv3-mlm20\nif use_exp3:\n    print(exp03_df.head())","metadata":{"_uuid":"ece90856-7c64-4964-a2bd-7bdb8c2cc659","_cell_guid":"c2839844-7d6c-4260-8153-a5c8d1cb7ad7","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T13:01:23.640685Z","iopub.execute_input":"2022-08-23T13:01:23.641619Z","iopub.status.idle":"2022-08-23T13:01:23.646731Z","shell.execute_reply.started":"2022-08-23T13:01:23.641569Z","shell.execute_reply":"2022-08-23T13:01:23.645871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# delv3-mlm20-data-issue-resolved\nif use_exp4:\n    print(exp04_df.head())","metadata":{"_uuid":"ab293356-3978-49e8-a672-26aa246ae17d","_cell_guid":"46d53fc0-ff0a-4048-aac1-7dfc3cb9fdfe","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T13:01:24.241319Z","iopub.execute_input":"2022-08-23T13:01:24.241963Z","iopub.status.idle":"2022-08-23T13:01:24.246134Z","shell.execute_reply.started":"2022-08-23T13:01:24.241926Z","shell.execute_reply":"2022-08-23T13:01:24.245061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# delv3-mlm40-8folds\nif use_exp1:\n    print(exp01_df.head())","metadata":{"_uuid":"8361c1b1-6f28-4a03-b3d8-938614d43182","_cell_guid":"973b47dc-54f1-4130-befd-b2250cdb2bb1","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T13:01:25.111888Z","iopub.execute_input":"2022-08-23T13:01:25.112438Z","iopub.status.idle":"2022-08-23T13:01:25.120479Z","shell.execute_reply.started":"2022-08-23T13:01:25.112401Z","shell.execute_reply":"2022-08-23T13:01:25.119531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# delv3-mlm40-5folds-MSD\nif use_exp102:\n    print(exp102_df.head())","metadata":{"_uuid":"31aadd86-8e52-4e85-86d2-580d2015f15b","_cell_guid":"d771c0e6-785d-4bc2-92da-0dc517be48f9","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T13:01:25.984258Z","iopub.execute_input":"2022-08-23T13:01:25.984886Z","iopub.status.idle":"2022-08-23T13:01:25.988911Z","shell.execute_reply.started":"2022-08-23T13:01:25.984851Z","shell.execute_reply":"2022-08-23T13:01:25.987915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dexl-mlm40-4folds\nif use_exp6:\n    print(exp06_df.head())","metadata":{"_uuid":"bda296d2-6e47-44c5-b758-b9ba0cfbfc3c","_cell_guid":"511e4c08-dd15-43ca-a862-7a8041669a13","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T13:01:26.690241Z","iopub.execute_input":"2022-08-23T13:01:26.690894Z","iopub.status.idle":"2022-08-23T13:01:26.695491Z","shell.execute_reply.started":"2022-08-23T13:01:26.690854Z","shell.execute_reply":"2022-08-23T13:01:26.694415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dexl-mlm40-8folds\nexp19_df.head()","metadata":{"_uuid":"cf989cc1-9fd7-4676-851a-472a8577e86a","_cell_guid":"654598e5-1c65-4554-97ce-8fee8c688402","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T13:01:28.340917Z","iopub.execute_input":"2022-08-23T13:01:28.341593Z","iopub.status.idle":"2022-08-23T13:01:28.351893Z","shell.execute_reply.started":"2022-08-23T13:01:28.341555Z","shell.execute_reply":"2022-08-23T13:01:28.351061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# kd\nif use_exp8:\n    print(exp08_df.head())","metadata":{"_uuid":"ce1269d1-2a18-4582-bc1f-79453b968a30","_cell_guid":"96b6aa98-ffc9-4b71-a38c-844227b097b8","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T13:01:29.372435Z","iopub.execute_input":"2022-08-23T13:01:29.373093Z","iopub.status.idle":"2022-08-23T13:01:29.377212Z","shell.execute_reply.started":"2022-08-23T13:01:29.373053Z","shell.execute_reply":"2022-08-23T13:01:29.376116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# deb-l 8 fold mlm40 prompt+ spanfix + msd\nexp213_df.head()","metadata":{"_uuid":"2224db7e-b26b-42b8-9c29-ec031ec668a8","_cell_guid":"7a66875b-05bf-4757-a25d-9fef5bff2d04","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T13:01:29.824834Z","iopub.execute_input":"2022-08-23T13:01:29.825309Z","iopub.status.idle":"2022-08-23T13:01:29.837549Z","shell.execute_reply.started":"2022-08-23T13:01:29.825272Z","shell.execute_reply":"2022-08-23T13:01:29.836581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"exp213a_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-23T13:01:30.588016Z","iopub.execute_input":"2022-08-23T13:01:30.58869Z","iopub.status.idle":"2022-08-23T13:01:30.599086Z","shell.execute_reply.started":"2022-08-23T13:01:30.588652Z","shell.execute_reply":"2022-08-23T13:01:30.598222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"exp209_df.head()","metadata":{"_uuid":"747357b3-76a8-4257-bff2-ce9bbf7a1c3a","_cell_guid":"5703b67a-0992-468e-8b50-967d338cb1ed","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T13:01:31.223092Z","iopub.execute_input":"2022-08-23T13:01:31.223919Z","iopub.status.idle":"2022-08-23T13:01:31.236239Z","shell.execute_reply.started":"2022-08-23T13:01:31.223883Z","shell.execute_reply":"2022-08-23T13:01:31.235306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if use_exp10:\n    print(exp10_df.head())","metadata":{"_uuid":"bb6b4cd9-9f51-4670-9d8f-2b1a910dda02","_cell_guid":"574d0a5d-636b-4658-81e6-84f6e53bff72","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T13:01:31.799616Z","iopub.execute_input":"2022-08-23T13:01:31.80032Z","iopub.status.idle":"2022-08-23T13:01:31.804909Z","shell.execute_reply.started":"2022-08-23T13:01:31.800282Z","shell.execute_reply":"2022-08-23T13:01:31.804135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if use_exp11:\n    print(exp11_df.head())","metadata":{"_uuid":"0af2df73-5526-4002-8605-6e4458bc48b7","_cell_guid":"4a5c444e-8fb2-4593-9d18-2f1dd5e1dda6","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T13:01:32.352622Z","iopub.execute_input":"2022-08-23T13:01:32.353388Z","iopub.status.idle":"2022-08-23T13:01:32.359683Z","shell.execute_reply.started":"2022-08-23T13:01:32.35335Z","shell.execute_reply":"2022-08-23T13:01:32.358852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"exp16_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-23T13:01:32.972107Z","iopub.execute_input":"2022-08-23T13:01:32.973133Z","iopub.status.idle":"2022-08-23T13:01:32.985238Z","shell.execute_reply.started":"2022-08-23T13:01:32.973097Z","shell.execute_reply":"2022-08-23T13:01:32.98439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if use_exp214:\n    print(exp214_df.head())","metadata":{"execution":{"iopub.status.busy":"2022-08-23T13:01:33.709742Z","iopub.execute_input":"2022-08-23T13:01:33.710412Z","iopub.status.idle":"2022-08-23T13:01:33.714769Z","shell.execute_reply.started":"2022-08-23T13:01:33.710375Z","shell.execute_reply":"2022-08-23T13:01:33.71389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Meta Data Prep","metadata":{"_uuid":"60fee082-d344-484b-9fd4-ed93c1288158","_cell_guid":"7281281c-341f-426b-8626-32fb452fd464","trusted":true}},{"cell_type":"code","source":"# \"\"\"\n# exp7_oof_dexl_0.5728.csv 0.11\n# exp8_oof_del_kd_0.5720.csv 0.07\n# exp10_oof_delv3_0.5729.csv 0.11\n# exp16_oof_delv3_10fold.csv 0.1\n# exp209_oof_debv3_l_10fold_prompt.csv 0.2\n# exp212_oof_longformer_l_prompt_0.5833.csv 0.11\n# exp213_oof_deb_l_prompt_10folds.csv 0.24\n# exp214_oof_debv2-xl_prompt.csv 0.06\n# Score: 0.55788\n# Wt sum: 1.0\n\n# \"\"\"\n\n# MODEL_WEIGHTS =  [\n#      0.11, # exp7 - dexl\n#      0.07, # exp8 - kd\n#      0.11, # exp10 - debv3-l 8 fold\n#      0.10, # exp16 - debv3-l 10 fold\n#      0.20, # exp209 - debv3-l\n#      0.11, # exp212 - lf\n#      0.24, # exp213 - deb-l\n#      0.06  # exp214 - v2-xl\n#      ]\n\n\n# print(f\"sum of weights {np.sum(MODEL_WEIGHTS)}\")\n\n# submission_df = pd.DataFrame()\n\n# pred_dfs  = [  \n#     exp19_df,\n#     exp08_df,\n#     exp10_df,\n#     exp16_df,\n#     exp209_df,\n#     exp212_df,\n#     exp213_df,\n#     exp214_df,\n# ]\n\n\n# submission_df[\"discourse_id\"] =  pred_dfs[0][\"discourse_id\"].values\n\n# for model_idx, model_preds in enumerate(pred_dfs):\n#     if model_idx == 0:\n#         submission_df[\"Ineffective\"]  =  MODEL_WEIGHTS[model_idx] * model_preds[\"Ineffective\"] \n#         submission_df[\"Adequate\"]     =  MODEL_WEIGHTS[model_idx] * model_preds[\"Adequate\"] \n#         submission_df[\"Effective\"]    =  MODEL_WEIGHTS[model_idx] * model_preds[\"Effective\"]\n#     else:\n#         submission_df[\"Ineffective\"]  +=  MODEL_WEIGHTS[model_idx] * model_preds[\"Ineffective\"] \n#         submission_df[\"Adequate\"]     +=  MODEL_WEIGHTS[model_idx] * model_preds[\"Adequate\"] \n#         submission_df[\"Effective\"]    +=  MODEL_WEIGHTS[model_idx] * model_preds[\"Effective\"]","metadata":{"_uuid":"7bd94dca-6d91-47c2-9a46-273fb16b7208","_cell_guid":"17a78140-46bd-48c7-8173-fba3e1413b86","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T13:01:35.045466Z","iopub.execute_input":"2022-08-23T13:01:35.046068Z","iopub.status.idle":"2022-08-23T13:01:35.051376Z","shell.execute_reply.started":"2022-08-23T13:01:35.046032Z","shell.execute_reply":"2022-08-23T13:01:35.050247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# submission_df.head(10)","metadata":{"_uuid":"7b5df5ee-57e1-46be-a681-b88ae2a2b559","_cell_guid":"16e98add-a78f-4069-aca5-a805b454f48a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T13:01:35.586813Z","iopub.execute_input":"2022-08-23T13:01:35.58783Z","iopub.status.idle":"2022-08-23T13:01:35.591834Z","shell.execute_reply.started":"2022-08-23T13:01:35.587784Z","shell.execute_reply":"2022-08-23T13:01:35.590965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hc_df = submission_df","metadata":{"_uuid":"842bc799-520f-414a-b393-bc7a26d6be66","_cell_guid":"fb8a99e6-202e-4ccc-8b3b-afe975fb0c07","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T13:01:35.936723Z","iopub.execute_input":"2022-08-23T13:01:35.937711Z","iopub.status.idle":"2022-08-23T13:01:35.942767Z","shell.execute_reply.started":"2022-08-23T13:01:35.937662Z","shell.execute_reply":"2022-08-23T13:01:35.941908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" # LSTM","metadata":{}},{"cell_type":"code","source":"oof_dfs  = [\n        exp01_df,\n        exp19_df,\n        exp16_df,\n        exp17_df,\n        exp209_df,\n        exp212_df,\n        exp213_df,\n        exp213a_df,\n]","metadata":{"execution":{"iopub.status.busy":"2022-08-23T13:01:36.739239Z","iopub.execute_input":"2022-08-23T13:01:36.740116Z","iopub.status.idle":"2022-08-23T13:01:36.744653Z","shell.execute_reply.started":"2022-08-23T13:01:36.740079Z","shell.execute_reply":"2022-08-23T13:01:36.743718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_cols = [\"Ineffective\", \"Adequate\", \"Effective\"]\nfor model_idx in range(len(oof_dfs)):\n    col_map = dict()\n    for col in pred_cols:\n        col_map[col] = f\"model_{model_idx}_{col}\"\n    oof_dfs[model_idx] = oof_dfs[model_idx].rename(columns=col_map)","metadata":{"execution":{"iopub.status.busy":"2022-08-23T13:01:37.204629Z","iopub.execute_input":"2022-08-23T13:01:37.205Z","iopub.status.idle":"2022-08-23T13:01:37.212823Z","shell.execute_reply.started":"2022-08-23T13:01:37.204961Z","shell.execute_reply":"2022-08-23T13:01:37.21203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_df = oof_dfs[0]\n\nfor df in oof_dfs[1:]:\n    keep_cols = [\"discourse_id\"] + [col for col in df.columns if col.startswith(\"model\")]\n    df = df[keep_cols].copy()\n    merged_df = pd.merge(merged_df, df, on=\"discourse_id\", how='inner')\nassert merged_df.shape[0] == oof_dfs[0].shape[0]","metadata":{"execution":{"iopub.status.busy":"2022-08-23T13:01:38.784464Z","iopub.execute_input":"2022-08-23T13:01:38.785048Z","iopub.status.idle":"2022-08-23T13:01:38.817815Z","shell.execute_reply.started":"2022-08-23T13:01:38.78501Z","shell.execute_reply":"2022-08-23T13:01:38.816988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_df.head(3).T","metadata":{"execution":{"iopub.status.busy":"2022-08-23T13:01:39.323082Z","iopub.execute_input":"2022-08-23T13:01:39.323784Z","iopub.status.idle":"2022-08-23T13:01:39.338571Z","shell.execute_reply.started":"2022-08-23T13:01:39.323749Z","shell.execute_reply":"2022-08-23T13:01:39.337605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_names = [col for col in merged_df.columns if col.startswith(\"model\")]\nfeature_names[:6]","metadata":{"execution":{"iopub.status.busy":"2022-08-23T13:01:40.465245Z","iopub.execute_input":"2022-08-23T13:01:40.466221Z","iopub.status.idle":"2022-08-23T13:01:40.473272Z","shell.execute_reply.started":"2022-08-23T13:01:40.466157Z","shell.execute_reply":"2022-08-23T13:01:40.472363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_map = dict(zip(merged_df[\"discourse_id\"], merged_df[feature_names].values))","metadata":{"execution":{"iopub.status.busy":"2022-08-23T13:01:41.428951Z","iopub.execute_input":"2022-08-23T13:01:41.429637Z","iopub.status.idle":"2022-08-23T13:01:41.436175Z","shell.execute_reply.started":"2022-08-23T13:01:41.4296Z","shell.execute_reply":"2022-08-23T13:01:41.435294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport re\nfrom copy import deepcopy\nfrom itertools import chain\n\nimport pandas as pd\nfrom datasets import Dataset\nfrom tokenizers import AddedToken\nfrom transformers import AutoTokenizer\n\n#--------------- Tokenizer ---------------------------------------------#\ndef get_tokenizer(config):\n    \"\"\"load the tokenizer\"\"\"\n\n    print(\"using auto tokenizer\")\n    tokenizer = AutoTokenizer.from_pretrained(config[\"base_model_path\"])\n\n    print(\"==\"*40)\n    print(f\"tokenizer len: {len(tokenizer)}\")\n    print(\n        f\"tokenizer test: {tokenizer.tokenize('Starts: [SOE] [LEAD] [CLAIM] [POSITION] [COUNTER_CLAIM]')}\")\n    print(\n        f\"tokenizer test: {tokenizer.tokenize('Starts: [EOE] [LEAD_END] [POSITION_END] [CLAIM_END]')}\")\n\n    print(\"==\"*40)\n    return tokenizer\n\n\n#--------------- Processing ---------------------------------------------#\nUSE_NEW_MAP = True\n\nTOKEN_MAP = {\n    \"Lead\": [\"Lead [LEAD]\", \"[LEAD_END]\"],\n    \"Position\": [\"Position [POSITION]\", \"[POSITION_END]\"],\n    \"Claim\": [\"Claim [CLAIM]\", \"[CLAIM_END]\"],\n    \"Counterclaim\": [\"Counterclaim [COUNTER_CLAIM]\", \"[COUNTER_CLAIM_END]\"],\n    \"Rebuttal\": [\"Rebuttal [REBUTTAL]\", \"[REBUTTAL_END]\"],\n    \"Evidence\": [\"Evidence [EVIDENCE]\", \"[EVIDENCE_END]\"],\n    \"Concluding Statement\": [\"Concluding Statement [CONCLUDING_STATEMENT]\", \"[CONCLUDING_STATEMENT_END]\"]\n}\n\nDISCOURSE_START_TOKENS = [\n    \"[LEAD]\",\n    \"[POSITION]\",\n    \"[CLAIM]\",\n    \"[COUNTER_CLAIM]\",\n    \"[REBUTTAL]\",\n    \"[EVIDENCE]\",\n    \"[CONCLUDING_STATEMENT]\"\n]\n\nDISCOURSE_END_TOKENS = [\n    \"[LEAD_END]\",\n    \"[POSITION_END]\",\n    \"[CLAIM_END]\",\n    \"[COUNTER_CLAIM_END]\",\n    \"[REBUTTAL_END]\",\n    \"[EVIDENCE_END]\",\n    \"[CONCLUDING_STATEMENT_END]\"\n]\n\nif USE_NEW_MAP:\n    \n    TOKEN_MAP = {\n        \"topic\": [\"Topic [TOPIC]\", \"[TOPIC END]\"],\n        \"Lead\": [\"Lead [LEAD]\", \"[LEAD END]\"],\n        \"Position\": [\"Position [POSITION]\", \"[POSITION END]\"],\n        \"Claim\": [\"Claim [CLAIM]\", \"[CLAIM END]\"],\n        \"Counterclaim\": [\"Counterclaim [COUNTER_CLAIM]\", \"[COUNTER_CLAIM END]\"],\n        \"Rebuttal\": [\"Rebuttal [REBUTTAL]\", \"[REBUTTAL END]\"],\n        \"Evidence\": [\"Evidence [EVIDENCE]\", \"[EVIDENCE END]\"],\n        \"Concluding Statement\": [\"Concluding Statement [CONCLUDING_STATEMENT]\", \"[CONCLUDING_STATEMENT END]\"]\n    }\n\n    DISCOURSE_START_TOKENS = [\n        \"[LEAD]\",\n        \"[POSITION]\",\n        \"[CLAIM]\",\n        \"[COUNTER_CLAIM]\",\n        \"[REBUTTAL]\",\n        \"[EVIDENCE]\",\n        \"[CONCLUDING_STATEMENT]\"\n    ]\n\n    DISCOURSE_END_TOKENS = [\n        \"[LEAD END]\",\n        \"[POSITION END]\",\n        \"[CLAIM END]\",\n        \"[COUNTER_CLAIM END]\",\n        \"[REBUTTAL END]\",\n        \"[EVIDENCE END]\",\n        \"[CONCLUDING_STATEMENT END]\",\n    ]\n    \ndef relaxed_search(text, substring, min_length=2, fraction=0.99999):\n    \"\"\"\n    Returns substring's span from the given text with the certain precision.\n    \"\"\"\n\n    position = text.find(substring)\n    substring_length = len(substring)\n    if position == -1:\n        half_length = int(substring_length * fraction)\n        half_substring = substring[:half_length]\n        half_substring_length = len(half_substring)\n        if half_substring_length < min_length:\n            return [-1, 0]\n        else:\n            return relaxed_search(text=text,\n                                  substring=half_substring,\n                                  min_length=min_length,\n                                  fraction=fraction)\n\n    span = [position, position+substring_length]\n    return span\n\n\ndef build_span_map(discourse_list, essay_text):\n    reading_head = 0\n    to_return = dict()\n\n    for cur_discourse in discourse_list:\n        if cur_discourse not in to_return:\n            to_return[cur_discourse] = []\n\n        matches = re.finditer(re.escape(r'{}'.format(cur_discourse)), essay_text)\n        for match in matches:\n            span_start, span_end = match.span()\n            if span_end <= reading_head:\n                continue\n            to_return[cur_discourse].append(match.span())\n            reading_head = span_end\n            break\n\n    # post process\n    for cur_discourse in discourse_list:\n        if not to_return[cur_discourse]:\n            print(\"resorting to relaxed search...\")\n            to_return[cur_discourse] = [relaxed_search(essay_text, cur_discourse)]\n    return to_return\n\n\ndef get_substring_span(texts, mapping):\n    result = []\n    for text in texts:\n        ans = mapping[text].pop(0)\n        result.append(ans)\n    return result\n\n\ndef process_essay(essay_id, essay_text, anno_df):\n    \"\"\"insert newly added tokens in the essay text\n    \"\"\"\n    tmp_df = anno_df[anno_df[\"essay_id\"] == essay_id].copy()\n    tmp_df = tmp_df.sort_values(by=\"discourse_start\")\n    buffer = 0\n\n    for _, row in tmp_df.iterrows():\n        s, e, d_type = int(row.discourse_start) + buffer, int(row.discourse_end) + buffer, row.discourse_type\n        s_tok, e_tok = TOKEN_MAP[d_type]\n        essay_text = \" \".join([essay_text[:s], s_tok, essay_text[s:e], e_tok, essay_text[e:]])\n        buffer += len(s_tok) + len(e_tok) + 4\n\n    essay_text = \"[SOE]\" + essay_text + \"[EOE]\"\n    return essay_text\n\n\ndef process_input_df(anno_df, notes_df):\n    \"\"\"pre-process input dataframe\n\n    :param df: input dataframe\n    :type df: pd.DataFrame\n    :return: processed dataframe\n    :rtype: pd.DataFrame\n    \"\"\"\n    notes_df = deepcopy(notes_df)\n    anno_df = deepcopy(anno_df)\n\n    #------------------- Pre-Process Essay Text --------------------------#\n    anno_df[\"discourse_text\"] = anno_df[\"discourse_text\"].apply(lambda x: x.strip())  # pre-process\n    if \"discourse_effectiveness\" in anno_df.columns:\n        anno_df = anno_df[[\"discourse_id\", \"essay_id\", \"discourse_text\",\n                           \"discourse_type\", \"discourse_effectiveness\", \"uid\"]].copy()\n    else:\n        anno_df = anno_df[[\"discourse_id\", \"essay_id\", \"discourse_text\", \"discourse_type\", \"uid\"]].copy()\n\n    tmp_df = anno_df.groupby(\"essay_id\")[[\"discourse_id\", \"discourse_text\"]].agg(list).reset_index()\n    tmp_df = pd.merge(tmp_df, notes_df, on=\"essay_id\", how=\"left\")\n    tmp_df[\"span_map\"] = tmp_df[[\"discourse_text\", \"essay_text\"]].apply(\n        lambda x: build_span_map(x[0], x[1]), axis=1)\n    tmp_df[\"span\"] = tmp_df[[\"discourse_text\", \"span_map\"]].apply(\n        lambda x: get_substring_span(x[0], x[1]), axis=1)\n\n    all_discourse_ids = list(chain(*tmp_df[\"discourse_id\"].values))\n    all_discourse_spans = list(chain(*tmp_df[\"span\"].values))\n    span_df = pd.DataFrame()\n    span_df[\"discourse_id\"] = all_discourse_ids\n    span_df[\"span\"] = all_discourse_spans\n    span_df[\"discourse_start\"] = span_df[\"span\"].apply(lambda x: x[0])\n    span_df[\"discourse_end\"] = span_df[\"span\"].apply(lambda x: x[1])\n    span_df = span_df.drop(columns=\"span\")\n\n    anno_df = pd.merge(anno_df, span_df, on=\"discourse_id\", how=\"left\")\n    # anno_df = pd.merge(anno_df, notes_df, on=\"essay_id\", how=\"left\")\n\n    print(\"==\"*40)\n    print(\"processing essay text and inserting new tokens at span boundaries\")\n    notes_df[\"essay_text\"] = notes_df[[\"essay_id\", \"essay_text\"]].apply(\n        lambda x: process_essay(x[0], x[1], anno_df), axis=1\n    )\n    print(\"==\"*40)\n\n    anno_df = anno_df.drop(columns=[\"discourse_start\", \"discourse_end\"])\n    notes_df = notes_df.drop_duplicates(subset=[\"essay_id\"])[[\"essay_id\", \"essay_text\"]].copy()\n\n    anno_df = pd.merge(anno_df, notes_df, on=\"essay_id\", how=\"left\")\n\n    if \"discourse_effectiveness\" in anno_df.columns:\n        grouped_df = anno_df.groupby(\"essay_id\")[[\"uid\", \"discourse_id\", \"discourse_effectiveness\", \"discourse_type\"]].agg(list).reset_index()\n    else:\n        grouped_df = anno_df.groupby(\"essay_id\")[[\"uid\", \"discourse_id\", \"discourse_type\"]].agg(list).reset_index()\n\n    grouped_df = pd.merge(grouped_df, notes_df, on=\"essay_id\", how=\"left\")\n    grouped_df = grouped_df.rename(columns={\"uid\": \"uids\", \"discourse_id\": \"discourse_ids\"})\n\n    return grouped_df\n\n\n#--------------- Dataset ----------------------------------------------#\n\n\nclass FeedbackDatasetMeta:\n    \"\"\"Dataset class for feedback prize effectiveness task\n    \"\"\"\n\n    def __init__(self, config):\n        self.config = config\n\n        self.label2id = {\n            \"Ineffective\": 0,\n            \"Adequate\": 1,\n            \"Effective\": 2,\n        }\n\n        self.discourse_type2id = {\n            \"Lead\": 1,\n            \"Position\": 2,\n            \"Claim\": 3,\n            \"Counterclaim\": 4,\n            \"Rebuttal\": 5,\n            \"Evidence\": 6,\n            \"Concluding Statement\": 7,\n        }\n\n        self.id2label = {v: k for k, v in self.label2id.items()}\n        self.load_tokenizer()\n\n    def load_tokenizer(self):\n        \"\"\"load tokenizer as per config \n        \"\"\"\n        self.tokenizer = get_tokenizer(self.config)\n        print(\"==\"*40)\n        print(\"token maps...\")\n        print(TOKEN_MAP)\n        print(\"==\"*40)\n        print(f\"tokenizer len: {len(self.tokenizer)}\")\n\n        self.discourse_token_ids = set(self.tokenizer.convert_tokens_to_ids(DISCOURSE_START_TOKENS))\n        self.discourse_end_ids = set(self.tokenizer.convert_tokens_to_ids(DISCOURSE_END_TOKENS))\n        self.global_tokens = self.discourse_token_ids.union(self.discourse_end_ids)\n\n    def tokenize_function(self, examples):\n        tz = self.tokenizer(\n            examples[\"essay_text\"],\n            padding=False,\n            truncation=False,  # no truncation at first\n            add_special_tokens=True,\n            return_offsets_mapping=True,\n        )\n        return tz\n\n    def process_spans(self, examples):\n\n        span_head_char_start_idxs, span_tail_char_end_idxs = [], []\n        span_head_idxs, span_tail_idxs = [], []\n\n        for example_input_ids, example_offset_mapping, example_uids in zip(examples[\"input_ids\"], examples[\"offset_mapping\"], examples[\"uids\"]):\n            example_span_head_idxs = [pos for pos, this_id in enumerate(\n                example_input_ids) if this_id in self.discourse_token_ids]\n            example_span_tail_idxs = [pos for pos, this_id in enumerate(\n                example_input_ids) if this_id in self.discourse_end_ids]\n\n            example_span_head_char_start_idxs = [example_offset_mapping[pos][0] for pos in example_span_head_idxs]\n            example_span_tail_char_end_idxs = [example_offset_mapping[pos][1] for pos in example_span_tail_idxs]\n\n            span_head_char_start_idxs.append(example_span_head_char_start_idxs)\n            span_tail_char_end_idxs.append(example_span_tail_char_end_idxs)\n\n            span_head_idxs.append(example_span_head_idxs)\n            span_tail_idxs.append(example_span_tail_idxs)\n\n        return {\n            \"span_head_idxs\": span_head_idxs,\n            \"span_tail_idxs\": span_tail_idxs,\n            \"span_head_char_start_idxs\": span_head_char_start_idxs,\n            \"span_tail_char_end_idxs\": span_tail_char_end_idxs,\n        }\n\n    def generate_labels(self, examples):\n        labels = []\n        for example_labels, example_uids in zip(examples[\"discourse_effectiveness\"], examples[\"uids\"]):\n            labels.append([self.label2id[l] for l in example_labels])\n        return {\"labels\": labels}\n\n    def generate_meta_features(self, examples):\n        meta_features = []\n        for example_ids in examples[\"discourse_ids\"]:\n            current_features = []\n            for didx in example_ids:\n                current_features.append(self.feature_map[didx])\n            meta_features.append(current_features)\n        return {\"meta_features\": meta_features}\n\n    def generate_discourse_type_ids(self, examples):\n        discourse_type_ids = []\n        for example_discourse_types in examples[\"discourse_type\"]:\n            discourse_type_ids.append([self.discourse_type2id[dt] for dt in example_discourse_types])\n        return {\"discourse_type_ids\": discourse_type_ids}\n\n    def compute_input_length(self, examples):\n        return {\"input_length\": [len(x) for x in examples[\"input_ids\"]]}\n\n    def sanity_check_head_tail(self, examples):\n        for head_idxs, tail_idxs in zip(examples[\"span_head_idxs\"], examples[\"span_tail_idxs\"]):\n            assert len(head_idxs) == len(tail_idxs)\n            for head, tail in zip(head_idxs, tail_idxs):\n                assert tail > head + 1\n\n    def sanity_check_head_labels(self, examples):\n        for head_idxs, head_labels in zip(examples[\"span_head_idxs\"], examples[\"labels\"]):\n            assert len(head_idxs) == len(head_labels)\n\n    def get_dataset(self, df, essay_df, feature_map, mode='train'):\n        \"\"\"main api for creating the Feedback dataset\n\n        :param df: input annotation dataframe\n        :type df: pd.DataFrame\n        :param essay_df: dataframe with essay texts\n        :type essay_df: pd.DataFrame\n        :param mode: check if required for train or infer, defaults to 'train'\n        :type mode: str, optional\n        :return: the created dataset\n        :rtype: Dataset\n        \"\"\"\n        self.feature_map = feature_map\n        df = process_input_df(df, essay_df)\n\n        # save a sample for sanity checks\n        sample_df = df.sample(min(16, len(df)))\n        sample_df.to_csv(os.path.join(self.config[\"model_dir\"], f\"{mode}_df_processed.csv\"), index=False)\n\n        task_dataset = Dataset.from_pandas(df)\n        task_dataset = task_dataset.map(self.tokenize_function, batched=True)\n        task_dataset = task_dataset.map(self.compute_input_length, batched=True)\n        task_dataset = task_dataset.map(self.process_spans, batched=True)\n        task_dataset = task_dataset.map(self.generate_meta_features, batched=True)\n\n        print(task_dataset)\n        # todo check edge cases\n        task_dataset = task_dataset.filter(lambda example: len(example['span_head_idxs']) == len(\n            example['span_tail_idxs']))  # no need to run on empty set\n        print(task_dataset)\n        task_dataset = task_dataset.map(self.generate_discourse_type_ids, batched=True)\n        task_dataset = task_dataset.map(self.sanity_check_head_tail, batched=True)\n\n        if mode != \"infer\":\n            task_dataset = task_dataset.map(self.generate_labels, batched=True)\n            task_dataset = task_dataset.map(self.sanity_check_head_labels, batched=True)\n\n        try:\n            task_dataset = task_dataset.remove_columns(column_names=[\"__index_level_0__\"])\n        except Exception as e:\n            pass\n        return task_dataset\n\n@dataclass\nclass CustomDataCollatorWithPadding(DataCollatorWithPadding):\n    \"\"\"\n    data collector for seq classification\n    \"\"\"\n    tokenizer = None\n    padding = True\n    max_length = None\n    pad_to_multiple_of = 512\n    return_tensors = \"pt\"\n\n    def __call__(self, features):\n        uids = [feature[\"uids\"] for feature in features]\n        discourse_type_ids = [feature[\"discourse_type_ids\"] for feature in features]\n        span_head_idxs = [feature[\"span_head_idxs\"] for feature in features]\n        span_tail_idxs = [feature[\"span_tail_idxs\"] for feature in features]\n        meta_features = [feature[\"meta_features\"] for feature in features]\n\n        span_attention_mask = [[1]*len(feature[\"span_head_idxs\"]) for feature in features]\n\n        labels = None\n        if \"labels\" in features[0].keys():\n            labels = [feature[\"labels\"] for feature in features]\n\n        batch = self.tokenizer.pad(\n            features,\n            padding=self.padding,\n            max_length=self.max_length,\n            pad_to_multiple_of=self.pad_to_multiple_of,\n            return_tensors=None,\n        )\n\n        b_max = max([len(l) for l in span_head_idxs])\n        max_len = len(batch[\"input_ids\"][0])\n\n        default_head_idx = max(max_len - 10, 1)  # for padding\n        default_tail_idx = max(max_len - 4, 1)  # for padding\n\n        batch[\"span_head_idxs\"] = [\n            ex_span_head_idxs + [default_head_idx] * (b_max - len(ex_span_head_idxs)) for ex_span_head_idxs in span_head_idxs\n        ]\n\n        batch[\"uids\"] = [ex_uids + [-1] * (b_max - len(ex_uids)) for ex_uids in uids]\n        batch[\"discourse_type_ids\"] = [ex_discourse_type_ids + [0] *\n                                       (b_max - len(ex_discourse_type_ids)) for ex_discourse_type_ids in discourse_type_ids]\n\n        batch[\"span_tail_idxs\"] = [\n            ex_span_tail_idxs + [default_tail_idx] * (b_max - len(ex_span_tail_idxs)) for ex_span_tail_idxs in span_tail_idxs\n        ]\n        \n\n        padded_meta_features = []\n        for ex_features in meta_features:\n            pad_len = b_max - len(ex_features)\n            pad_vector = [0. for _ in range(len(ex_features[0]))]\n            for _ in range(pad_len):\n                ex_features.append(pad_vector)\n            padded_meta_features.append(ex_features)\n        # set_trace()\n\n        batch[\"meta_features\"] = padded_meta_features\n\n        batch[\"span_attention_mask\"] = [\n            ex_discourse_masks + [0] * (b_max - len(ex_discourse_masks)) for ex_discourse_masks in span_attention_mask\n        ]\n\n        if labels is not None:\n            batch[\"labels\"] = [ex_labels + [-1] * (b_max - len(ex_labels)) for ex_labels in labels]\n\n        batch = {k: (torch.tensor(v, dtype=torch.int64) if k != \"meta_features\" else torch.tensor(\n            v, dtype=torch.float32)) for k, v in batch.items()}        \n        return batch","metadata":{"execution":{"iopub.status.busy":"2022-08-23T13:01:43.200116Z","iopub.execute_input":"2022-08-23T13:01:43.200548Z","iopub.status.idle":"2022-08-23T13:01:43.264015Z","shell.execute_reply.started":"2022-08-23T13:01:43.200512Z","shell.execute_reply":"2022-08-23T13:01:43.263024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config = {\n    \"base_model_path\": \"../input/tk-fpe-models-v2/exp205-debv3-l-prompt/mlm_model\",\n    \"model_dir\": \"./\",\n    \"valid_bs\": 16,\n}","metadata":{"execution":{"iopub.status.busy":"2022-08-23T13:01:43.49788Z","iopub.execute_input":"2022-08-23T13:01:43.49827Z","iopub.status.idle":"2022-08-23T13:01:43.502741Z","shell.execute_reply.started":"2022-08-23T13:01:43.498235Z","shell.execute_reply":"2022-08-23T13:01:43.501678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_creator = FeedbackDatasetMeta(config)\ninfer_dataset = dataset_creator.get_dataset(test_df, essay_df, feature_map, mode=\"infer\")","metadata":{"execution":{"iopub.status.busy":"2022-08-23T13:01:43.79097Z","iopub.execute_input":"2022-08-23T13:01:43.791756Z","iopub.status.idle":"2022-08-23T13:01:44.491483Z","shell.execute_reply.started":"2022-08-23T13:01:43.791716Z","shell.execute_reply":"2022-08-23T13:01:44.490631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = dataset_creator.tokenizer\nconfig[\"len_tokenizer\"] = len(tokenizer)\ndata_collector = CustomDataCollatorWithPadding(tokenizer=tokenizer)\n\n# sort valid dataset for faster evaluation\ninfer_dataset = infer_dataset.sort(\"input_length\")\n\ninfer_dataset.set_format(\n    type=None,\n    columns=['input_ids', 'attention_mask', 'token_type_ids', 'span_head_idxs',\n                'span_tail_idxs', 'discourse_type_ids', \"meta_features\", 'uids']\n    )\n\ninfer_dl = DataLoader(\n    infer_dataset,\n    batch_size=config[\"valid_bs\"],\n    shuffle=False,\n    collate_fn=data_collector,\n    pin_memory=True,\n)","metadata":{"execution":{"iopub.status.busy":"2022-08-23T13:01:44.493054Z","iopub.execute_input":"2022-08-23T13:01:44.493965Z","iopub.status.idle":"2022-08-23T13:01:44.512027Z","shell.execute_reply.started":"2022-08-23T13:01:44.493928Z","shell.execute_reply":"2022-08-23T13:01:44.511229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FeedbackMetaModelResidual(nn.Module):\n    \"\"\"\n    The feedback prize effectiveness meta model for fast approach\n    \"\"\"\n\n    def __init__(self, config):\n        print(\"==\" * 40)\n        print(\"initializing the feedback model...\")\n\n        super(FeedbackMetaModelResidual, self).__init__()\n\n        self.config = config\n        self.num_labels = config[\"num_labels\"]\n        self.num_meta_features = config[\"num_features\"]\n        # self.layer_norm_raw = LayerNorm(config[\"num_features\"], 1e-7)\n\n        print(f'Num fts: {self.num_meta_features}')\n        # dropouts\n        self.dropout = nn.Dropout(self.config[\"dropout\"])\n        hidden_size = 512\n        self.projection = nn.Linear(self.num_meta_features, hidden_size)\n        self.layer_norm = LayerNorm(hidden_size, 1e-7)\n\n        self.meta_rnn = nn.LSTM(\n            input_size=hidden_size,\n            hidden_size=hidden_size // 2,\n            num_layers=1,\n            batch_first=True,\n            bidirectional=True,\n        )\n        self.classifier = nn.Linear(hidden_size, self.num_labels)\n\n    def forward(\n            self,\n            meta_features,\n            attention_mask,\n            span_attention_mask,\n            discourse_type_ids,\n            labels=None,\n            **kwargs\n    ):\n        # projection\n        meta_features = self.projection(meta_features)\n\n        # layer normalization\n        meta_features = self.layer_norm(meta_features)\n\n        # dropout\n        meta_features = self.dropout(meta_features)\n\n        # run through rnn\n        meta_features_rnn = self.meta_rnn(meta_features)[0]\n\n        # dropout\n        meta_features = meta_features + meta_features_rnn\n        meta_features = self.dropout(meta_features)\n        logits = self.classifier(meta_features)\n\n        return logits","metadata":{"execution":{"iopub.status.busy":"2022-08-23T13:01:44.513629Z","iopub.execute_input":"2022-08-23T13:01:44.514213Z","iopub.status.idle":"2022-08-23T13:01:44.524833Z","shell.execute_reply.started":"2022-08-23T13:01:44.514161Z","shell.execute_reply":"2022-08-23T13:01:44.523988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_config = {\n    \"num_labels\": 3,\n    \"num_features\": len(feature_names),\n    \"dropout\": 0.15,\n}\nmodel_config","metadata":{"execution":{"iopub.status.busy":"2022-08-23T13:01:44.526818Z","iopub.execute_input":"2022-08-23T13:01:44.527451Z","iopub.status.idle":"2022-08-23T13:01:44.541248Z","shell.execute_reply.started":"2022-08-23T13:01:44.527415Z","shell.execute_reply":"2022-08-23T13:01:44.540061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"meta_checkpoints = [\n    \"../input/ens57-lstm-meta-8m/fpe_model_fold_0_best.pth.tar\",\n    \"../input/ens57-lstm-meta-8m/fpe_model_fold_1_best.pth.tar\",\n    \"../input/ens57-lstm-meta-8m/fpe_model_fold_2_best.pth.tar\",\n    \"../input/ens57-lstm-meta-8m/fpe_model_fold_3_best.pth.tar\",\n    \"../input/ens57-lstm-meta-8m/fpe_model_fold_4_best.pth.tar\",\n    \"../input/ens57-lstm-meta-8m/fpe_model_fold_5_best.pth.tar\",\n    \"../input/ens57-lstm-meta-8m/fpe_model_fold_6_best.pth.tar\",\n    \"../input/ens57-lstm-meta-8m/fpe_model_fold_7_best.pth.tar\",\n]","metadata":{"execution":{"iopub.status.busy":"2022-08-23T13:01:44.650546Z","iopub.execute_input":"2022-08-23T13:01:44.650868Z","iopub.status.idle":"2022-08-23T13:01:44.655455Z","shell.execute_reply.started":"2022-08-23T13:01:44.65084Z","shell.execute_reply":"2022-08-23T13:01:44.654241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def inference_fn(model, infer_dl, model_id):\n    all_preds = []\n    all_uids = []\n    accelerator = Accelerator()\n    model, infer_dl = accelerator.prepare(model, infer_dl)\n    \n    model.eval()\n    tk0 = tqdm(infer_dl, total=len(infer_dl))\n    \n    for batch in tk0:\n        with torch.no_grad():\n            logits = model(**batch) # (b, nd, 3)\n            batch_preds = F.softmax(logits, dim=-1)\n            batch_uids = batch[\"uids\"]\n        all_preds.append(batch_preds)\n        all_uids.append(batch_uids)\n    \n    all_preds = [p.to('cpu').detach().numpy().tolist() for p in all_preds]\n    all_preds = list(chain(*all_preds))\n    flat_preds = list(chain(*all_preds))\n    \n    all_uids = [p.to('cpu').detach().numpy().tolist() for p in all_uids]\n    all_uids = list(chain(*all_uids))\n    flat_uids = list(chain(*all_uids))    \n    \n    preds_df = pd.DataFrame(flat_preds)\n    preds_df.columns = [\"Ineffective\", \"Adequate\", \"Effective\"]\n    preds_df[\"span_uid\"] = flat_uids # SORTED_DISCOURSE_IDS\n    preds_df = preds_df[preds_df[\"span_uid\"]>=0].copy()\n    preds_df[\"discourse_id\"] = preds_df[\"span_uid\"].map(idx2discourse)\n    preds_df = preds_df[[\"discourse_id\", \"Ineffective\", \"Adequate\", \"Effective\"]].copy()\n    preds_df.to_csv(f\"meta_model_{model_id}.csv\", index=False)\n    \n\nfor model_id, checkpoint in enumerate(meta_checkpoints):\n    print(f\"infering from {checkpoint}\")\n    model = FeedbackMetaModelResidual(model_config)\n    ckpt = torch.load(checkpoint)\n    print(f\"validation score for fold {model_id} = {ckpt['loss']}\")\n    model.load_state_dict(ckpt['state_dict'])\n    inference_fn(model, infer_dl, model_id)\n    \ndel model\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2022-08-23T13:01:44.825222Z","iopub.execute_input":"2022-08-23T13:01:44.826054Z","iopub.status.idle":"2022-08-23T13:01:46.659815Z","shell.execute_reply.started":"2022-08-23T13:01:44.826017Z","shell.execute_reply":"2022-08-23T13:01:46.658803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import glob\nimport pandas as pd\n\ncsvs = glob.glob(\"meta_model_*.csv\")\n\nidx = []\npreds = []\n\n\nfor csv_idx, csv in enumerate(csvs):\n    \n    print(\"==\"*40)\n    print(f\"preds in {csv}\")\n    df = pd.read_csv(csv)\n    df = df.sort_values(by=[\"discourse_id\"])\n    print(df.head(10))\n    print(\"==\"*40)\n    \n    temp_preds = df.drop([\"discourse_id\"], axis=1).values\n    if csv_idx == 0:\n        idx = list(df[\"discourse_id\"])\n        preds = temp_preds\n    else:\n        preds += temp_preds\n\npreds = preds / len(csvs)\n\nmeta_pred_df = pd.DataFrame()\nmeta_pred_df[\"discourse_id\"] = idx\nmeta_pred_df[\"Ineffective\"]  = preds[:, 0]\nmeta_pred_df[\"Adequate\"]     = preds[:, 1]\nmeta_pred_df[\"Effective\"]    = preds[:, 2]\n\nmeta_pred_df = meta_pred_df.groupby(\"discourse_id\")[[\"Ineffective\", \"Adequate\", \"Effective\"]].agg(np.mean).reset_index()","metadata":{"execution":{"iopub.status.busy":"2022-08-23T13:01:46.661998Z","iopub.execute_input":"2022-08-23T13:01:46.662822Z","iopub.status.idle":"2022-08-23T13:01:46.718614Z","shell.execute_reply.started":"2022-08-23T13:01:46.662783Z","shell.execute_reply":"2022-08-23T13:01:46.716253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"meta_pred_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-23T13:01:46.719777Z","iopub.execute_input":"2022-08-23T13:01:46.72025Z","iopub.status.idle":"2022-08-23T13:01:46.731523Z","shell.execute_reply.started":"2022-08-23T13:01:46.720216Z","shell.execute_reply":"2022-08-23T13:01:46.730725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df = meta_pred_df.copy() # pd.DataFrame()\nsubmission_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-08-23T13:01:46.733571Z","iopub.execute_input":"2022-08-23T13:01:46.734176Z","iopub.status.idle":"2022-08-23T13:01:46.747592Z","shell.execute_reply.started":"2022-08-23T13:01:46.734138Z","shell.execute_reply":"2022-08-23T13:01:46.746696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LGB","metadata":{}},{"cell_type":"code","source":"merged_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-23T13:01:46.749896Z","iopub.execute_input":"2022-08-23T13:01:46.750557Z","iopub.status.idle":"2022-08-23T13:01:46.777264Z","shell.execute_reply.started":"2022-08-23T13:01:46.750522Z","shell.execute_reply":"2022-08-23T13:01:46.776507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from textblob import TextBlob\n\nmeta_df = pd.merge(merged_df, test_df, on=\"discourse_id\", how=\"left\")\nmeta_df = pd.merge(meta_df, essay_df, on=\"essay_id\", how=\"left\")\n\ndef get_substring_span(text, substring, min_length=10, fraction=0.999):\n    \"\"\"\n    Returns substring's span from the given text with the certain precision.\n    \"\"\"\n\n    position = text.find(substring)\n    substring_length = len(substring)\n    if position == -1:\n        half_length = int(substring_length * fraction)\n        half_substring = substring[:half_length]\n        half_substring_length = len(half_substring)\n        if half_substring_length < min_length:\n            return [-1, 0]\n        else:\n            return get_substring_span(text=text,\n                                      substring=half_substring,\n                                      min_length=min_length,\n                                      fraction=fraction)\n\n    span = [position, position+substring_length]\n    return span\n\ndef tags(text):\n    blob = TextBlob(text)\n    return blob\n\ndef count_typ_tags(text, typ):\n    return len([word for (word,tag) in text.tags if tag.startswith(typ)])\n\ndef get_features(meta_df):\n    config = dict()\n    feature_names = [col for col in merged_df.columns if col.startswith(\"model\")]\n\n    config[\"features\"] = feature_names \n    config[\"cat_features\"] = []\n    \n    print('Processing spans')\n    meta_df[\"discourse_span\"] = meta_df[[\"essay_text\", \"discourse_text\"]].apply(lambda x: get_substring_span(x[0], x[1]), axis=1)\n    meta_df[\"discourse_start\"] = meta_df[\"discourse_span\"].apply(lambda x: x[0])\n    meta_df[\"discourse_end\"] = meta_df[\"discourse_span\"].apply(lambda x: x[1])\n\n    meta_df['discourse_len'] = meta_df['discourse_end'] - meta_df['discourse_start']\n    meta_df['freq_of_essay_id'] = meta_df['essay_id'].map(dict(meta_df['essay_id'].value_counts()))\n    meta_df['blob_discourse'] = meta_df['discourse_text'].apply(tags)\n    meta_df['discourse_Adjectives'] = meta_df['blob_discourse'].apply(lambda x: count_typ_tags(x, 'JJ'))\n    meta_df['discourse_Verbs'] = meta_df['blob_discourse'].apply(lambda x: count_typ_tags(x, 'VB'))\n    meta_df['discourse_Adverbs'] = meta_df['blob_discourse'].apply(lambda x: count_typ_tags(x, 'RB'))\n    meta_df['discourse_Nouns'] = meta_df['blob_discourse'].apply(lambda x: count_typ_tags(x, 'NN'))\n    meta_df['discourse_VBP'] = meta_df['blob_discourse'].apply(lambda x: count_typ_tags(x, 'VBP'))\n    meta_df['discourse_PRP'] = meta_df['blob_discourse'].apply(lambda x: count_typ_tags(x, 'PRP'))\n    meta_df['count_next_line_essay'] = meta_df['essay_text'].apply(lambda x: x.count(\"\\n\\n\"))\n    \n    discourse_type2id = {\n    \"Lead\": 1,\n    \"Position\": 2,\n    \"Claim\": 3,\n    \"Counterclaim\": 4,\n    \"Rebuttal\": 5,\n    \"Evidence\": 6,\n    \"Concluding Statement\": 7,\n}\n\n    new_col = []\n    for unique in ['Claim']:\n        meta_df['is_' + unique] = meta_df['discourse_type'].apply(lambda x: 1 if x == unique else 0)\n        new_col.append('is_'+unique)\n        \n    meta_df = meta_df.sort_values(by = ['essay_id','discourse_id']).reset_index(drop = True)\n    \n    essay_discourse_list = meta_df.groupby(['essay_id']).apply(\\\n                                lambda x: x['discourse_type'].tolist()).reset_index()\n    essay_discourse_list.rename(columns = {0:'discourse_type_list'}, inplace = True)\n    essay_discourse_list['discourse_type_list'] = essay_discourse_list['discourse_type_list'].apply(lambda x: \" \".join(x))\n    meta_df = meta_df.merge(essay_discourse_list[['essay_id','discourse_type_list']], \\\n                  on = 'essay_id',how = 'left')\n    meta_df[\"discourse_type\"] = meta_df[\"discourse_type\"].map(discourse_type2id)\n    meta_df['discourse_type_fe'] = meta_df['discourse_type'].map(dict(meta_df['discourse_type'].value_counts()))\n\n    essay_discourse = meta_df.groupby(['essay_id']).apply(lambda x: \\\n                                x['discourse_type'].nunique()).reset_index()\n    essay_discourse.rename(columns = {0:'unique_discourse_type'}, inplace = True)\n    essay_discourse.head()\n\n    meta_df = meta_df.merge(essay_discourse[['essay_id','unique_discourse_type']], on = 'essay_id',\\\n                  how = 'left')\n\n    config[\"features\"].extend([\"discourse_type\",\n                \"discourse_type_fe\",\"discourse_len\",\"freq_of_essay_id\",\\\n                          \"unique_discourse_type\"]+new_col)\n    config['features'].extend(['discourse_Adjectives','discourse_Verbs',\\\n                              'discourse_Adverbs','discourse_Nouns',\\\n                              'count_next_line_essay','discourse_VBP','discourse_PRP'])\n    config[\"cat_features\"].append(\"discourse_type\")\n    \n    return meta_df, config","metadata":{"execution":{"iopub.status.busy":"2022-08-23T13:01:46.778806Z","iopub.execute_input":"2022-08-23T13:01:46.779355Z","iopub.status.idle":"2022-08-23T13:01:46.807299Z","shell.execute_reply.started":"2022-08-23T13:01:46.779321Z","shell.execute_reply":"2022-08-23T13:01:46.806297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"meta_df, config = get_features(meta_df)\nmeta_df.shape, len(config['features'])","metadata":{"execution":{"iopub.status.busy":"2022-08-23T13:01:46.808644Z","iopub.execute_input":"2022-08-23T13:01:46.809636Z","iopub.status.idle":"2022-08-23T13:01:47.031822Z","shell.execute_reply.started":"2022-08-23T13:01:46.809552Z","shell.execute_reply":"2022-08-23T13:01:47.031022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import lightgbm as lgbm\nimport joblib\n\nmodel_paths = [\n    \"../input/meta-lgbm-model/lgbm_model_fold_0.txt\",\n    \"../input/meta-lgbm-model/lgbm_model_fold_1.txt\",\n    \"../input/meta-lgbm-model/lgbm_model_fold_2.txt\",\n    \"../input/meta-lgbm-model/lgbm_model_fold_3.txt\",\n    \"../input/meta-lgbm-model/lgbm_model_fold_4.txt\",\n    \"../input/meta-lgbm-model/lgbm_model_fold_5.txt\",\n    \"../input/meta-lgbm-model/lgbm_model_fold_6.txt\",\n    \"../input/meta-lgbm-model/lgbm_model_fold_7.txt\",\n]","metadata":{"execution":{"iopub.status.busy":"2022-08-23T13:01:47.033319Z","iopub.execute_input":"2022-08-23T13:01:47.033851Z","iopub.status.idle":"2022-08-23T13:01:47.416904Z","shell.execute_reply.started":"2022-08-23T13:01:47.033813Z","shell.execute_reply":"2022-08-23T13:01:47.415996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for midx, mp in enumerate(model_paths):\n    model = lgbm.Booster(model_file=mp)\n    if midx == 0:\n        preds = model.predict(meta_df[config[\"features\"]], num_iteration=model.best_iteration)\n    else:\n        preds += model.predict(meta_df[config[\"features\"]], num_iteration=model.best_iteration)\npreds = preds/len(model_paths)\npreds\nsubmission_df1 = pd.DataFrame()\n\nsubmission_df1[\"discourse_id\"] =  meta_df[\"discourse_id\"].values\nsubmission_df1[\"Ineffective\"]  =  preds[:, 0]\nsubmission_df1[\"Adequate\"]     =  preds[:, 1]\nsubmission_df1[\"Effective\"]    =  preds[:, 2]","metadata":{"execution":{"iopub.status.busy":"2022-08-23T13:01:47.418284Z","iopub.execute_input":"2022-08-23T13:01:47.418741Z","iopub.status.idle":"2022-08-23T13:01:48.343755Z","shell.execute_reply.started":"2022-08-23T13:01:47.418701Z","shell.execute_reply":"2022-08-23T13:01:48.342691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# All Data","metadata":{}},{"cell_type":"code","source":"df_list = [\n    exp17f_df,\n    exp19f_df,\n    exp20f_df,\n    exp99_rb_all_df,\n    exp209a_df,\n    exp213f_df,\n\n]\n\nMODEL_WEIGHTS = [\n    0.08, # [2] luke -rb\n    0.18, # [2] dexl -rb\n    0.04, # [1] del kd -rb\n    0.15, # [2] delv3 -rb\n    0.20, # [1] delv3 - tk\n    0.35, # [2] del - tk\n] \n\nprint(f\"sum of weights {np.sum(MODEL_WEIGHTS)}\")\n\nall_data_df = pd.DataFrame()\nall_data_df[\"discourse_id\"] =  df_list[0][\"discourse_id\"].values\n\nfor model_idx, model_preds in enumerate(df_list):\n    if model_idx == 0:\n        all_data_df[\"Ineffective\"]  =  MODEL_WEIGHTS[model_idx] * model_preds[\"Ineffective\"] \n        all_data_df[\"Adequate\"]     =  MODEL_WEIGHTS[model_idx] * model_preds[\"Adequate\"] \n        all_data_df[\"Effective\"]    =  MODEL_WEIGHTS[model_idx] * model_preds[\"Effective\"]\n    else:\n        all_data_df[\"Ineffective\"]  +=  MODEL_WEIGHTS[model_idx] * model_preds[\"Ineffective\"] \n        all_data_df[\"Adequate\"]     +=  MODEL_WEIGHTS[model_idx] * model_preds[\"Adequate\"] \n        all_data_df[\"Effective\"]    +=  MODEL_WEIGHTS[model_idx] * model_preds[\"Effective\"]","metadata":{"execution":{"iopub.status.busy":"2022-08-23T13:01:48.348096Z","iopub.execute_input":"2022-08-23T13:01:48.34857Z","iopub.status.idle":"2022-08-23T13:01:48.395638Z","shell.execute_reply.started":"2022-08-23T13:01:48.348529Z","shell.execute_reply":"2022-08-23T13:01:48.394659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_data_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-23T13:01:48.397526Z","iopub.execute_input":"2022-08-23T13:01:48.398179Z","iopub.status.idle":"2022-08-23T13:01:48.418433Z","shell.execute_reply.started":"2022-08-23T13:01:48.398137Z","shell.execute_reply":"2022-08-23T13:01:48.41691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Final Ensemble","metadata":{}},{"cell_type":"code","source":"lgb_df = submission_df1.sort_values(by=\"discourse_id\")\nlstm_df = submission_df.sort_values(by=\"discourse_id\")\nall_data_df = all_data_df.sort_values(by=\"discourse_id\") # TODO: check for flag","metadata":{"execution":{"iopub.status.busy":"2022-08-23T13:01:48.422626Z","iopub.execute_input":"2022-08-23T13:01:48.423236Z","iopub.status.idle":"2022-08-23T13:01:48.434273Z","shell.execute_reply.started":"2022-08-23T13:01:48.423176Z","shell.execute_reply":"2022-08-23T13:01:48.433357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgb_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-23T13:01:48.438253Z","iopub.execute_input":"2022-08-23T13:01:48.438582Z","iopub.status.idle":"2022-08-23T13:01:48.456725Z","shell.execute_reply.started":"2022-08-23T13:01:48.43855Z","shell.execute_reply":"2022-08-23T13:01:48.455838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lstm_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-23T13:01:48.460469Z","iopub.execute_input":"2022-08-23T13:01:48.462833Z","iopub.status.idle":"2022-08-23T13:01:48.479883Z","shell.execute_reply.started":"2022-08-23T13:01:48.462799Z","shell.execute_reply":"2022-08-23T13:01:48.479193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_data_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-23T13:01:48.585816Z","iopub.execute_input":"2022-08-23T13:01:48.586258Z","iopub.status.idle":"2022-08-23T13:01:48.604969Z","shell.execute_reply.started":"2022-08-23T13:01:48.586217Z","shell.execute_reply":"2022-08-23T13:01:48.604303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 0.536321*0.25 + 0.616211*0.35 + 0.499419*0.4\n# exp99_rb_all_df.head(1)\n# exp209a_df.head(1)\n# exp213f_df.head(1)","metadata":{"execution":{"iopub.status.busy":"2022-08-23T13:01:48.787491Z","iopub.execute_input":"2022-08-23T13:01:48.788285Z","iopub.status.idle":"2022-08-23T13:01:48.793903Z","shell.execute_reply.started":"2022-08-23T13:01:48.788242Z","shell.execute_reply":"2022-08-23T13:01:48.792869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df = pd.DataFrame()\nsub_df[\"discourse_id\"] =  lgb_df[\"discourse_id\"].values\n\nlgb_vals = lgb_df[[\"Ineffective\", \"Adequate\", \"Effective\"]].values\nlstm_vals = lstm_df[[\"Ineffective\", \"Adequate\", \"Effective\"]].values\nall_vals = all_data_df[[\"Ineffective\", \"Adequate\", \"Effective\"]].values\n\n\nsub_df[\"Ineffective\"]  =  0.60 * (0.7* lstm_vals[:, 0] + 0.3 * lgb_vals[:, 0]) + 0.40 * all_vals[:, 0]\nsub_df[\"Adequate\"]     =  0.60 * (0.7* lstm_vals[:, 1] + 0.3 * lgb_vals[:, 1]) + 0.40 * all_vals[:, 1]\nsub_df[\"Effective\"]    =  0.60 * (0.7* lstm_vals[:, 2] + 0.3 * lgb_vals[:, 2]) + 0.40 * all_vals[:, 2]","metadata":{"execution":{"iopub.status.busy":"2022-08-23T13:01:48.97001Z","iopub.execute_input":"2022-08-23T13:01:48.970384Z","iopub.status.idle":"2022-08-23T13:01:48.985054Z","shell.execute_reply.started":"2022-08-23T13:01:48.970351Z","shell.execute_reply":"2022-08-23T13:01:48.984236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df.to_csv(\"submission.csv\", index=False)\nsub_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-23T13:01:49.180054Z","iopub.execute_input":"2022-08-23T13:01:49.180994Z","iopub.status.idle":"2022-08-23T13:01:49.199886Z","shell.execute_reply.started":"2022-08-23T13:01:49.180949Z","shell.execute_reply":"2022-08-23T13:01:49.198711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df.to_csv(\"submission.csv\", index=False)\nsub_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-23T13:01:49.39693Z","iopub.execute_input":"2022-08-23T13:01:49.397308Z","iopub.status.idle":"2022-08-23T13:01:49.411781Z","shell.execute_reply.started":"2022-08-23T13:01:49.397275Z","shell.execute_reply":"2022-08-23T13:01:49.410813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}