{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nfrom transformers import AutoTokenizer, AutoConfig, AutoModel\nimport os\nfrom types import SimpleNamespace\nimport yaml\nimport multiprocessing as mp\nfrom glob import glob\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nimport collections\nimport lightgbm\n\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-08-24T14:25:55.707393Z","iopub.execute_input":"2022-08-24T14:25:55.708289Z","iopub.status.idle":"2022-08-24T14:26:01.537777Z","shell.execute_reply.started":"2022-08-24T14:25:55.708199Z","shell.execute_reply":"2022-08-24T14:26:01.536712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EXP_NAME = \"efficiency-prize-v2\"\n\nN_CORES = mp.cpu_count()\n\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    data_folder = \"test\"\n    df = pd.read_csv(\"../input/feedback-prize-effectiveness/test.csv\")\n    CALC_SCORE = False\nelse:\n    data_folder = \"train\"\n    df = pd.read_csv(\"../input/feedback-prize-effectiveness/train.csv\")\n    ids = df.essay_id.unique()\n    np.random.seed(1337)\n    val_ids = np.random.choice(ids, size=3000, replace=False)\n    df = df[df.essay_id.isin(val_ids)]\n    df = df.reset_index(drop=True)\n    CALC_SCORE = True","metadata":{"execution":{"iopub.status.busy":"2022-08-24T14:26:01.543096Z","iopub.execute_input":"2022-08-24T14:26:01.544045Z","iopub.status.idle":"2022-08-24T14:26:01.867488Z","shell.execute_reply.started":"2022-08-24T14:26:01.544004Z","shell.execute_reply":"2022-08-24T14:26:01.866525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def _read_data(essay_id):\n    fname = f\"../input/feedback-prize-effectiveness/{data_folder}/{essay_id}.txt\"\n    with open(fname) as f:\n        lines = f.read()\n        \n    return lines\n\nessay_ids = df.essay_id.unique()\n\npool_obj = mp.Pool(N_CORES)\nresults = pool_obj.map(_read_data, essay_ids)\n\nessay_texts = dict(zip(essay_ids, results))\ndf[\"essay_text\"] = df.essay_id.map(essay_texts)","metadata":{"execution":{"iopub.status.busy":"2022-08-24T14:26:01.868938Z","iopub.execute_input":"2022-08-24T14:26:01.869322Z","iopub.status.idle":"2022-08-24T14:26:10.922092Z","shell.execute_reply.started":"2022-08-24T14:26:01.869283Z","shell.execute_reply":"2022-08-24T14:26:10.920373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cfg = yaml.safe_load(open(f\"../input/{EXP_NAME}/cfg.yaml\").read())\nfor k, v in cfg.items():\n    if type(v) == dict:\n        cfg[k] = SimpleNamespace(**v)\ncfg = SimpleNamespace(**cfg)\n\ncfg.architecture.cache_dir = f\"../input/{EXP_NAME}/deberta-v3-large/\"","metadata":{"execution":{"iopub.status.busy":"2022-08-24T14:26:10.926704Z","iopub.execute_input":"2022-08-24T14:26:10.92706Z","iopub.status.idle":"2022-08-24T14:26:10.945977Z","shell.execute_reply.started":"2022-08-24T14:26:10.92702Z","shell.execute_reply":"2022-08-24T14:26:10.945109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(cfg.architecture.cache_dir)\n\ncfg._tokenizer_sep_token = tokenizer.sep_token\n\ncfg._tokenizer_start_token_id = []\ncfg._tokenizer_end_token_id = []\n\nd_types = sorted(df.discourse_type.unique())\n\nfor t in d_types:\n    tokenizer.add_tokens([f\"[START_{t}]\"], special_tokens=True)\n    cfg._tokenizer_start_token_id.append(tokenizer.encode(f\"[START_{t}]\")[1])\n    \nfor t in d_types:\n    tokenizer.add_tokens([f\"[END_{t}]\"], special_tokens=True)\n    cfg._tokenizer_end_token_id.append(tokenizer.encode(f\"[END_{t}]\")[1])\n\ntokenizer.add_tokens([f\"\\n\"], special_tokens=True)\ncfg._tokenizer_size = len(tokenizer)","metadata":{"execution":{"iopub.status.busy":"2022-08-24T14:26:10.948882Z","iopub.execute_input":"2022-08-24T14:26:10.949329Z","iopub.status.idle":"2022-08-24T14:26:12.329068Z","shell.execute_reply.started":"2022-08-24T14:26:10.949293Z","shell.execute_reply":"2022-08-24T14:26:12.328086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grps = df.groupby(\"essay_id\", sort=False)\ngrp_texts = []\n\nfor grp in grps.groups:\n    g = grps.get_group(grp)\n    t = g.essay_text.values[0]\n\n    end = 0\n    for j in range(len(g)):\n        d = g.discourse_text.values[j]\n        start = t[end:].find(d.strip())\n        start = start + end\n\n        end = start + len(d.strip())\n        t = (\n            t[:start]\n            + f\" [START_{g.discourse_type.values[j]}]  \"\n            + t[start:end]\n            + f\" [END_{g.discourse_type.values[j]}] \"\n            + t[end:]\n        )\n\n    t = \" \".join(g.discourse_type.values) + f\" {cfg._tokenizer_sep_token} \" + t\n    grp_texts.append(t)","metadata":{"execution":{"iopub.status.busy":"2022-08-24T14:26:12.331629Z","iopub.execute_input":"2022-08-24T14:26:12.332299Z","iopub.status.idle":"2022-08-24T14:26:14.089073Z","shell.execute_reply.started":"2022-08-24T14:26:12.332262Z","shell.execute_reply":"2022-08-24T14:26:14.088065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def encode(text):\n    sample = dict()\n    encodings = tokenizer(\n        text,\n        return_tensors=\"pt\",\n        padding=\"max_length\",\n        truncation=True,\n        max_length=cfg.tokenizer.max_length,\n    )\n    sample[\"input_ids\"] = encodings[\"input_ids\"][0]\n    sample[\"attention_mask\"] = encodings[\"attention_mask\"][0]\n    return sample","metadata":{"execution":{"iopub.status.busy":"2022-08-24T14:26:14.090538Z","iopub.execute_input":"2022-08-24T14:26:14.090894Z","iopub.status.idle":"2022-08-24T14:26:14.096446Z","shell.execute_reply.started":"2022-08-24T14:26:14.090858Z","shell.execute_reply":"2022-08-24T14:26:14.095511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pool_obj = mp.Pool(N_CORES)\ngrp_texts = pool_obj.map(encode, grp_texts)\n\nlens = [torch.sum(x[\"attention_mask\"]).item() for x in grp_texts]\n\nlens_map = df[[\"essay_id\"]].drop_duplicates()\nlens_map[\"count\"] = lens\nlens_map[\"orig_essay_order\"] = range(len(lens_map))\n\ndf[\"orig_order\"] = range(len(df))\ndf = df.merge(lens_map)\ndf = df.sort_values([\"count\", \"essay_id\", \"orig_order\"], ascending=True).reset_index(drop=True)\ngrp_texts = [grp_texts[i] for i in df[\"orig_essay_order\"].unique()]","metadata":{"execution":{"iopub.status.busy":"2022-08-24T14:26:14.098089Z","iopub.execute_input":"2022-08-24T14:26:14.098707Z","iopub.status.idle":"2022-08-24T14:26:24.238683Z","shell.execute_reply.started":"2022-08-24T14:26:14.098668Z","shell.execute_reply":"2022-08-24T14:26:24.237413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FeedbackDataset(Dataset):\n    def __init__(self, grp_texts):\n        self.grp_texts = grp_texts\n\n    def __len__(self):\n        return len(self.grp_texts)\n\n    def batch_to_device(batch, device):\n        if isinstance(batch, torch.Tensor):\n            return batch.to(device)\n        elif isinstance(batch, collections.abc.Mapping):\n            return {\n                key: FeedbackDataset.batch_to_device(value, device)\n                for key, value in batch.items()\n            }\n\n    def __getitem__(self, idx):\n        sample = self.grp_texts[idx]\n        if idx == 0:\n            print(sample)\n        return sample","metadata":{"execution":{"iopub.status.busy":"2022-08-24T14:26:24.240684Z","iopub.execute_input":"2022-08-24T14:26:24.24109Z","iopub.status.idle":"2022-08-24T14:26:24.251161Z","shell.execute_reply.started":"2022-08-24T14:26:24.241041Z","shell.execute_reply":"2022-08-24T14:26:24.249059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class NLPAllclsTokenPooling(nn.Module):\n    def __init__(self, dim):\n        super(NLPAllclsTokenPooling, self).__init__()\n\n        self.dim = dim\n        self.feat_mult = 3\n\n    def forward(self, x, attention_mask, input_ids, cfg):\n        ret = []\n        for j in range(x.shape[0]):\n            idx0 = torch.where(\n                (input_ids[j] >= min(cfg._tokenizer_start_token_id))\n                & (input_ids[j] <= max(cfg._tokenizer_start_token_id))\n            )[0]\n            idx1 = torch.where(\n                (input_ids[j] >= min(cfg._tokenizer_end_token_id))\n                & (input_ids[j] <= max(cfg._tokenizer_end_token_id))\n            )[0]\n\n            xx = []\n            for jj in range(len(idx0)):\n                xx0 = x[j, idx0[jj]]\n                xx1 = x[j, idx1[jj]]\n                xx2 = x[j, idx0[jj] + 1 : idx1[jj]].mean(dim=0)\n                xxx = torch.cat([xx0, xx1, xx2]).unsqueeze(0)\n                xx.append(xxx)\n            xx = torch.cat(xx)\n            ret.append(xx)\n\n        return ret\n\n\nclass FeedbackModel(nn.Module):\n    def __init__(self, cfg):\n        super(FeedbackModel, self).__init__()\n\n        self.cfg = cfg\n        self.n_classes = 3\n        config = AutoConfig.from_pretrained(cfg.architecture.cache_dir)\n        self.backbone = AutoModel.from_config(config)\n        self.backbone.pooler = None\n        self.backbone.resize_token_embeddings(cfg._tokenizer_size)\n\n        self.pooling = NLPAllclsTokenPooling(\n            dim=1\n        )  # init pooling and pool over token dimension\n        self.head = nn.Linear(\n            self.backbone.config.hidden_size * self.pooling.feat_mult, self.n_classes\n        )\n\n    def get_features(self, batch):\n        attention_mask = batch[\"attention_mask\"]\n        input_ids = batch[\"input_ids\"]\n\n        x = self.backbone(\n            input_ids=input_ids, attention_mask=attention_mask\n        ).last_hidden_state\n\n        x = self.pooling(x, attention_mask, input_ids, cfg=self.cfg)\n        x = torch.cat(x)\n\n        return x\n\n    def forward(self, batch):\n        idx = int(torch.where(batch[\"attention_mask\"] == 1)[1].max())\n        idx += 1\n        batch[\"attention_mask\"] = batch[\"attention_mask\"][:, :idx]\n        batch[\"input_ids\"] = batch[\"input_ids\"][:, :idx]\n\n        x = self.get_features(batch)\n        logits = self.head(x)\n\n        return {\"logits\": logits}","metadata":{"execution":{"iopub.status.busy":"2022-08-24T14:26:24.255966Z","iopub.execute_input":"2022-08-24T14:26:24.256245Z","iopub.status.idle":"2022-08-24T14:26:24.274566Z","shell.execute_reply.started":"2022-08-24T14:26:24.25622Z","shell.execute_reply":"2022-08-24T14:26:24.273447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_predictions(cfg, grp_texts, bs=1):\n    ds = FeedbackDataset(grp_texts)\n\n    model = FeedbackModel(cfg).to(\"cuda\").eval()\n    d = torch.load(f\"../input/{EXP_NAME}/checkpoint.pth\", map_location=\"cuda\")\n    model.load_state_dict(collections.OrderedDict(d[\"model\"]), strict=True)\n\n    dl = DataLoader(ds, shuffle=False, batch_size=bs, num_workers=N_CORES)\n\n    with torch.inference_mode():\n        preds = []\n        for batch in dl:\n            batch = FeedbackDataset.batch_to_device(batch, \"cuda\")\n\n            with torch.cuda.amp.autocast():\n                out = model(batch)\n                preds.append(\n                    out[\"logits\"].float().softmax(dim=1).detach().cpu().numpy()\n                )\n\n    return np.concatenate(preds, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-08-24T14:26:24.276359Z","iopub.execute_input":"2022-08-24T14:26:24.276769Z","iopub.status.idle":"2022-08-24T14:26:24.286349Z","shell.execute_reply.started":"2022-08-24T14:26:24.276732Z","shell.execute_reply":"2022-08-24T14:26:24.285185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pp = run_predictions(cfg, grp_texts=grp_texts, bs=16)","metadata":{"execution":{"iopub.status.busy":"2022-08-24T14:26:24.288157Z","iopub.execute_input":"2022-08-24T14:26:24.28855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"Adequate\"] = pp[:, 0] \ndf[\"Effective\"] = pp[:, 1] \ndf[\"Ineffective\"] = pp[:, 2] ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"orig_preds = pp.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_cols = [\"Adequate\", \"Effective\", \"Ineffective\"]\noof_cols = []\nfor j, l in enumerate(label_cols):\n\n    df[f\"oof_{l}\"] = pp[:,j]\n    oof_cols.append(f\"oof_{l}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FeedbackStackerModel(nn.Module):\n    def __init__(self, n_features):\n        super(FeedbackStackerModel, self).__init__()\n\n        self.sizes = [256, 128, 64]\n\n        self.features = nn.Sequential(\n            nn.utils.weight_norm(nn.Linear(n_features, self.sizes[0])),\n            nn.PReLU(),\n            nn.Linear(self.sizes[0], self.sizes[1]),\n            nn.PReLU(),\n            nn.Linear(self.sizes[1], self.sizes[2]),\n            nn.PReLU(),\n        )\n        self.head = nn.Linear(self.sizes[-1], 3)\n\n        self.loss_fn = nn.CrossEntropyLoss()\n\n    def forward(self, x, y):\n        x = self.features(x)\n        x = self.head(x)\n\n        return {\"logits\": x}\n\n\nclass FeedbackStackerDataset(Dataset):\n    def __init__(self, df, mode):\n        self.df = df.copy().reset_index(drop=True)\n        self.mode = mode\n\n        self.feature_cols = oof_cols.copy()\n        self.label_cols = label_cols.copy()\n\n        df = self.df\n\n        df[\"len\"] = df.groupby(\"essay_id\")[f\"discourse_id\"].transform(\"count\") / 10\n        self.feature_cols.append(\"len\")\n\n        for j, l in enumerate(label_cols):\n            df[f\"oof_{l}_mean\"] = df.groupby(\"essay_id\")[f\"oof_{l}\"].transform(\"mean\")\n            self.feature_cols.append(f\"oof_{l}_mean\")\n\n            df[f\"oof_{l}_t_mean\"] = df.groupby([\"essay_id\", \"discourse_type\"])[\n                f\"oof_{l}\"\n            ].transform(\"mean\")\n            self.feature_cols.append(f\"oof_{l}_t_mean\")\n\n        self.num_features = len(self.feature_cols)\n\n        self.X = self.df[self.feature_cols].values\n        self.y = self.df[self.label_cols].values\n\n    def __getitem__(self, idx):\n        X = self.X[idx]\n        y = self.y[idx]\n\n        return torch.FloatTensor(X), torch.FloatTensor(y)\n\n    def __len__(self):\n        return self.df.shape[0]\n\n\nds = FeedbackStackerDataset(df.copy(), mode=\"val\")\n\n\ndef run_nn_stacker(exp_name, df, BS=64):\n    ds = FeedbackStackerDataset(df.iloc[:].copy(), mode=\"test\")\n\n    checkpoints = glob(f\"{exp_name}/*.pth\")\n\n    models = []\n    for checkpoint in checkpoints:\n        print(f\"running model {checkpoint}\")\n\n        model = FeedbackStackerModel(n_features=ds.num_features).to(\"cuda\").eval()\n        model_weights = torch.load(checkpoint, map_location=\"cuda\")\n\n        model.load_state_dict(collections.OrderedDict(model_weights), strict=True)\n        models.append(model)\n\n    dl = DataLoader(ds, shuffle=False, batch_size=BS, num_workers=N_CORES)\n\n    with torch.no_grad():\n        preds = []\n        for batch in dl:\n            data = [x.to(\"cuda\") for x in batch]\n            inputs, target = data\n            p = []\n            for model in models:\n                out = model(inputs, target)\n                p.append(out[\"logits\"].float().softmax(dim=1))\n            preds.append(torch.mean(torch.stack(p), dim=0).detach().cpu().numpy())\n\n    preds = np.concatenate(preds, axis=0)\n\n    return preds\n\n\nnn_stacker_preds = run_nn_stacker(\"../input/efficiency-prize-v2/nn\", df, BS=256)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def gen_x(values):\n    return np.histogram(\n        np.clip(values, 0.001, 0.999), bins=3, density=True, range=(0, 1)\n    )[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_groups = []\n\ngb = df.groupby(\"essay_id\", sort=False)\n\ndf[\"n_types\"] = gb[\"discourse_type\"].transform(lambda x: x.nunique())\ndf[\"mean_Ineffective\"] = gb[\"Ineffective\"].transform(\"mean\")\n\nfor name, group in gb:\n    class_name = \"Ineffective\"\n    for idx, val in enumerate(gen_x(group[class_name].values)):\n        group[f\"{class_name}_bin_{idx}\"] = val\n\n    all_groups.append(group)\n\ndf = pd.concat(all_groups).reset_index(drop=True)\n\ndisc_types_mapping = {\n    \"Lead\": 0,\n    \"Position\": 1,\n    \"Claim\": 2,\n    \"Evidence\": 3,\n    \"Counterclaim\": 4,\n    \"Rebuttal\": 5,\n    \"Concluding Statement\": 6,\n}\n\ndf[\"len_disc\"] = df.discourse_text.str.len()\ndf[\"discourse_type\"] = df[\"discourse_type\"].map(disc_types_mapping)\ndf[\"paragraph_cnt\"] = df.essay_text.map(lambda x: len(x.split(\"\\n\\n\")))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgb_stacker_preds = []\n\nfor fold in range(5):\n    gbm = lightgbm.Booster(model_file=f\"../input/efficiency-prize-v2/lightgbm/model_fold_{fold}.txt\")\n    valid_pred = gbm.predict(\n        df[\n            [\n                \"discourse_type\",\n                \"Adequate\",\n                \"Effective\",\n                \"Ineffective\",\n                \"n_types\",\n                \"Ineffective_bin_0\",\n                \"Ineffective_bin_1\",\n                \"Ineffective_bin_2\",\n                \"mean_Ineffective\",\n                \"len_disc\",\n                \"paragraph_cnt\",\n            ]\n        ]\n    )\n    lgb_stacker_preds.append(valid_pred)\n    \nlgb_stacker_preds = np.array(lgb_stacker_preds).mean(axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_preds = [\n    orig_preds,\n    lgb_stacker_preds,\n    nn_stacker_preds,\n]\n\nall_preds = np.average(all_preds, axis=0, weights=[2, 1, 1 ])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"Adequate\"] = all_preds[:, 0]\ndf[\"Effective\"] = all_preds[:, 1]\ndf[\"Ineffective\"] = all_preds[:, 2]\n\ndf[[\"discourse_id\", \"Ineffective\", \"Adequate\", \"Effective\"]].to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if CALC_SCORE:\n    from sklearn.metrics import log_loss\n    \n    label_cols = [\"Adequate\", \"Effective\", \"Ineffective\"]\n    \n    y = np.zeros_like(all_preds)\n    \n    for ii, jj in enumerate([label_cols.index(x) for x in df[\"discourse_effectiveness\"].values]):\n        y[ii,jj] = 1\n        \n    print(log_loss(y, df[label_cols]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}