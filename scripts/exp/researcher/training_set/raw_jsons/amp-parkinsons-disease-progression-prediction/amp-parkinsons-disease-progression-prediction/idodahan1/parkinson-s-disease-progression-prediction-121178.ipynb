{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import MinMaxScaler\nfrom functools import reduce\nfrom tqdm import tqdm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\ndef load_data(base_path = \"data\"):\n    proteins = pd.read_csv(f\"{base_path}/train_proteins.csv\")\n    peptides = pd.read_csv(f\"{base_path}/train_peptides.csv\")\n    clinical = pd.read_csv(f\"{base_path}/train_clinical_data.csv\")\n    supplement = pd.read_csv(f\"{base_path}/supplemental_clinical_data.csv\")\n    return proteins, peptides, clinical, supplement\n\nproteins, peptides, clinical, supplement = load_data(\"../input/amp-parkinsons-disease-progression-prediction\")\nsupplement.loc[supplement[\"visit_month\"] == 5, \"visit_month\"] = 6\n","metadata":{"execution":{"iopub.status.busy":"2023-07-16T21:12:18.846786Z","iopub.execute_input":"2023-07-16T21:12:18.847271Z","iopub.status.idle":"2023-07-16T21:12:21.452399Z","shell.execute_reply.started":"2023-07-16T21:12:18.847216Z","shell.execute_reply":"2023-07-16T21:12:21.451313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Protein Utilization","metadata":{}},{"cell_type":"code","source":"print(proteins)\nprint(f'Number of patients : {len(proteins[\"patient_id\"].unique())}')\nprint(f'Number of protiens : {len(proteins[\"UniProt\"].unique())}')\nagg_p = proteins.groupby(by = 'UniProt').agg({'patient_id' : 'nunique'})\nprint(f'Patient sharing the same protien expression, Mean : {agg_p.mean().round(2).values} , Std : {agg_p.std().round(2).values}')\n\nprint(f'Least occuring protein apprance rate across patiens : {round((agg_p.min().values[0] /248)* 100)}%')","metadata":{"execution":{"iopub.status.busy":"2023-07-16T21:12:21.454416Z","iopub.execute_input":"2023-07-16T21:12:21.45512Z","iopub.status.idle":"2023-07-16T21:12:21.54016Z","shell.execute_reply.started":"2023-07-16T21:12:21.455086Z","shell.execute_reply":"2023-07-16T21:12:21.538901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The least occuring protien appears in 75% of the patiens, we can't disregard this.\nLet's try to utilize all the protiens , although some of them are missing","metadata":{}},{"cell_type":"code","source":"t = proteins.groupby(['visit_id', 'UniProt']).agg({'NPX': 'mean'}).reset_index()\npivoted_df = t.pivot(index='visit_id', columns='UniProt', values='NPX')\nnan_columns = pivoted_df.columns[pivoted_df.isnull().any()].tolist()\n\npivoted_df = pivoted_df.drop(nan_columns,axis=1)\npivoted_df","metadata":{"execution":{"iopub.status.busy":"2023-07-16T21:12:21.541609Z","iopub.execute_input":"2023-07-16T21:12:21.541951Z","iopub.status.idle":"2023-07-16T21:12:21.85476Z","shell.execute_reply.started":"2023-07-16T21:12:21.541922Z","shell.execute_reply":"2023-07-16T21:12:21.853545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class proteinPrep():\n    def __init__(self, scaler = MinMaxScaler):\n        self.scaler = scaler\n        self.cols = None\n        self.scalers = None\n        \n        \n    def fit(self, df ,train = False,impute_zero = False):\n        if not train and (self.cols == None):\n            raise Exception('Must run train before fitting')\n        \n        if train:\n            #column list containing statistics for optional imputation\n            self.cols = {col : [df[col].mean() , df[col].std()] for col in df.columns}\n            self.scalers =  {col : self.scaler().fit(np.array(df[col]).reshape(-1,1)) for col in df.columns}\n        \n        fitted_df = pd.DataFrame(columns = list(self.cols.keys()))\n        out_df = df.copy()\n        for col in self.cols:\n            imputed_series = self.impute_col(col,out_df ,impute_zero)\n            #if imputed_series.isna().sum() >0:\n                #print(imputed_series)\n                \n            fitted_df[col] = self.scalers[col].transform(imputed_series.to_numpy().reshape(-1, 1)).flatten()\n            #print(f'After Imputation :  {fitted_df[col]}')\n        fitted_df.reindex()\n        return fitted_df\n            \n            \n    def impute_col(self, col_name, df ,impute_zero = False ):\n        \"\"\"\n        test_zero is a test case\n        \"\"\"\n        if col_name not in self.cols:\n            raise Exception(f'Column {col_name} did not appear in fitted set')\n        \n        if col_name not in df:\n            mean,std = self.cols[col_name]\n            imputed_values = np.random.normal(mean, std, df.shape[0])\n            #print(f'Imputed value  : {imputed_values} for col {col_name}')\n            return pd.Series(imputed_values , name = col_name)\n        else:\n            series = df[col_name]\n            nan_indices = series.isnull()\n            num_nans = len(nan_indices)\n            # If there are nulls in the column, imputes them\n            if num_nans > 0:\n                if impute_zero: # imputation of 0 - test case\n                    series[nan_indices] = 0\n                else:    #Impute them by sampling from the protien distribution\n                    mean,std = self.cols[col_name]\n                    imputed_values = np.random.normal(mean, std, num_nans)\n                    series[nan_indices] = imputed_values\n            return series\n            \n","metadata":{"execution":{"iopub.status.busy":"2023-07-16T21:12:21.858256Z","iopub.execute_input":"2023-07-16T21:12:21.858783Z","iopub.status.idle":"2023-07-16T21:12:21.879832Z","shell.execute_reply.started":"2023-07-16T21:12:21.858728Z","shell.execute_reply":"2023-07-16T21:12:21.878421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\n\n# Define the Autoencoder class\nclass Autoencoder(nn.Module):\n    def __init__(self, input_dim, hidden_dim, hidden_layers):\n        super(Autoencoder, self).__init__()\n        \n        dimensions = np.int16(np.linspace(input_dim , hidden_dim , hidden_layers+2))\n        # Encoder layers\n        encoder_layers = []\n        \n        prev_dim = input_dim\n        for cur_dim in dimensions[1:]:\n            #print(f'input : {prev_dim} , output :{cur_dim}')\n            encoder_layers.append(nn.Linear(prev_dim, cur_dim))\n            if cur_dim == hidden_dim:\n                encoder_layers.append(nn.LeakyReLU()) \n            else:\n                encoder_layers.append(nn.ReLU())\n            prev_dim = cur_dim\n\n        self.encoder = nn.Sequential(*encoder_layers)\n\n        # Decoder layers\n        decoder_layers = []\n        for cur_dim in dimensions[::-1][1:]:\n            #print(f'input : {prev_dim} , output :{cur_dim}')\n            decoder_layers.append(nn.Linear(prev_dim, cur_dim))\n            decoder_layers.append(nn.ReLU())\n            prev_dim = cur_dim\n\n        self.decoder = nn.Sequential(*decoder_layers)\n    def forward(self, x):\n        encoded = self.encoder(x)\n        decoded = self.decoder(encoded)\n        return decoded\n    \n    def encode(self, x):\n        return self.encoder(x)\n    \n    \ndef train_routine(X_train , X_test , **kwargs):\n    # Set the device\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # Define the hyperparameters , interchangably\n    input_dim =         kwargs.get('input_dim' ,X_train.shape[1])  # Dimensionality of input data\n    additional_layers = kwargs.get('additional_layers' ,1) #amount of layers between input/output and encoding layers\n    hidden_dim =        kwargs.get('hidden_dim' ,30)  # Dimensionality of hidden layer\n    learning_rate =     kwargs.get('learning_rate' ,0.001)  \n    num_epochs =        kwargs.get('num_epochs' ,50)   \n    batch_size =        kwargs.get('batch_size' ,32)   \n\n    # Create an instance of the Autoencoder model\n    model = Autoencoder(input_dim, hidden_dim, hidden_layers = additional_layers).to(device)\n\n    # Define the loss function\n    criterion = nn.MSELoss()\n\n    # Define the optimizer\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n    # Create Train and Test datasets and loaders\n    train_set = torch.tensor(X_train.values, dtype = torch.float32 ).to(device)\n    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n    \n    if X_test is not None:\n        test_set = torch.tensor(X_test.values, dtype = torch.float32).to(device)\n        test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=True)\n    \n    train_loss_arr = []\n    test_loss_arr = []\n    # Training loop\n    for epoch in range(num_epochs):\n        train_loss = 0.0\n        model.train()  # Set the model in training mode\n\n        # Training loop\n        for batch in train_loader:\n            \n            optimizer.zero_grad()\n            output = model(batch)\n            loss = criterion(output, batch)\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item() * batch.size(0)\n\n        # Compute the average training loss for the epoch\n        train_loss /= len(train_set)\n        if X_test is not None:\n            model.eval()  # Set the model in evaluation mode\n            test_loss = 0.0\n            if epoch % 10 == 0:\n                # Evaluation loop on the test set\n                with torch.no_grad():\n                    for batch in test_loader:\n                        output = model(batch)\n                        loss = criterion(output, batch)\n                        test_loss += loss.item() * batch.size(0)\n\n                # Compute the average test loss for the epoch\n                test_loss /= len(test_set)\n\n                # Print the average training and test loss for the epoch\n                print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}\")\n\n                # Append losses\n                test_loss_arr.append(test_loss)\n                train_loss_arr.append(train_loss)\n\n    # Save the trained model\n    torch.save(model.state_dict(), \"autoencoder.pth\")\n    return (model ,train_loss_arr, test_loss_arr)","metadata":{"execution":{"iopub.status.busy":"2023-07-16T21:12:21.882187Z","iopub.execute_input":"2023-07-16T21:12:21.882732Z","iopub.status.idle":"2023-07-16T21:12:24.478686Z","shell.execute_reply.started":"2023-07-16T21:12:21.882676Z","shell.execute_reply":"2023-07-16T21:12:24.47754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class run_cv():\n#   def __init__(self ,X , model_wrapper ,pp , num_folds = 5):\n\n#     self.X = X\n#     self.cv = KFold(n_splits=num_folds, random_state=2033, shuffle=True)\n#     self.model_wrapper = model_wrapper\n#     self.pp = pp\n\n#   def run_cv(self , cv_param_dict = None):\n#     def get_dictionary_union(d1, d2):\n#         result = d1.copy()\n#         result.update(d2)\n#         return result\n\n\n#     scores_lst_per_configuration = []\n\n\n#     if cv_param_dict is not None:\n#       #Generate Combinations for the parameters\n#       generate_comb = lambda d_list , d2 : [get_dictionary_union(d1 ,{key : val}) for key,values in d2.items() for val in values for d1 in d_list]\n#       param_combs = reduce(generate_comb,[[{}]] +[{key : values} for key,values in cv_param_dict.items()])\n\n#     else:\n#       param_combs = [{}]\n\n#     for i, (train_index, test_index) in enumerate(self.cv.split(self.X)):\n\n#       print(f\"Fold {i}:\")\n#       pp  = self.pp()\n\n#       #preprocess train\n#       X_train = pp.fit(self.X.iloc[train_index], True)\n\n#       #preprocess test\n#       X_test = pp.fit(self.X.iloc[test_index], False)\n\n#       for param_comb in tqdm(param_combs):\n\n#         # Initialize new model with some test configuration\n#         _, train_loss, test_loss = self.model_wrapper(X_train ,X_test, **param_comb)\n#         # Update results retrived from the model to be the last epoch scores\n#         score_and_fold ={'fold' : i ,\n#                          'train_loss' : train_loss[-1] ,\n#                          'test_loss' : test_loss[-1] }\n        \n#         #Update score dict with parameters, fold and score\n#         scores_lst_per_configuration.append(get_dictionary_union(param_comb ,score_and_fold))\n\n#     return pd.DataFrame(scores_lst_per_configuration)\n\n\n# cv_param_dict = {'num_epochs' : [500,1000],\n#               'additional_layers' : [2,3],\n#                   'learning_rate' : [0.001,0.0001],\n#                'batch_size' : [32,64]}\n\n# dropped_df = pivoted_df.dropna(axis = 1).reset_index().drop(columns = 'visit_id')\n# dropped_df\n# cv_wrapper = run_cv(dropped_df ,train_routine , proteinPrep)\n\n# scores = cv_wrapper.run_cv(cv_param_dict)\n# scores.write_csv('encoder_scores.csv')","metadata":{"execution":{"iopub.status.busy":"2023-07-16T21:12:24.480111Z","iopub.execute_input":"2023-07-16T21:12:24.48073Z","iopub.status.idle":"2023-07-16T21:12:24.490809Z","shell.execute_reply.started":"2023-07-16T21:12:24.480694Z","shell.execute_reply":"2023-07-16T21:12:24.48923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# scores= pd.read_csv('/kaggle/input/encoder-Scores/encoder_scores.csv', index_col = 0)\n# col_lst = ['num_epochs' ,'additional_layers','learning_rate','batch_size']\n# t_df = scores.copy()\n\n# #Convert all parameters columns to string for reducing hashing exceptions during groupby\n# # t_df[col_lst] = t_df[col_lst].astype(str)\n\n# #Groupby parameters and preset the results ordered by accuracy\n# res_df = t_df.drop(columns  = 'fold'\n#           ).groupby(by = col_lst\n#                     ).mean().reset_index().sort_values('test_loss' ,\n#                                                        ascending = True)\n# res_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2023-07-16T21:12:24.494829Z","iopub.execute_input":"2023-07-16T21:12:24.495913Z","iopub.status.idle":"2023-07-16T21:12:24.505297Z","shell.execute_reply.started":"2023-07-16T21:12:24.49587Z","shell.execute_reply":"2023-07-16T21:12:24.504059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Checking top ten configurations with missing values:","metadata":{}},{"cell_type":"code","source":"# cv_param_dict = {'num_epochs' : [500,1000],\n#               'additional_layers' : [1,2,3],\n#                   'learning_rate' : [0.01,0.001],\n#                'batch_size' : [16,32,64]}\n\n# with_na_df = pivoted_df.reset_index().drop(columns = 'patient_id')\n# cv_wrapper = run_cv(with_na_df ,train_routine , proteinPrep)\n\n# scores_folds = cv_wrapper.run_cv(cv_param_dict)\n# scores_folds.to_csv('encoder_with_na_scores.csv')\n# scores_folds.head(10)","metadata":{"execution":{"iopub.status.busy":"2023-07-16T21:12:24.507302Z","iopub.execute_input":"2023-07-16T21:12:24.507745Z","iopub.status.idle":"2023-07-16T21:12:24.516126Z","shell.execute_reply.started":"2023-07-16T21:12:24.507703Z","shell.execute_reply":"2023-07-16T21:12:24.51484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# scores_folds= pd.read_csv('encoder_with_na_scores.csv', index_col = 0)\n# col_lst = ['num_epochs' ,'additional_layers','learning_rate','batch_size']\n# t_df = scores_folds.copy()\n\n# #Convert all parameters columns to string for reducing hashing exceptions during groupby\n# # t_df[col_lst] = t_df[col_lst].astype(str)\n\n# #Groupby parameters and preset the results ordered by accuracy\n# res_df = t_df.drop(columns  = 'fold'\n#           ).groupby(by = col_lst\n#                     ).mean().reset_index().sort_values('test_loss' ,\n#                                                        ascending = True)\n# res_df.head(10)\n\n# # Cofiguration index 8 looks good - low scores on test set, and not overfitted (high train loss)\n# # Plus 500 epochs and 64 batch size means faster processing","metadata":{"execution":{"iopub.status.busy":"2023-07-16T21:12:24.517721Z","iopub.execute_input":"2023-07-16T21:12:24.518201Z","iopub.status.idle":"2023-07-16T21:12:24.527511Z","shell.execute_reply.started":"2023-07-16T21:12:24.518148Z","shell.execute_reply":"2023-07-16T21:12:24.526381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Final Configuration and Implementation","metadata":{}},{"cell_type":"code","source":"kwargs = {'num_epoches' : 1000,\n          'additional_layers' : 1,\n         'learning_rate' : 0.001,\n         'batch_size' :  64}\n\npp =  proteinPrep()\nall_df = pivoted_df.reset_index().drop(columns = 'visit_id')\nX = pp.fit(all_df, True)\nmodel = train_routine(X , None , **kwargs)\nmodel","metadata":{"execution":{"iopub.status.busy":"2023-07-16T21:12:24.533114Z","iopub.execute_input":"2023-07-16T21:12:24.533986Z","iopub.status.idle":"2023-07-16T21:12:31.876618Z","shell.execute_reply.started":"2023-07-16T21:12:24.53394Z","shell.execute_reply":"2023-07-16T21:12:31.875447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\n\nclass EncodeProteins():\n    def __init__(self , X):\n        \n        best_configuration = {'num_epoches' : 2000,\n          'additional_layers' : 2,\n         'hidden_dim' :32, \n         'learning_rate' : 0.001,\n         'batch_size' :  64}\n        \n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        \n        pp =  proteinPrep()\n        X = pp.fit(X, True)\n        model = train_routine(X , None , **best_configuration)[0]\n\n        self.model = model.to(device)\n        self.pp = pp\n    \n    def encode(self, X):\n        \n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        transformed = pp.fit(X , False)\n        tensor_X = torch.tensor(transformed.values, dtype = torch.float32).to(device = device)\n        self.model.to(device=device)\n        \n        return self.model.encode(tensor_X)\n        \nencoder = EncodeProteins(all_df)\n\n#encoder.encode(all_df[:3])\n\n#this line will only work after running the end of the notebook - This diables the need to run env() multiple times\n# with open('test_prot.pkl' , 'rb') as f:\n#     pivoted_test_arr = pickle.load(f)\n    \n    \n# for piv_test in pivoted_test_arr:\n#     print(encoder.encode(piv_test))\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2023-07-16T21:12:31.878745Z","iopub.execute_input":"2023-07-16T21:12:31.879625Z","iopub.status.idle":"2023-07-16T21:12:35.328068Z","shell.execute_reply.started":"2023-07-16T21:12:31.879574Z","shell.execute_reply":"2023-07-16T21:12:35.326846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def smape1p_ind(A, F):\n    val = 200 * np.abs(F - A) / (np.abs(A+1) + np.abs(F+1))\n    return val\n\ndef smape1p(A, F):\n    return smape1p_ind(A, F).mean()\n\ndef max_dif(val, lst):\n    lst0 = [x for x in lst if x < val]\n    if len(lst0) == 0:\n        return -1\n    return val - max(lst0)\n\ndef count_prev_visits(val, lst):\n    lst0 = [x for x in lst if x < val]\n    return len(lst0)\n\nclass DataPrep:\n    def __init__(self, target_horizons=[0, 6, 12, 24], test_vmonths = [0, 6, 12, 18, 24, 36, 48, 60, 72, 84]):\n        self.target_horizons = target_horizons\n        self.test_vmonths = test_vmonths\n\n    def fit(self, proteins_df, peptides_df, clinical_df):\n        pass\n\n    def fe(self, sample, proteins_df, peptides_df, clinical_df):\n        for v_month in [0, 6, 12, 18, 24, 36, 48, 60, 72, 84]:\n            p = list(clinical_df[clinical_df[\"visit_month\"] == v_month][\"patient_id\"].unique())\n            sample[f\"visit_{v_month}m\"] = sample.apply(lambda x: (x[\"patient_id\"] in p) and (x[\"visit_month\"] >= v_month), axis=1).astype(int)\n\n            p = list(proteins_df[proteins_df[\"visit_month\"] == v_month][\"patient_id\"].unique())\n            sample[f\"btest_{v_month}m\"] = sample.apply(lambda x: (x[\"patient_id\"] in p) and (x[\"visit_month\"] >= v_month), axis=1).astype(int)\n\n            sample[f\"t_month_eq_{v_month}\"] = (sample[\"target_month\"] == v_month).astype(int)\n            sample[f\"v_month_eq_{v_month}\"] = (sample[\"visit_month\"] == v_month).astype(int)\n\n        for hor in self.target_horizons:\n            sample[f\"hor_eq_{hor}\"] = (sample[\"horizon\"] == hor).astype(int)\n\n        sample[\"horizon_scaled\"] = sample[\"horizon\"] / 24.0\n\n        blood_samples = proteins_df[\"visit_id\"].unique()\n        sample[\"blood_taken\"] = sample.apply(lambda x: x[\"visit_id\"] in blood_samples, axis=1).astype(int)\n        \n        all_visits = clinical_df.groupby(\"patient_id\")[\"visit_month\"].apply(lambda x: list(set(x))).to_dict()\n        all_non12_visits = sample.apply(lambda x: [xx for xx in all_visits.get(x[\"patient_id\"], []) if xx <= x[\"visit_month\"] and xx%12 != 0], axis=1)\n        sample[\"count_non12_visits\"] = all_non12_visits.apply(lambda x: len(x)) \n\n        return sample\n\n    def transform_train(self, proteins_df, peptides_df, clinical_df):\n        sample = clinical_df.rename({\"visit_month\":\"target_month\", \"visit_id\":\"visit_id_target\"}, axis=1).\\\n            merge(clinical_df[[\"patient_id\", \"visit_month\", \"visit_id\"]], how=\"left\", on=\"patient_id\")\n        sample[\"horizon\"] = sample[\"target_month\"] - sample[\"visit_month\"]\n        sample = sample[sample[\"horizon\"].isin(self.target_horizons)]\n        sample = sample[sample[\"visit_month\"].isin(self.test_vmonths)]\n\n        # Features\n        sample = self.fe(sample,\n            proteins_df[proteins_df[\"visit_month\"].isin(self.test_vmonths)],\n            peptides_df[peptides_df[\"visit_month\"].isin(self.test_vmonths)],\n            clinical_df[clinical_df[\"visit_month\"].isin(self.test_vmonths)])\n\n        # Targets reshape\n        res = []\n        for tgt_i in np.arange(1, 5):\n            delta_df = sample.copy()\n            if f\"updrs_{tgt_i}\" in delta_df.columns:\n                delta_df[\"target\"] = delta_df[f\"updrs_{tgt_i}\"]\n                delta_df[\"target_norm\"] = delta_df[\"target\"] / 100\n            delta_df[\"target_i\"] = tgt_i\n            res.append(delta_df)\n\n        sample = pd.concat(res, axis=0).reset_index(drop=True)\n        if f\"updrs_1\" in sample.columns:\n            sample = sample.drop([\"updrs_1\", \"updrs_2\", \"updrs_3\", \"updrs_4\"], axis=1)\n        \n        for tgt_i in np.arange(1, 5):\n            sample[f\"target_n_{tgt_i}\"] = (sample[\"target_i\"] == tgt_i).astype(int)\n\n        return sample\n    \n    def transform_test(self, proteins_df, peptides_df, test_df, sub_df):\n        sub = sub_df.copy()\n        sub[\"patient_id\"] = sub[\"prediction_id\"].apply(lambda x: int(x.split(\"_\")[0]))\n        sub[\"visit_month\"] = sub[\"prediction_id\"].apply(lambda x: int(x.split(\"_\")[1]))\n        sub[\"visit_id\"] = sub.apply(lambda x: str(x[\"patient_id\"]) + \"_\" + str(x[\"visit_month\"]), axis=1)\n\n        sample = sub[[\"patient_id\", \"visit_month\", \"visit_id\", \"prediction_id\"]]\n\n        sample[\"horizon\"] = sample[\"prediction_id\"].apply(lambda x: int(x.split(\"_\")[5]))\n        sample[\"target_i\"] = sample[\"prediction_id\"].apply(lambda x: int(x.split(\"_\")[3]))\n        sample[\"visit_month\"] = sample[\"visit_month\"]\n        sample[\"target_month\"] = sample[\"visit_month\"] + sample[\"horizon\"]\n        del sample[\"prediction_id\"]\n\n        # Features\n        sample = self.fe(sample, proteins_df, peptides_df, test_df)\n\n        for tgt_i in np.arange(1, 5):\n            sample[f\"target_n_{tgt_i}\"] = (sample[\"target_i\"] == tgt_i).astype(int)\n\n        return sample\n\ndp3 = DataPrep()\ndp3.fit(proteins, peptides, clinical)\n\nsample3 = dp3.transform_train(proteins, peptides, clinical)\nsample3 = sample3[~sample3[\"target\"].isnull()]\nsample3[\"is_suppl\"] = 0\n\nsup_sample3 = dp3.transform_train(proteins, peptides, supplement)\nsup_sample3 = sup_sample3[~sup_sample3[\"target\"].isnull()]\nsup_sample3[\"is_suppl\"] = 1\n\nprint(sample3.shape)\nprint(sup_sample3.shape)","metadata":{"execution":{"iopub.status.busy":"2023-07-16T21:12:35.329894Z","iopub.execute_input":"2023-07-16T21:12:35.330614Z","iopub.status.idle":"2023-07-16T21:12:48.325568Z","shell.execute_reply.started":"2023-07-16T21:12:35.330567Z","shell.execute_reply":"2023-07-16T21:12:48.324323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def split_df(df, folds_mapping, fold_id:int = 0):\n    folds = df[\"patient_id\"].map(folds_mapping)\n\n    df_train = df[folds != fold_id]\n    df_train = df_train[~df_train[\"target\"].isnull()].reset_index(drop=True)\n\n    df_valid = df[folds == fold_id]\n    df_valid = df_valid[~df_valid[\"target\"].isnull()].reset_index(drop=True)\n    \n    return df_train, df_valid\n\ndef create_folds_mapping(df, n_folds=5, random_state=42):\n    folds_df = pd.DataFrame({\"patient_id\":df[\"patient_id\"].unique()})\n    folds_df[\"fold\"] = -1\n\n    for i, (_, test_index) in enumerate(KFold(n_splits=n_folds, \n            shuffle=True, random_state=random_state).split(folds_df)):\n        folds_df.loc[test_index, \"fold\"] = i\n    folds_mapping = folds_df.set_index([\"patient_id\"])[\"fold\"]\n    return folds_mapping\n\nfrom joblib import Parallel, delayed\n\ndef smape1p_ind(A, F):\n    val = 200 * np.abs(F - A) / (np.abs(A+1) + np.abs(F+1))\n    return val\n\ndef smape1p(A, F):\n    return smape1p_ind(A, F).mean()\n\ndef smape1p_opt(x):\n    #return np.median(x)\n    tgts = np.arange(0, 61)\n    #tgts = [smape(x, val) for val in np.arange(0, 61)]\n    scores = [smape1p(x, val) for val in tgts]\n    return tgts[np.argmin(scores)]\n\ndef split_df(df, folds_mapping, fold_id:int = 0):\n    folds = df[\"patient_id\"].map(folds_mapping)\n\n    df_train = df[folds != fold_id]\n    df_train = df_train[~df_train[\"target\"].isnull()].reset_index(drop=True)\n\n    df_valid = df[folds == fold_id]\n    df_valid = df_valid[~df_valid[\"target\"].isnull()].reset_index(drop=True)\n    \n    return df_train, df_valid\n\ndef create_folds_mapping(df, n_folds=5, random_state=42):\n    folds_df = pd.DataFrame({\"patient_id\":df[\"patient_id\"].unique()})\n    folds_df[\"fold\"] = -1\n\n    for i, (_, test_index) in enumerate(KFold(n_splits=n_folds, \n            shuffle=True, random_state=random_state).split(folds_df)):\n        folds_df.loc[test_index, \"fold\"] = i\n    folds_mapping = folds_df.set_index([\"patient_id\"])[\"fold\"]\n    return folds_mapping\n\ndef run_single_fit(model, df_train, df_valid, fold_id, seed, probs):\n    if probs:\n        p = model.fit_predict_proba(df_train, df_valid)\n        p = pd.DataFrame(p, columns=[f\"prob_{i}\" for i in range(p.shape[1])]).reset_index(drop=True)\n        res = pd.DataFrame({\"seed\":seed, \"fold\": fold_id, \\\n            \"patient_id\":df_valid[\"patient_id\"], \"visit_month\":df_valid[\"visit_month\"], \\\n            \"target_month\":df_valid[\"target_month\"], \"target_i\":df_valid[\"target_i\"], \\\n            \"target\":df_valid[\"target\"]}).reset_index(drop=True)\n        return pd.concat([res, p], axis=1)\n    else:\n        p = model.fit_predict(df_train, df_valid)\n        return pd.DataFrame({\"seed\":seed, \"fold\": fold_id, \\\n            \"patient_id\":df_valid[\"patient_id\"], \"visit_month\":df_valid[\"visit_month\"], \\\n            \"target_month\":df_valid[\"target_month\"], \"target_i\":df_valid[\"target_i\"], \\\n            \"target\":df_valid[\"target\"], \"preds\":p})\n\nclass BaseModel:\n    def fit(self, df_train):\n        raise \"NotImplemented\"\n\n    def predict(self, df_valid):\n        raise \"NotImplemented\"\n\n    def predict_proba(self, df_valid):\n        raise \"NotImplemented\"\n\n    def fit_predict(self, df_train, df_valid):\n        self.fit(df_train)\n        return self.predict(df_valid)\n\n    def fit_predict_proba(self, df_train, df_valid):\n        self.fit(df_train)\n        return self.predict_proba(df_valid)\n\n    def cv(self, sample, sup_sample=None, n_folds=5, random_state=42):\n        folds_mapping = create_folds_mapping(sample, n_folds, random_state)\n\n        res = None\n        for fold_id in sorted(folds_mapping.unique()):\n            df_train, df_valid = split_df(sample, folds_mapping, fold_id)\n            if sup_sample is not None:\n                df_train = pd.concat([df_train, sup_sample], axis=0)\n            p = self.fit_predict(df_train, df_valid)\n            delta = pd.DataFrame({\"fold\": fold_id,  \\\n                    \"patient_id\":df_valid[\"patient_id\"], \"visit_month\":df_valid[\"visit_month\"], \\\n                    \"target_month\":df_valid[\"target_month\"], \"target_i\":df_valid[\"target_i\"], \\\n                    \"target\":df_valid[\"target\"], \"preds\":p})\n            res = pd.concat([res, delta], axis=0)\n\n        return res\n\n    def cvx(self, sample, sup_sample=None, n_runs=1, n_folds=5, random_state=42, probs=False):\n        np.random.seed(random_state)\n        seeds = np.random.randint(0, 1e6, n_runs)\n\n        run_args = []\n        for seed in seeds:\n            folds_mapping = create_folds_mapping(sample, n_folds, seed)\n            for fold_id in sorted(folds_mapping.unique()):\n                df_train, df_valid = split_df(sample, folds_mapping, fold_id)\n                if sup_sample is not None:\n                    df_train = pd.concat([df_train, sup_sample], axis=0)\n                run_args.append(dict(\n                    df_train = df_train,\n                    df_valid = df_valid,\n                    fold_id = fold_id,\n                    seed = seed,\n                    probs = probs\n                ))\n\n        res = Parallel(-1)(delayed(run_single_fit)(self, **args) for args in run_args)\n        #res = [run_single_fit(self, **args) for args in run_args]\n        return pd.concat(res, axis=0)\n\n    def loo(self, sample, sup_sample=None, probs=False, sample2=None):\n        if sample2 is None:\n            sample2 = sample\n        run_args = []\n        for patient_id in sample[\"patient_id\"].unique():\n            df_train = sample[sample[\"patient_id\"] != patient_id]\n            df_valid = sample2[sample2[\"patient_id\"] == patient_id]\n            if sup_sample is not None:\n                df_train = pd.concat([df_train, sup_sample], axis=0)\n            run_args.append(dict(\n                df_train = df_train,\n                df_valid = df_valid,\n                fold_id = None,\n                seed = None,\n                probs=probs\n            ))\n\n        res = Parallel(-1)(delayed(run_single_fit)(self, **args) for args in run_args)\n        return pd.concat(res, axis=0)\n\ndef print_cvx_summary(res_df):\n    scores = res_df.groupby([\"seed\", \"fold\"]).apply(lambda x: smape1p(x[\"target\"], x[\"preds\"])).values\n    print(\"# \", len(scores), \" runs\")\n    #print(\"# 05   :      \", np.quantile(scores, 0.05))\n    #print(\"# 25   :   \", np.quantile(scores, 0.25))\n    print(\"# mean :\", scores.mean())\n    #print(\"# 75   :   \", np.quantile(scores, 0.75))\n    #print(\"# 95   :      \", np.quantile(scores, 0.95))\n    print(\"# ovrl :\", smape1p(res_df[\"target\"], res_df[\"preds\"]))\n\ndef print_loo_summary(res_df):\n    scores = res_df.groupby([\"patient_id\"]).apply(lambda x: smape1p(x[\"target\"], x[\"preds\"])).values\n    print(\"# \", len(scores), \" runs\")\n    #print(\"# 05   :      \", np.quantile(scores, 0.05))\n    #print(\"# 25   :   \", np.quantile(scores, 0.25))\n    print(\"# mean :\", scores.mean())\n    #print(\"# 75   :   \", np.quantile(scores, 0.75))\n    #print(\"# 95   :      \", np.quantile(scores, 0.95))\n    print(\"# ovrl :\", smape1p(res_df[\"target\"], res_df[\"preds\"]))\n\n","metadata":{"execution":{"iopub.status.busy":"2023-07-16T21:12:48.327639Z","iopub.execute_input":"2023-07-16T21:12:48.328417Z","iopub.status.idle":"2023-07-16T21:12:48.390628Z","shell.execute_reply.started":"2023-07-16T21:12:48.328362Z","shell.execute_reply":"2023-07-16T21:12:48.389302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport sys\nimport random\nfrom tqdm import tqdm\nimport gc\nimport torch\nfrom torch import optim\nfrom torch.cuda.amp import GradScaler, autocast\nfrom collections import defaultdict\nfrom copy import copy\nimport os\nfrom transformers import get_cosine_schedule_with_warmup\nfrom torch.utils.data import SequentialSampler, DataLoader\nfrom sklearn.metrics import roc_auc_score, f1_score, cohen_kappa_score\nfrom torch.utils.data import Dataset\nfrom scipy.special import softmax\n\ntorch.set_num_threads(1)\n\ndef single_smape1p(preds, tgt):\n    x = np.tile(np.arange(preds.shape[1]), (preds.shape[0], 1))\n    x = np.abs(x - tgt) / (2 + x + tgt)\n    return (x * preds).sum(axis=1)\n\ndef opt_smape1p(preds):\n    x = np.hstack([single_smape1p(preds, i).reshape(-1,1) for i in range(preds.shape[1])])\n    return x.argmin(axis=1)\n\n\nfrom types import SimpleNamespace\nfrom torch import nn\nimport torch\n\nclass CustomDataset(Dataset):\n    def __init__(self, df, cfg, aug, mode=\"train\"):\n        self.cfg = cfg\n        self.mode = mode\n        self.df = df.copy()\n        self.features = df[cfg.features].values\n        if self.mode != \"test\":\n            self.targets = df[self.cfg.target_column].values.astype(np.float32)\n        else:\n            self.targets = np.zeros(len(df))\n\n    def __getitem__(self, idx):\n        features = self.features[idx]\n        targets = self.targets[idx]\n        \n        feature_dict = {\n            \"input\": torch.tensor(features),\n            \"target_norm\": torch.tensor(targets),\n        }\n        return feature_dict\n\n    def __len__(self):\n        return len(self.df)\n\n\nclass Net(nn.Module):\n    def __init__(self, cfg):\n        super(Net, self).__init__()\n        self.cfg = cfg\n        self.n_classes = cfg.n_classes\n        self.cnn = nn.Sequential(*([\n            nn.Linear(len(self.cfg.features), cfg.n_hidden),\n            nn.LeakyReLU(),\n            ] +\n            [\n            nn.Linear(cfg.n_hidden, cfg.n_hidden),\n            nn.LeakyReLU(),\n            ] * self.cfg.n_layers)\n        )\n\n        self.head = nn.Sequential(\n            nn.Linear(cfg.n_hidden, self.n_classes),\n            nn.LeakyReLU(),\n        )\n\n    def forward(self, batch):\n        input = batch[\"input\"].float()\n        y = batch[\"target_norm\"]\n        x = input\n        x = self.cnn(x)\n        preds = self.head(x).squeeze(-1)\n        loss = (torch.abs(y - preds) / (torch.abs(0.01 + y) + torch.abs(0.01 + preds))).mean()\n        return {\"loss\": loss, \"preds\": preds, \"target_norm\": y}\n\n\ndef worker_init_fn(worker_id):\n    np.random.seed(np.random.get_state()[1][0] + worker_id)\n\ndef get_train_dataloader(train_ds, cfg, verbose):\n    train_dataloader = DataLoader(\n        train_ds,\n        sampler=None,\n        shuffle=True,\n        batch_size=cfg.batch_size,\n        num_workers=cfg.num_workers,\n        pin_memory=False,\n        collate_fn=cfg.tr_collate_fn,\n        drop_last=cfg.drop_last,\n        worker_init_fn=worker_init_fn,\n    )\n    if verbose:\n        print(f\"train: dataset {len(train_ds)}, dataloader {len(train_dataloader)}\")\n    return train_dataloader\n\n\ndef get_val_dataloader(val_ds, cfg, verbose):\n    sampler = SequentialSampler(val_ds)\n    if cfg.batch_size_val is not None:\n        batch_size = cfg.batch_size_val\n    else:\n        batch_size = cfg.batch_size\n    val_dataloader = DataLoader(\n        val_ds,\n        sampler=sampler,\n        batch_size=batch_size,\n        num_workers=cfg.num_workers,\n        pin_memory=False,\n        collate_fn=cfg.val_collate_fn,\n        worker_init_fn=worker_init_fn,\n    )\n    if verbose:\n        print(f\"valid: dataset {len(val_ds)}, dataloader {len(val_dataloader)}\")\n    return val_dataloader\n\n\ndef get_scheduler(cfg, optimizer, total_steps):\n    scheduler = get_cosine_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=cfg.warmup * (total_steps // cfg.batch_size),\n        num_training_steps=cfg.epochs * (total_steps // cfg.batch_size),\n    )\n    return scheduler\n\n\ndef set_seed(seed=1234):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    #torch.backends.cudnn.deterministic = False\n    #torch.backends.cudnn.benchmark = True\n\ndef batch_to_device(batch, device):\n    batch_dict = {key: batch[key].to(device) for key in batch}\n    return batch_dict\n\ndef run_eval(model, val_dataloader, cfg, pre=\"val\", verbose=True):\n    model.eval()\n    torch.set_grad_enabled(False)\n    val_data = defaultdict(list)\n    if verbose:\n        progress_bar = tqdm(val_dataloader)\n    else:\n        progress_bar = val_dataloader\n    for data in progress_bar:\n        batch = batch_to_device(data, cfg.device)\n        if cfg.mixed_precision:\n            with autocast():\n                output = model(batch)\n        else:\n            output = model(batch)\n        for key, val in output.items():\n            val_data[key] += [output[key]]\n    for key, val in output.items():\n        value = val_data[key]\n        if len(value[0].shape) == 0:\n            val_data[key] = torch.stack(value)\n        else:\n            val_data[key] = torch.cat(value, dim=0)\n\n    preds = val_data[\"preds\"].cpu().numpy()\n    if (pre == \"val\") and verbose:\n        metric = smape1p(100*val_data[\"target_norm\"].cpu().numpy(), 100*preds)\n        print(f\"{pre}_metric 1 \", metric)\n        metric = smape1p(100*val_data[\"target_norm\"].cpu().numpy(), np.round(100*preds))\n        print(f\"{pre}_metric 2 \", metric)\n    \n    return 100*preds\n\n\ndef run_train(cfg, train_df, val_df, test_df=None, verbose=True):\n    os.makedirs(str(cfg.output_dir + \"/\"), exist_ok=True)\n\n    if cfg.seed < 0:\n        cfg.seed = np.random.randint(1_000_000)\n    if verbose:\n        print(\"seed\", cfg.seed)\n    set_seed(cfg.seed)\n\n    train_dataset = CustomDataset(train_df, cfg, aug=None, mode=\"train\")\n    train_dataloader = get_train_dataloader(train_dataset, cfg, verbose)\n    \n    if val_df is not None:\n        val_dataset = CustomDataset(val_df, cfg, aug=None, mode=\"val\")\n        val_dataloader = get_val_dataloader(val_dataset, cfg, verbose)\n\n    if test_df is not None:\n        test_dataset = CustomDataset(test_df, cfg, aug=None, mode=\"test\")\n        test_dataloader = get_val_dataloader(test_dataset, cfg, verbose)\n\n    model = Net(cfg)\n    model.to(cfg.device)\n\n    total_steps = len(train_dataset)\n    params = model.parameters()\n    optimizer = optim.Adam(params, lr=cfg.lr, weight_decay=0)\n    scheduler = get_scheduler(cfg, optimizer, total_steps)\n\n    if cfg.mixed_precision:\n        scaler = GradScaler()\n    else:\n        scaler = None\n\n    cfg.curr_step = 0\n    i = 0\n    optimizer.zero_grad()\n    for epoch in range(cfg.epochs):\n        set_seed(cfg.seed + epoch)\n        if verbose:\n            print(\"EPOCH:\", epoch)\n            progress_bar = tqdm(range(len(train_dataloader)))\n        else:\n            progress_bar = range(len(train_dataloader))\n        tr_it = iter(train_dataloader)\n        losses = []\n        gc.collect()\n\n        for itr in progress_bar:\n            i += 1\n            data = next(tr_it)\n            model.train()\n            torch.set_grad_enabled(True)\n            batch = batch_to_device(data, cfg.device)\n            if cfg.mixed_precision:\n                with autocast():\n                    output_dict = model(batch)\n            else:\n                output_dict = model(batch)\n            loss = output_dict[\"loss\"]\n            losses.append(loss.item())\n            if cfg.mixed_precision:\n                scaler.scale(loss).backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.gradient_clip)\n                scaler.step(optimizer)\n                scaler.update()\n                optimizer.zero_grad()\n            else:\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.gradient_clip)\n                optimizer.step()\n                optimizer.zero_grad()\n            if scheduler is not None:\n                scheduler.step()\n        if val_df is not None:\n            if (epoch + 1) % cfg.eval_epochs == 0 or (epoch + 1) == cfg.epochs:\n                run_eval(model, val_dataloader, cfg, pre=\"val\", verbose=verbose)\n\n    if test_df is not None:\n        return run_eval(model, test_dataloader, cfg, pre=\"test\", verbose=verbose)\n    else:\n        return model\n\ndef run_test(model, cfg, test_df):\n    test_dataset = CustomDataset(test_df, cfg, aug=None, mode=\"test\")\n    test_dataloader = get_val_dataloader(test_dataset, cfg, verbose=False)\n    return run_eval(model, test_dataloader, cfg, pre=\"test\", verbose=False)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-16T21:12:48.392562Z","iopub.execute_input":"2023-07-16T21:12:48.393231Z","iopub.status.idle":"2023-07-16T21:12:57.787376Z","shell.execute_reply.started":"2023-07-16T21:12:48.393169Z","shell.execute_reply":"2023-07-16T21:12:57.786083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import lightgbm as lgb\n\ndef single_smape1p(preds, tgt):\n    x = np.tile(np.arange(preds.shape[1]), (preds.shape[0], 1))\n    x = np.abs(x - tgt) / (2 + x + tgt)\n    return (x * preds).sum(axis=1)\n\ndef opt_smape1p(preds):\n    x = np.hstack([single_smape1p(preds, i).reshape(-1,1) for i in range(preds.shape[1])])\n    return x.argmin(axis=1)\n\nclass LGBClassModel1(BaseModel):\n    def __init__(self, params, features) -> None:\n        self.params = params\n        self.features = features\n    \n    def fit(self, df_train):\n        if self.features is None:\n            self.features = [col for col in df_train.columns if col.startswith(\"v_\")]\n        lgb_train = lgb.Dataset(df_train[self.features], df_train[\"target\"])\n        params0 = {k:v for k,v in self.params.items() if k not in [\"n_estimators\"]}\n        self.m_gbm = lgb.train(params0, lgb_train, num_boost_round=self.params[\"n_estimators\"])\n        return self\n\n    def predict_proba(self, df_valid):\n        return self.m_gbm.predict(df_valid[self.features])\n\n    def predict(self, df_valid):\n        return opt_smape1p(self.predict_proba(df_valid))\n\n\n\nparams = {\n        'boosting_type': 'gbdt',\n        'objective': 'multiclass',\n        'num_class': 87,\n        \"n_estimators\": 300,\n\n        'learning_rate': 0.019673004699536346,\n        'num_leaves': 208,\n        'max_depth': 14,\n        'min_data_in_leaf': 850,\n        'feature_fraction': 0.5190632906197453,\n        'lambda_l1': 7.405660751699475e-08,\n        'lambda_l2': 0.14583961675675494,\n        'max_bin': 240,\n    \n        'verbose': -1,\n        'force_col_wise': True,\n        'n_jobs': -1,\n    }\n\nfeatures = [\"target_i\", \"target_month\", \"horizon\", \"visit_month\", \"visit_6m\", \"blood_taken\"]\nfeatures += [\"visit_18m\", \"is_suppl\"]\nfeatures += [\"count_non12_visits\"]\nfeatures += [\"visit_48m\"]\n\nmodel_lgb = LGBClassModel1(params, features)\nmodel_lgb = model_lgb.fit(pd.concat([sample3, sup_sample3], axis=0))\n    ","metadata":{"execution":{"iopub.status.busy":"2023-07-16T21:12:57.789568Z","iopub.execute_input":"2023-07-16T21:12:57.7907Z","iopub.status.idle":"2023-07-16T21:14:32.180936Z","shell.execute_reply.started":"2023-07-16T21:12:57.790651Z","shell.execute_reply":"2023-07-16T21:14:32.179756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class NNRegModel1(BaseModel):\n#     def __init__(self, cfg, features=None) -> None:\n#         self.cfg = cfg\n#         #self.features = features\n    \n#     def fit(self, df_train):\n#         self.models = [run_train(self.cfg, df_train, None, None, verbose=False) for _ in range(self.cfg.bag_size)]\n#         return self\n\n#     def predict(self, df_valid):\n#         preds = np.vstack([run_test(model, self.cfg, df_valid) for model in self.models])\n#         if self.cfg.bag_agg_function == \"max\":\n#             return np.max(preds, axis=0)\n#         elif self.cfg.bag_agg_function == \"median\":\n#             return np.median(preds, axis=0)\n#         else:\n#             return np.mean(preds, axis=0)\n\n\n# cfg = SimpleNamespace(**{})\n\n# cfg.tr_collate_fn = None\n# cfg.val_collate_fn = None\n# #cfg.CustomDataset = CustomDataset\n# #cfg.net = Net\n\n# cfg.target_column = \"target_norm\"\n# cfg.output_dir = \"results/nn_temp\"\n# cfg.seed = -1\n# cfg.eval_epochs = 1\n# cfg.mixed_precision = False\n# #cfg.device = \"cuda:2\"\n# cfg.device = \"cpu\"\n\n# cfg.n_classes = 1\n# cfg.batch_size = 128\n# cfg.batch_size_val = 256\n# cfg.n_hidden = 64\n# cfg.n_layers = 2 #3\n# cfg.num_workers = 0\n# cfg.drop_last = False\n# cfg.gradient_clip = 1.0\n\n# cfg.bag_size = 1\n# cfg.bag_agg_function = \"mean\"\n# cfg.lr = 2e-3\n# cfg.warmup = 0\n# cfg.epochs = 10\n\n# cfg.features = [\"visit_6m\"]\n# #cfg.features += [\"blood_taken\"]\n# cfg.features += [c for c in sample3.columns if c.startswith(\"t_month_eq_\")]\n# cfg.features += [c for c in sample3.columns if c.startswith(\"v_month_eq_\")]\n# cfg.features += [c for c in sample3.columns if c.startswith(\"hor_eq_\")]\n# cfg.features += [c for c in sample3.columns if c.startswith(\"target_n_\")]\n# cfg.features += [\"visit_18m\"]\n# cfg.features += [\"visit_48m\"]\n# cfg.features += [\"is_suppl\"]\n# cfg.features += [\"horizon_scaled\"]\n\n# model_nn = NNRegModel1(cfg)\n# model_nn = model_nn.fit(pd.concat([sample3, sup_sample3], axis=0))\n","metadata":{"execution":{"iopub.status.busy":"2023-07-16T21:14:32.182592Z","iopub.execute_input":"2023-07-16T21:14:32.183498Z","iopub.status.idle":"2023-07-16T21:14:32.191488Z","shell.execute_reply.started":"2023-07-16T21:14:32.183453Z","shell.execute_reply":"2023-07-16T21:14:32.190237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.path.append('/kaggle/input/amp-pd')\n\nimport amp_pd_peptide\nenv = amp_pd_peptide.make_env()\niter_test = env.iter_test()\n\ndef repl(x1, x2, cond):\n    res = x1.copy()\n    res[cond] = x2[cond]\n    return res\n\nall_test_peptides = None\nall_test_proteins = None\nall_test_df = None\ntest_prot = []\nfor (test_df, test_peptides, test_proteins, sample_submission) in iter_test:\n    test_prot.append(test_proteins)\n    all_test_df = pd.concat([all_test_df, test_df], axis=0)\n    all_test_proteins = pd.concat([all_test_proteins, test_proteins], axis=0)\n    all_test_peptides = pd.concat([all_test_peptides, test_peptides], axis=0)\n    sample_test = dp3.transform_test(all_test_proteins, all_test_peptides, all_test_df, sample_submission)\n    sample_test[\"is_suppl\"] = 0\n    \n    sample_test[\"preds_lgb\"] = model_lgb.predict(sample_test)\n#     sample_test[\"preds_nn\"] = np.round(np.clip(model_nn.predict(sample_test), 0, None))\n    \n    sample_submission[\"rating\"] = np.round(sample_test[\"preds_lgb\"])\n#                                              + sample_test[\"preds_nn\"]) / 2)\n                                        \n    \n    env.predict(sample_submission)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-16T21:14:32.193467Z","iopub.execute_input":"2023-07-16T21:14:32.193912Z","iopub.status.idle":"2023-07-16T21:14:32.545684Z","shell.execute_reply.started":"2023-07-16T21:14:32.193855Z","shell.execute_reply":"2023-07-16T21:14:32.544483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_prot","metadata":{"execution":{"iopub.status.busy":"2023-07-16T21:14:32.547365Z","iopub.execute_input":"2023-07-16T21:14:32.54781Z","iopub.status.idle":"2023-07-16T21:14:32.568108Z","shell.execute_reply.started":"2023-07-16T21:14:32.547762Z","shell.execute_reply":"2023-07-16T21:14:32.566598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pivoted_test_arr = []\nfor t_prot in test_prot:\n    test_grouped = t_prot.groupby(['patient_id', 'UniProt']).agg({'NPX': 'mean'}).reset_index()\n    pivoted_test = test_grouped.pivot(index='patient_id', columns='UniProt', values='NPX')\n    pivoted_test = pivoted_test.drop(columns=[col for col in nan_columns if col in pivoted_test.columns])\n    pivoted_test_arr.append(pivoted_test)\n\nwith open('test_prot.pkl' , 'wb') as f:\n    pickle.dump(pivoted_test_arr , f)\n    \nfor piv_test in pivoted_test_arr:\n    print(encoder.encode(piv_test))","metadata":{"execution":{"iopub.status.busy":"2023-07-16T21:14:32.570461Z","iopub.execute_input":"2023-07-16T21:14:32.571617Z","iopub.status.idle":"2023-07-16T21:14:34.599767Z","shell.execute_reply.started":"2023-07-16T21:14:32.571571Z","shell.execute_reply":"2023-07-16T21:14:34.597593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.read_csv('/kaggle/working/submission.csv')\nsub","metadata":{"execution":{"iopub.status.busy":"2023-07-16T21:14:34.601369Z","iopub.execute_input":"2023-07-16T21:14:34.601744Z","iopub.status.idle":"2023-07-16T21:14:34.618127Z","shell.execute_reply.started":"2023-07-16T21:14:34.601708Z","shell.execute_reply":"2023-07-16T21:14:34.617087Z"},"trusted":true},"execution_count":null,"outputs":[]}]}