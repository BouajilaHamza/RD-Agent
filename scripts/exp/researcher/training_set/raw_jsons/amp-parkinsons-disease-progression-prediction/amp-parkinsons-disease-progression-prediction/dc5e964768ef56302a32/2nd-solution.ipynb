{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from tqdm.notebook import tqdm\nimport numpy as np\nimport pandas as pd\nimport sys, os\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nimport transformers\n\nclass MLPModel(nn.Module):\n    def __init__(self,fe,out_num):\n        super(MLPModel, self).__init__()\n        inp_num=len(fe)\n        \n        hid1=2048*2\n        hid2=2048\n        hid3=2048\n        hid4=2048\n        \n        self.fc1 = nn.Linear(inp_num,hid1)\n        self.fc2 = nn.Linear(hid1, hid2)\n        self.fc3 = nn.Linear(hid2, hid3)\n        self.fc4 = nn.Linear(hid3, hid4)\n        \n        self.output = nn.Linear(hid2,out_num)\n        \n        self.bn = nn.BatchNorm1d(inp_num)\n        self.ln = nn.LayerNorm(inp_num)\n        self.relu = nn.ReLU()\n\n        self.dropout1 = nn.Dropout(0.1)\n        self.dropout2 = nn.Dropout(0.2)\n        self.dropout3 = nn.Dropout(0.3)\n        self.dropout4 = nn.Dropout(0.4)\n        self.dropout5 = nn.Dropout(0.5)\n        \n#         self.fc1_list=nn.ModuleList([nn.Linear(hid_num,256) for i in range(10)])\n#         self.fc2_list=nn.ModuleList([nn.Linear(256,256) for i in range(10)])\n#         self.fc3_list=nn.ModuleList([nn.Linear(256,256) for i in range(10)])\n#         self.output_list=nn.ModuleList([nn.Linear(256, len(label_1_name)+len(label_2_name)) for i in range(10)])\n        \n    def forward(self, X):\n        #X=self.ln(X)\n        X = self.fc1(X)\n        X = self.relu(X)\n        \n        X = self.fc2(X)\n        X = self.relu(X)\n        \n#         X = self.fc3(X)\n#         X = self.relu(X)\n        \n#         X = self.fc4(X)\n#         X = self.relu(X)\n        \n#         X = self.fc3(X)\n#         #X = self.dropout2(X)\n#         X = self.relu(X)\n        preds = self.output(X)\n        #print(preds.shape)\n        return preds\n    \n############################################################################################################\n############################################################################################################\n############################################################################################################\n    \ntrain_peptides=pd.read_csv(\"/kaggle/input/amp-parkinsons-disease-progression-prediction/train_peptides.csv\")\ntrain_proteins=pd.read_csv(\"/kaggle/input/amp-parkinsons-disease-progression-prediction/train_proteins.csv\")\nPeptide=pd.DataFrame({\"Peptide\":list(train_peptides[\"Peptide\"].unique())})\nNPX=pd.DataFrame({\"NPX\":list(train_proteins[\"NPX\"].unique())})\n\ntrain_peptides[\"PeptideAbundance\"]=train_peptides[\"PeptideAbundance\"]/(train_peptides.groupby(\"visit_id\").PeptideAbundance.transform(\"sum\")+1)\ntrain_proteins[\"NPX\"]=train_proteins[\"NPX\"]/(train_proteins.groupby(\"visit_id\").NPX.transform(\"sum\")+1)\n\ntrain_proteins_pivot=train_proteins.pivot(index=[\"visit_id\"],columns=[\"UniProt\"],values=[\"NPX\"]).reset_index()\ntrain_proteins_pivot.columns=[\"_\".join(i).strip(\"_\") for i in train_proteins_pivot.columns]\n\ntrain_peptides_pivot=train_peptides.pivot(index=[\"visit_id\"],columns=[\"Peptide\"],values=[\"PeptideAbundance\"]).reset_index()\ntrain_peptides_pivot.columns=[\"_\".join(i).strip(\"_\") for i in train_peptides_pivot.columns]\n\n\n############################################################################################################\n############################################################################################################\n############################################################################################################\n\n\nlabel_1_name=[\"label\"]\nlabel_2_name=[\"label_updrs_1\",\"label_updrs_2\",\"label_updrs_3\",\"label_updrs_4\"]\nlabel_3_name=[\"label_0\",\"label_6\",\"label_12\",\"label_24\"]\nlabel_4_name=[i+\"_\"+str(j) for i in ['updrs_1', 'updrs_2','updrs_3', 'updrs_4'] for j in [0,6,12,24]]\n\nvalid_month=[0, 6, 12, 18, 24 , 36, 48, 60, 72, 84]\np_features=list(train_proteins_pivot.columns)[1:]+list(train_peptides_pivot.columns)[1:]\nfeatures_name1=[\"have_%s\"%i for i in valid_month]+[\"month_%s\"%i for i in valid_month]+label_4_name\nfeatures_name2=[\"have_%s\"%i for i in valid_month]+[\"month_%s\"%i for i in valid_month]+label_3_name\nfeatures_name3=[\"have_%s\"%i for i in valid_month]+[\"month_%s\"%i for i in valid_month]+label_2_name\nfeatures_name4=[\"have_%s\"%i for i in valid_month]+[\"month_%s\"%i for i in valid_month]\nfeatures_name5=features_name1+p_features\nhistory_month_dic={}\n\n\nmodel_version_path=\"/kaggle/input/amp-parkinson/models_ensemble/label1/\"\nmodel1_list=[]\nfor w in os.listdir(model_version_path):\n    model=MLPModel(features_name1,1)\n    model.cuda()\n    model.load_state_dict(torch.load(model_version_path+w))\n    model1_list.append(model)\n    \nmodel_version_path=\"/kaggle/input/amp-parkinson/models_ensemble/label4/\"\nmodel2_list=[]\nfor w in os.listdir(model_version_path):\n    model=MLPModel(features_name2,4)\n    model.cuda()\n    model.load_state_dict(torch.load(model_version_path+w))\n    model2_list.append(model)\n    \nmodel_version_path=\"/kaggle/input/amp-parkinson/models_ensemble/time4/\"\nmodel3_list=[]\nfor w in os.listdir(model_version_path):\n    model=MLPModel(features_name3,4)\n    model.cuda()\n    model.load_state_dict(torch.load(model_version_path+w))\n    model3_list.append(model)\n    \nmodel_version_path=\"/kaggle/input/amp-parkinson/models_ensemble/label16/\"\nmodel4_list=[]\nfor w in os.listdir(model_version_path):\n    model=MLPModel(features_name4,16)\n    model.cuda()\n    model.load_state_dict(torch.load(model_version_path+w))\n    model4_list.append(model)\n    \n# model_version_path=\"/kaggle/input/amp-parkinson/models_seq2/\"\n# model_list5=[]\n# for w in os.listdir(model_version_path):\n#     model=MLPModel(features_name5,1)\n#     model.cuda()\n#     model.load_state_dict(torch.load(model_version_path+w))\n#     model_list5.append(model)\n    \n    \nmodel_version_path=\"/kaggle/input/amp-parkinson/models_ensemble2/label1/\"\nmodel1_list2=[]\nfor w in os.listdir(model_version_path):\n    model=MLPModel(features_name1+p_features,1)\n    model.cuda()\n    model.load_state_dict(torch.load(model_version_path+w))\n    model1_list2.append(model)\n    \nmodel_version_path=\"/kaggle/input/amp-parkinson/models_ensemble2/label4/\"\nmodel2_list2=[]\nfor w in os.listdir(model_version_path):\n    model=MLPModel(features_name2+p_features,4)\n    model.cuda()\n    model.load_state_dict(torch.load(model_version_path+w))\n    model2_list2.append(model)\n    \nmodel_version_path=\"/kaggle/input/amp-parkinson/models_ensemble2/time4/\"\nmodel3_list2=[]\nfor w in os.listdir(model_version_path):\n    model=MLPModel(features_name3+p_features,4)\n    model.cuda()\n    model.load_state_dict(torch.load(model_version_path+w))\n    model3_list2.append(model)\n    \nmodel_version_path=\"/kaggle/input/amp-parkinson/models_ensemble2/label16/\"\nmodel4_list2=[]\nfor w in os.listdir(model_version_path):\n    model=MLPModel(features_name4+p_features,16)\n    model.cuda()\n    model.load_state_dict(torch.load(model_version_path+w))\n    model4_list2.append(model)\n    \n    \ndef have_month(m,p,i,dic):\n    if i>m:\n        return -1\n    else:\n        if i in dic.get(p,[]):\n            return 1\n        else:\n            return 0\n        ","metadata":{"execution":{"iopub.status.busy":"2023-05-11T06:34:21.639014Z","iopub.execute_input":"2023-05-11T06:34:21.63951Z","iopub.status.idle":"2023-05-11T06:35:18.667673Z","shell.execute_reply.started":"2023-05-11T06:34:21.639466Z","shell.execute_reply":"2023-05-11T06:35:18.666415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_pred(df_test, df_test_peptides, df_test_proteins, df_submission):\n    df_submission['patient_id'] = df_submission.apply('prediction_id').str.split('_', expand=True)[0].astype(int)\n    df_submission['visit_month'] = df_submission.apply('prediction_id').str.split('_', expand=True)[1].astype(int)\n    df_submission['visit_id']=df_submission['patient_id'].astype(str)+\"_\"+df_submission['visit_month'].astype(str)\n    \n    \n    all_data=df_submission[[\"patient_id\",\"visit_month\",\"visit_id\"]].drop_duplicates().reset_index(drop=True)\n    for row in all_data.itertuples():\n        if row.patient_id not in history_month_dic.keys():\n            history_month_dic[row.patient_id]=[row.visit_month]\n        else:\n            history_month_dic[row.patient_id].append(row.visit_month)\n\n    #1\n    all_data1=all_data.merge(pd.DataFrame({\"out_type\":label_4_name}),how=\"cross\")\n    for i in valid_month:\n        all_data1[\"have_%s\"%i]=list(map(lambda m,p:have_month(m,p,i,history_month_dic),all_data1[\"visit_month\"],all_data1[\"patient_id\"]))\n        all_data1[\"month_%s\"%i]=(all_data1[\"visit_month\"]==i).astype(int)\n\n    for i in label_4_name:\n        all_data1[i]=(all_data1[\"out_type\"]==i).astype(int)\n\n    #2\n    all_data2=all_data.merge(pd.DataFrame({\"out_type\":label_3_name}),how=\"cross\")\n    for i in valid_month:\n        all_data2[\"have_%s\"%i]=list(map(lambda m,p:have_month(m,p,i,history_month_dic),all_data2[\"visit_month\"],all_data2[\"patient_id\"]))\n        all_data2[\"month_%s\"%i]=(all_data2[\"visit_month\"]==i).astype(int)\n\n    for i in label_3_name:\n        all_data2[i]=(all_data2[\"out_type\"]==i).astype(int)\n\n    #3\n    all_data3=all_data.merge(pd.DataFrame({\"out_type\":label_2_name}),how=\"cross\")\n    for i in valid_month:\n        all_data3[\"have_%s\"%i]=list(map(lambda m,p:have_month(m,p,i,history_month_dic),all_data3[\"visit_month\"],all_data3[\"patient_id\"]))\n        all_data3[\"month_%s\"%i]=(all_data3[\"visit_month\"]==i).astype(int)\n\n    for i in label_2_name:\n        all_data3[i]=(all_data3[\"out_type\"]==i).astype(int)\n\n    #4\n    all_data4=all_data.copy()\n    for i in valid_month:\n        all_data4[\"have_%s\"%i]=list(map(lambda m,p:have_month(m,p,i,history_month_dic),all_data4[\"visit_month\"],all_data4[\"patient_id\"]))\n        all_data4[\"month_%s\"%i]=(all_data4[\"visit_month\"]==i).astype(int)  \n        \n        \n    #5\n#     all_data=all_data.merge(pd.DataFrame({\"out_type\":label_4_name}),how=\"cross\")\n#     for i in valid_month:\n#         all_data[\"have_%s\"%i]=list(map(lambda m,p:have_month_ori(m,p,i),all_data[\"visit_month\"],all_data[\"patient_id\"]))\n#         all_data[\"month_%s\"%i]=(all_data[\"visit_month\"]==i).astype(int)\n\n#     for i in label_4_name:\n#         all_data[i]=(all_data[\"out_type\"]==i).astype(int)\n        \n\n    train_peptides=Peptide.merge(df_test_peptides,on=\"Peptide\",how=\"left\").fillna(0)\n    train_proteins_pivot=NPX.merge(df_test_proteins,on=\"NPX\",how=\"left\").fillna(0)\n\n    train_peptides[\"PeptideAbundance\"]=train_peptides[\"PeptideAbundance\"]/(train_peptides.groupby(\"visit_id\").PeptideAbundance.transform(\"sum\")+1)\n    train_proteins[\"NPX\"]=train_proteins[\"NPX\"]/(train_proteins.groupby(\"visit_id\").NPX.transform(\"sum\")+1)\n\n    train_proteins_pivot=train_proteins.pivot(index=[\"visit_id\"],columns=[\"UniProt\"],values=[\"NPX\"]).reset_index()\n    train_proteins_pivot.columns=[\"_\".join(i).strip(\"_\") for i in train_proteins_pivot.columns]\n\n    train_peptides_pivot=train_peptides.pivot(index=[\"visit_id\"],columns=[\"Peptide\"],values=[\"PeptideAbundance\"]).reset_index()\n    train_peptides_pivot.columns=[\"_\".join(i).strip(\"_\") for i in train_peptides_pivot.columns]\n    \n    #all_data=all_data.merge(train_proteins_pivot,on=\"visit_id\",how=\"left\").merge(train_peptides_pivot,on=\"visit_id\",how=\"left\").fillna(0)\n\n    pred_list=[]\n    pred_list2=[]\n    with torch.no_grad():\n        for model in model1_list:\n            model.eval()\n            pred = model(torch.Tensor(all_data1[features_name1].values).cuda())\n            pred=pred.cpu().detach().numpy().reshape(-1)\n            pred_list.append(pred)\n        for model in model2_list:\n            model.eval()\n            pred = model(torch.Tensor(all_data2[features_name2].values).cuda())\n            pred=pred.cpu().detach().numpy().reshape(-1,4,4)\n            pred=np.concatenate([pred[i,:] for i in range(len(pred))],axis=1).T.reshape(-1)\n            pred_list.append(pred)\n        for model in model3_list:\n            model.eval()\n            pred = model(torch.Tensor(all_data3[features_name3].values).cuda())\n            pred=pred.cpu().detach().numpy().reshape(-1)\n            pred_list.append(pred)\n        for model in model4_list:\n            model.eval()\n            pred = model(torch.Tensor(all_data4[features_name4].values).cuda())\n            pred=pred.cpu().detach().numpy().reshape(-1)\n            pred_list.append(pred)\n\n\n        for model in model1_list2:\n            model.eval()\n            pred = model(torch.Tensor(all_data1.merge(train_proteins_pivot,on=\"visit_id\",how=\"left\").merge(train_peptides_pivot,on=\"visit_id\",how=\"left\").fillna(0)[features_name1+p_features].values).cuda())\n            pred=pred.cpu().detach().numpy().reshape(-1)\n            pred_list.append(pred)\n        for model in model2_list2:\n            model.eval()\n            pred = model(torch.Tensor(all_data2.merge(train_proteins_pivot,on=\"visit_id\",how=\"left\").merge(train_peptides_pivot,on=\"visit_id\",how=\"left\").fillna(0)[features_name2+p_features].values).cuda())\n            pred=pred.cpu().detach().numpy().reshape(-1,4,4)\n            pred=np.concatenate([pred[i,:] for i in range(len(pred))],axis=1).T.reshape(-1)\n            pred_list2.append(pred)\n        for model in model3_list2:\n            model.eval()\n            pred = model(torch.Tensor(all_data3.merge(train_proteins_pivot,on=\"visit_id\",how=\"left\").merge(train_peptides_pivot,on=\"visit_id\",how=\"left\").fillna(0)[features_name3+p_features].values).cuda())\n            pred=pred.cpu().detach().numpy().reshape(-1)\n            pred_list.append(pred)\n        for model in model4_list2:\n            model.eval()\n            pred = model(torch.Tensor(all_data4.merge(train_proteins_pivot,on=\"visit_id\",how=\"left\").merge(train_peptides_pivot,on=\"visit_id\",how=\"left\").fillna(0)[features_name4+p_features].values).cuda())\n            pred=pred.cpu().detach().numpy().reshape(-1)\n            pred_list.append(pred)\n\n    pred_avg=np.mean(np.array(pred_list),axis=0)\n    pred_avg[pred_avg<0]=0\n    pred_avg2=np.mean(np.array(pred_list2),axis=0)\n    pred_avg2[pred_avg2<0]=0 \n    \n    return pred_avg,pred_avg2","metadata":{"execution":{"iopub.status.busy":"2023-05-11T06:35:18.670091Z","iopub.execute_input":"2023-05-11T06:35:18.67051Z","iopub.status.idle":"2023-05-11T06:35:18.707197Z","shell.execute_reply.started":"2023-05-11T06:35:18.670469Z","shell.execute_reply":"2023-05-11T06:35:18.706071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import amp_pd_peptide\n\nenv = amp_pd_peptide.make_env()\ntest_iterator = env.iter_test()\n        \n\nfor (df_test, df_test_peptides, df_test_proteins, df_submission) in test_iterator:\n    pred_avg,pred_avg2=get_pred(df_test, df_test_peptides, df_test_proteins, df_submission)\n    \n    df_submission[\"rating1\"]=pred_avg\n    df_submission[\"rating2\"]=pred_avg2\n    \n    df_submission[\"rating\"]=list(map(lambda a,b,c:a*0.45+b*0.55 if c in [0,6] else a*0.55+b*0.45,df_submission[\"rating1\"],df_submission[\"rating2\"],df_submission[\"visit_month\"]))\n    df_submission[\"rating\"]=df_submission[\"rating\"].apply(lambda x:np.round(x,0))\n    \n    df_submission = df_submission.loc[:, ['prediction_id', 'rating']]\n    env.predict(df_submission)","metadata":{"execution":{"iopub.status.busy":"2023-05-11T06:35:18.709068Z","iopub.execute_input":"2023-05-11T06:35:18.70969Z","iopub.status.idle":"2023-05-11T06:35:20.857124Z","shell.execute_reply.started":"2023-05-11T06:35:18.709649Z","shell.execute_reply":"2023-05-11T06:35:20.855932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_submission","metadata":{"execution":{"iopub.status.busy":"2023-05-11T06:35:33.509135Z","iopub.execute_input":"2023-05-11T06:35:33.509803Z","iopub.status.idle":"2023-05-11T06:35:33.526961Z","shell.execute_reply.started":"2023-05-11T06:35:33.509761Z","shell.execute_reply":"2023-05-11T06:35:33.525638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}