{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\n\nimport plotly.graph_objs as go\nfrom plotly.subplots import make_subplots\nfrom plotly.offline import init_notebook_mode\ninit_notebook_mode(connected=True)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom tqdm import tqdm\ntqdm.pandas()\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport gc\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import mean_absolute_error as mae\n\nfrom umap import UMAP\n\nimport optuna\nimport catboost as cb\n\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.layers import Dense, Dropout, Input\n\nseed = 228\nnp.random.seed(seed)\ntf.random.set_seed(seed)\n\n# Read files\npath = '/kaggle/input/playground-series-s3e2/'\ntrain = pd.read_csv(path+'train.csv')\ntest = pd.read_csv(path+'test.csv')\nss = pd.read_csv(path+'sample_submission.csv')\n\norig_train = pd.read_csv('/kaggle/input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv')\ntrain.drop('id', axis=1, inplace=True)\norig_train.drop('id', axis=1, inplace=True)\ntest.drop('id', axis=1, inplace=True)\n\n# Show all columns\npd.set_option('display.max_columns', None)\n\n# Bold font\nbold = ['\\033[1m', '\\033[0m']","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-01-16T23:31:44.477923Z","iopub.execute_input":"2023-01-16T23:31:44.478428Z","iopub.status.idle":"2023-01-16T23:32:06.342745Z","shell.execute_reply.started":"2023-01-16T23:31:44.478339Z","shell.execute_reply":"2023-01-16T23:32:06.341776Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![](https://www.brainandlife.org/siteassets/online-exclusives/covid-19/stroke-main.jpg)","metadata":{}},{"cell_type":"markdown","source":"# <div style=\"background-color:#72bfd6;text-align:center;color:white;font-size:150%;font-family:Calibri;border-radius:10px\"> <b>EDA</b></div>\n\n<b><span style='color:#444444;font-size:200%;font-family:Calibri'>|</span><span style='color:#72bfd6;font-size:200%;font-family:Calibri'> Missing values</span></b>\n\nWe have missing values only in original data for bmi (3.93%).","metadata":{}},{"cell_type":"code","source":"def mv(df, name):\n    l = len(df)\n    df = df.isna().sum().to_frame().reset_index() \\\n             .rename({'index': 'feature', 0: name}, axis=1)\n    df[name] = round(df[name]/l*100, 2)\n    return df\n    \ntrain_mv = mv(train, 'train (kaggle), %')\norig_train_mv = mv(orig_train, 'train (original), %')\ntest_mv = mv(test, 'test, %')\nmv_df = train_mv.merge(orig_train_mv, on='feature', how='right') \\\n                .merge(test_mv, on='feature', how='right')\n\nfor col in mv_df.columns[1:]:\n    mv_df[col] = mv_df[col].astype('str')\n    \nmv_df = mv_df.style.hide_index() \\\n        .set_table_styles([\n        {'selector': 'th.col_heading', 'props': 'font-size: 12pt; text-align: center; font-weight: bold; border: 1px solid black !important'},\n        {'selector': 'td', 'props': 'text-align: center; font-size: 10pt; border: 1px solid black !important'},\n        {'selector': 'th:not(.index_name)', 'props': 'background-color: #72bfd6; color: white;'}\n        ], overwrite=False).set_properties(subset=['feature'], **{'font-weight': 'bold'})\nmv_df","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-01-16T23:32:06.344327Z","iopub.execute_input":"2023-01-16T23:32:06.345322Z","iopub.status.idle":"2023-01-16T23:32:06.466867Z","shell.execute_reply.started":"2023-01-16T23:32:06.345276Z","shell.execute_reply":"2023-01-16T23:32:06.465736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I'll try to restore them with KNN. It's better than just putting the average.","metadata":{}},{"cell_type":"code","source":"num_cols = test.select_dtypes(include=np.number).columns.tolist()\nnum_cols.remove('hypertension') # in fact, this is a categorical feature\nnum_cols.remove('heart_disease') # and this is too\ncat_cols = list(set(test.columns) - set(num_cols))\ndf = orig_train.copy()\n\nfor col in cat_cols:\n    le = LabelEncoder()\n    df[col] = le.fit_transform(df[col])\n    \ncols = ['age', 'ever_married', 'work_type', \n        'smoking_status', 'avg_glucose_level', \n        'hypertension', 'heart_disease']\nknn = KNeighborsRegressor(n_neighbors=100,\n                          metric='minkowski',\n                          n_jobs=-1)\nknn.fit(df[cols], df.index)\ndists, nears = knn.kneighbors(df[cols], return_distance=True)\n\n# Check results\nresult = []\nfor i, n in enumerate(nears):\n    n = list(n)\n    n.remove(i)\n    try:\n        avg_bmi = df.iloc[n]['bmi'].median()\n        result.append(mae(np.array([df.iloc[i]['bmi']]), \n                          np.array([avg_bmi])))\n    except:\n        continue\n        \nprint(f'MAE mean score: {round(np.mean(result), 2)}')\nprint(f'MAE median score: {round(np.median(result), 2)}')\n\n# Restore missing values\nresult = []\nfor i in df.query('bmi!=bmi').index:\n    result.append(round(df.iloc[nears[i]]['bmi'].median(),1))\ndf.loc[orig_train.query('bmi!=bmi').index, 'bmi'] = result\norig_train['bmi'] = df['bmi']","metadata":{"execution":{"iopub.status.busy":"2023-01-16T23:32:06.468568Z","iopub.execute_input":"2023-01-16T23:32:06.469144Z","iopub.status.idle":"2023-01-16T23:32:11.4754Z","shell.execute_reply.started":"2023-01-16T23:32:06.469059Z","shell.execute_reply":"2023-01-16T23:32:11.473995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b><span style='color:#444444;font-size:200%;font-family:Calibri'>|</span><span style='color:#72bfd6;font-size:200%;font-family:Calibri'> Features distribution</span></b>\n\nThe distribution of original train data is different from the distribution of the generated train and test data.","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize = (15, 5))\nfor i, col in enumerate(num_cols):\n    plt.subplot(1,3,i+1)\n    plt.title(col, size=18, y=1.03, fontname='Calibri', \n              fontweight='bold', color='#444444')\n    a = sns.kdeplot(train[col], color='#d1d1d1', \n                    shade=True, label='train kaggle', \n                    alpha=0.6, edgecolor='black')\n    sns.kdeplot(orig_train[col], color='#9c2f3b', \n                shade=True, label='train original', \n                alpha=0.6, edgecolor='black')\n    sns.kdeplot(test[col], color='#72bfd6', \n                shade=True, label='test', \n                alpha=0.6, edgecolor='black')\n    plt.ylabel('')\n    plt.xlabel('')\n    plt.xticks(fontname='Calibri', size=12)\n    plt.yticks([])\n    for s in ['right', 'top', 'left', 'bottom']:\n        a.spines[s].set_visible(False)\n        \nfig.tight_layout(pad=3)\nfig.legend(['train (kaggle)', 'train (original)', 'test'], bbox_to_anchor=(0.65, 1.07), \n           ncol=3, borderpad=0.5, frameon=True, fontsize=11, title_fontsize=12)\nplt.figtext(0.5, 1.1, 'Numerical features distribution', \n            fontname='Calibri', fontweight='bold',\n            size=22, color='#444444', ha='center') \nplt.show()\n\n###########################################\n\nprint()\nprint()\ncat_cols = list(set(test.columns) - set(num_cols))\n\ntrain['data'] = 'kaggle'\norig_train['data'] = 'original'\ntest['data'] = 'test'\ndf = pd.concat([train, orig_train, test])\n\nl = {'kaggle': len(train), 'original': len(orig_train), 'test': len(test)}\n\ndef change_width(ax, new_value) :\n    for patch in ax.patches :\n        current_width = patch.get_width()\n        diff = current_width - new_value\n        patch.set_width(new_value)\n        patch.set_x(patch.get_x() + diff * .5)\n\nfig = plt.figure(figsize=(15, 15))\nfor i, col in enumerate(cat_cols):\n    df_plot = df.groupby(['data', col], as_index=False) \\\n                  ['age'].count().rename({'age': 'count'}, axis=1)\n    df_plot['len_data'] = df_plot['data'].apply(lambda x: l[x])\n    df_plot['count'] = round(df_plot['count'] / df_plot['len_data'] * 100, 2)\n    \n    plt.subplot(4,2,i+1)\n    plt.title(col, size=16, y=1.03, fontname='Calibri', \n              fontweight='bold', color='#444444')\n    a = sns.barplot(data=df_plot, x=col, y='count', hue='data',\n                    palette=['#d1d1d1', '#9c2f3b', '#72bfd6'], \n                    linestyle=\"-\", linewidth=1, edgecolor=\"black\", \n                    saturation=0.9)\n    plt.ylabel('')\n    plt.xlabel('')\n    plt.xticks(fontname='Calibri', size=11)\n    a.set_yticks([0, 100])\n    plt.yticks([])\n\n    for p in a.patches:\n        height = p.get_height()\n        a.annotate(f'{height:g}%', (p.get_x() + p.get_width() / 2, p.get_height()+2), \n                   ha='center', va='center', size=8,xytext=(0, 5), \n                   textcoords='offset points')\n    \n    for s in ['right', 'top', 'left']:\n        a.spines[s].set_visible(False)\n\n    a.legend().set_visible(False)\n    change_width(a, 0.15)\n\nfig.tight_layout(pad=6)\nplt.figtext(0.5, 0.98, 'Categorical features distribution', \n            fontname='Calibri', fontweight='bold',\n            size=22, color='#444444', ha='center') \nplt.show()","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-01-16T23:32:11.477822Z","iopub.execute_input":"2023-01-16T23:32:11.478172Z","iopub.status.idle":"2023-01-16T23:32:14.025986Z","shell.execute_reply.started":"2023-01-16T23:32:11.47814Z","shell.execute_reply":"2023-01-16T23:32:14.024969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"text-align:justify;\">\nAs the analysis shows, it is unsafe to use data from the original dataset in this competition due to the difference in distribution. It may seem insignificant, however, as the training of models has shown, when using the original data, the final result becomes worse. Marking up data about generation increases the CV score, but at the same time worsens the final results. However, the data on patients who have had a stroke from original data will be added to the training model, as they are an important addition to improving the possibility of separating the two classes.\n</p>\n\n<b><span style='color:#444444;font-size:200%;font-family:Calibri'>|</span><span style='color:#72bfd6;font-size:200%;font-family:Calibri'> Target distribution</span></b>","metadata":{}},{"cell_type":"code","source":"fig = make_subplots(rows=1, cols=2, \n                    vertical_spacing=0.1,\n                    specs=[[{\"type\": \"pie\"}, {\"type\": \"pie\"}]],\n                    subplot_titles=('<b>Kaggle</b>',  \n                                    '<b>Original</b>'))\nfor i, df in enumerate([train, orig_train]):\n    info = df['stroke'].value_counts().reset_index()\n    info['index'] = info['index'].map({0: 'No', 1: 'Yes'})\n    fig.add_trace(go.Pie(labels=info['index'],\n                         values=info['stroke'],\n                         textinfo='label+percent',\n                         textposition = 'inside',\n                         hoverinfo='value',\n                         textfont_size=22,\n                         marker=dict(colors=['#72bfd6', '#9c2f3b'],\n                                     line=dict(color='#444444', width=1)),\n                         hole=0.75,\n                         showlegend=False),\n                  row=1, col=i+1)\n    \nfig.update_layout(width=750, height=450, plot_bgcolor='white',\n                  font_family=\"Calibri\", font_color=\"#444444\",\n                  hoverlabel=dict(font_size=14, font_family=\"Calibri\"))\n\nfig.layout.annotations[0].update(x=0.225, y=0.46, font_size=24)\nfig.layout.annotations[1].update(x=0.775, y=0.46, font_size=24)\n\nfig.show()","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-01-16T23:32:14.027487Z","iopub.execute_input":"2023-01-16T23:32:14.027835Z","iopub.status.idle":"2023-01-16T23:32:14.338439Z","shell.execute_reply.started":"2023-01-16T23:32:14.027803Z","shell.execute_reply":"2023-01-16T23:32:14.337548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize = (15, 5))\nfor i, col in enumerate(num_cols):\n    plt.subplot(1,3,i+1)\n    plt.title(col, size=18, y=1.03, fontname='Calibri', \n              fontweight='bold', color='#444444')\n    a = sns.kdeplot(train[train['stroke']==0][col], \n                    color='#72bfd6', label='No stroke', \n                    shade=True, alpha=0.8, edgecolor='black')\n    sns.kdeplot(train[train['stroke']==1][col], color='#9c2f3b', \n                shade=True, label='Stroke', \n                alpha=0.8, edgecolor='black')\n    plt.ylabel('')\n    plt.xlabel('')\n    plt.xticks(fontname='Calibri', size=12)\n    plt.yticks([])\n    for s in ['right', 'top', 'left', 'bottom']:\n        a.spines[s].set_visible(False)\n        \nfig.tight_layout(pad=3)\nfig.legend(['No stroke', 'Stroke'], bbox_to_anchor=(0.58, 1.07), \n           ncol=2, borderpad=0.5, frameon=True, fontsize=11, title_fontsize=12)\nplt.figtext(0.5, 1.1, 'Distribution of stroke by numerical features', \n            fontname='Calibri', fontweight='bold',\n            size=22, color='#444444', ha='center') \nplt.show()\n\n###########################################\n\nprint()\nprint()\n\ndef change_width(ax, new_value) :\n    for patch in ax.patches :\n        current_width = patch.get_width()\n        diff = current_width - new_value\n        patch.set_width(new_value)\n        patch.set_x(patch.get_x() + diff * .5)\n\nfig = plt.figure(figsize=(15, 15))\nfor i, col in enumerate(cat_cols):\n    df_plot = df.groupby(['stroke', col], as_index=False) \\\n                  ['age'].count().rename({'age': 'count'}, axis=1)\n    df_plot['count'] = round(df_plot['count'] / len(train) * 100, 2)\n    \n    plt.subplot(4,2,i+1)\n    plt.title(col, size=16, y=1.03, fontname='Calibri', \n              fontweight='bold', color='#444444')\n    a = sns.barplot(data=df_plot, x=col, y='count', hue='stroke',\n                    palette=['#72bfd6', '#9c2f3b'], \n                    linestyle=\"-\", linewidth=1, edgecolor=\"black\", \n                    saturation=0.9)\n    plt.ylabel('')\n    plt.xlabel('')\n    plt.xticks(fontname='Calibri', size=11)\n    a.set_yticks([0, 50])\n    plt.yticks([])\n\n    for p in a.patches:\n        height = p.get_height()\n        a.annotate(f'{height:g}%', (p.get_x() + p.get_width() / 2, p.get_height()+1), \n                   ha='center', va='center', size=12, xytext=(0, 5), \n                   textcoords='offset points')\n    \n    for s in ['right', 'top', 'left']:\n        a.spines[s].set_visible(False)\n\n    a.legend().set_visible(False)\n    change_width(a, 0.35)\n\nfig.tight_layout(pad=6)\nplt.figtext(0.5, 0.98, 'Distribution of stroke by categorical features', \n            fontname='Calibri', fontweight='bold',\n            size=22, color='#444444', ha='center') \nplt.show()","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-01-16T23:32:14.33982Z","iopub.execute_input":"2023-01-16T23:32:14.340158Z","iopub.status.idle":"2023-01-16T23:32:16.344655Z","shell.execute_reply.started":"2023-01-16T23:32:14.340127Z","shell.execute_reply":"2023-01-16T23:32:16.343463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b><span style='color:#444444;font-size:200%;font-family:Calibri'>|</span><span style='color:#72bfd6;font-size:200%;font-family:Calibri'> Correlation map</span></b>\n\nBefore building a correlation map, I will finally combine the necessary data from the original.","metadata":{}},{"cell_type":"code","source":"orig_train = orig_train[orig_train['stroke']==1]\ntrain = pd.concat([orig_train, train], ignore_index=True)\ntrain.drop('data', axis=1, inplace=True) # This column was created to build a barplot\ntest.drop('data', axis=1, inplace=True)\ndf = train.copy()\n\nfor col in cat_cols:\n    le = LabelEncoder()\n    df[col] = le.fit_transform(df[col])","metadata":{"execution":{"iopub.status.busy":"2023-01-16T23:32:16.346324Z","iopub.execute_input":"2023-01-16T23:32:16.34678Z","iopub.status.idle":"2023-01-16T23:32:16.394655Z","shell.execute_reply.started":"2023-01-16T23:32:16.346738Z","shell.execute_reply":"2023-01-16T23:32:16.393657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"matrix = np.triu(df.corr())\nplt.figure(figsize = (15, 12))\na = sns.heatmap(df.corr(), mask=matrix, annot=True, annot_kws={'fontsize': 12}, \n            cmap=sns.diverging_palette(230, 10, as_cmap=True), cbar=True,\n            vmin=-1, vmax=1, fmt='.2f', linewidths=0.1, linecolor='white')\nplt.xticks(size=12, fontname='Calibri')\nplt.yticks(size=12, fontname='Calibri')\na.set_xticks(a.get_xticks()[:-1])\na.set_yticks(a.get_yticks()[1:])\nplt.yticks()[-1][-1].set_fontsize(14)\nplt.yticks()[-1][-1].set_fontweight('bold')\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-01-16T23:32:16.39631Z","iopub.execute_input":"2023-01-16T23:32:16.396849Z","iopub.status.idle":"2023-01-16T23:32:17.012667Z","shell.execute_reply.started":"2023-01-16T23:32:16.396817Z","shell.execute_reply":"2023-01-16T23:32:17.011519Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b><span style='color:#444444;font-size:200%;font-family:Calibri'>|</span><span style='color:#72bfd6;font-size:200%;font-family:Calibri'> UMAP</span></b>","metadata":{}},{"cell_type":"code","source":"umap = UMAP(n_components=2, \n            n_neighbors=50, \n            min_dist=1,\n            random_state=seed)\nresults=umap.fit_transform(df.drop('stroke', axis=1))\n\nplt.figure(figsize=(15, 12))\na = sns.scatterplot(x=results[:,0],\n                    y=results[:,1],\n                    hue=df['stroke'],\n                    s=6,\n                    edgecolor='none',\n                    palette=['#72bfd6', '#9c2f3b'])\nplt.xticks([])\nplt.yticks([])\n\nfor s in ['right', 'left', 'top', 'bottom']:\n    a.spines[s].set_visible(False)\na.legend().set_visible(False)\n\nplt.show()","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-01-16T23:32:17.014268Z","iopub.execute_input":"2023-01-16T23:32:17.014711Z","iopub.status.idle":"2023-01-16T23:32:59.583746Z","shell.execute_reply.started":"2023-01-16T23:32:17.014673Z","shell.execute_reply":"2023-01-16T23:32:59.582796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b><span style='color:#444444;font-size:200%;font-family:Calibri'>|</span><span style='color:#72bfd6;font-size:200%;font-family:Calibri'> Preprocessing</span></b>","metadata":{}},{"cell_type":"code","source":"def f_importance_plot(f_imp):\n    f_imp['avg_imp'] = f_imp[f_imp.columns[1:]].mean(axis=1)\n    f_imp.sort_values('avg_imp', ascending=False, inplace=True)\n    fig = plt.figure(figsize = (15, 0.35*len(f_imp)))\n    plt.title('Feature importances', size=25, y=1.05, \n              fontname='Calibri', fontweight='bold', color='#444444')\n    a = sns.barplot(data=f_imp, x='avg_imp', y='feature', \n                    palette='Blues_d', linestyle=\"-\", \n                    linewidth=1, edgecolor=\"black\")\n    plt.xlabel('')\n    plt.xticks([])\n    plt.ylabel('')\n    plt.yticks(size=11, color='#444444')\n    \n    for j in ['right', 'top', 'bottom']:\n        a.spines[j].set_visible(False)\n    for j in ['left']:\n        a.spines[j].set_linewidth(0.5)\n    plt.show()\n    \ndef norm_0to1(preds):\n    return (preds - np.min(preds)) / (np.max(preds) - np.min(preds))\n\ndef preds_plot(preds):\n    plt.figure(figsize=(15, 7))\n    plt.title('Distribution of predictions', \n          size=25, y=1.03, fontname='Calibri', \n          fontweight='bold', color='#444444')\n    a = sns.histplot(preds, color='#72bfd6', bins=100)\n    plt.ylabel('')\n    plt.xlabel('')\n    plt.xticks(fontname='Calibri', size=12)\n    plt.yticks([])\n    for s in ['right', 'top', 'left']:\n        a.spines[s].set_visible(False)\n    plt.show()","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-01-16T23:32:59.586781Z","iopub.execute_input":"2023-01-16T23:32:59.587842Z","iopub.status.idle":"2023-01-16T23:32:59.600566Z","shell.execute_reply.started":"2023-01-16T23:32:59.587805Z","shell.execute_reply":"2023-01-16T23:32:59.599574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Removing 8 outliers gave an excellent improvement for CatBoost, but a deterioration for NN (according to LB).","metadata":{}},{"cell_type":"code","source":"train_cb = train.copy()\ntrain_cb.drop(train_cb[(train_cb['age']<31)&\n                       (train_cb['stroke']==1)].index, inplace=True) # 4 patients\n\ntrain_cb.drop(train_cb[(train_cb['age']>70)&\n                       (train_cb['avg_glucose_level']>200)&\n                       (train_cb['hypertension']==1)&\n                       (train_cb['heart_disease']==1)&\n                       (train_cb['ever_married']=='Yes')&\n                       (train_cb['stroke']==0)].index, inplace=True) # 4 patients\n\ntrain = pd.get_dummies(train)\ntrain_cb = pd.get_dummies(train_cb)\ntest = pd.get_dummies(test)\n\nX_cb = train_cb.drop('stroke', axis=1)\ny_cb = train_cb['stroke']\n    \nX = train.drop('stroke', axis=1)\ny = train['stroke']\n\nseed = 228\nFOLDS = 10","metadata":{"execution":{"iopub.status.busy":"2023-01-16T23:32:59.601946Z","iopub.execute_input":"2023-01-16T23:32:59.602952Z","iopub.status.idle":"2023-01-16T23:32:59.683943Z","shell.execute_reply.started":"2023-01-16T23:32:59.602918Z","shell.execute_reply":"2023-01-16T23:32:59.682549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div style=\"background-color:#72bfd6;text-align:center;color:white;font-size:150%;font-family:Calibri;border-radius:10px\"> <b>CatBoost</b></div>","metadata":{}},{"cell_type":"code","source":"cb_params = {\n    'depth': 3,\n    'learning_rate': 0.01,\n    'rsm': 0.5,\n    'subsample': 0.931,\n    'l2_leaf_reg': 69,\n    'min_data_in_leaf': 20,\n    'random_strength': 0.175,\n    \n    'random_seed': seed,\n    'use_best_model': True,\n    'task_type': 'CPU',\n    'bootstrap_type': 'Bernoulli',\n    'grow_policy': 'SymmetricTree',\n    'loss_function': 'Logloss',\n    'eval_metric': 'AUC'\n}\n\nf_imp = pd.DataFrame({'feature': X.columns})\npredictions, scores = np.zeros(len(test)), []\n\nk = StratifiedKFold(n_splits=FOLDS, random_state=seed, shuffle=True)\nfor fold, (train_idx, val_idx) in enumerate(k.split(X_cb, y_cb)):    \n    cb_train = cb.Pool(data=X_cb.iloc[train_idx],\n                       label=y_cb.iloc[train_idx])\n    cb_valid = cb.Pool(data=X_cb.iloc[val_idx],\n                       label=y_cb.iloc[val_idx])\n    \n    model = cb.train(params=cb_params,\n                     dtrain=cb_train,\n                     num_boost_round=10000,\n                     evals=cb_valid, \n                     early_stopping_rounds=500,\n                     verbose=False)\n    \n    f_imp['fold_'+str(fold+1)] = model.get_feature_importance()\n    val_preds = model.predict(cb_valid)\n    val_score = roc_auc_score(y_cb.iloc[val_idx], val_preds)\n    scores.append(val_score)\n    \n    predictions += model.predict(test) / FOLDS\n    print(f'- FOLD {fold+1} AUC: {round(val_score, 4)} -')\n    \n    del cb_train, cb_valid, val_preds, val_score, model\n    gc.collect()\n\nprint('*'*45)\nprint(f'Mean AUC: {bold[0]}{round(np.mean(scores), 4)}{bold[1]}')\n\npredictions = norm_0to1(predictions)\ncb_preds = predictions.copy()\nf_importance_plot(f_imp)\npreds_plot(predictions)","metadata":{"execution":{"iopub.status.busy":"2023-01-16T23:32:59.685633Z","iopub.execute_input":"2023-01-16T23:32:59.686252Z","iopub.status.idle":"2023-01-16T23:35:05.879701Z","shell.execute_reply.started":"2023-01-16T23:32:59.686201Z","shell.execute_reply":"2023-01-16T23:35:05.878467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div style=\"background-color:#72bfd6;text-align:center;color:white;font-size:150%;font-family:Calibri;border-radius:10px\"> <b>Lasso</b></div>\n<p style=\"text-align:justify;\">\nAll the code was taken from <a href=\"https://www.kaggle.com/code/tilii7/modeling-stroke-dataset-with-lasso-regression\">here</a>! Many thanks to the author and, frankly, I would never have thought of such a thing myself. The only difference is that I added stroke patients from the original dataset and used StratifiedKFold. Upvote his work, because he reminded us an important thing that trees and neural networks are not always the best and sometimes we need to remember about simple models!\n</p>","metadata":{}},{"cell_type":"code","source":"lasso_preds = pd.read_csv('/kaggle/input/ps01-preds/lasso_skf.csv')['stroke']\nlasso_preds = norm_0to1(lasso_preds)\npreds_plot(lasso_preds)","metadata":{"execution":{"iopub.status.busy":"2023-01-16T23:35:05.882078Z","iopub.execute_input":"2023-01-16T23:35:05.882536Z","iopub.status.idle":"2023-01-16T23:35:06.260296Z","shell.execute_reply.started":"2023-01-16T23:35:05.882501Z","shell.execute_reply":"2023-01-16T23:35:06.258976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div style=\"background-color:#72bfd6;text-align:center;color:white;font-size:150%;font-family:Calibri;border-radius:10px\"> <b>NN</b></div>","metadata":{}},{"cell_type":"code","source":"sc = StandardScaler()\nX[num_cols] = sc.fit_transform(X[num_cols])\ntest[num_cols] = sc.transform(test[num_cols])\n\nX = X.values\ny = y.values\ntest = test.values","metadata":{"execution":{"iopub.status.busy":"2023-01-16T23:35:06.261999Z","iopub.execute_input":"2023-01-16T23:35:06.262406Z","iopub.status.idle":"2023-01-16T23:35:06.279638Z","shell.execute_reply.started":"2023-01-16T23:35:06.262371Z","shell.execute_reply":"2023-01-16T23:35:06.278482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def my_model():\n    x_input = Input(shape=(X.shape[-1]), name='input')\n    x1 = Dense(256, activation='relu')(x_input)\n    d1 = Dropout(0.1)(x1)\n    x2 = Dense(128, activation='relu')(d1)\n    d2 = Dropout(0.1)(x2)\n    x3 = Dense(64, activation='relu')(d2)\n    d3 = Dropout(0.1)(x3)\n    output = Dense(1, activation='sigmoid', name='output')(d3)\n    \n    model = Model(x_input, output, name='nn_model')\n    return model\n\nmodel = my_model()\nplot_model(model, to_file='nn_model.png', \n           show_shapes=True, show_layer_names=True)","metadata":{"execution":{"iopub.status.busy":"2023-01-16T23:35:06.281012Z","iopub.execute_input":"2023-01-16T23:35:06.281513Z","iopub.status.idle":"2023-01-16T23:35:07.61203Z","shell.execute_reply.started":"2023-01-16T23:35:06.28147Z","shell.execute_reply":"2023-01-16T23:35:07.610558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"VERBOSE = False\nBATCH_SIZE = 32\npredictions, scores = [], []\n\nlr = ReduceLROnPlateau(monitor='val_auc', factor=0.7, \n                       patience=5, verbose=VERBOSE)\n\nes = EarlyStopping(monitor='val_auc', patience=15, \n                   verbose=VERBOSE, mode='max', \n                   restore_best_weights=True)\n\nk = StratifiedKFold(n_splits=FOLDS, random_state=seed, shuffle=True)\nfor fold, (train_idx, val_idx) in enumerate(k.split(X, y)):\n    X_train, X_val = X[train_idx], X[val_idx]\n    y_train, y_val = y[train_idx], y[val_idx]\n        \n    model = my_model()\n    model.compile(optimizer='adam', \n                  loss=tfa.losses.SigmoidFocalCrossEntropy(alpha=0.1, gamma=3),\n                  metrics='AUC')\n        \n    model.fit(X_train, y_train, \n              validation_data=(X_val, y_val), \n              epochs=300,\n              verbose=VERBOSE,\n              batch_size=BATCH_SIZE, \n              class_weight={0: 1, 1: 10},\n              callbacks=[lr, es])\n        \n    y_pred = norm_0to1(model.predict(X_val))\n    val_score = roc_auc_score(y_val, y_pred)\n    scores.append(val_score)\n    \n    predictions.append(model.predict(test))\n    print(f'- FOLD {fold+1} AUC: {round(val_score, 4)} -')\n    \nprint(f'Mean AUC: {bold[0]}{round(np.mean(scores), 4)}{bold[1]}')\npredictions = norm_0to1(np.squeeze(np.mean(predictions, axis=0)))\nnn_preds = predictions.copy()\npreds_plot(predictions)","metadata":{"execution":{"iopub.status.busy":"2023-01-16T23:35:07.614279Z","iopub.execute_input":"2023-01-16T23:35:07.6147Z","iopub.status.idle":"2023-01-16T23:39:53.742709Z","shell.execute_reply.started":"2023-01-16T23:35:07.614658Z","shell.execute_reply":"2023-01-16T23:39:53.741821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div style=\"background-color:#72bfd6;text-align:center;color:white;font-size:150%;font-family:Calibri;border-radius:10px\"> <b>Blending</b></div>\n<p style=\"text-align:justify;\">\nYou may ask why I only used CatBoost? Because the results of this model turned out to be so much better than the results of LGBM and XGB. Interestingly, in the previous competition, CatBoost had the worst results compared to other trees, and the neural network (at least for me) produced very poor results. Here it's the opposite. It turned out to be a very unusual competition and I am sure that in a day the community will have a way to reach 0.895.\n</p>","metadata":{}},{"cell_type":"code","source":"ss['stroke'] = (cb_preds*0.7 + nn_preds*0.3)*0.4 + lasso_preds*0.6 \nss.to_csv('submission_.csv', index=False)\npreds_plot(ss['stroke'])","metadata":{"execution":{"iopub.status.busy":"2023-01-16T23:40:16.772085Z","iopub.execute_input":"2023-01-16T23:40:16.772599Z","iopub.status.idle":"2023-01-16T23:40:17.170742Z","shell.execute_reply.started":"2023-01-16T23:40:16.772558Z","shell.execute_reply":"2023-01-16T23:40:17.169709Z"},"trusted":true},"execution_count":null,"outputs":[]}]}