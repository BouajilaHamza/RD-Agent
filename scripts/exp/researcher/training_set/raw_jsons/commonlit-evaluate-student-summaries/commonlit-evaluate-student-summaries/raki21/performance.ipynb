{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Basics","metadata":{}},{"cell_type":"code","source":"MODE = 'test'\nPOSTPROCESS = True","metadata":{"execution":{"iopub.status.busy":"2023-10-11T17:41:52.512953Z","iopub.execute_input":"2023-10-11T17:41:52.513353Z","iopub.status.idle":"2023-10-11T17:41:52.525861Z","shell.execute_reply.started":"2023-10-11T17:41:52.513319Z","shell.execute_reply":"2023-10-11T17:41:52.524766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"# Native\nimport os\nimport re\nimport warnings\nimport tempfile\n\n# Torch: BSD\nimport torch                                      \nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Huggingface: Apache 2\nfrom transformers import AutoTokenizer, AutoConfig, AutoModelForSequenceClassification, DataCollatorWithPadding, TrainingArguments, Trainer\nfrom datasets import Dataset                      \n\nimport numpy as np                                # BSD\nimport pandas as pd                               # BSD\nfrom tqdm import tqdm                             # MIT\nimport lightgbm as lgb                            # MIT\n\n# Not needed for inference\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import mean_squared_error\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2023-10-11T17:41:52.528027Z","iopub.execute_input":"2023-10-11T17:41:52.528842Z","iopub.status.idle":"2023-10-11T17:42:11.023115Z","shell.execute_reply.started":"2023-10-11T17:41:52.528803Z","shell.execute_reply":"2023-10-11T17:42:11.022068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Base\nbase = '/kaggle/input/commonlit-evaluate-student-summaries/'\nprompts = pd.read_csv(base + f'prompts_{MODE}.csv')\nsummaries = pd.read_csv(base + f'summaries_{MODE}.csv')\ndf = prompts.merge(summaries, on='prompt_id') #[::100]","metadata":{"execution":{"iopub.status.busy":"2023-10-11T17:42:11.02503Z","iopub.execute_input":"2023-10-11T17:42:11.025636Z","iopub.status.idle":"2023-10-11T17:42:11.061893Z","shell.execute_reply.started":"2023-10-11T17:42:11.0256Z","shell.execute_reply":"2023-10-11T17:42:11.060864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Mark","metadata":{}},{"cell_type":"code","source":"#df = df[::10].reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2023-10-11T17:42:11.063473Z","iopub.execute_input":"2023-10-11T17:42:11.064068Z","iopub.status.idle":"2023-10-11T17:42:11.068788Z","shell.execute_reply.started":"2023-10-11T17:42:11.064033Z","shell.execute_reply":"2023-10-11T17:42:11.067493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_summary(row):\n    text = row['text']\n    clean_summary = re.sub(r'\\s+', ' ', text).strip()\n    return clean_summary\ndef clean_prompt_text(row):\n    text = row['prompt_text']\n    clean_prompt_text = re.sub(r'\\s+', ' ', text).strip()\n    return clean_prompt_text","metadata":{"execution":{"iopub.status.busy":"2023-10-11T17:42:11.071966Z","iopub.execute_input":"2023-10-11T17:42:11.072673Z","iopub.status.idle":"2023-10-11T17:42:11.08263Z","shell.execute_reply.started":"2023-10-11T17:42:11.072646Z","shell.execute_reply":"2023-10-11T17:42:11.081543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['clean_summary'] = df.apply(clean_summary, axis=1)\ndf['clean_prompt_text'] = df.apply(clean_prompt_text, axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-10-11T17:42:11.084304Z","iopub.execute_input":"2023-10-11T17:42:11.085151Z","iopub.status.idle":"2023-10-11T17:42:11.098969Z","shell.execute_reply.started":"2023-10-11T17:42:11.085106Z","shell.execute_reply":"2023-10-11T17:42:11.097845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to find common sequences\ndef find_common_sequences(text, prompt_text):\n    text_words = text.split()\n    prompt_words = prompt_text.split()\n    common_sequences = []\n    words_copied = 0\n    for length in range(min(len(prompt_words), len(text_words), 512), 2, -1):  # Start with the longest sequences and end with sequences of length 3\n        for start_idx in range(len(text_words) - length + 1):\n            sequence_words = text_words[start_idx:start_idx + length]\n            sequence = ' '.join(sequence_words)\n            if sequence in prompt_text:\n                common_sequences.append(sequence)\n                prompt_text = prompt_text.replace(sequence, '')  # Remove this sequence from prompt_text to avoid duplicate matches \n                words_copied += len(sequence.split())\n    return common_sequences, words_copied\n\ndef standardize_summary(row):\n    summary = row['clean_summary']\n    original = row['clean_prompt_text']  #row['prompt_title'] + ' ' + row['prompt_text'] + ' ' + row['prompt_question']\n    common_sequences, words_copied = find_common_sequences(summary, original)\n    standardized_summary = summary\n    for sequence in common_sequences:\n        replacement = f\"< {sequence} >\"  \n        standardized_summary = standardized_summary.replace(sequence, replacement)\n    \n    return standardized_summary, words_copied","metadata":{"execution":{"iopub.status.busy":"2023-10-11T17:42:11.100654Z","iopub.execute_input":"2023-10-11T17:42:11.101737Z","iopub.status.idle":"2023-10-11T17:42:11.111477Z","shell.execute_reply.started":"2023-10-11T17:42:11.101701Z","shell.execute_reply":"2023-10-11T17:42:11.110428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to find common sequences\ndef find_common_sequences(text, prompt_text):\n    common_sequences = []\n    words_total = len(text.split())\n    for length in range(min(len(prompt_text.split()), len(text.split()), 512), 2, -1):  # Start with the longest sequences and end with sequences of length 3\n        text_words = text.split()  # Move text_words inside the loop to ensure it's updated\n        prompt_words = prompt_text.split()\n        for start_idx in range(len(text_words) - length + 1):\n            sequence_words = text_words[start_idx:start_idx + length]\n            sequence = ' '.join(sequence_words)\n            if sequence in prompt_text:\n                common_sequences.append(sequence)\n                text = text.replace(sequence, '')  # Remove this sequence from text to avoid duplicate matches\n    words_copied = words_total - len(text.split()) # reduce by leftover after removal\n    return common_sequences, words_copied\n\ndef mark_summary(row):\n    summary = row['clean_summary']\n    original = row['clean_prompt_text'] # row['prompt_title'] + ' ' + row['clean_prompt_text'] + ' ' + row['clean_prompt_question'] # \n    common_sequences, words_copied = find_common_sequences(summary, original)\n    standardized_summary = summary\n    braces_summary = summary\n    for sequence in common_sequences:\n        replacement = f\"< {sequence} >\"  \n        standardized_summary = standardized_summary.replace(sequence, replacement)\n        \n        replacement_braces = f\"{ {sequence} }\"\n        braces_summary = braces_summary.replace(sequence, replacement_braces)\n    return braces_summary, standardized_summary, words_copied","metadata":{"execution":{"iopub.status.busy":"2023-10-11T17:42:11.113694Z","iopub.execute_input":"2023-10-11T17:42:11.114963Z","iopub.status.idle":"2023-10-11T17:42:11.129626Z","shell.execute_reply.started":"2023-10-11T17:42:11.114926Z","shell.execute_reply":"2023-10-11T17:42:11.128574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[['braces_summary', 'standardized_summary', 'words_copied']] = df.apply(mark_summary, axis=1, result_type='expand') ","metadata":{"execution":{"iopub.status.busy":"2023-10-11T17:42:11.131372Z","iopub.execute_input":"2023-10-11T17:42:11.131972Z","iopub.status.idle":"2023-10-11T17:42:11.150859Z","shell.execute_reply.started":"2023-10-11T17:42:11.131936Z","shell.execute_reply":"2023-10-11T17:42:11.14977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['len'] = df.text.apply(lambda x: len(x.split()))\ndf['rel_copy'] = df['words_copied']/df['len']","metadata":{"execution":{"iopub.status.busy":"2023-10-11T17:42:11.152587Z","iopub.execute_input":"2023-10-11T17:42:11.153237Z","iopub.status.idle":"2023-10-11T17:42:11.167481Z","shell.execute_reply.started":"2023-10-11T17:42:11.153183Z","shell.execute_reply":"2023-10-11T17:42:11.166296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Transformer","metadata":{}},{"cell_type":"markdown","source":"## Functions and Seeding","metadata":{}},{"cell_type":"code","source":"def seed_everything(seed: int): \n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True \nseed_everything(seed=42)","metadata":{"execution":{"iopub.status.busy":"2023-10-11T17:42:11.172871Z","iopub.execute_input":"2023-10-11T17:42:11.173191Z","iopub.status.idle":"2023-10-11T17:42:11.185582Z","shell.execute_reply.started":"2023-10-11T17:42:11.173164Z","shell.execute_reply":"2023-10-11T17:42:11.184428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ContentScoreRegressor:\n    def __init__(self, \n                model_name: str,\n                model_dir: str,\n                target: str,\n                hidden_dropout_prob: float,\n                attention_probs_dropout_prob: float,\n                max_length: int,\n                ):\n        self.input_col = \"input\" \n        self.text_cols = [self.input_col] \n        self.target = target\n        self.target_cols = [target]\n        self.model_name = model_name\n        self.model_dir = model_dir\n        self.max_length = max_length\n        self.tokenizer = AutoTokenizer.from_pretrained(f\"{model_name}\")  \n        self.model_config = AutoConfig.from_pretrained(f\"{model_name}\")\n        self.model_config.update({\n            \"hidden_dropout_prob\": hidden_dropout_prob,\n            \"attention_probs_dropout_prob\": attention_probs_dropout_prob,\n            \"num_labels\": 1,                                                         # 2 in 1! \n            \"problem_type\": \"regression\",\n        })   \n        seed_everything(seed=42)\n        self.data_collator = DataCollatorWithPadding(\n            tokenizer=self.tokenizer\n        )\n\n    def tokenize_function(self, examples: pd.DataFrame):\n        labels = [examples[self.target]]\n        tokenized = self.tokenizer(examples[self.input_col],\n                         padding=False,\n                         truncation=True,\n                         max_length=self.max_length)\n        return {\n            **tokenized,\n            \"labels\": labels,\n        }\n    \n    def tokenize_function_test(self, examples: pd.DataFrame):\n        tokenized = self.tokenizer(examples[self.input_col],\n                         padding=False,\n                         truncation=True,\n                         max_length=self.max_length)\n        return tokenized\n\n    def predict(self, \n                test_df: pd.DataFrame,\n                fold: int,\n               ):\n        sep = self.tokenizer.sep_token  \n        in_text = (\n            test_df[\"prompt_title\"] + sep \n            + test_df[\"prompt_question\"] + sep \n            + test_df[summary_input]\n                  )\n\n        test_df[self.input_col] = in_text\n        test_ = test_df[[self.input_col]][test_df.fold == fold]             \n        test_dataset = Dataset.from_pandas(test_, preserve_index=False) \n        test_tokenized_dataset = test_dataset.map(self.tokenize_function_test, batched=False)\n        model = AutoModelForSequenceClassification.from_pretrained(f\"{self.model_dir}\")\n        model.eval()\n        model_fold_dir = os.path.join(self.model_dir, str(fold)) \n        \n        test_args = TrainingArguments(\n            output_dir=tempfile.mkdtemp(),\n            do_train = False,\n            do_predict = True,\n            per_device_eval_batch_size = CFG.eval_batch,   # SWITCHED SO IT WORKS BEST WITH CPU\n            dataloader_drop_last = False,\n            logging_dir=tempfile.mkdtemp(),\n            report_to=[],  # Disable all reporting\n        )\n        infer_content = Trainer(\n                      model = model, \n                      tokenizer=self.tokenizer,\n                      data_collator=self.data_collator,\n                      args = test_args)\n\n        preds = infer_content.predict(test_tokenized_dataset)[0]\n        return preds","metadata":{"execution":{"iopub.status.busy":"2023-10-11T17:42:11.187521Z","iopub.execute_input":"2023-10-11T17:42:11.18836Z","iopub.status.idle":"2023-10-11T17:42:11.204489Z","shell.execute_reply.started":"2023-10-11T17:42:11.188321Z","shell.execute_reply":"2023-10-11T17:42:11.203414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(\n    test_df: pd.DataFrame,\n    target:str,\n    model_name: str,\n    hidden_dropout_prob: float,\n    attention_probs_dropout_prob: float,\n    max_length : int\n    ):\n    if MODE == 'test':\n        test_df[f\"{target}_p{ENSEMBLE}\"] = 0\n    \n    for fold in range(4):\n        print(f\"fold {fold}:\")\n        for model_file in os.listdir(f\"{model_name}\"):\n            if f\"fold_{fold}_{target}\" in model_file:\n                model_dir =  os.path.join(f\"{model_name}\", model_file)\n        csr = ContentScoreRegressor(\n            model_name=model_name,\n            target=target,\n            model_dir = model_dir, \n            hidden_dropout_prob=hidden_dropout_prob,\n            attention_probs_dropout_prob=attention_probs_dropout_prob,\n            max_length=max_length,\n           )\n        if MODE == 'train':\n            pred = csr.predict(\n                test_df=test_df, \n                fold=fold\n            )\n            test_df.loc[test_df.fold == fold, f\"{target}_p{ENSEMBLE}\"] = pred\n        else: \n            pred = csr.predict(\n                test_df=test_df, \n                fold=0\n            ) \n            test_df[f\"{target}_p{ENSEMBLE}\"] += pred.flatten()\n    if MODE == 'test':\n        test_df[f\"{target}_p{ENSEMBLE}\"] /= 4\n        \n    return test_df","metadata":{"execution":{"iopub.status.busy":"2023-10-11T17:42:11.206176Z","iopub.execute_input":"2023-10-11T17:42:11.206898Z","iopub.status.idle":"2023-10-11T17:42:11.222957Z","shell.execute_reply.started":"2023-10-11T17:42:11.206863Z","shell.execute_reply":"2023-10-11T17:42:11.221764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ContentScoreRegressorBoth:\n    def __init__(self, \n                model_name: str,\n                model_dir: str,\n                target1: str,\n                target2: str,\n                hidden_dropout_prob: float,\n                attention_probs_dropout_prob: float,\n                max_length: int,\n                ):\n        self.input_col = \"input\" \n        self.text_cols = [self.input_col] \n        self.target1 = target1\n        self.target2 = target2\n        self.target_cols = [target1, target2]\n        self.model_name = model_name\n        self.model_dir = model_dir\n        self.max_length = max_length\n        self.tokenizer = AutoTokenizer.from_pretrained(f\"{model_name}\")  \n        self.model_config = AutoConfig.from_pretrained(f\"{model_name}\")\n        self.model_config.update({\n            \"hidden_dropout_prob\": hidden_dropout_prob,\n            \"attention_probs_dropout_prob\": attention_probs_dropout_prob,\n            \"num_labels\": 2,  # Updated to 2 for two targets\n            \"problem_type\": \"regression\",\n        })   \n        seed_everything(seed=42)\n        self.data_collator = DataCollatorWithPadding(\n            tokenizer=self.tokenizer\n        )\n\n    def tokenize_function(self, examples: pd.DataFrame):\n        labels = [examples[self.target1], examples[self.target2]]   # Updated to include both targets\n        tokenized = self.tokenizer(examples[self.input_col],\n                         padding=False,\n                         truncation=True,\n                         max_length=self.max_length)\n        return {\n            **tokenized,\n            \"labels\": labels,\n        }\n    \n    def tokenize_function_test(self, examples: pd.DataFrame):\n        tokenized = self.tokenizer(examples[self.input_col],\n                         padding=False,\n                         truncation=True,\n                         max_length=self.max_length)\n        return tokenized\n        \n    def predict(self, \n                test_df: pd.DataFrame,\n                fold: int,\n               ):\n        sep = self.tokenizer.sep_token  \n\n        in_text = (\n            test_df[\"prompt_title\"] + sep \n            + test_df[\"prompt_question\"] + sep \n            + test_df[summary_input]\n                  )\n \n        test_df[self.input_col] = in_text\n        test_ = test_df[[self.input_col]][test_df.fold == fold]             \n        test_dataset = Dataset.from_pandas(test_, preserve_index=False) \n        test_tokenized_dataset = test_dataset.map(self.tokenize_function_test, batched=False)\n        model = AutoModelForSequenceClassification.from_pretrained(f\"{self.model_dir}\")\n        model.eval()\n        \n        model_fold_dir = os.path.join(self.model_dir, str(fold)) \n        test_args = TrainingArguments(\n            output_dir=tempfile.mkdtemp(),\n            do_train = False,\n            do_predict = True,\n            per_device_eval_batch_size = CFG.eval_batch,   # SWITCHED SO IT WORKS BEST WITH CPU\n            dataloader_drop_last = False,\n            logging_dir=tempfile.mkdtemp(),\n            report_to=[],  # Disable all reporting\n        )\n        infer_content = Trainer(\n                      model = model, \n                      tokenizer=self.tokenizer,\n                      data_collator=self.data_collator,\n                      args = test_args)\n        \n        preds = infer_content.predict(test_tokenized_dataset)[0]\n        return preds","metadata":{"execution":{"iopub.status.busy":"2023-10-11T17:42:11.224879Z","iopub.execute_input":"2023-10-11T17:42:11.225753Z","iopub.status.idle":"2023-10-11T17:42:11.241747Z","shell.execute_reply.started":"2023-10-11T17:42:11.225719Z","shell.execute_reply":"2023-10-11T17:42:11.240592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_both(\n    test_df: pd.DataFrame,\n    target1: str,\n    target2: str,\n    model_name: str,\n    hidden_dropout_prob: float,\n    attention_probs_dropout_prob: float,\n    max_length : int\n    ):\n    if MODE == 'test':\n        test_df[f\"{target1}_p{ENSEMBLE}\"] = 0\n        test_df[f\"{target2}_p{ENSEMBLE}\"] = 0\n    for fold in range(4):\n        print(f\"fold {fold}:\")\n        for model_file in os.listdir(f\"{model_name}\"):\n            if f\"fold_{fold}_\" in model_file:\n                model_dir =  os.path.join(f\"{model_name}\", model_file)\n        csr = ContentScoreRegressorBoth(\n            model_name=model_name,\n            target1=target1,\n            target2=target2,\n            model_dir=model_dir, \n            hidden_dropout_prob=hidden_dropout_prob,\n            attention_probs_dropout_prob=attention_probs_dropout_prob,\n            max_length=max_length,\n           )\n        if MODE == 'train':\n            pred = csr.predict(\n                test_df=test_df, \n                fold=fold\n            )\n            test_df.loc[test_df.fold == fold, f\"{target1}_p{ENSEMBLE}\"] = pred[:, 0]\n            test_df.loc[test_df.fold == fold, f\"{target2}_p{ENSEMBLE}\"] = pred[:, 1]\n        else: \n            pred = csr.predict(\n                test_df=test_df, \n                fold=0\n            ) \n            test_df[f\"{target1}_p{ENSEMBLE}\"] += pred[:, 0].flatten()\n            test_df[f\"{target2}_p{ENSEMBLE}\"] += pred[:, 1].flatten()\n    if MODE == 'test':\n        test_df[f\"{target1}_p{ENSEMBLE}\"] /= 4\n        test_df[f\"{target2}_p{ENSEMBLE}\"] /= 4\n        \n    return test_df","metadata":{"execution":{"iopub.status.busy":"2023-10-11T17:42:11.243591Z","iopub.execute_input":"2023-10-11T17:42:11.24437Z","iopub.status.idle":"2023-10-11T17:42:11.260792Z","shell.execute_reply.started":"2023-10-11T17:42:11.244328Z","shell.execute_reply":"2023-10-11T17:42:11.259596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predict","metadata":{}},{"cell_type":"code","source":"if MODE == 'train':\n    gkf = GroupKFold(n_splits=4)\n    for i, (_, val_index) in enumerate(gkf.split(df, groups=df[\"prompt_id\"])):\n        df.loc[val_index, \"fold\"] = i\nelse:\n    df['fold'] = 0","metadata":{"execution":{"iopub.status.busy":"2023-10-11T17:42:11.262506Z","iopub.execute_input":"2023-10-11T17:42:11.263283Z","iopub.status.idle":"2023-10-11T17:42:11.279448Z","shell.execute_reply.started":"2023-10-11T17:42:11.263246Z","shell.execute_reply":"2023-10-11T17:42:11.278235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    max_length=512\n    eval_batch=8\nsummary_input = 'standardized_summary'","metadata":{"execution":{"iopub.status.busy":"2023-10-11T17:42:11.281107Z","iopub.execute_input":"2023-10-11T17:42:11.28175Z","iopub.status.idle":"2023-10-11T17:42:11.292904Z","shell.execute_reply.started":"2023-10-11T17:42:11.281715Z","shell.execute_reply":"2023-10-11T17:42:11.29188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name=\"/kaggle/input/commonlit-big\"\nENSEMBLE = 0\nfor global_target in ['content','wording']:\n    df = predict(\n        df,\n        target=global_target,\n        model_name=model_name,\n        hidden_dropout_prob=0,\n        attention_probs_dropout_prob=0,\n        max_length=CFG.max_length\n    )","metadata":{"execution":{"iopub.status.busy":"2023-10-11T17:42:11.294601Z","iopub.execute_input":"2023-10-11T17:42:11.295305Z","iopub.status.idle":"2023-10-11T17:44:35.426632Z","shell.execute_reply.started":"2023-10-11T17:42:11.295267Z","shell.execute_reply":"2023-10-11T17:44:35.42562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name=\"/kaggle/input/commonlit-v3-large-freeze8clean\"\nENSEMBLE = 1\nfor global_target in ['content','wording']:\n    df = predict(\n        df,\n        target=global_target,\n        model_name=model_name,\n        hidden_dropout_prob=0,\n        attention_probs_dropout_prob=0,\n        max_length=CFG.max_length\n    )","metadata":{"execution":{"iopub.status.busy":"2023-10-11T17:44:35.428296Z","iopub.execute_input":"2023-10-11T17:44:35.428881Z","iopub.status.idle":"2023-10-11T17:46:46.072627Z","shell.execute_reply.started":"2023-10-11T17:44:35.428847Z","shell.execute_reply":"2023-10-11T17:46:46.071544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name=\"/kaggle/input/v3large-freeze8-remove-summary\"\nENSEMBLE = 2\nfor global_target in ['content','wording']:\n    df = predict(\n        df,\n        target=global_target,\n        model_name=model_name,\n        hidden_dropout_prob=0,\n        attention_probs_dropout_prob=0,\n        max_length=CFG.max_length\n    )","metadata":{"execution":{"iopub.status.busy":"2023-10-11T17:46:46.074578Z","iopub.execute_input":"2023-10-11T17:46:46.075211Z","iopub.status.idle":"2023-10-11T17:48:58.970057Z","shell.execute_reply.started":"2023-10-11T17:46:46.075173Z","shell.execute_reply":"2023-10-11T17:48:58.968994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name=\"/kaggle/input/cv3large-more-regularization-freeze68\"\nENSEMBLE = 3\nfor global_target in ['content','wording']:\n    df = predict(\n        df,\n        target=global_target,\n        model_name=model_name,\n        hidden_dropout_prob=0,\n        attention_probs_dropout_prob=0,\n        max_length=CFG.max_length\n    )","metadata":{"execution":{"iopub.status.busy":"2023-10-11T17:48:58.97204Z","iopub.execute_input":"2023-10-11T17:48:58.972418Z","iopub.status.idle":"2023-10-11T17:51:08.972351Z","shell.execute_reply.started":"2023-10-11T17:48:58.972384Z","shell.execute_reply":"2023-10-11T17:51:08.971251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summary_input = 'braces_summary' \nmodel_name=\"/kaggle/input/v3l-f5-braces-both\"\nENSEMBLE = 4\ndf = predict_both(\n    df,\n    target1='content',\n    target2='wording',\n    model_name=model_name,\n    hidden_dropout_prob=0,\n    attention_probs_dropout_prob=0,\n    max_length=CFG.max_length\n)","metadata":{"execution":{"iopub.status.busy":"2023-10-11T17:51:08.97391Z","iopub.execute_input":"2023-10-11T17:51:08.974965Z","iopub.status.idle":"2023-10-11T17:52:14.08073Z","shell.execute_reply.started":"2023-10-11T17:51:08.974929Z","shell.execute_reply":"2023-10-11T17:52:14.079662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summary_input = 'braces_summary' \nmodel_name=\"/kaggle/input/v3l-f8-braces-both\"\nENSEMBLE = 5\ndf = predict_both(\n    df,\n    target1='content',\n    target2='wording',\n    model_name=model_name,\n    hidden_dropout_prob=0,\n    attention_probs_dropout_prob=0,\n    max_length=CFG.max_length\n)","metadata":{"execution":{"iopub.status.busy":"2023-10-11T17:52:14.08218Z","iopub.execute_input":"2023-10-11T17:52:14.082538Z","iopub.status.idle":"2023-10-11T17:53:18.435909Z","shell.execute_reply.started":"2023-10-11T17:52:14.082504Z","shell.execute_reply":"2023-10-11T17:53:18.434793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if False:\n    summary_input = 'braces_summary' \n    model_name=\"/kaggle/input/v3l-f7-braces-both\"\n    ENSEMBLE = 6\n    df = predict_both(\n        df,\n        target1='content',\n        target2='wording',\n        model_name=model_name,\n        hidden_dropout_prob=0,\n        attention_probs_dropout_prob=0,\n        max_length=CFG.max_length\n    )","metadata":{"execution":{"iopub.status.busy":"2023-10-11T17:53:18.43789Z","iopub.execute_input":"2023-10-11T17:53:18.438628Z","iopub.status.idle":"2023-10-11T17:53:18.444386Z","shell.execute_reply.started":"2023-10-11T17:53:18.43859Z","shell.execute_reply":"2023-10-11T17:53:18.443182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summary_input = 'braces_summary' \nmodel_name=\"/kaggle/input/v3l-f12-braces-both\"\nENSEMBLE = 7\ndf = predict_both(\n    df,\n    target1='content',\n    target2='wording',\n    model_name=model_name,\n    hidden_dropout_prob=0,\n    attention_probs_dropout_prob=0,\n    max_length=CFG.max_length\n)","metadata":{"execution":{"iopub.status.busy":"2023-10-11T17:53:18.446Z","iopub.execute_input":"2023-10-11T17:53:18.446762Z","iopub.status.idle":"2023-10-11T17:54:24.404578Z","shell.execute_reply.started":"2023-10-11T17:53:18.44671Z","shell.execute_reply":"2023-10-11T17:54:24.399282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ensembles = [0,1,2,3,4,5,7]\n\ndf['content_mean'] = 0\ndf['wording_mean'] = 0\nfor ENSEMBLE in ensembles:\n    df['content_mean'] += df[f'content_p{ENSEMBLE}'] / len(ensembles)\n    df['wording_mean'] += df[f'wording_p{ENSEMBLE}'] / len(ensembles)\n    \n    \nif MODE == 'train':\n    for ENSEMBLE in ensembles:\n        content_rmse = np.sqrt(((df['content'] - df[f'content_p{ENSEMBLE}']) ** 2).mean())\n        wording_rmse = np.sqrt(((df['wording'] - df[f'wording_p{ENSEMBLE}']) ** 2).mean())\n\n        rmse = (content_rmse + wording_rmse)/2\n        #print(f'{content_rmse}\\n{wording_rmse}\\n{rmse}')\n\n        df['content_dif'] = df['content'] - df[f'content_p{ENSEMBLE}']\n        df['wording_dif'] = df['wording'] - df[f'wording_p{ENSEMBLE}']\n\n    content_rmse = np.sqrt(((df['content'] - df[f'content_mean']) ** 2).mean())\n    wording_rmse = np.sqrt(((df['wording'] - df[f'wording_mean']) ** 2).mean())\n\n    rmse = (content_rmse + wording_rmse)/2\n    print(f'{content_rmse}\\n{wording_rmse}\\n{rmse}')\n\n    df['content_dif'] = df['content'] - df[f'content_mean']\n    df['wording_dif'] = df['wording'] - df[f'wording_mean']","metadata":{"execution":{"iopub.status.busy":"2023-10-11T17:54:24.406592Z","iopub.execute_input":"2023-10-11T17:54:24.407249Z","iopub.status.idle":"2023-10-11T17:54:24.425062Z","shell.execute_reply.started":"2023-10-11T17:54:24.407194Z","shell.execute_reply":"2023-10-11T17:54:24.423692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"    df::10 with commonlit-big trained with marking and removal from prompt and freeze8clean trained on removal from summary\n    with removal from prompt\n    0.420812660489145\n    0.5436554022594821\n    0.48223403137431353\n    0.4225474840718467\n    0.5390831076476523\n    0.4808152958597495\n    0.41635675494060786\n    0.53341817650472\n    0.47488746572266394\n    \n    With removal from summary:\n    0.4145619676413469\n    0.5405258671127942\n    0.4775439173770705\n    0.4209995014473389\n    0.5328895652573283\n    0.4769445333523336\n    0.4117069578676729\n    0.5292576582820044\n    0.4704823080748387\n    \n    Full\n    0.4800887487439707  0 (single + <>)\n    0.4846268136752474  1\n    0.4804681110307913  2\n    0.4815882008131167  3\n    0.4794552627000645  4 (both + braces)\n    0.4810434541529361  5 \n    \n    \n    0.4755065724485361  0+1\n    0.4729761046873674  0-2\n    0.4713179620663737  0-3\n    0.4677167678019531  0-4\n    0.4666305670723127  0-5 postprocess: 0.4636365941271711 Faulty!\n    0.4658944885731245  0,1,2,3,4,5,7 non grouped lgbm: 0.45228009742238695\n    \n    => Noarmilized features and mean removed content and wording difference across folds lgb: \n    mcrmse : 0.46427082773567485, content: 0.40641374126603336, wording: 0.5221279142053163","metadata":{}},{"cell_type":"code","source":"df['content_prediction'] = df.content_mean\ndf['wording_prediction'] = df.wording_mean","metadata":{"execution":{"iopub.status.busy":"2023-10-11T17:54:24.427047Z","iopub.execute_input":"2023-10-11T17:54:24.427627Z","iopub.status.idle":"2023-10-11T17:54:24.4421Z","shell.execute_reply.started":"2023-10-11T17:54:24.427583Z","shell.execute_reply":"2023-10-11T17:54:24.440958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Post-Process","metadata":{}},{"cell_type":"code","source":"if MODE == 'train':\n    content_rmse = np.sqrt(((df['content'] - df['content_prediction']) ** 2).mean())\n    wording_rmse = np.sqrt(((df['wording'] - df['wording_prediction']) ** 2).mean())\n    rmse = (content_rmse + wording_rmse)/2\n    print(f'{content_rmse}\\n{wording_rmse}\\n{rmse}')","metadata":{"execution":{"iopub.status.busy":"2023-10-11T17:54:24.443806Z","iopub.execute_input":"2023-10-11T17:54:24.444452Z","iopub.status.idle":"2023-10-11T17:54:24.45661Z","shell.execute_reply.started":"2023-10-11T17:54:24.444414Z","shell.execute_reply":"2023-10-11T17:54:24.455375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if POSTPROCESS:\n    prompt_count = len(df.prompt_id.unique())\n    if MODE == 'test':\n        gkf = GroupKFold(n_splits=prompt_count)\n        for i, (_, val_index) in enumerate(gkf.split(df, groups=df[\"prompt_id\"])):\n            df.loc[val_index, \"fold\"] = i","metadata":{"execution":{"iopub.status.busy":"2023-10-11T17:54:24.462957Z","iopub.execute_input":"2023-10-11T17:54:24.463352Z","iopub.status.idle":"2023-10-11T17:54:24.476897Z","shell.execute_reply.started":"2023-10-11T17:54:24.463311Z","shell.execute_reply":"2023-10-11T17:54:24.475605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if POSTPROCESS:\n    model_name = '/kaggle/input/v3-large-base'\n    tokenizer = AutoTokenizer.from_pretrained(model_name) \n    tokenized = tokenizer(df['braces_summary'].tolist())\n    token_counts = [len(toks) for toks in tokenized['input_ids']]\n    df['token_count'] = token_counts\n    df['copy_description'] = df.text.apply(lambda x: 'take notes on' in x)","metadata":{"execution":{"iopub.status.busy":"2023-10-11T17:54:24.478749Z","iopub.execute_input":"2023-10-11T17:54:24.479652Z","iopub.status.idle":"2023-10-11T17:54:24.80165Z","shell.execute_reply.started":"2023-10-11T17:54:24.479601Z","shell.execute_reply":"2023-10-11T17:54:24.800605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    model_name= '/kaggle/input/deberta-v3-large'\n    hidden_dropout_prob=0        \n    attention_probs_dropout_prob=0 \n    max_length=512\ndevice = 'cuda'","metadata":{"execution":{"iopub.status.busy":"2023-10-11T17:54:24.803057Z","iopub.execute_input":"2023-10-11T17:54:24.804002Z","iopub.status.idle":"2023-10-11T17:54:24.809489Z","shell.execute_reply.started":"2023-10-11T17:54:24.803968Z","shell.execute_reply":"2023-10-11T17:54:24.808417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if POSTPROCESS:\n    df = df.sort_values(by='fold', kind='mergesort').reset_index(drop=True)\n    outputs = []\n    model = AutoModelForSequenceClassification.from_pretrained(f\"{model_name}\") #/fold_{0}_{N_FREEZE_LAYER}\")\n    model.eval().cuda()\n    for fold in range(prompt_count):\n        df_fold = df[df.fold==fold]\n\n        # Tokenization\n        sep = tokenizer.sep_token  \n        in_text = df_fold['text']\n        inputs = []\n        for text in in_text:\n            tokens = tokenizer(text, padding=False, truncation=True, return_tensors=\"pt\", max_length=CFG.max_length)\n            inputs.append(tokens['input_ids'])\n\n        # Inference\n        for i in inputs:\n            with torch.no_grad():\n                out = model(i.cuda(), output_hidden_states=True)['hidden_states']\n                outputs.append(np.array([np.array(h.cpu()).squeeze().mean(axis=0) for h in out]))\n    outputs = np.array(outputs)\n    pt_out = []\n    for fold in range(prompt_count):\n        df_fold = df[df.fold==fold]\n\n        # Tokenization\n        for text in (df_fold[\"prompt_title\"] + sep + df_fold[\"prompt_text\"] + sep + df_fold[\"prompt_question\"]):\n            tokens = tokenizer(text, padding=False, truncation=True, return_tensors=\"pt\", max_length=2048)\n            with torch.no_grad():\n                out = model(tokens['input_ids'].cuda(), output_hidden_states=True)['hidden_states']\n                pt_out.append(np.array([np.array(h.cpu()).squeeze().mean(axis=0) for h in out]))\n            break\n    pt_out = np.array(pt_out).squeeze()","metadata":{"execution":{"iopub.status.busy":"2023-10-11T17:54:24.811032Z","iopub.execute_input":"2023-10-11T17:54:24.811426Z","iopub.status.idle":"2023-10-11T17:54:34.982138Z","shell.execute_reply.started":"2023-10-11T17:54:24.81139Z","shell.execute_reply":"2023-10-11T17:54:34.981064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if POSTPROCESS:\n    df = df.sort_values(by='fold', kind='mergesort').reset_index(drop=True)\n    NORMALIZE = True\n    for LAYER in range(25):\n        reduced_features = outputs[:,LAYER]\n        if NORMALIZE:\n            reduced_features /= np.linalg.norm(reduced_features, axis=1, keepdims=True)\n        prompt_text_pca = pt_out[:,LAYER]\n        if NORMALIZE:\n            prompt_text_pca /= np.linalg.norm(prompt_text_pca, axis=1, keepdims=True)\n        cosine_sims = np.zeros(len(df))\n        start = 0\n        for fold in range(prompt_count):\n            l = len(df[df.fold==fold])\n            cosine_fold = (reduced_features[start:start+l] * prompt_text_pca[fold]).sum(axis=1)\n            cosine_sims[start:start+l] = cosine_fold\n            start += l\n        df[f'cos_sim{LAYER}'] = cosine_sims ","metadata":{"execution":{"iopub.status.busy":"2023-10-11T17:54:34.983676Z","iopub.execute_input":"2023-10-11T17:54:34.984277Z","iopub.status.idle":"2023-10-11T17:54:35.037005Z","shell.execute_reply.started":"2023-10-11T17:54:34.984224Z","shell.execute_reply":"2023-10-11T17:54:35.036019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if POSTPROCESS:\n    # Find stuff that works for postprocessing\n    to_normalized = ['rel_copy', 'token_count'] + [f'cos_sim{layer}' for layer in range(25)] \n    for fold in range(prompt_count):\n        fold_mask = df.fold == fold\n        for column in to_normalized:\n            df.loc[fold_mask, f'{column}_normalized'] = (df.loc[fold_mask, column] - df.loc[fold_mask, column].mean()) / df.loc[fold_mask, column].std()\n        [f'cos_sim{layer}' for layer in range(25)] \n    if MODE == 'train':\n        df['content_dif'] = df['content'] - df[f'content_mean']\n        df['wording_dif'] = df['wording'] - df[f'wording_mean']\n        to_normalized = ['content_dif', 'wording_dif']\n        for fold in range(prompt_count):\n            fold_mask = df.fold == fold\n            for column in to_normalized:\n                df.loc[fold_mask, f'{column}_normalized'] = (df.loc[fold_mask, column] - df.loc[fold_mask, column].mean())","metadata":{"execution":{"iopub.status.busy":"2023-10-11T17:54:35.038097Z","iopub.execute_input":"2023-10-11T17:54:35.038488Z","iopub.status.idle":"2023-10-11T17:54:35.135278Z","shell.execute_reply.started":"2023-10-11T17:54:35.038456Z","shell.execute_reply":"2023-10-11T17:54:35.134038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_lgb(target, drop_columns, l1=0.1, l2=0.1, max_depth=3, lr=0.1, display=False, save=False): \n    # Training\n    model_dict = {}  \n    for target in targets:\n        models = []  \n        for fold in range(4):\n            X_train_cv = df[df[\"fold\"] != fold].drop(columns=drop_columns)\n            y_train_cv = df[df[\"fold\"] != fold][target]\n            X_eval_cv = df[df[\"fold\"] == fold].drop(columns=drop_columns)\n            y_eval_cv = df[df[\"fold\"] == fold][target]\n            dtrain = lgb.Dataset(X_train_cv, label=y_train_cv)\n            dval = lgb.Dataset(X_eval_cv, label=y_eval_cv)\n            params = {\n                'boosting_type': 'gbdt',\n                'random_state': 42,\n                'objective': 'regression',\n                'metric': 'rmse',\n                'learning_rate': lr,\n                'max_depth': max_depth, \n                'lambda_l1': l1,\n                'lambda_l2': l2,\n                'verbose': -1\n            }\n            evaluation_results = {}\n            model = lgb.train(params,\n                              num_boost_round=1000,\n                              valid_names=['train', 'valid'],\n                              train_set=dtrain,\n                              valid_sets=[dtrain, dval],\n                              callbacks=[\n                                  lgb.early_stopping(stopping_rounds=30, verbose=True),\n                                   lgb.log_evaluation(100),\n                                  lgb.callback.record_evaluation(evaluation_results)\n                                ],\n                              )\n            models.append(model)\n        model_dict[target] = models\n        \n    # Inference\n    rmses = []\n    for target in ['content_dif','wording_dif']:\n        models = model_dict[f\"{target}_normalized\"]\n        preds = []\n        trues = []\n        for fold, model in enumerate(models):\n            if save:\n                model.save_model(f\"{target}_model_{fold}.txt\")\n            X_eval_cv = df[df[\"fold\"] == fold].drop(columns=drop_columns)\n            y_eval_cv = df[df[\"fold\"] == fold][target]\n            pred = model.predict(X_eval_cv)  \n            trues.extend(y_eval_cv)\n            preds.extend(pred)\n        rmse = np.sqrt(((np.array(trues)-np.array(preds))**2).mean())\n        print(f\"{target}_rmse : {rmse}\")\n        rmses = rmses + [rmse]\n    print(f\"mcrmse : {sum(rmses) / len(rmses)}, content: {rmses[0]}, wording: {rmses[1]}\")\n    if display:\n        for target, models in model_dict.items():\n            for i, model in enumerate(models):\n                lgb.plot_importance(model, importance_type='split')\n                plt.title(f'Model for target {target}, fold {i}')\n                plt.show()\n    return sum(rmses) / len(rmses)","metadata":{"execution":{"iopub.status.busy":"2023-10-11T17:54:35.137121Z","iopub.execute_input":"2023-10-11T17:54:35.137544Z","iopub.status.idle":"2023-10-11T17:54:35.152486Z","shell.execute_reply.started":"2023-10-11T17:54:35.137503Z","shell.execute_reply":"2023-10-11T17:54:35.151187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if POSTPROCESS:\n    targets = ['content_dif_normalized', 'wording_dif_normalized']\n    if MODE == 'train':\n        drop_columns = ['prompt_id', 'prompt_question', 'prompt_title', 'prompt_text', 'student_id', 'text', \n                        'clean_summary', 'clean_prompt_text', 'braces_summary', 'standardized_summary',\n                        'fold', 'input', 'content_prediction', 'wording_prediction',\n                        'content_dif', 'wording_dif', \n                        'content', 'wording', \n                       ] + targets \n        targets = ['content_dif_normalized', 'wording_dif_normalized']\n        train_lgb(targets, drop_columns, l1=0.1, l2=0.25, max_depth=15, lr=0.07, display=False, save=True)\n    else:\n        drop_columns = ['prompt_id', 'prompt_question', 'prompt_title', 'prompt_text', 'student_id', 'text', \n                        'clean_summary', 'clean_prompt_text', 'braces_summary', 'standardized_summary',\n                        'fold', 'input', 'content_prediction', 'wording_prediction'] ","metadata":{"execution":{"iopub.status.busy":"2023-10-11T17:54:35.154283Z","iopub.execute_input":"2023-10-11T17:54:35.155373Z","iopub.status.idle":"2023-10-11T17:54:35.170567Z","shell.execute_reply.started":"2023-10-11T17:54:35.155334Z","shell.execute_reply":"2023-10-11T17:54:35.16921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if POSTPROCESS:\n    model_dict = {}\n    for target in ['content_dif','wording_dif']:\n        models = []\n        for fold in range(4):\n            model = lgb.Booster(model_file=f\"/kaggle/input/ensemble-lgb-fold-corrected/{target}_model_{fold}.txt\")\n            models.append(model)\n        model_dict[target] = models\n\n    if MODE == 'train':\n        rmses = []\n        for target in ['content_dif', 'wording_dif']:\n            models = model_dict[f\"{target}\"]\n            preds = []\n            trues = []\n            for fold, model in enumerate(models):\n                X_eval_cv = df[df[\"fold\"] == fold].drop(columns=drop_columns)\n                y_eval_cv = df[df[\"fold\"] == fold][target]\n                pred = model.predict(X_eval_cv)\n                trues.extend(y_eval_cv)\n                preds.extend(pred)\n            rmse = np.sqrt(((np.array(trues) - np.array(preds)) ** 2).mean())\n            print(f\"{target}_rmse : {rmse}\")\n            rmses = rmses + [rmse]\n        print(f\"mcrmse : {sum(rmses) / len(rmses)}, content: {rmses[0]}, wording: {rmses[1]}\")\n    else:\n        for target in ['content', 'wording']:\n            models = model_dict[f\"{target}_dif\"]\n            preds = np.zeros(len(df))\n            for fold, model in enumerate(models):\n                X_eval_cv = df.drop(columns=drop_columns)\n                pred = model.predict(X_eval_cv)\n                preds += pred / 4\n            df[f'{target}_prediction'] += preds","metadata":{"execution":{"iopub.status.busy":"2023-10-11T17:54:35.172369Z","iopub.execute_input":"2023-10-11T17:54:35.17344Z","iopub.status.idle":"2023-10-11T17:54:35.344025Z","shell.execute_reply.started":"2023-10-11T17:54:35.173399Z","shell.execute_reply":"2023-10-11T17:54:35.342843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df = df[['student_id', 'content_prediction', 'wording_prediction']].copy()\nsub_df.rename(columns={'content_prediction': 'content', 'wording_prediction': 'wording'}, inplace=True)\nsub_df","metadata":{"execution":{"iopub.status.busy":"2023-10-11T17:54:35.345814Z","iopub.execute_input":"2023-10-11T17:54:35.346207Z","iopub.status.idle":"2023-10-11T17:54:35.365786Z","shell.execute_reply.started":"2023-10-11T17:54:35.346167Z","shell.execute_reply":"2023-10-11T17:54:35.364664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-10-11T17:54:35.367195Z","iopub.execute_input":"2023-10-11T17:54:35.368207Z","iopub.status.idle":"2023-10-11T17:54:35.378949Z","shell.execute_reply.started":"2023-10-11T17:54:35.368166Z","shell.execute_reply":"2023-10-11T17:54:35.3774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}