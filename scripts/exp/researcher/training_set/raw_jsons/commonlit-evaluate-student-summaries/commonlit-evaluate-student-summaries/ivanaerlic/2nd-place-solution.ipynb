{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig\n\nimport sys\nsys.path.append('/kaggle/input/ess-utilities')\nimport utilities\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2023-10-12T11:59:51.866909Z","iopub.execute_input":"2023-10-12T11:59:51.867345Z","iopub.status.idle":"2023-10-12T11:59:59.711793Z","shell.execute_reply.started":"2023-10-12T11:59:51.867295Z","shell.execute_reply":"2023-10-12T11:59:59.710837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Extra_Head_1(nn.Module):\n    def __init__(self, embedding_size):\n        super().__init__()  \n\n        self.backbone = nn.Sequential(nn.Linear(embedding_size, embedding_size), nn.GELU())\n\n        self.head = nn.Linear(embedding_size, 2)\n        self.aux_head = nn.Linear(embedding_size, 6)\n\n    def forward(self, x):\n\n        x = self.backbone(x)\n\n        x_preds = self.head(x)\n\n        return x_preds","metadata":{"execution":{"iopub.status.busy":"2023-10-12T11:59:59.713859Z","iopub.execute_input":"2023-10-12T11:59:59.714792Z","iopub.status.idle":"2023-10-12T11:59:59.72125Z","shell.execute_reply.started":"2023-10-12T11:59:59.714757Z","shell.execute_reply":"2023-10-12T11:59:59.720254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Weighted_Linear(nn.Module):\n    def __init__(self, hidden_size, model_instance):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.cat_size = hidden_size*3\n\n        self.layer_pooler = utilities.WeightedLayerPooling(model_instance.n_layers) \n        self.sequence_pooler = utilities.MeanPooling(.0 if model_instance.use_prompt_text else 1e-9)  \n \n        self.head = nn.Sequential(nn.Linear(self.hidden_size, CFG.n_classes))\n        \n        if model_instance.aux:\n            self.aux_head = nn.Sequential(nn.Linear(self.hidden_size, 6))\n        \n        self.dropout = utilities.Multisample_Dropout()\n                                \n    def forward(self, x, mask):\n        \n        x = self.layer_pooler(x.hidden_states) \n\n        x = self.sequence_pooler(x, mask).half()\n\n        #x = self.dropout(x, self.head) \n\n        return self.head(x)\n\nclass Cat_LSTM(nn.Module):\n    def __init__(self, hidden_size, model_instance):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.cat_size = hidden_size*model_instance.n_layers\n        self.n_layers = model_instance.n_layers\n\n        #self.layer_pooler = utilities.WeightedLayerPooling(6) \n        self.sequence_pooler = utilities.MeanPooling(.0 if model_instance.use_prompt_text else 1e-9)  \n        self.rnn = utilities.Bi_RNN_FOUT(self.cat_size, self.cat_size//2)   \n \n        self.head = nn.Sequential(nn.Linear(self.cat_size, CFG.n_classes)) \n        if model_instance.aux:\n            self.aux_head = nn.Sequential(nn.Linear(self.cat_size, 6))\n        \n        #self.dropout = utilities.Multisample_Dropout()\n        self.extra_heads = nn.ModuleList()\n        if model_instance.extra_head_instances:\n            for extra_head_instance in model_instance.extra_head_instances:\n                \n                head_path = extra_head_instance.folds[model_instance.current_fold]\n            \n                head = extra_head_instance.head(extra_head_instance.emb_size)\n                head.load_state_dict(torch.load(head_path))\n                #head = head.to(CFG.device).half()\n                #head = nn.DataParallel(head).half()\n                self.extra_heads.append(head)\n                                \n    def forward(self, x, mask): \n        \n        x = torch.cat(x.hidden_states[-self.n_layers:], dim=-1)  \n        \n        hidden_mask = mask.unsqueeze(-1).expand(x.size()).float()\n        x = (x * hidden_mask).half() \n        \n        x = self.rnn(x) \n        x = self.sequence_pooler(x, mask).half() \n\n        #x_preds = self.dropout(x, self.head) \n        #aux = self.dropout(x, self.aux_head) \n        \n        output = self.head(x) \n        if self.extra_heads:\n            for head in self.extra_heads:\n                output += head(x)\n            \n            output = output / (len(self.extra_heads) + 1)\n        \n        return output\n    \nclass Pool_LSTM(nn.Module):\n    def __init__(self, hidden_size, model_instance):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.cat_size = hidden_size*model_instance.n_layers\n        self.n_layers = model_instance.n_layers\n\n        self.pooler = utilities.LSTMPooling(hidden_size, num_hidden_layers=self.n_layers) \n        #self.rnn = utilities.Bi_RNN_FOUT(self.cat_size, self.cat_size//2)   \n \n        self.head = nn.Sequential(nn.Linear(self.hidden_size, 2)) \n    \n        if model_instance.aux:\n            self.aux_head = nn.Sequential(nn.Linear(self.hidden_size, 6)) \n\n        self.extra_heads = nn.ModuleList()\n        if model_instance.extra_head_instances:\n            for extra_head_instance in model_instance.extra_head_instances:\n                \n                head_path = extra_head_instance.folds[model_instance.current_fold]\n            \n                head = extra_head_instance.head(extra_head_instance.emb_size)\n                head.load_state_dict(torch.load(head_path))\n                #head = head.to(CFG.device).half()\n                #head = nn.DataParallel(head).half()\n                self.extra_heads.append(head)\n                                \n    def forward(self, x, mask): \n        \n        \n        x = self.pooler(x.hidden_states, mask) \n\n        output = self.head(x) \n        if self.extra_heads:\n            for head in self.extra_heads:\n                output += head(x)\n            \n            output = output / (len(self.extra_heads) + 1) \n\n        return output","metadata":{"execution":{"iopub.status.busy":"2023-10-12T11:59:59.72279Z","iopub.execute_input":"2023-10-12T11:59:59.723484Z","iopub.status.idle":"2023-10-12T11:59:59.742217Z","shell.execute_reply.started":"2023-10-12T11:59:59.723453Z","shell.execute_reply":"2023-10-12T11:59:59.741159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n\n    n_classes = 2\n\n    n_workers = 2\n\n    device = torch.device('cuda')\n    #autocast = True\n        ","metadata":{"execution":{"iopub.status.busy":"2023-10-12T11:59:59.746317Z","iopub.execute_input":"2023-10-12T11:59:59.747033Z","iopub.status.idle":"2023-10-12T11:59:59.758935Z","shell.execute_reply.started":"2023-10-12T11:59:59.746991Z","shell.execute_reply":"2023-10-12T11:59:59.757641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Extra_Head_Instance(nn.Module):\n    def __init__(self, head, emb_size, folds):\n        super().__init__()\n        self.folds = folds\n        self.head = head\n        self.emb_size = emb_size\n","metadata":{"execution":{"iopub.status.busy":"2023-10-12T11:59:59.762357Z","iopub.execute_input":"2023-10-12T11:59:59.762738Z","iopub.status.idle":"2023-10-12T11:59:59.769684Z","shell.execute_reply.started":"2023-10-12T11:59:59.762712Z","shell.execute_reply":"2023-10-12T11:59:59.768539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Model_Instance(nn.Module):\n    def __init__(self, batch_size, max_len, model_name, tokenizer, config, folds, weight, head, extra_head_instances=None, n_layers=6, aux=False, use_prompt_text=True):\n        super().__init__()\n        self.batch_size = batch_size\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n        self.config = config\n        self.folds = folds\n        self.n_layers = n_layers\n        self.use_prompt_text = use_prompt_text\n        self.aux=aux\n        self.weight = weight\n        self.head = head\n        self.extra_head_instances = extra_head_instances\n        self.current_fold = 0\n\n","metadata":{"execution":{"iopub.status.busy":"2023-10-12T11:59:59.772152Z","iopub.execute_input":"2023-10-12T11:59:59.773718Z","iopub.status.idle":"2023-10-12T11:59:59.782295Z","shell.execute_reply.started":"2023-10-12T11:59:59.773693Z","shell.execute_reply":"2023-10-12T11:59:59.781109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"deberta_v3_base_long_v3 = []\ndeberta_v3_base_long_v3.append(Extra_Head_Instance(\n    head=Extra_Head_1,\n    emb_size=1536,\n    folds=[#'/kaggle/input/deberta-v3-base-long-v3-h3/deberta-v3-base-long-v3-h3/microsoft-deberta-v3-base-0-0.433',\n           #'/kaggle/input/deberta-v3-base-long-v3-h3/deberta-v3-base-long-v3-h3/microsoft-deberta-v3-base-1-0.496',\n           '/kaggle/input/deberta-v3-base-long-v3-h3/deberta-v3-base-long-v3-h3/microsoft-deberta-v3-base-2-0.419',\n           '/kaggle/input/deberta-v3-base-long-v3-h3/deberta-v3-base-long-v3-h3/microsoft-deberta-v3-base-3-0.502',\n    ]\n))","metadata":{"execution":{"iopub.status.busy":"2023-10-12T11:59:59.784021Z","iopub.execute_input":"2023-10-12T11:59:59.784357Z","iopub.status.idle":"2023-10-12T11:59:59.793236Z","shell.execute_reply.started":"2023-10-12T11:59:59.784307Z","shell.execute_reply":"2023-10-12T11:59:59.792227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"deberta_v3_large_long_v9 = []\ndeberta_v3_large_long_v9.append(Extra_Head_Instance(\n    head=Extra_Head_1,\n    emb_size=1024,\n    folds=[#'/kaggle/input/deberta-v3-large-long-v3-h2/deberta-v3-large-long-v3-h2/microsoft-deberta-v3-large-0-0.421',\n           #'/kaggle/input/deberta-v3-large-long-v3-h2/deberta-v3-large-long-v3-h2/microsoft-deberta-v3-large-1-0.488',\n           '/kaggle/input/deberta-v3-large-long-v3-h2/deberta-v3-large-long-v3-h2/microsoft-deberta-v3-large-2-0.415',\n           '/kaggle/input/deberta-v3-large-long-v3-h2/deberta-v3-large-long-v3-h2/microsoft-deberta-v3-large-3-0.504',\n          ]\n))","metadata":{"execution":{"iopub.status.busy":"2023-10-12T11:59:59.795029Z","iopub.execute_input":"2023-10-12T11:59:59.795267Z","iopub.status.idle":"2023-10-12T11:59:59.80884Z","shell.execute_reply.started":"2023-10-12T11:59:59.795236Z","shell.execute_reply":"2023-10-12T11:59:59.807843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"OpenAssistant_v1 = []\nOpenAssistant_v1.append(Extra_Head_Instance(\n    head=Extra_Head_1,\n    emb_size=1024,\n    folds=['/kaggle/input/openassistant-large-v2-long-v1-h1/OpenAssistant--large-v2-long-v1-h1/microsoft-deberta-v3-large-0-0.421',\n           '/kaggle/input/openassistant-large-v2-long-v1-h1/OpenAssistant--large-v2-long-v1-h1/microsoft-deberta-v3-large-1-0.482',\n           #'/kaggle/input/openassistant-large-v2-long-v1-h1/OpenAssistant--large-v2-long-v1-h1/microsoft-deberta-v3-large-2-0.424',\n           #'/kaggle/input/openassistant-large-v2-long-v1-h1/OpenAssistant--large-v2-long-v1-h1/microsoft-deberta-v3-large-3-0.516',\n          ]\n))","metadata":{"execution":{"iopub.status.busy":"2023-10-12T11:59:59.811366Z","iopub.execute_input":"2023-10-12T11:59:59.812101Z","iopub.status.idle":"2023-10-12T11:59:59.819307Z","shell.execute_reply.started":"2023-10-12T11:59:59.812069Z","shell.execute_reply":"2023-10-12T11:59:59.818215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_instances = []\nmodel_instances.append(Model_Instance(batch_size=32,\n                                      max_len=1792,  \n                                      model_name='microsoft/deberta-v3-large', \n                                      tokenizer='/kaggle/input/deberta-v3-large-long-v9/deberta-v3-large-long-v9/microsoft-deberta-v3-large-tokenizer',\n                                      config='/kaggle/input/deberta-v3-large-long-v9/deberta-v3-large-long-v9/microsoft-deberta-v3-large-config',\n                                      folds = [#'/kaggle/input/deberta-v3-large-long-v9/deberta-v3-large-long-v9/microsoft-deberta-v3-large-0-0.424',\n                                               #'/kaggle/input/deberta-v3-large-long-v9/deberta-v3-large-long-v9/microsoft-deberta-v3-large-1-0.489',  \n                                               '/kaggle/input/deberta-v3-large-long-v9/deberta-v3-large-long-v9/microsoft-deberta-v3-large-2-0.417',\n                                               '/kaggle/input/deberta-v3-large-long-v9/deberta-v3-large-long-v9/microsoft-deberta-v3-large-3-0.513',\n                                              ],\n                                      weight=0.2,\n                                      aux=True,\n                                      head=Pool_LSTM,\n                                      extra_head_instances=deberta_v3_large_long_v9,\n                                      ))\nmodel_instances.append(Model_Instance(batch_size=32, \n                                      max_len=2048,  \n                                      model_name='microsoft/deberta-v3-base', \n                                      tokenizer='/kaggle/input/deberta-v3-base-long-v3/deberta-v3-base-long-v3/microsoft-deberta-v3-base-tokenizer',\n                                      config='/kaggle/input/deberta-v3-base-long-v3/deberta-v3-base-long-v3/microsoft-deberta-v3-base-config',\n                                      folds = [#'/kaggle/input/deberta-v3-base-long-v3/deberta-v3-base-long-v3/microsoft-deberta-v3-base-0-0.436',\n                                               #'/kaggle/input/deberta-v3-base-long-v3/deberta-v3-base-long-v3/microsoft-deberta-v3-base-1-0.502',  \n                                               '/kaggle/input/deberta-v3-base-long-v3/deberta-v3-base-long-v3/microsoft-deberta-v3-base-2-0.422',\n                                               '/kaggle/input/deberta-v3-base-long-v3/deberta-v3-base-long-v3/microsoft-deberta-v3-base-3-0.513',\n                                              ],  \n                                      n_layers=2,  \n                                      weight=.2,\n                                      aux=True,\n                                      head=Cat_LSTM,\n                                      extra_head_instances=deberta_v3_base_long_v3,\n                                      ))\nmodel_instances.append(Model_Instance(batch_size=32, \n                                      max_len=1792,  \n                                      model_name='OpenAssistant/reward-model-deberta-v3-large-v2', \n                                      tokenizer='/kaggle/input/openassistant-large-v2-long-v1/OpenAssistant--large-v2-long-v1/OpenAssistant-reward-model-deberta-v3-large-v2-tokenizer',\n                                      config='/kaggle/input/openassistant-large-v2-long-v1/OpenAssistant--large-v2-long-v1/OpenAssistant-reward-model-deberta-v3-large-v2-config',\n                                      folds = ['/kaggle/input/openassistant-large-v2-long-v1/OpenAssistant--large-v2-long-v1/OpenAssistant-reward-model-deberta-v3-large-v2-0-0.42',\n                                               '/kaggle/input/openassistant-large-v2-long-v1/OpenAssistant--large-v2-long-v1/OpenAssistant-reward-model-deberta-v3-large-v2-1-0.485', \n                                               #'/kaggle/input/openassistant-large-v2-long-v1/OpenAssistant--large-v2-long-v1/OpenAssistant-reward-model-deberta-v3-large-v2-2-0.426',\n                                               #'/kaggle/input/openassistant-large-v2-long-v1/OpenAssistant--large-v2-long-v1/OpenAssistant-reward-model-deberta-v3-large-v2-3-0.526',\n                                              ],\n                                      weight=0.2,\n                                      aux=True,\n                                      head=Pool_LSTM,\n                                      extra_head_instances=OpenAssistant_v1,\n                                      ))\nmodel_instances.append(Model_Instance(batch_size=32,\n                                      max_len=1792,  \n                                      model_name='microsoft/deberta-large', \n                                      tokenizer='/kaggle/input/deberta-large-long-v5/deberta-large-long-v5/microsoft-deberta-large-tokenizer',\n                                      config='/kaggle/input/deberta-large-long-v5/deberta-large-long-v5/microsoft-deberta-large-config',\n                                      folds = [#'/kaggle/input/deberta-large-long-v5/deberta-large-long-v5/microsoft-deberta-large-0-0.425',\n                                               #'/kaggle/input/deberta-large-long-v5/deberta-large-long-v5/microsoft-deberta-large-1-0.492', \n                                               '/kaggle/input/deberta-large-long-v5/deberta-large-long-v5/microsoft-deberta-large-2-0.432',\n                                               '/kaggle/input/deberta-large-long-v5/deberta-large-long-v5/microsoft-deberta-large-3-0.515',\n                                              ],\n                                      aux=True,\n                                      weight=0.2,\n                                      head=Weighted_Linear,\n                                      ))\nmodel_instances.append(Model_Instance(batch_size=32, \n                                      max_len=1792,  \n                                      model_name='microsoft/deberta-v3-base', \n                                      tokenizer='/kaggle/input/deberta-v3-large-long-v7/deberta-v3-large-long-v7/microsoft-deberta-v3-large-tokenizer',\n                                      config='/kaggle/input/deberta-v3-large-long-v7/deberta-v3-large-long-v7/microsoft-deberta-v3-large-config',\n                                      folds = ['/kaggle/input/deberta-v3-large-long-v7/deberta-v3-large-long-v7/microsoft-deberta-v3-large-0-0.426',\n                                               '/kaggle/input/deberta-v3-large-long-v7/deberta-v3-large-long-v7/microsoft-deberta-v3-large-1-0.494',  \n                                               #'/kaggle/input/deberta-v3-large-long-v7/deberta-v3-large-long-v7/microsoft-deberta-v3-large-2-0.421',\n                                               #'/kaggle/input/deberta-v3-large-long-v7/deberta-v3-large-long-v7/microsoft-deberta-v3-large-3-0.504',\n                                              ],  \n                                      n_layers=2,  \n                                      weight=.2,\n                                      aux=True,\n                                      head=Cat_LSTM,\n                                      ))\n","metadata":{"execution":{"iopub.status.busy":"2023-10-12T11:59:59.824051Z","iopub.execute_input":"2023-10-12T11:59:59.824473Z","iopub.status.idle":"2023-10-12T11:59:59.836631Z","shell.execute_reply.started":"2023-10-12T11:59:59.824448Z","shell.execute_reply":"2023-10-12T11:59:59.835617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompts_path = '/kaggle/input/commonlit-evaluate-student-summaries/prompts_test.csv'\nsummaries_path = '/kaggle/input/commonlit-evaluate-student-summaries/summaries_test.csv' \n\n#prompts_path = '/kaggle/input/commonlit-evaluate-student-summaries/prompts_train.csv'\n#summaries_path = '/kaggle/input/commonlit-evaluate-student-summaries/summaries_train.csv'","metadata":{"execution":{"iopub.status.busy":"2023-10-12T11:59:59.838125Z","iopub.execute_input":"2023-10-12T11:59:59.839041Z","iopub.status.idle":"2023-10-12T11:59:59.852561Z","shell.execute_reply.started":"2023-10-12T11:59:59.839017Z","shell.execute_reply":"2023-10-12T11:59:59.851402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompts_df = pd.read_csv(prompts_path)\nsummaries_df = pd.read_csv(summaries_path)","metadata":{"execution":{"iopub.status.busy":"2023-10-12T11:59:59.854497Z","iopub.execute_input":"2023-10-12T11:59:59.855204Z","iopub.status.idle":"2023-10-12T11:59:59.881187Z","shell.execute_reply.started":"2023-10-12T11:59:59.855169Z","shell.execute_reply":"2023-10-12T11:59:59.880261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_df = summaries_df.merge(prompts_df, how='inner', on=None, left_on='prompt_id', right_on='prompt_id')","metadata":{"execution":{"iopub.status.busy":"2023-10-12T11:59:59.883043Z","iopub.execute_input":"2023-10-12T11:59:59.883374Z","iopub.status.idle":"2023-10-12T11:59:59.903624Z","shell.execute_reply.started":"2023-10-12T11:59:59.883343Z","shell.execute_reply":"2023-10-12T11:59:59.902648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-10-12T11:59:59.905561Z","iopub.execute_input":"2023-10-12T11:59:59.906013Z","iopub.status.idle":"2023-10-12T11:59:59.927951Z","shell.execute_reply.started":"2023-10-12T11:59:59.905959Z","shell.execute_reply":"2023-10-12T11:59:59.927068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_df['concat'] = merged_df.prompt_question + merged_df.text + merged_df.prompt_text","metadata":{"execution":{"iopub.status.busy":"2023-10-12T11:59:59.930735Z","iopub.execute_input":"2023-10-12T11:59:59.931059Z","iopub.status.idle":"2023-10-12T11:59:59.940341Z","shell.execute_reply.started":"2023-10-12T11:59:59.931036Z","shell.execute_reply":"2023-10-12T11:59:59.938784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_df['concat_len'] = merged_df['concat'].apply(lambda x: len(x))","metadata":{"execution":{"iopub.status.busy":"2023-10-12T11:59:59.941527Z","iopub.execute_input":"2023-10-12T11:59:59.944663Z","iopub.status.idle":"2023-10-12T11:59:59.954379Z","shell.execute_reply.started":"2023-10-12T11:59:59.94463Z","shell.execute_reply":"2023-10-12T11:59:59.953385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_df = merged_df.sort_values(by=['concat_len'], ascending=True)","metadata":{"execution":{"iopub.status.busy":"2023-10-12T11:59:59.956573Z","iopub.execute_input":"2023-10-12T11:59:59.957655Z","iopub.status.idle":"2023-10-12T11:59:59.970669Z","shell.execute_reply.started":"2023-10-12T11:59:59.957616Z","shell.execute_reply":"2023-10-12T11:59:59.969483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_df.head(20)","metadata":{"execution":{"iopub.status.busy":"2023-10-12T11:59:59.971973Z","iopub.execute_input":"2023-10-12T11:59:59.972821Z","iopub.status.idle":"2023-10-12T11:59:59.997346Z","shell.execute_reply.started":"2023-10-12T11:59:59.972787Z","shell.execute_reply":"2023-10-12T11:59:59.996391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Summary_DS(Dataset):\n    def __init__(self, df, tokenizer, use_prompt_text):\n        self.use_prompt_text = use_prompt_text\n        self.df = df\n        self.tokenizer = tokenizer\n        self.seperator = \" \" + self.tokenizer.sep_token + \" \"\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):\n\n        row = self.df.iloc[index]\n        \n        prompt_text = (self.seperator + row.prompt_text) if self.use_prompt_text else ''\n\n        input_text = 'Think through this step by step : ' + row.prompt_question + self.seperator + 'Pay attention to the content and wording : ' + row.text + prompt_text\n\n        tokenized_dict = self.tokenizer(input_text, add_special_tokens = False) \n\n        input_ids = tokenized_dict.input_ids\n        attention_mask = tokenized_dict.attention_mask \n\n        head_mask = []\n        use_full = False\n        for token in tokenized_dict.input_ids:\n            \n            if token == self.tokenizer.sep_token_id:\n                use_full = not use_full  \n\n            head_mask.append(1 if use_full else .0) \n\n        return {'ids':input_ids,'mask':attention_mask, 'head_mask':head_mask}","metadata":{"execution":{"iopub.status.busy":"2023-10-12T12:00:00.000714Z","iopub.execute_input":"2023-10-12T12:00:00.004211Z","iopub.status.idle":"2023-10-12T12:00:00.017013Z","shell.execute_reply.started":"2023-10-12T12:00:00.004173Z","shell.execute_reply":"2023-10-12T12:00:00.015847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Collate:\n    def __init__(self, tokenizer, max_len):\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __call__(self, batch):\n        output = dict()\n        output[\"ids\"] = [sample[\"ids\"] for sample in batch]\n        output[\"mask\"] = [sample[\"mask\"] for sample in batch]\n        output[\"head_mask\"] = [sample[\"head_mask\"] for sample in batch]\n\n        # calculate max token length of this batch\n        batch_max = max([len(ids) for ids in output[\"ids\"]]) \n\n        batch_max = min(batch_max, self.max_len) \n\n        output[\"ids\"] = [s[:batch_max] for s in output[\"ids\"]]\n        output[\"mask\"] = [s[:batch_max] for s in output[\"mask\"]] \n        output[\"head_mask\"] = [s[:batch_max] for s in output[\"head_mask\"]] \n\n        output[\"ids\"] = [s + (batch_max - len(s)) * [self.tokenizer.pad_token_id] for s in output[\"ids\"]]\n        output[\"mask\"] = [s + (batch_max - len(s)) * [0] for s in output[\"mask\"]]\n        output[\"head_mask\"] = [s + (batch_max - len(s)) * [0] for s in output[\"head_mask\"]]\n\n\n        # convert to tensors\n        output[\"ids\"] = torch.tensor(output[\"ids\"], dtype=torch.long)\n        output[\"mask\"] = torch.tensor(output[\"mask\"], dtype=torch.long)\n        output[\"head_mask\"] = torch.tensor(output[\"head_mask\"], dtype=torch.float)\n\n        return output","metadata":{"execution":{"iopub.status.busy":"2023-10-12T12:00:00.02241Z","iopub.execute_input":"2023-10-12T12:00:00.023452Z","iopub.status.idle":"2023-10-12T12:00:00.038072Z","shell.execute_reply.started":"2023-10-12T12:00:00.023414Z","shell.execute_reply":"2023-10-12T12:00:00.036459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, model_instance):\n        super().__init__()\n        self.config = AutoConfig.from_pretrained(model_instance.config)\n        self.transformer = AutoModel.from_config(\n            config=self.config\n        )\n        self.use_prompt_text = model_instance.use_prompt_text\n        self.head = model_instance.head(self.config.hidden_size, model_instance)\n\n    def forward(self, input_ids, attention_mask, head_mask):\n\n        x = self.transformer(input_ids, attention_mask = attention_mask)\n\n        x = self.head(x, head_mask if self.use_prompt_text else attention_mask)\n        \n        return x\n","metadata":{"execution":{"iopub.status.busy":"2023-10-12T12:00:00.040457Z","iopub.execute_input":"2023-10-12T12:00:00.041175Z","iopub.status.idle":"2023-10-12T12:00:00.058157Z","shell.execute_reply.started":"2023-10-12T12:00:00.04114Z","shell.execute_reply":"2023-10-12T12:00:00.056782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def eval(model, valid_loader, fold):\n    with torch.no_grad():\n        model.eval() \n\n        all_preds=torch.tensor([],dtype=torch.float)\n\n        bar = tqdm(valid_loader)\n        for i, data in enumerate(bar): \n\n            input_ids = data['ids'].to(CFG.device)\n            attention_mask = data['mask'].to(CFG.device)\n            head_mask = data['head_mask'].to(CFG.device)\n\n            preds = model(input_ids, attention_mask, head_mask)\n            \n            preds = preds.cpu().detach()\n            \n            all_preds = torch.cat([all_preds, preds], dim=0)\n\n            bar.set_postfix(fold=fold)\n\n    return all_preds","metadata":{"execution":{"iopub.status.busy":"2023-10-12T12:00:00.059472Z","iopub.execute_input":"2023-10-12T12:00:00.060595Z","iopub.status.idle":"2023-10-12T12:00:00.074171Z","shell.execute_reply.started":"2023-10-12T12:00:00.060562Z","shell.execute_reply":"2023-10-12T12:00:00.073094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_list = []\noof_total = 0  \nfor model_instance in model_instances:\n    tokenizer = AutoTokenizer.from_pretrained(model_instance.tokenizer)\n    for fold,path in enumerate(model_instance.folds):  \n        model_instance.current_fold = fold\n        model = Model(model_instance) \n        model.load_state_dict(torch.load(path),strict=False)\n        model = model.to(CFG.device).half()\n        model = nn.DataParallel(model).half()\n        \n        ds = Summary_DS(merged_df, tokenizer, model_instance.use_prompt_text)\n        loader = DataLoader(ds, batch_size = model_instance.batch_size, num_workers=CFG.n_workers, shuffle=False, drop_last=False, collate_fn=Collate(tokenizer, model_instance.max_len))\n\n        preds = eval(model, loader, fold)\n        preds = preds * (model_instance.weight/len(model_instance.folds))\n        preds_list.append(preds)\n        \n        del model\n        torch.cuda.empty_cache()\n\n","metadata":{"execution":{"iopub.status.busy":"2023-10-12T12:00:00.075647Z","iopub.execute_input":"2023-10-12T12:00:00.076939Z","iopub.status.idle":"2023-10-12T12:03:41.421601Z","shell.execute_reply.started":"2023-10-12T12:00:00.076901Z","shell.execute_reply":"2023-10-12T12:03:41.420291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_np = np.stack(preds_list, 0)","metadata":{"execution":{"iopub.status.busy":"2023-10-12T12:03:41.424118Z","iopub.execute_input":"2023-10-12T12:03:41.4249Z","iopub.status.idle":"2023-10-12T12:03:41.430941Z","shell.execute_reply.started":"2023-10-12T12:03:41.424861Z","shell.execute_reply":"2023-10-12T12:03:41.429863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_preds = preds_np.sum(0)","metadata":{"execution":{"iopub.status.busy":"2023-10-12T12:03:41.432727Z","iopub.execute_input":"2023-10-12T12:03:41.433909Z","iopub.status.idle":"2023-10-12T12:03:41.445203Z","shell.execute_reply.started":"2023-10-12T12:03:41.433873Z","shell.execute_reply":"2023-10-12T12:03:41.443899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_preds.shape","metadata":{"execution":{"iopub.status.busy":"2023-10-12T12:03:41.446713Z","iopub.execute_input":"2023-10-12T12:03:41.447936Z","iopub.status.idle":"2023-10-12T12:03:41.460446Z","shell.execute_reply.started":"2023-10-12T12:03:41.447899Z","shell.execute_reply":"2023-10-12T12:03:41.459301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df = pd.DataFrame({'student_id':merged_df.student_id,'content':final_preds[:,0],'wording':final_preds[:,1]})","metadata":{"execution":{"iopub.status.busy":"2023-10-12T12:03:41.462002Z","iopub.execute_input":"2023-10-12T12:03:41.46301Z","iopub.status.idle":"2023-10-12T12:03:41.472451Z","shell.execute_reply.started":"2023-10-12T12:03:41.462968Z","shell.execute_reply":"2023-10-12T12:03:41.47136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-10-12T12:03:41.478225Z","iopub.execute_input":"2023-10-12T12:03:41.479114Z","iopub.status.idle":"2023-10-12T12:03:41.491674Z","shell.execute_reply.started":"2023-10-12T12:03:41.479079Z","shell.execute_reply":"2023-10-12T12:03:41.490205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.to_csv(\"/kaggle/working/submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-10-12T12:03:41.493563Z","iopub.execute_input":"2023-10-12T12:03:41.494352Z","iopub.status.idle":"2023-10-12T12:03:41.505551Z","shell.execute_reply.started":"2023-10-12T12:03:41.494298Z","shell.execute_reply":"2023-10-12T12:03:41.504496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}