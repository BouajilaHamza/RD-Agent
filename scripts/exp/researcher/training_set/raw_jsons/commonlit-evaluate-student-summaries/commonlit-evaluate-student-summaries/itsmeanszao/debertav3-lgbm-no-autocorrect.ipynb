{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install /kaggle/input/pyspellchecker/pyspellchecker-0.7.2-py3-none-any.whl\nfrom typing import List\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport warnings\nimport logging\nimport os\nimport gc\nimport shutil\nimport json\nimport transformers\nfrom transformers import AutoModel, AutoTokenizer, AutoConfig, AutoModelForSequenceClassification\nfrom transformers import DataCollatorWithPadding\nfrom datasets import Dataset,load_dataset, load_from_disk\nfrom transformers import TrainingArguments, Trainer\nfrom datasets import load_metric, disable_progress_bar\nfrom sklearn.metrics import mean_squared_error\nimport torch\nfrom sklearn.model_selection import KFold, GroupKFold\nfrom tqdm import tqdm\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nimport spacy\nimport re\nfrom spellchecker import SpellChecker\nimport lightgbm as lgb\nimport torch.nn as nn\n\n# logging setting \n\nwarnings.simplefilter(\"ignore\")\nlogging.disable(logging.ERROR)\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \ndisable_progress_bar()\ntqdm.pandas()","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-10-26T12:27:40.816772Z","iopub.execute_input":"2023-10-26T12:27:40.817265Z","iopub.status.idle":"2023-10-26T12:28:34.327444Z","shell.execute_reply.started":"2023-10-26T12:27:40.817222Z","shell.execute_reply":"2023-10-26T12:28:34.326438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set random seed\ndef seed_everything(seed: int):\n    import random, os\n    import numpy as np\n    import torch\n    \n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    \n    \nseed_everything(seed=42)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-10-26T12:28:34.329475Z","iopub.execute_input":"2023-10-26T12:28:34.329769Z","iopub.status.idle":"2023-10-26T12:28:34.341554Z","shell.execute_reply.started":"2023-10-26T12:28:34.329726Z","shell.execute_reply":"2023-10-26T12:28:34.340665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    model_name=\"deberta-v3-large\"\n    learning_rate=1e-5\n    weight_decay=1e-8\n    hidden_dropout_prob=0.\n    attention_probs_dropout_prob=0.\n    num_train_epochs=2\n    n_splits=4\n    batch_size=4\n    random_seed=42\n    save_steps=200\n    max_length=1700\n    folds=[0,1,2,3]\n    n_classes = 2\n    pl_dir='kaggle/input/pl/'","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-10-26T12:28:34.342832Z","iopub.execute_input":"2023-10-26T12:28:34.343078Z","iopub.status.idle":"2023-10-26T12:28:34.350148Z","shell.execute_reply.started":"2023-10-26T12:28:34.343051Z","shell.execute_reply":"2023-10-26T12:28:34.349187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataload","metadata":{}},{"cell_type":"code","source":"DATA_DIR = \"/kaggle/input/commonlit-evaluate-student-summaries/\"\n\nprompts_train = pd.read_csv(DATA_DIR + \"prompts_train.csv\")\nprompts_test = pd.read_csv(DATA_DIR + \"prompts_test.csv\")\nsummaries_train = pd.read_csv(DATA_DIR + \"summaries_train.csv\")\nsummaries_test = pd.read_csv(DATA_DIR + \"summaries_test.csv\")\nsample_submission = pd.read_csv(DATA_DIR + \"sample_submission.csv\")\n\n\n# summaries_train = summaries_train.head(10) # for dev mode\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-10-26T12:28:34.353289Z","iopub.execute_input":"2023-10-26T12:28:34.353558Z","iopub.status.idle":"2023-10-26T12:28:34.490525Z","shell.execute_reply.started":"2023-10-26T12:28:34.353524Z","shell.execute_reply":"2023-10-26T12:28:34.489517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Preprocessor:\n    def __init__(self, \n                model_name: str,\n                ) -> None:\n        self.tokenizer = AutoTokenizer.from_pretrained(f\"/kaggle/input/commonlitexp07/deberta-v3-large-finetune-hm/fold_0\")\n        self.STOP_WORDS = set(stopwords.words('english'))\n        \n        self.spacy_ner_model = spacy.load('en_core_web_sm',)\n        self.speller = SpellChecker() #Speller(lang='en')\n        \n    def count_text_length(self, df: pd.DataFrame, col:str) -> pd.Series:\n        \"\"\" text length \"\"\"\n        tokenizer=self.tokenizer\n        return df[col].progress_apply(lambda x: len(tokenizer.encode(x)))\n\n    def word_overlap_count(self, row):\n        \"\"\" intersection(prompt_text, text) \"\"\"        \n        def check_is_stop_word(word):\n            return word in self.STOP_WORDS\n        \n        prompt_words = row['prompt_tokens']\n        summary_words = row['summary_tokens']\n        if self.STOP_WORDS:\n            prompt_words = list(filter(check_is_stop_word, prompt_words))\n            summary_words = list(filter(check_is_stop_word, summary_words))\n        return len(set(prompt_words).intersection(set(summary_words)))\n            \n    def ngrams(self, token, n):\n        # Use the zip function to help us generate n-grams\n        # Concatentate the tokens into ngrams and return\n        ngrams = zip(*[token[i:] for i in range(n)])\n        return [\" \".join(ngram) for ngram in ngrams]\n\n    def ngram_co_occurrence(self, row, n: int):\n        # Tokenize the original text and summary into words\n        original_tokens = row['prompt_tokens']\n        summary_tokens = row['summary_tokens']\n\n        # Generate n-grams for the original text and summary\n        original_ngrams = set(self.ngrams(original_tokens, n))\n        summary_ngrams = set(self.ngrams(summary_tokens, n))\n\n        # Calculate the number of common n-grams\n        common_ngrams = original_ngrams.intersection(summary_ngrams)\n\n        # # Optionally, you can get the frequency of common n-grams for a more nuanced analysis\n        # original_ngram_freq = Counter(ngrams(original_words, n))\n        # summary_ngram_freq = Counter(ngrams(summary_words, n))\n        # common_ngram_freq = {ngram: min(original_ngram_freq[ngram], summary_ngram_freq[ngram]) for ngram in common_ngrams}\n\n        return len(common_ngrams)\n    \n    def ner_overlap_count(self, row, mode:str):\n        model = self.spacy_ner_model\n        def clean_ners(ner_list):\n            return set([(ner[0].lower(), ner[1]) for ner in ner_list])\n        prompt = model(row['prompt_text'])\n        summary = model(row['text'])\n\n        if \"spacy\" in str(model):\n            prompt_ner = set([(token.text, token.label_) for token in prompt.ents])\n            summary_ner = set([(token.text, token.label_) for token in summary.ents])\n        elif \"stanza\" in str(model):\n            prompt_ner = set([(token.text, token.type) for token in prompt.ents])\n            summary_ner = set([(token.text, token.type) for token in summary.ents])\n        else:\n            raise Exception(\"Model not supported\")\n\n        prompt_ner = clean_ners(prompt_ner)\n        summary_ner = clean_ners(summary_ner)\n\n        intersecting_ners = prompt_ner.intersection(summary_ner)\n        \n        ner_dict = dict(Counter([ner[1] for ner in intersecting_ners]))\n        \n        if mode == \"train\":\n            return ner_dict\n        elif mode == \"test\":\n            return {key: ner_dict.get(key) for key in self.ner_keys}\n\n    \n    def quotes_count(self, row):\n        summary = row['text']\n        text = row['prompt_text']\n        quotes_from_summary = re.findall(r'\"([^\"]*)\"', summary)\n        if len(quotes_from_summary)>0:\n            return [quote in text for quote in quotes_from_summary].count(True)\n        else:\n            return 0\n\n    def spelling(self, text):\n        \n        wordlist=text.split()\n        amount_miss = len(list(self.speller.unknown(wordlist)))\n\n        return amount_miss\n    \n    def run(self, \n            prompts: pd.DataFrame,\n            summaries:pd.DataFrame,\n            mode:str\n        ) -> pd.DataFrame:\n        \n        # before merge preprocess\n        prompts[\"prompt_length\"] = prompts[\"prompt_text\"].apply(\n            lambda x: len(self.tokenizer.encode(x))\n        )\n        prompts[\"prompt_tokens\"] = prompts[\"prompt_text\"].apply(\n            lambda x: self.tokenizer.convert_ids_to_tokens(\n                self.tokenizer.encode(x), \n                skip_special_tokens=True\n            )\n        )\n\n        summaries[\"summary_length\"] = summaries[\"text\"].apply(\n            lambda x: len(self.tokenizer.encode(x))\n        )\n        summaries[\"summary_tokens\"] = summaries[\"text\"].apply(\n            lambda x: self.tokenizer.convert_ids_to_tokens(\n                self.tokenizer.encode(x), \n                skip_special_tokens=True\n            )\n\n        )\n        summaries[\"splling_err_num\"] = summaries[\"text\"].progress_apply(self.spelling)\n\n        # merge prompts and summaries\n        input_df = summaries.merge(prompts, how=\"left\", on=\"prompt_id\")\n\n        # after merge preprocess\n        input_df['length_ratio'] = input_df['summary_length'] / input_df['prompt_length']\n        \n        input_df['word_overlap_count'] = input_df.progress_apply(self.word_overlap_count, axis=1)\n        input_df['bigram_overlap_count'] = input_df.progress_apply(\n            self.ngram_co_occurrence,args=(2,), axis=1 \n        )\n        input_df['trigram_overlap_count'] = input_df.progress_apply(\n            self.ngram_co_occurrence, args=(3,), axis=1\n        )\n        \n        # Crate dataframe with count of each category NERs overlap for all the summaries\n        # Because it spends too much time for this feature, I don't use this time.\n#         ners_count_df  = input_df.progress_apply(\n#             lambda row: pd.Series(self.ner_overlap_count(row, mode=mode), dtype='float64'), axis=1\n#         ).fillna(0)\n#         self.ner_keys = ners_count_df.columns\n#         ners_count_df['sum'] = ners_count_df.sum(axis=1)\n#         ners_count_df.columns = ['NER_' + col for col in ners_count_df.columns]\n#         # join ner count dataframe with train dataframe\n#         input_df = pd.concat([input_df, ners_count_df], axis=1)\n        \n        input_df['quotes_count'] = input_df.progress_apply(self.quotes_count, axis=1)\n        \n        return input_df.drop(columns=[\"summary_tokens\", \"prompt_tokens\"])\n    \npreprocessor = Preprocessor(model_name=CFG.model_name)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-10-26T12:28:34.492077Z","iopub.execute_input":"2023-10-26T12:28:34.492382Z","iopub.status.idle":"2023-10-26T12:28:36.299264Z","shell.execute_reply.started":"2023-10-26T12:28:34.492344Z","shell.execute_reply":"2023-10-26T12:28:36.298345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = preprocessor.run(prompts_train, summaries_train, mode=\"train\")\ntest = preprocessor.run(prompts_test, summaries_test, mode=\"test\")\ntest['length'] = test['summary_length'] + test['prompt_length']\ntest = test.sort_values('length', ascending=True).reset_index(drop=True)\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2023-10-26T12:28:36.300456Z","iopub.execute_input":"2023-10-26T12:28:36.300677Z","iopub.status.idle":"2023-10-26T12:29:01.481474Z","shell.execute_reply.started":"2023-10-26T12:28:36.30065Z","shell.execute_reply":"2023-10-26T12:29:01.480441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gkf = GroupKFold(n_splits=CFG.n_splits)\n\nfor i, (_, val_index) in enumerate(gkf.split(train, groups=train[\"prompt_id\"])):\n    train.loc[val_index, \"fold\"] = i\n\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2023-10-26T12:29:01.483086Z","iopub.execute_input":"2023-10-26T12:29:01.483393Z","iopub.status.idle":"2023-10-26T12:29:01.520168Z","shell.execute_reply.started":"2023-10-26T12:29:01.483352Z","shell.execute_reply":"2023-10-26T12:29:01.51923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Function Definition","metadata":{}},{"cell_type":"code","source":"def compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    rmse = mean_squared_error(labels, predictions, squared=False)\n    return {\"rmse\": rmse}\n\ndef compute_mcrmse(eval_pred):\n    \"\"\"\n    Calculates mean columnwise root mean squared error\n    https://www.kaggle.com/competitions/commonlit-evaluate-student-summaries/overview/evaluation\n    \"\"\"\n    preds, labels = eval_pred\n\n    col_rmse = np.sqrt(np.mean((preds - labels) ** 2, axis=0))\n    mcrmse = np.mean(col_rmse)\n\n    return {\n        \"content_rmse\": col_rmse[0],\n        \"wording_rmse\": col_rmse[1],\n        \"mcrmse\": mcrmse,\n    }\nclass MeanPooling(nn.Module):\n    def __init__(self):\n        super(MeanPooling, self).__init__()\n        \n    def forward(self, last_hidden_state, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings / sum_mask\n        return mean_embeddings\n    \nclass Bi_RNN_FOUT(nn.Module):\n    def __init__(self, size, hidden_size, layers=1):\n        super().__init__()\n        self.layers = layers\n        self.hidden_size = hidden_size\n        self.rnn = nn.LSTM(size, hidden_size, num_layers=layers, bidirectional=True, bias=False, batch_first=True)\n                                \n    def forward(self, x):\n        x, hidden = self.rnn(x)\n        return x\n    \ndef compt_score(content_true, content_pred, wording_true, wording_pred):\n    content_score = mean_squared_error(content_true, content_pred)**(1/2)\n    wording_score = mean_squared_error(wording_true, wording_pred)**(1/2)\n    \n    return (content_score + wording_score)/2\n\nclass Head(nn.Module):\n    def __init__(self, hidden_size):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.cat_size = hidden_size*2\n\n        #self.layer_pooler = utilities.WeightedLayerPooling(6) \n        self.sequence_pooler = MeanPooling()\n        self.rnn = Bi_RNN_FOUT(self.cat_size, self.cat_size//2)   \n \n        self.head = nn.Sequential(nn.Linear(self.cat_size, 2)) \n        # self.aux_head = nn.Sequential(nn.Linear(self.cat_size, len(CFG.aux_classes))) \n\n        #self.dropout = utilities.Multisample_Dropout()\n                                \n    def forward(self, x, mask): \n        \n        x = torch.cat(x.hidden_states[-2:], dim=-1)  \n        \n        hidden_mask = mask.unsqueeze(-1).expand(x.size()).float()\n        x = x * hidden_mask \n        \n        x = self.rnn(x) \n        x = self.sequence_pooler(x, mask) \n\n        #x_preds = self.dropout(x, self.head) \n        #aux = self.dropout(x, self.aux_head) \n        x_preds = self.head(x)  \n        # aux = self.aux_head(x)  \n        return x_preds#, aux\n        \n    def process_outputs(self, out):\n        n_layers = int(len(out)/2)\n        out = torch.stack(out[-n_layers:], dim=0) \n        out_mean = torch.mean(out, dim=0)\n        out_max, _ = torch.max(out, dim=0)\n        out_std = torch.std(out, dim=0)\n        last_hidden_states = torch.cat((out_mean, out_max, out_std), dim=-1)\n        return last_hidden_states\n    \nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.config = AutoConfig.from_pretrained(\n            '/kaggle/input/commonlitexp07/deberta-v3-large-finetune-hm/fold_0',\n            output_hidden_states=True,\n            hidden_dropout_prob=.0,\n            attention_probs_dropout_prob=.0,\n            num_hidden_layers=24,\n        )\n        self.transformer = AutoModel.from_pretrained(\n            '/kaggle/input/commonlitexp07/deberta-v3-large-finetune-hm/fold_0', \n            config=self.config\n        )\n        \n        self.transformer.gradient_checkpointing_enable()\n\n        self.head = Head(self.config.hidden_size)\n\n    def forward(self, input_ids, attention_mask, head_mask, labels=None):\n\n        x = self.transformer(input_ids, attention_mask = attention_mask)\n\n        x = self.head(x, head_mask)\n        # print(x)\n        if labels is not None:\n            loss_fct = nn.MSELoss()\n            # x = x.view(-1).to(labels.dtype)\n            # loss = loss_fn(x, labels.view(-1))\n            loss = loss_fct(x, labels)\n            return {\n                'loss':loss,\n                'predictions':x\n            }\n        return (x,)\n\n    def get_parameters(self):\n\n        parameter_settings = []\n        parameter_settings.extend(self.get_parameter_section([(n, p) for n, p in self.transformer.named_parameters()], lr=CFG.backbone_lr, wd=CFG.backbone_weight_decay, freeze=CFG.freeze)) \n        parameter_settings.extend(self.get_parameter_section([(n, p) for n, p in self.head.named_parameters()], lr=CFG.head_lr, wd=CFG.head_weight_decay)) \n\n\n        return parameter_settings\n\n    def get_parameter_section(self, parameters, lr=None, wd=None, freeze=None): \n        parameter_settings = []\n        freeze_layers = ['LayerNorm']\n\n        lr_is_dict = isinstance(lr, dict)\n        wd_is_dict = isinstance(wd, dict)\n\n        layer_no = None\n        for no, (n,p) in enumerate(parameters):\n            \n            for split in n.split('.'):\n                if split.isnumeric():\n                    layer_no = int(split)\n            \n            if not layer_no:\n                layer_no = 0\n\n            if freeze and layer_no < freeze:\n                p.requires_grad = False      \n\n            if lr_is_dict:\n                for k,v in lr.items():\n                    if layer_no < int(k):\n                        temp_lr = v\n                        break\n            else:\n                temp_lr = lr\n\n            if wd_is_dict:\n                for k,v in wd.items():\n                    if layer_no < int(k):\n                        temp_wd = v\n                        break\n            else:\n                temp_wd = wd\n\n            weight_decay = 0.0 if 'bias' in n else temp_wd\n\n            parameter_setting = {\"params\" : p, \"lr\" : temp_lr, \"weight_decay\" : temp_wd}\n\n            parameter_settings.append(parameter_setting)\n\n            #print(f'no {no} | params {n} | lr {temp_lr} | weight_decay {weight_decay} | requires_grad {p.requires_grad}')\n\n        return parameter_settings","metadata":{"execution":{"iopub.status.busy":"2023-10-26T12:29:01.521678Z","iopub.execute_input":"2023-10-26T12:29:01.521935Z","iopub.status.idle":"2023-10-26T12:29:01.554374Z","shell.execute_reply.started":"2023-10-26T12:29:01.521906Z","shell.execute_reply":"2023-10-26T12:29:01.553455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Collate:\n    def __init__(self, tokenizer, test=False):\n        self.tokenizer = tokenizer\n        self.test = test\n\n    def __call__(self, batch):\n        output = dict()\n        # print(batch)\n        output[\"input_ids\"] = [sample[\"input_ids\"] for sample in batch]\n        output[\"attention_mask\"] = [sample[\"attention_mask\"] for sample in batch]\n        if not self.test:\n            output[\"labels\"] = [sample[\"labels\"] for sample in batch]\n        # output[\"aux\"] = [sample[\"aux\"] for sample in batch]\n        output[\"head_mask\"] = [sample[\"head_mask\"] for sample in batch]\n\n        # calculate max token length of this batch\n        batch_max = max([len(ids) for ids in output[\"input_ids\"]]) \n\n        batch_max = min(batch_max, CFG.max_length) \n\n        output[\"input_ids\"] = [s[:batch_max] for s in output[\"input_ids\"]]\n        output[\"attention_mask\"] = [s[:batch_max] for s in output[\"attention_mask\"]] \n        output[\"head_mask\"] = [s[:batch_max] for s in output[\"head_mask\"]] \n\n        output[\"input_ids\"] = [s + (batch_max - len(s)) * [self.tokenizer.pad_token_id] for s in output[\"input_ids\"]]\n        output[\"attention_mask\"] = [s + (batch_max - len(s)) * [0] for s in output[\"attention_mask\"]]\n        output[\"head_mask\"] = [s + (batch_max - len(s)) * [0] for s in output[\"head_mask\"]]\n\n\n        # convert to tensors\n        output[\"input_ids\"] = torch.tensor(output[\"input_ids\"], dtype=torch.long)\n        output[\"attention_mask\"] = torch.tensor(output[\"attention_mask\"], dtype=torch.long)\n        if not self.test:\n            output[\"labels\"] = torch.tensor(output[\"labels\"], dtype=torch.float) \n        # output[\"aux\"] = torch.tensor(output[\"aux\"], dtype=torch.float) \n        output[\"head_mask\"] = torch.tensor(output[\"head_mask\"], dtype=torch.float)\n\n        return output","metadata":{"execution":{"iopub.status.busy":"2023-10-26T12:29:01.555596Z","iopub.execute_input":"2023-10-26T12:29:01.555846Z","iopub.status.idle":"2023-10-26T12:29:01.571442Z","shell.execute_reply.started":"2023-10-26T12:29:01.555818Z","shell.execute_reply":"2023-10-26T12:29:01.570514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Deberta Regressor","metadata":{}},{"cell_type":"code","source":"class ScoreRegressor:\n    def __init__(self, \n                model_name: str,\n                model_dir: str,\n                inputs: List[str],\n                target_cols: List[str],\n                hidden_dropout_prob: float,\n                attention_probs_dropout_prob: float,\n                max_length: int,\n                test: bool,\n                ):\n        \n        self.input_col = \"input\" # col name of model input after text concat sep token\n        \n        self.input_text_cols = inputs\n        self.target_cols = target_cols\n        self.model_name = model_name\n        self.model_dir = model_dir\n        self.max_length = max_length\n        self.test=test\n        \n        self.tokenizer = AutoTokenizer.from_pretrained(f\"/kaggle/input/commonlitexp07/deberta-v3-large-finetune-hm/fold_0\")\n#         self.model_config = AutoConfig.from_pretrained(f\"microsoft/deberta-v3-large\")\n        \n#         self.model_config.update({\n#             \"hidden_dropout_prob\": hidden_dropout_prob,\n#             \"attention_probs_dropout_prob\": attention_probs_dropout_prob,\n#             \"num_labels\": 2,\n#             \"problem_type\": \"regression\",\n#         })\n\n        self.data_collator = Collate(\n            tokenizer=self.tokenizer, test=self.test\n        )\n        self.seperator = ' ' + self.tokenizer.sep_token + ' '\n\n    def concatenate_with_sep_token(self, row):\n        full_text = 'Think through this step by step : ' + row.prompt_question + self.seperator + 'Pay attention to the content and wording : ' + row.text + self.seperator + row.prompt_text\n        return full_text\n\n    def tokenize_function(self, examples: pd.DataFrame):\n        labels = [examples[\"content\"], examples[\"wording\"]]\n        tokenized_dict = self.tokenizer(examples[self.input_col],\n                        padding=\"longest\",\n                        truncation=True,\n                        max_length=self.max_length)\n        input_ids = tokenized_dict.input_ids\n        attention_mask = tokenized_dict.attention_mask \n\n        head_mask = []\n        use_full = False\n        for token in tokenized_dict.input_ids:\n            \n            if token == self.tokenizer.sep_token_id:\n                use_full = not use_full  \n\n            head_mask.append(1 if use_full else .0) \n        return {'input_ids':input_ids,'attention_mask':attention_mask, 'labels':torch.tensor(labels, dtype=torch.float) ,'head_mask':head_mask}\n    \n    def tokenize_function_test(self, examples: pd.DataFrame):\n        tokenized_dict = self.tokenizer(examples[self.input_col],\n                        padding=\"longest\",\n                        truncation=True,\n                        max_length=self.max_length)\n        input_ids = tokenized_dict.input_ids\n        attention_mask = tokenized_dict.attention_mask \n\n        head_mask = []\n        use_full = False\n        for token in tokenized_dict.input_ids:\n            \n            if token == self.tokenizer.sep_token_id:\n                use_full = not use_full  \n\n            head_mask.append(1 if use_full else .0) \n        return {'input_ids':input_ids,'attention_mask':attention_mask,'head_mask':head_mask}\n        \n    def train(self, \n            fold: int,\n            train_df: pd.DataFrame,\n            valid_df: pd.DataFrame,\n            batch_size: int,\n            learning_rate: float,\n            weight_decay: float,\n            num_train_epochs: float,\n            save_steps: int,\n        ) -> None:\n        \"\"\"fine-tuning\"\"\"\n        \n        train_df[self.input_col] = train_df.apply(self.concatenate_with_sep_token, axis=1)\n        # print(train_df)\n        valid_df[self.input_col] = valid_df.apply(self.concatenate_with_sep_token, axis=1)        \n\n        train_df = train_df[[self.input_col] + self.target_cols]\n        valid_df = valid_df[[self.input_col] + self.target_cols]\n        print(train_df.shape)\n        \n        model = Model()\n        state = torch.load(f\"deberta-v3-large-pre/fold_{fold}/pytorch_model.bin\",\n                   map_location=torch.device('cpu'))\n\n        model.load_state_dict(state)\n        train_dataset = Dataset.from_pandas(train_df, preserve_index=False) \n        val_dataset = Dataset.from_pandas(valid_df, preserve_index=False) \n    \n        train_tokenized_datasets = train_dataset.map(self.tokenize_function, batched=False)\n        # print(train_tokenized_datasets[0])\n        val_tokenized_datasets = val_dataset.map(self.tokenize_function, batched=False)\n\n        # eg. \"bert/fold_0/\"\n        model_fold_dir = os.path.join(self.model_dir, str(fold)) \n        \n        training_args = TrainingArguments(\n            output_dir=model_fold_dir,\n            load_best_model_at_end=True, # select best model\n            learning_rate=learning_rate,\n            per_device_train_batch_size=batch_size, \n            per_device_eval_batch_size=batch_size,\n            num_train_epochs=num_train_epochs,\n            weight_decay=weight_decay,\n            # gradient_checkpointing=True,\n            report_to='none',\n            greater_is_better=False,\n            save_strategy=\"steps\",\n            evaluation_strategy=\"steps\",\n            eval_steps=save_steps,\n            save_steps=save_steps,\n            metric_for_best_model=\"mcrmse\",\n            save_total_limit=1,\n            fp16=True,\n            auto_find_batch_size=True,\n        )\n\n        trainer = Trainer(\n            model=model,\n            args=training_args,\n            train_dataset=train_tokenized_datasets,\n            eval_dataset=val_tokenized_datasets,\n            tokenizer=self.tokenizer,\n            compute_metrics=compute_mcrmse,\n            data_collator=self.data_collator\n        )\n\n        trainer.train()\n        \n        # model.save_pretrained(self.model_dir)\n        torch.save(model.state_dict(),\n            self.model_dir+\"/pytorch_model.bin\")\n        self.tokenizer.save_pretrained(self.model_dir)\n\n        model.cpu()\n        del model\n        gc.collect()\n        torch.cuda.empty_cache()\n\n        \n    def predict(self, \n                test_df: pd.DataFrame,\n                batch_size: int,\n                fold: int,\n               ):\n        \"\"\"predict content score\"\"\"\n        \n        test_df[self.input_col] = test_df.apply(self.concatenate_with_sep_token, axis=1)\n\n        test_dataset = Dataset.from_pandas(test_df[[self.input_col]], preserve_index=False) \n        test_tokenized_dataset = test_dataset.map(self.tokenize_function_test, batched=False)\n\n        model = Model()\n        state = torch.load(f\"/kaggle/input/commonlitexp07/deberta-v3-large-finetune-hm/fold_{fold}/pytorch_model.bin\",\n                   map_location=torch.device('cpu'))\n\n        model.load_state_dict(state,strict=False)\n        model.eval()\n        \n        # e.g. \"bert/fold_0/\"\n        model_fold_dir = os.path.join(self.model_dir, str(fold)) \n\n        test_args = TrainingArguments(\n            output_dir=model_fold_dir,\n            do_train=False,\n            do_predict=True,\n            per_device_eval_batch_size=batch_size,\n            dataloader_drop_last=False,\n            fp16=True,\n            auto_find_batch_size=True,\n        )\n\n        # init trainer\n        infer_content = Trainer(\n                      model = model, \n                      tokenizer=self.tokenizer,\n                      data_collator=self.data_collator,\n                      args = test_args)\n\n        preds = infer_content.predict(test_tokenized_dataset)[0]\n        pred_df = pd.DataFrame(\n            preds, \n            columns=[\n                f\"content_pred\", \n                f\"wording_pred\"\n           ]\n        )\n        \n        model.cpu()\n        del model\n        gc.collect()\n        torch.cuda.empty_cache()\n\n        return pred_df","metadata":{"execution":{"iopub.status.busy":"2023-10-26T12:34:53.731963Z","iopub.execute_input":"2023-10-26T12:34:53.73228Z","iopub.status.idle":"2023-10-26T12:34:54.5657Z","shell.execute_reply.started":"2023-10-26T12:34:53.732247Z","shell.execute_reply":"2023-10-26T12:34:54.564806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_by_fold(\n        train_df: pd.DataFrame,\n        model_name: str,\n        targets: List[str],\n        inputs: List[str],\n        save_each_model: bool,\n        n_splits: int,\n        batch_size: int,\n        learning_rate: int,\n        hidden_dropout_prob: float,\n        attention_probs_dropout_prob: float,\n        weight_decay: float,\n        num_train_epochs: int,\n        save_steps: int,\n        max_length:int\n    ):\n\n    # delete old model files\n    if os.path.exists(model_name):\n        shutil.rmtree(model_name)\n    \n    os.mkdir(model_name)\n        \n    for fold in CFG.folds:\n        print(f\"fold {fold}:\")\n        train_data = train_df[train_df[\"fold\"] != fold]\n        # train_data = pd.read_csv(CFG.pl_dir + f'pl_{fold}.csv')#.head(5)\n        print(train_data.shape)\n        valid_data = train_df[train_df[\"fold\"] == fold]\n        \n        model_dir =  f\"{model_name}/fold_{fold}\"\n\n        csr = ScoreRegressor(\n            model_name=model_name,\n            target_cols=targets,\n            inputs= inputs,\n            model_dir = model_dir, \n            hidden_dropout_prob=hidden_dropout_prob,\n            attention_probs_dropout_prob=attention_probs_dropout_prob,\n            max_length=max_length,\n            test=False,\n           )\n        \n        csr.train(\n            fold=fold,\n            train_df=train_data,\n            valid_df=valid_data, \n            batch_size=batch_size,\n            learning_rate=learning_rate,\n            weight_decay=weight_decay,\n            num_train_epochs=num_train_epochs,\n            save_steps=save_steps,\n        )\n\ndef validate(\n    train_df: pd.DataFrame,\n    mode: str,\n    targets: List[str],\n    inputs: List[str],\n    save_each_model: bool,\n    n_splits: int,\n    batch_size: int,\n    model_name: str,\n    hidden_dropout_prob: float,\n    attention_probs_dropout_prob: float,\n    max_length : int\n    ) -> pd.DataFrame:\n    \"\"\"predict oof data\"\"\"\n    \n    columns = list(train_df.columns.values)\n    \n    for fold in CFG.folds:\n        print(f\"fold {fold}:\")\n        \n        valid_data = train_df[train_df[\"fold\"] == fold]\n        \n        model_dir =  f\"{model_name}/fold_{fold}\"\n        \n        csr = ScoreRegressor(\n            model_name=model_name,\n            target_cols=targets,\n            inputs= inputs,\n            model_dir = model_dir,\n            hidden_dropout_prob=hidden_dropout_prob,\n            attention_probs_dropout_prob=attention_probs_dropout_prob,\n            max_length=max_length,\n            test=True,\n           )\n        \n        pred_df = csr.predict(\n            test_df=valid_data, \n            batch_size=batch_size,\n            fold=fold\n        )\n\n        train_df.loc[valid_data.index, f\"content_{mode}_pred\"] = pred_df[f\"content_pred\"].values\n        train_df.loc[valid_data.index, f\"wording_{mode}_pred\"] = pred_df[f\"wording_pred\"].values\n                \n    return train_df[columns + [f\"content_{mode}_pred\", f\"wording_{mode}_pred\"]]\n    \ndef predict(\n    test_df: pd.DataFrame,\n    mode: str,\n    targets:List[str],\n    inputs: List[str],\n    save_each_model: bool,\n    n_splits: int,\n    batch_size: int,\n    model_name: str,\n    hidden_dropout_prob: float,\n    attention_probs_dropout_prob: float,\n    max_length : int\n    ):\n    \"\"\"predict using mean folds\"\"\"\n    \n    columns = list(test_df.columns.values)\n\n    for fold in CFG.folds:\n        print(f\"fold {fold}:\")\n        \n        model_dir =  f\"{model_name}/fold_{fold}\"\n\n        csr = ScoreRegressor(\n            model_name=model_name,\n            target_cols=targets,\n            inputs= inputs,\n            model_dir = model_dir, \n            hidden_dropout_prob=hidden_dropout_prob,\n            attention_probs_dropout_prob=attention_probs_dropout_prob,\n            max_length=max_length,\n            test=True,\n           )\n        \n        pred_df = csr.predict(\n            test_df=test_df, \n            batch_size=batch_size,\n            fold=fold\n        )\n\n        test_df[f\"content_{mode}_pred_{fold}\"] = pred_df[f\"content_pred\"].values\n        test_df[f\"wording_{mode}_pred_{fold}\"] = pred_df[f\"wording_pred\"].values\n\n    test_df[f\"content_{mode}_pred\"] = test_df[[f\"content_{mode}_pred_{fold}\" for fold in range(n_splits)]].mean(axis=1)\n    test_df[f\"wording_{mode}_pred\"] = test_df[[f\"wording_{mode}_pred_{fold}\" for fold in range(n_splits)]].mean(axis=1)\n    \n    return test_df[columns + [f\"content_{mode}_pred\", f\"wording_{mode}_pred\"]]","metadata":{"execution":{"iopub.status.busy":"2023-10-26T12:29:01.610836Z","iopub.execute_input":"2023-10-26T12:29:01.61107Z","iopub.status.idle":"2023-10-26T12:29:01.635249Z","shell.execute_reply.started":"2023-10-26T12:29:01.611042Z","shell.execute_reply":"2023-10-26T12:29:01.634217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"targets = [\"wording\", \"content\"]\nmode = \"multi\"\ninput_cols = [\"text\", \"prompt_question\", \"prompt_text\"]\nmodel_cfg = CFG\n\n# train_by_fold(\n#     train,\n#     model_name=model_cfg.model_name,\n#     save_each_model=False,\n#     targets=targets,\n#     inputs=input_cols,\n#     learning_rate=model_cfg.learning_rate,\n#     hidden_dropout_prob=model_cfg.hidden_dropout_prob,\n#     attention_probs_dropout_prob=model_cfg.attention_probs_dropout_prob,\n#     weight_decay=model_cfg.weight_decay,\n#     num_train_epochs=model_cfg.num_train_epochs,\n#     n_splits=CFG.n_splits,\n#     batch_size=model_cfg.batch_size,\n#     save_steps=model_cfg.save_steps,\n#     max_length=model_cfg.max_length\n# )\n\n\"\"\"\ntrain = validate(\n    train,\n    mode=mode,\n    targets=targets,\n    inputs=input_cols,\n    save_each_model=False,\n    n_splits=CFG.n_splits,\n    batch_size=model_cfg.batch_size,\n    model_name=model_cfg.model_name,\n    hidden_dropout_prob=model_cfg.hidden_dropout_prob,\n    attention_probs_dropout_prob=model_cfg.attention_probs_dropout_prob,\n    max_length=model_cfg.max_length\n)\n\n# set validate result\nfor target in [\"content\", \"wording\"]:\n    rmse = mean_squared_error(train[target], train[f\"{target}_{mode}_pred\"], squared=False)\n    print(f\"cv {target} rmse: {rmse}\")\n\"\"\"\ntest = predict(\n    test,\n    mode=mode,\n    targets=targets,\n    inputs=input_cols,\n    save_each_model=False,\n    batch_size=model_cfg.batch_size,\n    n_splits=CFG.n_splits,\n    model_name=model_cfg.model_name,\n    hidden_dropout_prob=model_cfg.hidden_dropout_prob,\n    attention_probs_dropout_prob=model_cfg.attention_probs_dropout_prob,\n    max_length=model_cfg.max_length\n)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-10-26T12:35:08.445602Z","iopub.execute_input":"2023-10-26T12:35:08.446454Z","iopub.status.idle":"2023-10-26T12:36:41.127521Z","shell.execute_reply.started":"2023-10-26T12:35:08.446403Z","shell.execute_reply":"2023-10-26T12:36:41.126439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LGBM model","metadata":{}},{"cell_type":"code","source":"# targets = [\"content\", \"wording\"]\n\n# drop_columns = [\"fold\", \"student_id\", \"prompt_id\", \"text\", \n#                 \"prompt_question\", \"prompt_title\", \n#                 \"prompt_text\"\n#                ] + targets","metadata":{"execution":{"iopub.status.busy":"2023-10-26T12:29:26.706081Z","iopub.status.idle":"2023-10-26T12:29:26.706464Z","shell.execute_reply.started":"2023-10-26T12:29:26.70624Z","shell.execute_reply":"2023-10-26T12:29:26.706266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model_dict = {}\n\n# for target in targets:\n#     models = []\n    \n#     for fold in range(CFG.n_splits):\n\n#         X_train_cv = train[train[\"fold\"] != fold].drop(columns=drop_columns)\n#         y_train_cv = train[train[\"fold\"] != fold][target]\n\n#         X_eval_cv = train[train[\"fold\"] == fold].drop(columns=drop_columns)\n#         y_eval_cv = train[train[\"fold\"] == fold][target]\n\n#         dtrain = lgb.Dataset(X_train_cv, label=y_train_cv)\n#         dval = lgb.Dataset(X_eval_cv, label=y_eval_cv)\n\n#         params = {\n#                   'boosting_type': 'gbdt',\n#                   'random_state': 42,\n#                   'objective': 'regression',\n#                   'metric': 'rmse',\n#                   'learning_rate': 0.05,\n#                   }\n\n#         evaluation_results = {}\n#         model = lgb.train(params,\n#                           num_boost_round=10000,\n#                             #categorical_feature = categorical_features,\n#                           valid_names=['train', 'valid'],\n#                           train_set=dtrain,\n#                           valid_sets=dval,\n#                           callbacks=[\n#                               lgb.early_stopping(stopping_rounds=30, verbose=True),\n#                                lgb.log_evaluation(100),\n#                               lgb.callback.record_evaluation(evaluation_results)\n#                             ],\n#                           )\n#         models.append(model)\n    \n#     model_dict[target] = models\n","metadata":{"execution":{"iopub.status.busy":"2023-10-26T12:29:26.707531Z","iopub.status.idle":"2023-10-26T12:29:26.707926Z","shell.execute_reply.started":"2023-10-26T12:29:26.707692Z","shell.execute_reply":"2023-10-26T12:29:26.707718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CV Score","metadata":{}},{"cell_type":"code","source":"# # cv\n# rmses = []\n\n# for target in targets:\n#     models = model_dict[target]\n\n#     preds = []\n#     trues = []\n    \n#     for fold, model in enumerate(models):\n#         # ilocで取り出す行を指定\n#         X_eval_cv = train[train[\"fold\"] == fold].drop(columns=drop_columns)\n#         y_eval_cv = train[train[\"fold\"] == fold][target]\n\n#         pred = model.predict(X_eval_cv)\n\n#         trues.extend(y_eval_cv)\n#         preds.extend(pred)\n        \n#     rmse = np.sqrt(mean_squared_error(trues, preds))\n#     print(f\"{target}_rmse : {rmse}\")\n#     rmses = rmses + [rmse]\n\n# print(f\"mcrmse : {sum(rmses) / len(rmses)}\")","metadata":{"execution":{"iopub.status.busy":"2023-10-26T12:29:26.709245Z","iopub.status.idle":"2023-10-26T12:29:26.709539Z","shell.execute_reply.started":"2023-10-26T12:29:26.709373Z","shell.execute_reply":"2023-10-26T12:29:26.709388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predict ","metadata":{}},{"cell_type":"code","source":"# drop_columns = [\n#                 #\"fold\", \n#                 \"student_id\", \"prompt_id\", \"text\", \n#                 \"prompt_question\", \"prompt_title\", \n#                 \"prompt_text\"\n#                ]","metadata":{"execution":{"iopub.status.busy":"2023-10-26T12:29:26.710552Z","iopub.status.idle":"2023-10-26T12:29:26.710939Z","shell.execute_reply.started":"2023-10-26T12:29:26.710709Z","shell.execute_reply":"2023-10-26T12:29:26.710733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pred_dict = {}\n# for target in targets:\n#     models = model_dict[target]\n#     preds = []\n\n#     for fold, model in enumerate(models):\n#         # ilocで取り出す行を指定\n#         X_eval_cv = test.drop(columns=drop_columns)\n\n#         pred = model.predict(X_eval_cv)\n#         preds.append(pred)\n    \n#     pred_dict[target] = preds","metadata":{"execution":{"iopub.status.busy":"2023-10-26T12:29:26.712745Z","iopub.status.idle":"2023-10-26T12:29:26.71311Z","shell.execute_reply.started":"2023-10-26T12:29:26.712913Z","shell.execute_reply":"2023-10-26T12:29:26.712939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for target in targets:\n#     preds = pred_dict[target]\n#     for i, pred in enumerate(preds):\n#         test[f\"{target}_pred_{i}\"] = pred\n\n#     test[target] = test[[f\"{target}_pred_{fold}\" for fold in range(CFG.n_splits)]].mean(axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-10-26T12:29:26.714373Z","iopub.status.idle":"2023-10-26T12:29:26.714685Z","shell.execute_reply.started":"2023-10-26T12:29:26.714522Z","shell.execute_reply":"2023-10-26T12:29:26.714537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test","metadata":{"execution":{"iopub.status.busy":"2023-10-26T12:29:26.71643Z","iopub.status.idle":"2023-10-26T12:29:26.716768Z","shell.execute_reply.started":"2023-10-26T12:29:26.716568Z","shell.execute_reply":"2023-10-26T12:29:26.716593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create Submission file","metadata":{}},{"cell_type":"code","source":"targets = [\"content\", \"wording\"]\ntest[targets] = test[['content_multi_pred','wording_multi_pred']].values\nsample_submission = sample_submission.drop(columns=targets).merge(test[['student_id'] + targets], on='student_id', how='left')\ndisplay(sample_submission.head())\nsample_submission[['student_id'] + targets].to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-10-26T12:38:59.961689Z","iopub.execute_input":"2023-10-26T12:38:59.962002Z","iopub.status.idle":"2023-10-26T12:38:59.990125Z","shell.execute_reply.started":"2023-10-26T12:38:59.961974Z","shell.execute_reply":"2023-10-26T12:38:59.989149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sample_submission","metadata":{"execution":{"iopub.status.busy":"2023-10-26T12:29:26.718297Z","iopub.status.idle":"2023-10-26T12:29:26.718596Z","shell.execute_reply.started":"2023-10-26T12:29:26.718435Z","shell.execute_reply":"2023-10-26T12:29:26.71845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test[[\"student_id\", \"content\", \"wording\"]].to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-10-26T12:29:26.719662Z","iopub.status.idle":"2023-10-26T12:29:26.719986Z","shell.execute_reply.started":"2023-10-26T12:29:26.719819Z","shell.execute_reply":"2023-10-26T12:29:26.719835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}