{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!touch submission.csv","metadata":{"_uuid":"a22ea663-5426-4af6-aef9-93b39d823882","_cell_guid":"89514f93-b7f2-4388-a38c-9e74c30a4b81","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T04:39:31.441734Z","iopub.execute_input":"2023-10-11T04:39:31.442341Z","iopub.status.idle":"2023-10-11T04:39:32.520738Z","shell.execute_reply.started":"2023-10-11T04:39:31.442312Z","shell.execute_reply":"2023-10-11T04:39:32.51946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.path.append(\"/kaggle/input/pip-install-nlp-mit\")","metadata":{"_uuid":"f7fd6dee-f540-4900-abc1-ebb376cb2e8d","_cell_guid":"3bfffc8d-edf2-4d7c-9ab1-5721a0bb664b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T04:39:32.523158Z","iopub.execute_input":"2023-10-11T04:39:32.523843Z","iopub.status.idle":"2023-10-11T04:39:32.52864Z","shell.execute_reply.started":"2023-10-11T04:39:32.523809Z","shell.execute_reply":"2023-10-11T04:39:32.527797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install \"/kaggle/input/worddifficulty/py_readability_metrics-1.4.5-py3-none-any.whl\"","metadata":{"_uuid":"5f0bd945-c5ba-4a29-95f4-468e8f03954b","_cell_guid":"66724e7f-bfbc-4c46-890c-132b5d3b9003","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T04:39:32.530077Z","iopub.execute_input":"2023-10-11T04:39:32.531041Z","iopub.status.idle":"2023-10-11T04:40:07.825244Z","shell.execute_reply.started":"2023-10-11T04:39:32.531011Z","shell.execute_reply":"2023-10-11T04:40:07.824143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install \"/kaggle/input/autocorrect/autocorrect-2.6.1.tar\"\n!pip install \"/kaggle/input/pyspellchecker/pyspellchecker-0.7.2-py3-none-any.whl\"","metadata":{"_uuid":"1490d882-0383-4907-9c70-f4f69803581c","_cell_guid":"5b367cb6-6ec3-4fab-b74b-62c5ebfb1e4c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T04:40:07.827954Z","iopub.execute_input":"2023-10-11T04:40:07.828305Z","iopub.status.idle":"2023-10-11T04:40:40.424585Z","shell.execute_reply.started":"2023-10-11T04:40:07.828272Z","shell.execute_reply":"2023-10-11T04:40:40.423413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from typing import List\nimport numpy as np\nimport pandas as pd\nimport warnings\nimport logging\nimport os\nimport shutil\nimport json\nimport transformers\nfrom transformers import AutoModel, AutoTokenizer, AutoConfig, AutoModelForSequenceClassification\nfrom transformers import DataCollatorWithPadding\nfrom datasets import Dataset,load_dataset, load_from_disk\nfrom transformers import TrainingArguments, Trainer\nfrom datasets import load_metric, disable_progress_bar\nfrom sklearn.metrics import mean_squared_error\nimport torch\nfrom sklearn.model_selection import KFold, GroupKFold\nfrom tqdm import tqdm\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize.treebank import TreebankWordDetokenizer\nfrom collections import Counter\nimport spacy\nimport re\nfrom autocorrect import Speller\nfrom spellchecker import SpellChecker\nimport lightgbm as lgb\n\nwarnings.simplefilter(\"ignore\")\nlogging.disable(logging.ERROR)\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \ndisable_progress_bar()\ntqdm.pandas()","metadata":{"_uuid":"3bfb38a1-6e64-4325-8bbb-9acd43c19929","_cell_guid":"5f372950-3cb0-4872-a4cf-8f1cc92c0ae4","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T04:40:40.426475Z","iopub.execute_input":"2023-10-11T04:40:40.426869Z","iopub.status.idle":"2023-10-11T04:40:57.493975Z","shell.execute_reply.started":"2023-10-11T04:40:40.426835Z","shell.execute_reply":"2023-10-11T04:40:57.493166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed: int):\n    import random, os\n    import numpy as np\n    import torch\n    \n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    \nseed_everything(seed=42)","metadata":{"_uuid":"ca9fc2a8-e2cc-497b-86b1-5b96bf7933cc","_cell_guid":"a55c9b72-8a68-4b5d-82a6-c6082ac1cbfe","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T04:40:57.495186Z","iopub.execute_input":"2023-10-11T04:40:57.495899Z","iopub.status.idle":"2023-10-11T04:40:57.505683Z","shell.execute_reply.started":"2023-10-11T04:40:57.495867Z","shell.execute_reply":"2023-10-11T04:40:57.504916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    model_name = \"another-bert\"\n    learning_rate=1.5e-5\n    weight_decay=0.02\n    hidden_dropout_prob=0.007\n    attention_probs_dropout_prob=0.007\n    num_train_epochs=5\n    n_splits=4\n    batch_size=12\n    \n    random_seed=42\n    save_steps=20\n    max_length=512","metadata":{"_uuid":"17afa223-9c54-4073-bdeb-0ab63e4afb4c","_cell_guid":"4bedd952-c084-4e4a-87f1-3cccf2103ec6","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T04:40:57.507392Z","iopub.execute_input":"2023-10-11T04:40:57.507643Z","iopub.status.idle":"2023-10-11T04:40:57.514317Z","shell.execute_reply.started":"2023-10-11T04:40:57.507614Z","shell.execute_reply":"2023-10-11T04:40:57.513471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MODEL_DIR = '/kaggle/input/commitlit-deberta-v3-large-trained3'\nINIT_MODEL = f\"{MODEL_DIR}/content/{CFG.model_name}/fold_0\"","metadata":{"_uuid":"0857b06d-1a77-47f3-a02b-5b06e28a16be","_cell_guid":"f62c9857-d280-405e-bd1d-15c3d8699516","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T04:40:57.515764Z","iopub.execute_input":"2023-10-11T04:40:57.516191Z","iopub.status.idle":"2023-10-11T04:40:57.528713Z","shell.execute_reply.started":"2023-10-11T04:40:57.516162Z","shell.execute_reply":"2023-10-11T04:40:57.527842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataload","metadata":{"_uuid":"c5bec2f0-8f58-4364-9e38-01de461b81e1","_cell_guid":"3cef7e9c-e819-4194-be7c-477981a583bd","trusted":true}},{"cell_type":"code","source":"DATA_DIR = \"/kaggle/input/commonlit-evaluate-student-summaries/\"\n\nprompts_train = pd.read_csv(DATA_DIR + \"prompts_train.csv\")\nprompts_test = pd.read_csv(DATA_DIR + \"prompts_test.csv\")\nsummaries_train = pd.read_csv(DATA_DIR + \"summaries_train.csv\")\nsummaries_test = pd.read_csv(DATA_DIR + \"summaries_test.csv\")\nsample_submission = pd.read_csv(DATA_DIR + \"sample_submission.csv\")","metadata":{"_uuid":"4d55167a-0c16-4797-99eb-1a966cf3ef99","_cell_guid":"5de02d1b-f5de-4a52-b68c-cf8d608be399","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T04:40:57.530081Z","iopub.execute_input":"2023-10-11T04:40:57.530374Z","iopub.status.idle":"2023-10-11T04:40:57.657217Z","shell.execute_reply.started":"2023-10-11T04:40:57.530345Z","shell.execute_reply":"2023-10-11T04:40:57.656258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocess\n\n[Using features]\n\n- Text Length\n- Length Ratio\n- Word Overlap\n- N-grams Co-occurrence\n  - count\n  - ratio\n- Quotes Overlap\n- Grammar Check\n  - spelling: pyspellchecker","metadata":{"_uuid":"9c204531-7ee7-4f4a-85f9-9b2fd83c6ac0","_cell_guid":"617668ba-d4a1-4d23-ab1d-f8c9f9b64887","trusted":true}},{"cell_type":"code","source":"class Preprocessor:\n    def __init__(self, \n                model_name: str,\n                ) -> None:\n        self.tokenizer = AutoTokenizer.from_pretrained(INIT_MODEL)\n        self.twd = TreebankWordDetokenizer()\n        self.STOP_WORDS = set(stopwords.words('english'))\n        \n        self.spacy_ner_model = spacy.load('en_core_web_sm',)\n        self.speller = Speller(lang='en')\n        self.spellchecker = SpellChecker() \n        \n    def word_overlap_count(self, row):\n        \"\"\" intersection(prompt_text, text) \"\"\"        \n        def check_is_stop_word(word):\n            return word in self.STOP_WORDS\n        \n        prompt_words = row['prompt_tokens']\n        summary_words = row['summary_tokens']\n        if self.STOP_WORDS:\n            prompt_words = list(filter(check_is_stop_word, prompt_words))\n            summary_words = list(filter(check_is_stop_word, summary_words))\n        return len(set(prompt_words).intersection(set(summary_words)))\n            \n    def ngrams(self, token, n):\n        # Use the zip function to help us generate n-grams\n        # Concatentate the tokens into ngrams and return\n        ngrams = zip(*[token[i:] for i in range(n)])\n        return [\" \".join(ngram) for ngram in ngrams]\n\n    def ngram_co_occurrence(self, row, n: int) -> int:\n        # Tokenize the original text and summary into words\n        original_tokens = row['prompt_tokens']\n        summary_tokens = row['summary_tokens']\n\n        # Generate n-grams for the original text and summary\n        original_ngrams = set(self.ngrams(original_tokens, n))\n        summary_ngrams = set(self.ngrams(summary_tokens, n))\n\n        # Calculate the number of common n-grams\n        common_ngrams = original_ngrams.intersection(summary_ngrams)\n        return len(common_ngrams)\n    \n    def ner_overlap_count(self, row, mode:str):\n        model = self.spacy_ner_model\n        def clean_ners(ner_list):\n            return set([(ner[0].lower(), ner[1]) for ner in ner_list])\n        prompt = model(row['prompt_text'])\n        summary = model(row['text'])\n\n        if \"spacy\" in str(model):\n            prompt_ner = set([(token.text, token.label_) for token in prompt.ents])\n            summary_ner = set([(token.text, token.label_) for token in summary.ents])\n        elif \"stanza\" in str(model):\n            prompt_ner = set([(token.text, token.type) for token in prompt.ents])\n            summary_ner = set([(token.text, token.type) for token in summary.ents])\n        else:\n            raise Exception(\"Model not supported\")\n\n        prompt_ner = clean_ners(prompt_ner)\n        summary_ner = clean_ners(summary_ner)\n\n        intersecting_ners = prompt_ner.intersection(summary_ner)\n        \n        ner_dict = dict(Counter([ner[1] for ner in intersecting_ners]))\n        \n        if mode == \"train\":\n            return ner_dict\n        elif mode == \"test\":\n            return {key: ner_dict.get(key) for key in self.ner_keys}\n\n    \n    def quotes_count(self, row):\n        summary = row['text']\n        text = row['prompt_text']\n        quotes_from_summary = re.findall(r'\"([^\"]*)\"', summary)\n        if len(quotes_from_summary)>0:\n            return [quote in text for quote in quotes_from_summary].count(True)\n        else:\n            return 0\n\n    def spelling(self, text):\n        \n        wordlist=text.split()\n        amount_miss = len(list(self.spellchecker.unknown(wordlist)))\n\n        return amount_miss\n    \n    def add_spelling_dictionary(self, tokens: List[str]) -> List[str]:\n        \"\"\"dictionary update for pyspell checker and autocorrect\"\"\"\n        self.spellchecker.word_frequency.load_words(tokens)\n        self.speller.nlp_data.update({token:1000 for token in tokens})\n    \n    def run(self, \n            prompts: pd.DataFrame,\n            summaries:pd.DataFrame,\n            mode:str\n        ) -> pd.DataFrame:\n        \n        # before merge preprocess\n        prompts[\"prompt_length\"] = prompts[\"prompt_text\"].apply(\n            lambda x: len(word_tokenize(x))\n        )\n        prompts[\"prompt_tokens\"] = prompts[\"prompt_text\"].apply(\n            lambda x: word_tokenize(x)\n        )\n\n        summaries[\"summary_length\"] = summaries[\"text\"].apply(\n            lambda x: len(word_tokenize(x))\n        )\n        summaries[\"summary_tokens\"] = summaries[\"text\"].apply(\n            lambda x: word_tokenize(x)\n        )\n        \n        # Add prompt tokens into spelling checker dictionary\n        prompts[\"prompt_tokens\"].apply(\n            lambda x: self.add_spelling_dictionary(x)\n        )\n        \n#         from IPython.core.debugger import Pdb; Pdb().set_trace()\n        # fix misspelling\n        summaries[\"fixed_summary_text\"] = summaries[\"text\"].progress_apply(\n            lambda x: self.speller(x)\n        )\n        \n        # count misspelling\n        summaries[\"splling_err_num\"] = summaries[\"text\"].progress_apply(self.spelling)\n        \n        # merge prompts and summaries\n        input_df = summaries.merge(prompts, how=\"left\", on=\"prompt_id\")\n\n        # after merge preprocess\n        input_df['length_ratio'] = input_df['summary_length'] / input_df['prompt_length']\n\n        input_df['word_overlap_count'] = input_df.progress_apply(self.word_overlap_count, axis=1)\n        input_df['bigram_overlap_count'] = input_df.progress_apply(\n            self.ngram_co_occurrence,args=(2,), axis=1 \n        )\n        input_df['bigram_overlap_ratio'] = input_df['bigram_overlap_count'] / (input_df['summary_length'] - 1)\n        \n        input_df['trigram_overlap_count'] = input_df.progress_apply(\n            self.ngram_co_occurrence, args=(3,), axis=1\n        )\n        input_df['trigram_overlap_ratio'] = input_df['trigram_overlap_count'] / (input_df['summary_length'] - 2)\n        \n        input_df['quotes_count'] = input_df.progress_apply(self.quotes_count, axis=1)\n        \n        return input_df.drop(columns=[\"summary_tokens\", \"prompt_tokens\"])\n    \npreprocessor = Preprocessor(model_name=CFG.model_name)","metadata":{"_uuid":"551c4d8e-4364-4755-b01c-ecb2ad7fd499","_cell_guid":"f1a3f6d9-05df-46ef-91ab-5f7bd4e29130","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T04:40:57.661196Z","iopub.execute_input":"2023-10-11T04:40:57.661375Z","iopub.status.idle":"2023-10-11T04:40:58.939068Z","shell.execute_reply.started":"2023-10-11T04:40:57.661354Z","shell.execute_reply":"2023-10-11T04:40:58.938117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summaries_train.head()","metadata":{"_uuid":"63c15516-fccd-4fad-b39c-10fbc0eddb6c","_cell_guid":"d6a59bf1-3e5f-4c77-9b89-fb14dd2d88dc","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T04:40:58.940493Z","iopub.execute_input":"2023-10-11T04:40:58.940727Z","iopub.status.idle":"2023-10-11T04:40:58.96089Z","shell.execute_reply.started":"2023-10-11T04:40:58.940696Z","shell.execute_reply":"2023-10-11T04:40:58.96001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\nwith open(MODEL_DIR+'/pickled.pkl', 'rb') as f:\n    train = pickle.load(f)\n    test = pickle.load(f)\n\n# train = preprocessor.run(prompts_train, summaries_train, mode=\"train\")\ntest = preprocessor.run(prompts_test, summaries_test, mode=\"test\")\n\n# train.head()","metadata":{"_uuid":"ece91580-c8ec-4a84-afb2-a3c66782ddb4","_cell_guid":"c54ba4c7-57cc-4c06-9c9d-98011263e5c1","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T04:40:58.962462Z","iopub.execute_input":"2023-10-11T04:40:58.963025Z","iopub.status.idle":"2023-10-11T04:40:59.215274Z","shell.execute_reply.started":"2023-10-11T04:40:58.96299Z","shell.execute_reply":"2023-10-11T04:40:59.214481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train","metadata":{"_uuid":"2506a0fc-05ce-4806-ba6a-37ee30b21839","_cell_guid":"b58e5bba-7de6-4d5e-bebc-a575edd3d6ca","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T04:40:59.21661Z","iopub.execute_input":"2023-10-11T04:40:59.217368Z","iopub.status.idle":"2023-10-11T04:40:59.221808Z","shell.execute_reply.started":"2023-10-11T04:40:59.217334Z","shell.execute_reply":"2023-10-11T04:40:59.220681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train.drop(columns = ['length_ratio'])\ntest = test.drop(columns = ['length_ratio'])","metadata":{"_uuid":"0deb1bf3-d46e-470b-886c-056196026dc2","_cell_guid":"45b9fbc8-a5d7-4089-b579-be94f547edc5","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T04:40:59.222965Z","iopub.execute_input":"2023-10-11T04:40:59.223679Z","iopub.status.idle":"2023-10-11T04:40:59.238167Z","shell.execute_reply.started":"2023-10-11T04:40:59.22364Z","shell.execute_reply":"2023-10-11T04:40:59.237302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gkf = GroupKFold(n_splits=CFG.n_splits)\n\nfor i, (_, val_index) in enumerate(gkf.split(train, groups=train[\"prompt_id\"])):\n    train.loc[val_index, \"fold\"] = i\n\ntrain.head()","metadata":{"_uuid":"77b1b234-42a1-4ef1-80e3-113949a978b1","_cell_guid":"c166e387-2668-4bd3-8bfc-73c1a45408c5","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T04:40:59.239809Z","iopub.execute_input":"2023-10-11T04:40:59.240442Z","iopub.status.idle":"2023-10-11T04:40:59.27228Z","shell.execute_reply.started":"2023-10-11T04:40:59.240392Z","shell.execute_reply":"2023-10-11T04:40:59.271491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Function Definition","metadata":{"_uuid":"e273c014-65bf-4f07-8277-d9ffc67d2764","_cell_guid":"a9242205-2689-402c-b9a7-9c0e1e50fe2c","trusted":true}},{"cell_type":"code","source":"import shutil\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    rmse = mean_squared_error(labels, predictions, squared=False)\n    return {\"rmse\": rmse}\n\ndef compute_mcrmse(eval_pred):\n    \"\"\"\n    Calculates mean columnwise root mean squared error\n    https://www.kaggle.com/competitions/commonlit-evaluate-student-summaries/overview/evaluation\n    \"\"\"\n    preds, labels = eval_pred\n\n    col_rmse = np.sqrt(np.mean((preds - labels) ** 2, axis=0))\n    mcrmse = np.mean(col_rmse)\n\n    return {\n        \"content_rmse\": col_rmse[0],\n        \"wording_rmse\": col_rmse[1],\n        \"mcrmse\": mcrmse,\n    }\n\ndef compt_score(content_true, content_pred, wording_true, wording_pred):\n    content_score = mean_squared_error(content_true, content_pred)**(1/2)\n    wording_score = mean_squared_error(wording_true, wording_pred)**(1/2)\n    \n    return (content_score + wording_score)/2","metadata":{"_uuid":"7594bd04-6f1a-4c83-bf22-94f113cae29a","_cell_guid":"370b12a0-8bfe-41bc-9a04-cf34471014e6","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T04:40:59.273588Z","iopub.execute_input":"2023-10-11T04:40:59.274039Z","iopub.status.idle":"2023-10-11T04:40:59.282014Z","shell.execute_reply.started":"2023-10-11T04:40:59.274004Z","shell.execute_reply":"2023-10-11T04:40:59.281134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Deberta Regressor","metadata":{"_uuid":"9c4ce611-312d-4068-9afd-84678bb990a3","_cell_guid":"9d38482e-2c9e-4a39-88c6-e726f106dc9a","trusted":true}},{"cell_type":"code","source":"class ContentScoreRegressor:\n    def __init__(self, \n                model_name: str,\n                model_dir: str,\n                target: str,\n                hidden_dropout_prob: float,\n                attention_probs_dropout_prob: float,\n                max_length: int,\n                ):\n        self.inputs = [\"prompt_text\", \"prompt_title\", \"prompt_question\", \"fixed_summary_text\"]\n        self.input_col = \"input\"\n        \n        self.text_cols = [self.input_col] \n        self.target = target\n        self.target_cols = [target]\n\n        self.model_name = model_name\n        self.model_dir = model_dir\n        self.max_length = max_length\n        \n        self.tokenizer = AutoTokenizer.from_pretrained(INIT_MODEL)\n        self.model_config = AutoConfig.from_pretrained(INIT_MODEL)\n        \n        self.model_config.update({\n            \"hidden_dropout_prob\": hidden_dropout_prob,\n            \"attention_probs_dropout_prob\": attention_probs_dropout_prob,\n            \"num_labels\": 1,\n            \"problem_type\": \"regression\",\n        })\n        \n        seed_everything(seed=42)\n\n        self.data_collator = DataCollatorWithPadding(\n            tokenizer=self.tokenizer\n        )\n\n\n    def tokenize_function(self, examples: pd.DataFrame):\n        labels = [examples[self.target]]\n        tokenized = self.tokenizer(examples[self.input_col],\n                         padding=False,\n                         truncation=True,\n                         max_length=self.max_length)\n        return {\n            **tokenized,\n            \"labels\": labels,\n        }\n    \n    def tokenize_function_test(self, examples: pd.DataFrame):\n        tokenized = self.tokenizer(examples[self.input_col],\n                         padding=False,\n                         truncation=True,\n                         max_length=self.max_length)\n        return tokenized\n        \n    def train(self, \n            fold: int,\n            train_df: pd.DataFrame,\n            valid_df: pd.DataFrame,\n            batch_size: int,\n            learning_rate: float,\n            weight_decay: float,\n            num_train_epochs: float,\n            save_steps: int,\n        ) -> None:\n        \"\"\"fine-tuning\"\"\"\n        \n        sep = self.tokenizer.sep_token\n        train_df[self.input_col] = (\n                    train_df[\"prompt_title\"] + sep \n                    + train_df[\"prompt_question\"] + sep \n                    + train_df[\"fixed_summary_text\"]\n                  )\n\n        valid_df[self.input_col] = (\n                    valid_df[\"prompt_title\"] + sep \n                    + valid_df[\"prompt_question\"] + sep \n                    + valid_df[\"fixed_summary_text\"]\n                  )\n        \n        train_df = train_df[[self.input_col] + self.target_cols]\n        valid_df = valid_df[[self.input_col] + self.target_cols]\n        \n        model_content = AutoModelForSequenceClassification.from_pretrained(\n            f\"/kaggle/input/{self.model_name}\", \n            config=self.model_config,\n            ignore_mismatched_sizes=True\n        )\n\n        train_dataset = Dataset.from_pandas(train_df, preserve_index=False) \n        val_dataset = Dataset.from_pandas(valid_df, preserve_index=False) \n    \n        train_tokenized_datasets = train_dataset.map(self.tokenize_function, batched=False)\n        val_tokenized_datasets = val_dataset.map(self.tokenize_function, batched=False)\n\n        # eg. \"bert/fold_0/\"\n        model_fold_dir = os.path.join(self.model_dir, str(fold)) \n        \n        training_args = TrainingArguments(\n            output_dir=model_fold_dir,\n            load_best_model_at_end=True, # select best model\n            learning_rate=learning_rate,\n            per_device_train_batch_size=batch_size,\n            per_device_eval_batch_size=8,\n            num_train_epochs=num_train_epochs,\n            weight_decay=weight_decay,\n            report_to='none',\n            greater_is_better=False,\n            save_strategy=\"steps\",\n            evaluation_strategy=\"steps\",\n            eval_steps=save_steps,\n            save_steps=save_steps,\n            metric_for_best_model=\"rmse\",\n            save_total_limit=1\n        )\n\n        trainer = Trainer(\n            model=model_content,\n            args=training_args,\n            train_dataset=train_tokenized_datasets,\n            eval_dataset=val_tokenized_datasets,\n            tokenizer=self.tokenizer,\n            compute_metrics=compute_metrics,\n            data_collator=self.data_collator\n        )\n\n        trainer.train()\n        \n        shutil.rmtree(self.model_dir)\n        \n        model_content.save_pretrained(self.model_dir)\n        self.tokenizer.save_pretrained(self.model_dir)\n\n        \n    def predict(self, \n                test_df: pd.DataFrame,\n                fold: int,\n               ):\n        \"\"\"predict content score\"\"\"\n        \n        sep = self.tokenizer.sep_token\n        in_text = (\n                    test_df[\"prompt_title\"] + sep \n                    + test_df[\"prompt_question\"] + sep \n                    + test_df[\"fixed_summary_text\"]\n                  )\n        test_df[self.input_col] = in_text\n\n        test_ = test_df[[self.input_col]]\n    \n        test_dataset = Dataset.from_pandas(test_, preserve_index=False) \n        test_tokenized_dataset = test_dataset.map(self.tokenize_function_test, batched=False)\n\n        model_content = AutoModelForSequenceClassification.from_pretrained(f\"{self.model_dir}\")\n        model_content.eval()\n        \n        # e.g. \"bert/fold_0/\"\n        model_fold_dir = os.path.join(self.model_dir, str(fold)) \n        model_fold_dir = \"valid_log\" #f\"bert-{fold}\"\n#         print(\"model_fold_dir\", model_fold_dir)\n        \n        test_args = TrainingArguments(\n            output_dir=model_fold_dir,\n            do_train = False,\n            do_predict = True,\n            per_device_eval_batch_size = 4,   \n            dataloader_drop_last = False,\n        )\n\n        # init trainer\n        infer_content = Trainer(\n                      model = model_content, \n                      tokenizer=self.tokenizer,\n                      data_collator=self.data_collator,\n                      args = test_args)\n\n        preds = infer_content.predict(test_tokenized_dataset)[0]\n\n        return preds","metadata":{"_uuid":"9fa2efe4-12f6-4a7e-9e99-52681f0dee3e","_cell_guid":"adef2ea6-24b4-43fc-9c23-48e5791ebe08","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T04:40:59.283574Z","iopub.execute_input":"2023-10-11T04:40:59.284032Z","iopub.status.idle":"2023-10-11T04:40:59.30648Z","shell.execute_reply.started":"2023-10-11T04:40:59.283999Z","shell.execute_reply":"2023-10-11T04:40:59.305577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_by_fold(\n        train_df: pd.DataFrame,\n        model_name: str,\n        target:str,\n        save_each_model: bool,\n        n_splits: int,\n        batch_size: int,\n        learning_rate: int,\n        hidden_dropout_prob: float,\n        attention_probs_dropout_prob: float,\n        weight_decay: float,\n        num_train_epochs: int,\n        save_steps: int,\n        max_length:int\n    ):\n\n    # delete old model files\n    if os.path.exists(model_name):\n        shutil.rmtree(model_name)\n    \n    os.mkdir(model_name)\n        \n    for fold in range(CFG.n_splits):\n        print(f\"fold {fold}:\")\n        \n        train_data = train_df[train_df[\"fold\"] != fold]\n        valid_data = train_df[train_df[\"fold\"] == fold]\n        \n        if save_each_model == True:\n            model_dir =  f\"{target}/{model_name}/fold_{fold}\"\n        else: \n            model_dir =  f\"{model_name}/fold_{fold}\"\n\n        csr = ContentScoreRegressor(\n            model_name=model_name,\n            target=target,\n            model_dir = model_dir, \n            hidden_dropout_prob=hidden_dropout_prob,\n            attention_probs_dropout_prob=attention_probs_dropout_prob,\n            max_length=max_length,\n           )\n        \n        csr.train(\n            fold=fold,\n            train_df=train_data,\n            valid_df=valid_data, \n            batch_size=batch_size,\n            learning_rate=learning_rate,\n            weight_decay=weight_decay,\n            num_train_epochs=num_train_epochs,\n            save_steps=save_steps,\n        )\n\ndef validate(\n    train_df: pd.DataFrame,\n    target:str,\n    save_each_model: bool,\n    model_name: str,\n    hidden_dropout_prob: float,\n    attention_probs_dropout_prob: float,\n    max_length : int\n    ) -> pd.DataFrame:\n    \"\"\"predict oof data\"\"\"\n    for fold in range(CFG.n_splits):\n        print(f\"fold {fold}:\")\n        \n        valid_data = train_df[train_df[\"fold\"] == fold]\n        \n        if save_each_model == True:\n            model_dir =  f\"{MODEL_DIR}/{target}/{model_name}/fold_{fold}\"\n        else: \n            model_dir =  f\"{MODEL_DIR}/{model_name}/fold_{fold}\"\n        \n#         print(model_dir, model_name)\n        csr = ContentScoreRegressor(\n            model_name=model_name,\n            target=target,\n            model_dir = model_dir,\n            hidden_dropout_prob=hidden_dropout_prob,\n            attention_probs_dropout_prob=attention_probs_dropout_prob,\n            max_length=max_length,\n           )\n        \n        pred = csr.predict(\n            test_df=valid_data, \n            fold=fold\n        )\n        \n        train_df.loc[valid_data.index, f\"{target}_pred\"] = pred\n\n    return train_df\n    \ndef predict(\n    test_df: pd.DataFrame,\n    target:str,\n    save_each_model: bool,\n    model_name: str,\n    hidden_dropout_prob: float,\n    attention_probs_dropout_prob: float,\n    max_length : int\n    ):\n    \"\"\"predict using mean folds\"\"\"\n\n    for fold in range(CFG.n_splits):\n        print(f\"fold {fold}:\")\n        \n        if save_each_model == True:\n            model_dir =  f\"{MODEL_DIR}/{target}/{model_name}/fold_{fold}\"\n        else: \n            model_dir =  f\"{MODEL_DIR}/{model_name}/fold_{fold}\"\n\n        csr = ContentScoreRegressor(\n            model_name=model_name,\n            target=target,\n            model_dir = model_dir, \n            hidden_dropout_prob=hidden_dropout_prob,\n            attention_probs_dropout_prob=attention_probs_dropout_prob,\n            max_length=max_length,\n           )\n        \n        pred = csr.predict(\n            test_df=test_df, \n            fold=fold\n        )\n        \n        test_df[f\"{target}_pred_{fold}\"] = pred\n    \n    test_df[f\"{target}\"] = test_df[[f\"{target}_pred_{fold}\" for fold in range(CFG.n_splits)]].mean(axis=1)\n\n    return test_df","metadata":{"_uuid":"0318bf89-467e-4d84-a6c5-cf59c9884fcd","_cell_guid":"883d2b7e-11a3-443a-bc23-ffdfb48d72b9","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T04:40:59.307935Z","iopub.execute_input":"2023-10-11T04:40:59.30839Z","iopub.status.idle":"2023-10-11T04:40:59.323493Z","shell.execute_reply.started":"2023-10-11T04:40:59.308357Z","shell.execute_reply":"2023-10-11T04:40:59.322694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for target in [\"content\", \"wording\"]:\n#     train_by_fold(\n#         train,\n#         model_name=CFG.model_name,\n#         save_each_model=True,\n#         target=target,\n#         learning_rate=CFG.learning_rate,\n#         hidden_dropout_prob=CFG.hidden_dropout_prob,\n#         attention_probs_dropout_prob=CFG.attention_probs_dropout_prob,\n#         weight_decay=CFG.weight_decay,\n#         num_train_epochs=CFG.num_train_epochs,\n#         n_splits=CFG.n_splits,\n#         batch_size=CFG.batch_size,\n#         save_steps=CFG.save_steps,\n#         max_length=CFG.max_length\n#     )\n    \n    print(\"[validate]\")\n    train = validate(\n        train,\n        target=target,\n        save_each_model=True,\n        model_name=CFG.model_name,\n        hidden_dropout_prob=CFG.hidden_dropout_prob,\n        attention_probs_dropout_prob=CFG.attention_probs_dropout_prob,\n        max_length=CFG.max_length\n    )\n\n    rmse = mean_squared_error(train[target], train[f\"{target}_pred\"], squared=False)\n    print(f\"cv {target} rmse: {rmse}\")\n\n    print(\"[test]\")\n    test = predict(\n        test,\n        target=target,\n        save_each_model=True,\n        model_name=CFG.model_name,\n        hidden_dropout_prob=CFG.hidden_dropout_prob,\n        attention_probs_dropout_prob=CFG.attention_probs_dropout_prob,\n        max_length=CFG.max_length\n    )","metadata":{"_uuid":"6668b40c-14db-4ca8-bb0a-f2df99bc271f","_cell_guid":"980a5129-79b1-4b2f-bf17-d2b6347666f1","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T04:40:59.324831Z","iopub.execute_input":"2023-10-11T04:40:59.325322Z","iopub.status.idle":"2023-10-11T04:53:09.195755Z","shell.execute_reply.started":"2023-10-11T04:40:59.325291Z","shell.execute_reply":"2023-10-11T04:53:09.193721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !rm -r wording content","metadata":{"_uuid":"51d8f82d-64f0-40ba-855f-94c777cd6459","_cell_guid":"b2a6e3db-d37a-452c-9d6f-99ffced9ee74","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T04:53:09.201149Z","iopub.execute_input":"2023-10-11T04:53:09.201858Z","iopub.status.idle":"2023-10-11T04:53:09.209796Z","shell.execute_reply.started":"2023-10-11T04:53:09.201819Z","shell.execute_reply":"2023-10-11T04:53:09.20885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"_uuid":"40ee3c9a-fe73-4067-9e86-129c66a4468e","_cell_guid":"eebd8eb2-cd40-45f5-8695-a581cb0a4ad9","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T04:53:09.211295Z","iopub.execute_input":"2023-10-11T04:53:09.211912Z","iopub.status.idle":"2023-10-11T04:53:09.268751Z","shell.execute_reply.started":"2023-10-11T04:53:09.21188Z","shell.execute_reply":"2023-10-11T04:53:09.267781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Add Features","metadata":{"_uuid":"adc28720-5b84-4e75-aca2-962a5eb452d3","_cell_guid":"f9bd0969-3957-456c-86fe-25bac5bc476b","trusted":true}},{"cell_type":"code","source":"wd = pd.read_csv('/kaggle/input/worddifficulty/WordDifficulty.csv')\ndic = dict(zip(wd['Word'], wd['I_Zscore']))","metadata":{"_uuid":"e0480832-1515-4505-99b9-6507f5dbecf3","_cell_guid":"27812e4e-a0aa-40d9-b9e9-6aeb3cdf2454","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T04:53:09.270433Z","iopub.execute_input":"2023-10-11T04:53:09.271306Z","iopub.status.idle":"2023-10-11T04:53:09.362913Z","shell.execute_reply.started":"2023-10-11T04:53:09.271252Z","shell.execute_reply":"2023-10-11T04:53:09.362046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def difficulty(data) :\n    words = word_tokenize(data['text'])\n#     s = ['``','\\'\\'','.',',']\n#     stop_words = set(stopwords.words('english') + s)\n#     filtered_words = [word for word in words if word.lower() not in stop_words]\n    filtered_words = words\n    score = 0\n    num = 0\n    sep = 0\n    for e in filtered_words:\n        if e in dic:\n            score += dic[e]\n            num+=1\n        elif e == '.' or e == ',' :\n            sep+=1\n        else:\n            pass\n#             print(e,\"**\")\n\n    nn = max(1, len(filtered_words))\n    sep = max(1, sep)\n    num = max(1, num)\n    return score/num, score/nn, nn/sep, (nn-num)/nn\n\n\n\n#         prompts[\"prompt_tokens\"] = prompts[\"prompt_text\"].apply(\n#             lambda x: word_tokenize(x)\n#         )\nlabels = ['difficulty0', 'difficulty1', 'ave_text_len', 'unknown_words']\ntrain[labels]=train.apply(lambda x:difficulty(x),axis=1, result_type='expand')\ntest[labels]=test.apply(lambda x:difficulty(x),axis=1, result_type='expand')","metadata":{"_uuid":"a2c40112-5e6d-4e2a-b256-f4cb267665d1","_cell_guid":"133ccada-b57c-48d6-9697-fad432e723ec","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T04:53:09.364323Z","iopub.execute_input":"2023-10-11T04:53:09.364554Z","iopub.status.idle":"2023-10-11T04:53:13.217657Z","shell.execute_reply.started":"2023-10-11T04:53:09.364526Z","shell.execute_reply":"2023-10-11T04:53:13.216807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from readability import Readability","metadata":{"_uuid":"e66fe33d-aa57-4b88-8829-013f5edcb33c","_cell_guid":"d9408f71-a541-4c26-b96d-4c41e9df3f74","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T04:53:13.218821Z","iopub.execute_input":"2023-10-11T04:53:13.219056Z","iopub.status.idle":"2023-10-11T04:53:13.231326Z","shell.execute_reply.started":"2023-10-11T04:53:13.219027Z","shell.execute_reply":"2023-10-11T04:53:13.23029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"no_score = -10000\ndef rscore(data) :\n    txt = data['text']\n    words = word_tokenize(txt)\n    n = len(words) + 1\n    if n == 0 : \n        return no_score, no_score, no_score, no_score, no_score, no_score, no_score, no_score\n    tot = n\n    new = txt\n    while tot < 200 :\n        new += \" \" + txt\n        tot += n\n    r = Readability(new)\n    try :\n        ret = (r.flesch_kincaid().score, r.flesch().score, r.gunning_fog().score,\n               r.coleman_liau().score,r.dale_chall().score, r.ari().score,\n               r.linsear_write().score, r.spache().score)\n    except:\n        return no_score, no_score, no_score, no_score, no_score, no_score, no_score, no_score\n    return ret\n\nlabels = ['flesch_kincaid', 'flesch', 'gunning_fog', 'coleman_liau',\n         'dale_chall','ari','linsear_write','spache']\n\ntrain[labels]=train.progress_apply(lambda x:rscore(x),axis=1, result_type='expand')\ntest[labels]=test.progress_apply(lambda x:rscore(x),axis=1, result_type='expand')","metadata":{"_uuid":"9bedf40f-9dbd-49b7-ba43-12ce131fa141","_cell_guid":"0e3fcc4a-2fb5-4e46-b70a-f60335858b46","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T04:53:13.232601Z","iopub.execute_input":"2023-10-11T04:53:13.232861Z","iopub.status.idle":"2023-10-11T04:54:25.762732Z","shell.execute_reply.started":"2023-10-11T04:53:13.232827Z","shell.execute_reply":"2023-10-11T04:54:25.761917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport nltk\n\ndic = dict()\nfor i in range(len(prompts_train)) :\n    pid = prompts_train['prompt_id'][i]\n    txt = prompts_train['prompt_text'][i]\n    original_tokens = nltk.word_tokenize(txt)\n    original_text = ' '.join(original_tokens)\n    dic[pid] = original_text\nfor i in range(len(prompts_test)) :\n    pid = prompts_test['prompt_id'][i]\n    txt = prompts_test['prompt_text'][i]\n    original_tokens = nltk.word_tokenize(txt)\n    original_text = ' '.join(original_tokens)\n    dic[pid] = original_text\n    \ndic.keys()\n\ndef cosine_sim(data):\n    original_text = dic[data['prompt_id']]\n    summary = data['fixed_summary_text']\n#     original_tokens = nltk.word_tokenize(original_text)\n    summary_tokens = nltk.word_tokenize(summary)\n\n    # トークンを結合して文に戻す\n#     original_text = ' '.join(original_tokens)\n    summary = ' '.join(summary_tokens)\n\n    # CountVectorizerを使用して文をベクトル化\n    vectorizer = CountVectorizer().fit_transform([original_text, summary])\n\n    # コサイン類似度を計算\n    cosine_scores = cosine_similarity(vectorizer)\n\n    # 要約と元の文章の類似度を表示\n    similarity_score = cosine_scores[0][1]\n    \n    return similarity_score","metadata":{"_uuid":"f65c5645-da57-4e24-830e-b26b5d997f37","_cell_guid":"53e49bbc-91be-4bd4-95ca-2959aa26ed56","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T04:54:25.764133Z","iopub.execute_input":"2023-10-11T04:54:25.764585Z","iopub.status.idle":"2023-10-11T04:54:25.791341Z","shell.execute_reply.started":"2023-10-11T04:54:25.764552Z","shell.execute_reply":"2023-10-11T04:54:25.790334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cosine_sim(train.iloc[0])\ntrain['cos_sim']=train.progress_apply(lambda x:cosine_sim(x),axis=1, result_type='expand')\ntest['cos_sim']=test.progress_apply(lambda x:cosine_sim(x),axis=1, result_type='expand')","metadata":{"_uuid":"9aa85842-339f-45ba-972e-cc1ccf3e39ed","_cell_guid":"b87f2b3b-69f3-4ab5-b167-9d221eba6579","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T04:54:25.79251Z","iopub.execute_input":"2023-10-11T04:54:25.792764Z","iopub.status.idle":"2023-10-11T04:54:44.547645Z","shell.execute_reply.started":"2023-10-11T04:54:25.792733Z","shell.execute_reply":"2023-10-11T04:54:44.546683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import textstat\ndef txts(data):\n    text = data['text']\n    return (\n        textstat.flesch_reading_ease(text),\n        textstat.flesch_kincaid_grade(text),\n        textstat.gunning_fog(text),\n        textstat.smog_index(text),\n        textstat.automated_readability_index(text),\n        textstat.coleman_liau_index(text),\n        textstat.linsear_write_formula(text),\n        textstat.dale_chall_readability_score(text),\n        textstat.text_standard(text, float_output=True),\n        textstat.reading_time(text, ms_per_char=14.69),\n        textstat.syllable_count(text),\n        textstat.lexicon_count(text, removepunct=True),\n        textstat.sentence_count(text),\n        textstat.char_count(text, ignore_spaces=True),\n        textstat.letter_count(text, ignore_spaces=True),\n        textstat.monosyllabcount(text)\n    )\n\nsample = txts(train.iloc[1])\nprint(sample)\nlabels = [f'f{i}' for i in range(len(sample))]\ntrain[labels]=train.progress_apply(lambda x:txts(x),axis=1, result_type='expand')\ntest[labels]=test.progress_apply(lambda x:txts(x),axis=1, result_type='expand')","metadata":{"_uuid":"17e25dff-49de-4ade-b65c-291994350c50","_cell_guid":"d5721b2f-7356-4aef-83cf-4ad4b3acf36b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T04:54:44.55479Z","iopub.execute_input":"2023-10-11T04:54:44.555595Z","iopub.status.idle":"2023-10-11T04:54:51.923579Z","shell.execute_reply.started":"2023-10-11T04:54:44.555559Z","shell.execute_reply":"2023-10-11T04:54:51.922761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train","metadata":{"_uuid":"fde342e3-6aba-4759-93c4-faad2c7340a7","_cell_guid":"9afe4f0b-5815-466f-ab86-7fb1836bebda","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T04:54:51.92476Z","iopub.execute_input":"2023-10-11T04:54:51.925457Z","iopub.status.idle":"2023-10-11T04:54:51.959475Z","shell.execute_reply.started":"2023-10-11T04:54:51.925409Z","shell.execute_reply":"2023-10-11T04:54:51.958671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.to_csv(\"train0.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-10-11T04:54:51.960818Z","iopub.execute_input":"2023-10-11T04:54:51.961263Z","iopub.status.idle":"2023-10-11T04:54:53.158944Z","shell.execute_reply.started":"2023-10-11T04:54:51.961229Z","shell.execute_reply":"2023-10-11T04:54:53.158067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LGBM model","metadata":{"_uuid":"e017febd-bb8a-47b2-9758-2a6785a25a65","_cell_guid":"8f92b222-3f4a-45fa-979e-250b1cda06a6","trusted":true}},{"cell_type":"code","source":"targets = [\"content\", \"wording\"]\n\ndrop_columns = [\"fold\", \"student_id\", \"prompt_id\", \"text\", \"fixed_summary_text\",\n                \"prompt_question\", \"prompt_title\", \n                \"prompt_text\"\n               ] + targets","metadata":{"_uuid":"86d85c47-94cc-429c-8994-4f0346a2e288","_cell_guid":"06b20091-6512-4d48-acf7-eb7016da6b30","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T04:54:53.160554Z","iopub.execute_input":"2023-10-11T04:54:53.161085Z","iopub.status.idle":"2023-10-11T04:54:53.165908Z","shell.execute_reply.started":"2023-10-11T04:54:53.161051Z","shell.execute_reply":"2023-10-11T04:54:53.165035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[train[\"fold\"] != 0].drop(columns=drop_columns)","metadata":{"_uuid":"96f71686-ec1d-4452-8a47-a66025afabfd","_cell_guid":"78aa25ec-8764-4f3e-a506-ea294de29031","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T04:54:53.167193Z","iopub.execute_input":"2023-10-11T04:54:53.168047Z","iopub.status.idle":"2023-10-11T04:54:53.20133Z","shell.execute_reply.started":"2023-10-11T04:54:53.168016Z","shell.execute_reply":"2023-10-11T04:54:53.200572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import optuna\n# def objective(trial):\n    \n#     model_dict = {}\n\n#     for target in targets:\n#         models = []\n\n#         for fold in range(CFG.n_splits):\n\n#             X_train_cv = train[train[\"fold\"] != fold].drop(columns=drop_columns)\n#             y_train_cv = train[train[\"fold\"] != fold][target]\n\n#             X_eval_cv = train[train[\"fold\"] == fold].drop(columns=drop_columns)\n#             y_eval_cv = train[train[\"fold\"] == fold][target]\n\n#             dtrain = lgb.Dataset(X_train_cv, label=y_train_cv)\n#             dval = lgb.Dataset(X_eval_cv, label=y_eval_cv)\n\n#             params = {\n#                 'boosting_type': 'gbdt',\n#                 'random_state': 42,\n#                 'objective': 'regression',\n#                 'metric': 'rmse',\n#                 'learning_rate': trial.suggest_uniform('learning_rate', 0.01, 0.2),\n#                 'max_depth': trial.suggest_int('max_depth', 3, 10),\n#                 'lambda_l1': trial.suggest_uniform('lambda_l1', 0, 0.01),\n#                 'lambda_l2': trial.suggest_uniform('lambda_l2', 0, 0.10),\n#                 'num_leaves': trial.suggest_int('num_leaves', 16, 64),\n#                 'verbosity': -1\n#             }\n\n#             evaluation_results = {}\n#             model = lgb.train(params,\n#                               num_boost_round=10000,\n#                                 #categorical_feature = categorical_features,\n#                               valid_names=['train', 'valid'],\n#                               train_set=dtrain,\n#                               valid_sets=dval,\n#                               callbacks=[\n#                                   lgb.early_stopping(stopping_rounds=30, verbose=False),\n# #                                   lgb.log_evaluation(100),\n# #                                   lgb.callback.record_evaluation(evaluation_results)\n#                                 ],\n#                               )\n#             models.append(model)\n\n#         model_dict[target] = models\n\n#         rmses = []\n\n#     for target in targets:\n#         models = model_dict[target]\n\n#         preds = []\n#         trues = []\n\n#         for fold, model in enumerate(models):\n#             X_eval_cv = train[train[\"fold\"] == fold].drop(columns=drop_columns)\n#             y_eval_cv = train[train[\"fold\"] == fold][target]\n\n#             pred = model.predict(X_eval_cv)\n\n#             trues.extend(y_eval_cv)\n#             preds.extend(pred)\n\n#         rmse = np.sqrt(mean_squared_error(trues, preds))\n# #         print(f\"{target}_rmse : {rmse}\")\n#         rmses = rmses + [rmse]\n\n#     print(f\"mcrmse : {sum(rmses) / len(rmses)}\")\n#     return sum(rmses) / len(rmses)\n        \n\n# study = optuna.create_study(direction='minimize')\n# study.optimize(objective, n_trials=30)\n\n# print('Number of finished trials:', len(study.trials))\n# print('Best trial:', study.best_trial.params)","metadata":{"_uuid":"7b65245b-b3d8-4746-afd0-0400162aed46","_cell_guid":"e801b18b-71ef-4f09-9c80-268263056b85","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T04:54:53.202899Z","iopub.execute_input":"2023-10-11T04:54:53.203127Z","iopub.status.idle":"2023-10-11T04:54:53.209893Z","shell.execute_reply.started":"2023-10-11T04:54:53.203097Z","shell.execute_reply":"2023-10-11T04:54:53.209114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_dict = {}\n\nfor target in targets:\n    models = []\n    \n    for fold in range(CFG.n_splits):\n\n        X_train_cv = train[train[\"fold\"] != fold].drop(columns=drop_columns)\n        y_train_cv = train[train[\"fold\"] != fold][target]\n\n        X_eval_cv = train[train[\"fold\"] == fold].drop(columns=drop_columns)\n        y_eval_cv = train[train[\"fold\"] == fold][target]\n\n        dtrain = lgb.Dataset(X_train_cv, label=y_train_cv)\n        dval = lgb.Dataset(X_eval_cv, label=y_eval_cv)\n\n        params = {\n            'boosting_type': 'gbdt',\n            'random_state': 42,\n            'objective': 'regression',\n            'metric': 'rmse',\n            'learning_rate': 0.048,\n            'max_depth': 4,\n            'lambda_l1': 0.001,\n            'lambda_l2': 0.011\n        }\n\n        evaluation_results = {}\n        model = lgb.train(params,\n                          num_boost_round=10000,\n                            #categorical_feature = categorical_features,\n                          valid_names=['train', 'valid'],\n                          train_set=dtrain,\n                          valid_sets=dval,\n                          callbacks=[\n                              lgb.early_stopping(stopping_rounds=30, verbose=True),\n                              lgb.log_evaluation(100),\n                              lgb.callback.record_evaluation(evaluation_results)\n                            ],\n                          )\n        models.append(model)\n    \n    model_dict[target] = models","metadata":{"_uuid":"870c5829-305e-40b5-83b8-98fd556048dc","_cell_guid":"71db9659-beb4-4ac4-9913-d566a8b5d7eb","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T04:54:53.211405Z","iopub.execute_input":"2023-10-11T04:54:53.211998Z","iopub.status.idle":"2023-10-11T04:54:55.788184Z","shell.execute_reply.started":"2023-10-11T04:54:53.211964Z","shell.execute_reply":"2023-10-11T04:54:55.787366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CV Score","metadata":{"_uuid":"60cb81aa-d4aa-4b0a-aad5-1dfd3b207484","_cell_guid":"23411a48-39b2-4372-8c1b-ad6b0a252098","trusted":true}},{"cell_type":"code","source":"# cv\nrmses = []\n\nfor target in targets:\n    models = model_dict[target]\n\n    preds = []\n    trues = []\n    \n    for fold, model in enumerate(models):\n        X_eval_cv = train[train[\"fold\"] == fold].drop(columns=drop_columns)\n        y_eval_cv = train[train[\"fold\"] == fold][target]\n\n        pred = model.predict(X_eval_cv)\n\n        trues.extend(y_eval_cv)\n        preds.extend(pred)\n        \n    rmse = np.sqrt(mean_squared_error(trues, preds))\n    print(f\"{target}_rmse : {rmse}\")\n    rmses = rmses + [rmse]\n\nprint(f\"mcrmse : {sum(rmses) / len(rmses)}\")","metadata":{"_uuid":"9852b67a-a867-43b3-9349-5ac07b900165","_cell_guid":"a3dc69bb-e51b-45b6-8815-6424bfaa2836","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T04:54:55.789379Z","iopub.execute_input":"2023-10-11T04:54:55.790157Z","iopub.status.idle":"2023-10-11T04:54:55.889301Z","shell.execute_reply.started":"2023-10-11T04:54:55.790124Z","shell.execute_reply":"2023-10-11T04:54:55.888461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predict","metadata":{"_uuid":"f8023dd1-41b3-470b-bc09-048a1c580fa5","_cell_guid":"e55f8a1d-1ef1-4618-aaf8-166fa7c2343e","trusted":true}},{"cell_type":"code","source":"drop_columns = [\n                #\"fold\", \n                \"student_id\", \"prompt_id\", \"text\", \"fixed_summary_text\",\n                \"prompt_question\", \"prompt_title\", \n                \"prompt_text\",\n                \"input\"\n               ] + [\n                f\"content_pred_{i}\" for i in range(CFG.n_splits)\n                ] + [\n                f\"wording_pred_{i}\" for i in range(CFG.n_splits)\n                ]","metadata":{"_uuid":"31ec3bc9-fd51-4970-b609-1f2616475fa8","_cell_guid":"3e551948-a833-4f18-9f75-0ae88056b5c4","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T04:54:55.890772Z","iopub.execute_input":"2023-10-11T04:54:55.891003Z","iopub.status.idle":"2023-10-11T04:54:55.896011Z","shell.execute_reply.started":"2023-10-11T04:54:55.890972Z","shell.execute_reply":"2023-10-11T04:54:55.895187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_dict = {}\nfor target in targets:\n    models = model_dict[target]\n    preds = []\n\n    for fold, model in enumerate(models):\n        X_eval_cv = test.drop(columns=drop_columns)\n\n        pred = model.predict(X_eval_cv)\n        preds.append(pred)\n    \n    pred_dict[target] = preds","metadata":{"_uuid":"1fd45293-6670-4577-b0f1-97fe39a6789a","_cell_guid":"c3c4e2b3-7bae-4a0c-8103-b6557732f087","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T04:54:55.897428Z","iopub.execute_input":"2023-10-11T04:54:55.89844Z","iopub.status.idle":"2023-10-11T04:54:55.923526Z","shell.execute_reply.started":"2023-10-11T04:54:55.898392Z","shell.execute_reply":"2023-10-11T04:54:55.922794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for target in targets:\n    preds = pred_dict[target]\n    for i, pred in enumerate(preds):\n        test[f\"{target}_pred_{i}\"] = pred\n\n    test[target] = test[[f\"{target}_pred_{fold}\" for fold in range(CFG.n_splits)]].mean(axis=1)","metadata":{"_uuid":"e90f3096-9310-49ae-9509-6411257cf3be","_cell_guid":"20f73a8d-52f0-4558-86f7-0b7264dff0ed","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T04:54:55.925367Z","iopub.execute_input":"2023-10-11T04:54:55.925814Z","iopub.status.idle":"2023-10-11T04:54:55.935428Z","shell.execute_reply.started":"2023-10-11T04:54:55.925782Z","shell.execute_reply":"2023-10-11T04:54:55.934564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test","metadata":{"_uuid":"3abd7cbc-7496-47ab-92fa-25f09c190749","_cell_guid":"45325d98-6021-4197-ad31-a30059f366cd","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T04:54:55.936938Z","iopub.execute_input":"2023-10-11T04:54:55.937367Z","iopub.status.idle":"2023-10-11T04:54:55.959621Z","shell.execute_reply.started":"2023-10-11T04:54:55.937333Z","shell.execute_reply":"2023-10-11T04:54:55.958732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create Submission file","metadata":{"_uuid":"ab90bd8c-0a4f-4704-b439-6611698aff6d","_cell_guid":"9c8d82ce-a0e6-4bc6-b2b4-d62a6cda5fe8","trusted":true}},{"cell_type":"code","source":"sample_submission","metadata":{"_uuid":"dc7203b4-a331-4a13-b64c-6f3e4d050743","_cell_guid":"7ed95aad-7615-4b19-8a52-4a31fdb80c55","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T04:54:55.960824Z","iopub.execute_input":"2023-10-11T04:54:55.96135Z","iopub.status.idle":"2023-10-11T04:54:55.970737Z","shell.execute_reply.started":"2023-10-11T04:54:55.961318Z","shell.execute_reply":"2023-10-11T04:54:55.969798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test[[\"student_id\", \"content\", \"wording\"]]","metadata":{"_uuid":"d8db8641-264c-438e-945c-57b55e7f744d","_cell_guid":"995f80b0-7d38-4b15-90f6-1995a4deba27","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T04:54:55.971976Z","iopub.execute_input":"2023-10-11T04:54:55.972557Z","iopub.status.idle":"2023-10-11T04:54:55.985021Z","shell.execute_reply.started":"2023-10-11T04:54:55.972527Z","shell.execute_reply":"2023-10-11T04:54:55.984205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test[[\"student_id\", \"content\", \"wording\"]].to_csv(\"submission.csv\", index=False)","metadata":{"_uuid":"f6eaa8ed-c858-4d41-acf0-02e4f36c7618","_cell_guid":"156af440-8e66-484b-b5fa-db04f996979a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T04:54:55.986464Z","iopub.execute_input":"2023-10-11T04:54:55.986988Z","iopub.status.idle":"2023-10-11T04:54:55.997126Z","shell.execute_reply.started":"2023-10-11T04:54:55.986958Z","shell.execute_reply":"2023-10-11T04:54:55.996273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Summary\n\nCV result is like this.\n\n| | content rmse |wording rmse | mcrmse | LB| |\n| -- | -- | -- | -- | -- | -- |\n|baseline| 0.494 | 0.630 | 0.562 | 0.509 | [link](https://www.kaggle.com/code/tsunotsuno/debertav3-baseline-content-and-wording-models)|\n| use title and question field | 0.476| 0.619 | 0.548 | 0.508 | [link](https://www.kaggle.com/code/tsunotsuno/debertav3-w-prompt-title-question-fields) |\n| Debertav3 + LGBM | 0.451 | 0.591 | 0.521 | 0.461 | [link](https://www.kaggle.com/code/tsunotsuno/debertav3-lgbm-with-feature-engineering) |\n| Debertav3 + LGBM with spell autocorrect | 0.448 | 0.581 | 0.514 | 0.459 |nogawanogawa's original code\n| Debertav3 + LGBM with spell autocorrect and tuning | 0.442 | 0.566 | 0.504 | 0.453 | this notebook |\n\nThe CV values improved slightly, and the LB value is improved.","metadata":{"_uuid":"66e53e98-80f3-475d-874c-91af609dcc87","_cell_guid":"b66f8d3d-5b48-4441-98e4-77f6e72b2e58","trusted":true}},{"cell_type":"code","source":"test_zero = test","metadata":{"_uuid":"862eebcc-e8cd-4c7a-83a5-be51fc5c4743","_cell_guid":"7ec393c2-1b40-4ab9-9fb0-856af1aab144","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T04:54:55.998632Z","iopub.execute_input":"2023-10-11T04:54:55.999154Z","iopub.status.idle":"2023-10-11T04:54:56.007385Z","shell.execute_reply.started":"2023-10-11T04:54:55.999124Z","shell.execute_reply":"2023-10-11T04:54:56.006475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"53694c6f-839a-481a-8ce8-7f2a372cd01b","_cell_guid":"5c413f78-3577-4c77-b24c-fa77e4924e57","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"#  [code] {\"jupyter\":{\"outputs_hidden\":false}}","metadata":{"_uuid":"25698b6f-2d00-4b4f-939e-0794fb926f8e","_cell_guid":"a31226c8-fad1-47de-89c4-f7e4969a3523","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T04:54:56.008967Z","iopub.execute_input":"2023-10-11T04:54:56.009308Z","iopub.status.idle":"2023-10-11T04:54:56.019393Z","shell.execute_reply.started":"2023-10-11T04:54:56.00923Z","shell.execute_reply":"2023-10-11T04:54:56.01851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import sys\n# sys.path.append(\"/kaggle/input/pip-install-nlp-mit\")","metadata":{"_uuid":"bab995f2-98ef-409c-8a29-175b3ce9f3d7","_cell_guid":"e479e817-2741-422c-b42f-7041416b8164","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T04:54:56.020651Z","iopub.execute_input":"2023-10-11T04:54:56.021793Z","iopub.status.idle":"2023-10-11T04:54:56.029629Z","shell.execute_reply.started":"2023-10-11T04:54:56.021749Z","shell.execute_reply":"2023-10-11T04:54:56.028792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install \"/kaggle/input/worddifficulty/py_readability_metrics-1.4.5-py3-none-any.whl\"","metadata":{"_uuid":"c03df1f4-80f3-43f2-a8a3-94f818c1d89e","_cell_guid":"8bc5908d-68cb-4345-9e74-9b4d1d2a7815","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T04:54:56.030937Z","iopub.execute_input":"2023-10-11T04:54:56.031464Z","iopub.status.idle":"2023-10-11T04:54:56.040051Z","shell.execute_reply.started":"2023-10-11T04:54:56.031435Z","shell.execute_reply":"2023-10-11T04:54:56.039238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1st MODEL","metadata":{"_uuid":"8b15a2fa-44bc-4dbe-a3ce-2f6dda30fa7f","_cell_guid":"dc493868-112d-41cd-af63-6243c8e859bd","trusted":true}},{"cell_type":"code","source":"# !pip install \"/kaggle/input/autocorrect/autocorrect-2.6.1.tar\"\n# !pip install \"/kaggle/input/pyspellchecker/pyspellchecker-0.7.2-py3-none-any.whl\"","metadata":{"_uuid":"1947f9a8-b4e1-4f01-ad69-9a09abc9d369","_cell_guid":"31ba055b-2b47-485c-beb3-f5da365f86d2","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T04:54:56.04141Z","iopub.execute_input":"2023-10-11T04:54:56.041921Z","iopub.status.idle":"2023-10-11T04:54:56.050566Z","shell.execute_reply.started":"2023-10-11T04:54:56.041891Z","shell.execute_reply":"2023-10-11T04:54:56.04969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from typing import List\nimport numpy as np\nimport pandas as pd\nimport warnings\nimport logging\nimport os\nimport shutil\nimport json\nimport transformers\nfrom transformers import AutoModel, AutoTokenizer, AutoConfig, AutoModelForSequenceClassification\nfrom transformers import DataCollatorWithPadding\nfrom datasets import Dataset,load_dataset, load_from_disk\nfrom transformers import TrainingArguments, Trainer\nfrom datasets import load_metric, disable_progress_bar\nfrom sklearn.metrics import mean_squared_error\nimport torch\nfrom sklearn.model_selection import KFold, GroupKFold\nfrom tqdm import tqdm\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize.treebank import TreebankWordDetokenizer\nfrom collections import Counter\nimport spacy\nimport re\nfrom autocorrect import Speller\nfrom spellchecker import SpellChecker\nimport lightgbm as lgb\n\nwarnings.simplefilter(\"ignore\")\nlogging.disable(logging.ERROR)\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \ndisable_progress_bar()\ntqdm.pandas()","metadata":{"_uuid":"cb460702-e731-42b7-bf11-123714a52d4b","_cell_guid":"b8cae13f-56bc-4f7e-ac40-fd3c3b8e42f4","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T04:54:56.051716Z","iopub.execute_input":"2023-10-11T04:54:56.052411Z","iopub.status.idle":"2023-10-11T04:54:56.067321Z","shell.execute_reply.started":"2023-10-11T04:54:56.052359Z","shell.execute_reply":"2023-10-11T04:54:56.066402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed: int):\n    import random, os\n    import numpy as np\n    import torch\n    \n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    \nseed_everything(seed=42)","metadata":{"_uuid":"dbcfa4e4-5e7b-4340-9f96-c0f3ae0623e5","_cell_guid":"c6ca0b3a-dd06-4f80-907e-a6cd4412a6d1","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T04:54:56.06858Z","iopub.execute_input":"2023-10-11T04:54:56.069554Z","iopub.status.idle":"2023-10-11T04:54:56.07876Z","shell.execute_reply.started":"2023-10-11T04:54:56.069523Z","shell.execute_reply":"2023-10-11T04:54:56.077836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    model_name = \"another-bert\"\n    learning_rate=1.5e-5\n    weight_decay=0.02\n    hidden_dropout_prob=0.007\n    attention_probs_dropout_prob=0.007\n    num_train_epochs=5\n    n_splits=4\n    batch_size=12\n    \n    random_seed=42\n    save_steps=20\n    max_length=512","metadata":{"_uuid":"1e2d2d64-8143-416b-8c08-7f1c2b477f85","_cell_guid":"0ca1e0fc-8c05-44ed-8079-a1b81b07619e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T04:54:56.080168Z","iopub.execute_input":"2023-10-11T04:54:56.080392Z","iopub.status.idle":"2023-10-11T04:54:56.090211Z","shell.execute_reply.started":"2023-10-11T04:54:56.080363Z","shell.execute_reply":"2023-10-11T04:54:56.089477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MODEL_DIR = '/kaggle/input/commitlit-deberta-v3-large-trained'\nINIT_MODEL = f\"{MODEL_DIR}/content/{CFG.model_name}/fold_0\"","metadata":{"_uuid":"750f1aa3-2010-4f8b-be6d-72bb8a27eaa3","_cell_guid":"a5f1fbd2-49b5-476b-a6be-f04bfdb52eff","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T04:54:56.091756Z","iopub.execute_input":"2023-10-11T04:54:56.09198Z","iopub.status.idle":"2023-10-11T04:54:56.101194Z","shell.execute_reply.started":"2023-10-11T04:54:56.091952Z","shell.execute_reply":"2023-10-11T04:54:56.100305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataload","metadata":{"_uuid":"3fedebce-9666-453f-9e0a-c3eae689e1a8","_cell_guid":"3a227745-6ca9-481a-aa3f-9797e216292b","trusted":true}},{"cell_type":"code","source":"DATA_DIR = \"/kaggle/input/commonlit-evaluate-student-summaries/\"\n\nprompts_train = pd.read_csv(DATA_DIR + \"prompts_train.csv\")\nprompts_test = pd.read_csv(DATA_DIR + \"prompts_test.csv\")\nsummaries_train = pd.read_csv(DATA_DIR + \"summaries_train.csv\")\nsummaries_test = pd.read_csv(DATA_DIR + \"summaries_test.csv\")\nsample_submission = pd.read_csv(DATA_DIR + \"sample_submission.csv\")","metadata":{"_uuid":"805e089a-e539-4258-9909-cc3b8ea06ccf","_cell_guid":"68a6c340-bd71-49a5-a76e-15ab322f0800","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T04:54:56.102718Z","iopub.execute_input":"2023-10-11T04:54:56.102957Z","iopub.status.idle":"2023-10-11T04:54:56.208065Z","shell.execute_reply.started":"2023-10-11T04:54:56.102929Z","shell.execute_reply":"2023-10-11T04:54:56.207161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Meta Data Cleansing","metadata":{"_uuid":"db4c803d-24c7-4409-ad53-737f1d984098","_cell_guid":"f7dacaa3-f90b-4559-900c-9924afde4c2d","trusted":true}},{"cell_type":"code","source":"import pandas as pd\nimport spacy\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\nimport logging\n\n# Initialize logging\nlogging.basicConfig(level=logging.INFO)\n\n# Load Spacy model\nnlp = spacy.load('en_core_web_sm')\n\nclass FeatureEngineering:\n    \n    def __init__(self, df):\n        self.df = df\n        self.df['grade'].fillna(0, inplace=True)  # Fill NA values in 'grade' with 0\n\n    def classify_author(self, author):\n        doc = nlp(author)\n        for ent in doc.ents:\n            if ent.label_ == 'PERSON':\n                return 'person'\n        return 'org'\n\n    def encode_author_type(self):\n        self.df['author_type'] = self.df['author'].apply(self.classify_author)\n        le = LabelEncoder()\n        self.df['author_type'] = le.fit_transform(self.df['author_type'])\n\n    def frequency_encoding(self):\n        logging.info(\"Applying Frequency Encoding on 'author'\")\n        self.df['author_frequency'] = self.df['author'].map(self.df['author'].value_counts())\n\n    def one_hot_encoding(self):\n        logging.info(\"Applying One-Hot Encoding on 'genre'\")\n        onehot_encoder = OneHotEncoder(sparse=False)\n        genre_onehot = onehot_encoder.fit_transform(self.df[['genre']])\n        df_onehot = pd.DataFrame(genre_onehot, columns=onehot_encoder.get_feature_names_out(['genre']))\n        self.df = pd.concat([self.df, df_onehot], axis=1)\n\n    def feature_scaling(self):\n        logging.info(\"Applying Feature Scaling on 'lexile'\")\n        scaler = StandardScaler()\n        self.df['lexile_scaled'] = scaler.fit_transform(self.df[['lexile']])\n\n    def transform(self):\n        self.encode_author_type()\n        self.frequency_encoding()\n#         self.one_hot_encoding()\n        self.feature_scaling()\n        return self.df\n\n# Initialize FeatureEngineering class and apply transformations\nprompt_grade = pd.read_csv(r'/kaggle/input/commonlit-texts/commonlit_texts.csv')\nfeature_engineer = FeatureEngineering(prompt_grade)\ntransformed_df = feature_engineer.transform()\n\n# Display the transformed DataFrame\nprompt_grade = transformed_df","metadata":{"_uuid":"40a7723a-b69e-4a01-b599-7601583b4b34","_cell_guid":"adb6a272-8a0a-48a7-b8f1-12b3e4062c5a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T04:54:56.209322Z","iopub.execute_input":"2023-10-11T04:54:56.209779Z","iopub.status.idle":"2023-10-11T04:55:10.139985Z","shell.execute_reply.started":"2023-10-11T04:54:56.209748Z","shell.execute_reply":"2023-10-11T04:55:10.139119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"keep_columns = ['title','author','description','grade','genre','lexile','lexile_scaled','is_prose','author_type','author_frequency']\nprompt_grade = prompt_grade[keep_columns]\n\ndef preprocess_and_join(df1, df2, df1_title_col, df2_title_col, grade_col):\n    # Copy dataframes to avoid modifying the originals\n    df1 = df1.copy()\n    df2 = df2.copy()\n\n    # Preprocess titles\n    df1[df1_title_col] = df1[df1_title_col].str.replace('\"', '').str.strip()\n    df2[df2_title_col] = df2[df2_title_col].str.replace('\"', '').str.strip()\n\n    # Remove duplicate grades\n    df2 = df2.drop_duplicates(subset=df2_title_col, keep='first')\n\n    # Join dataframes\n    merged_df = df1.merge(df2, how='left', left_on=df1_title_col, right_on=df2_title_col)\n    \n\n    # Postprocess grades\n    merged_df[grade_col] = merged_df[grade_col].fillna(0)\n    merged_df[grade_col] = merged_df[grade_col].astype(int).astype('category')\n\n \n    return merged_df\n\nprompts_train = preprocess_and_join(prompts_train, prompt_grade, 'prompt_title', 'title', 'grade')\nprompts_test = preprocess_and_join(prompts_test, prompt_grade, 'prompt_title', 'title', 'grade')","metadata":{"_uuid":"d6d74931-bbe3-4328-9e7b-263ebb411c68","_cell_guid":"d0a905c2-89a3-4d0c-8073-cce703f78af7","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T04:55:10.141306Z","iopub.execute_input":"2023-10-11T04:55:10.141544Z","iopub.status.idle":"2023-10-11T04:55:10.17876Z","shell.execute_reply.started":"2023-10-11T04:55:10.141516Z","shell.execute_reply":"2023-10-11T04:55:10.177921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install \"/kaggle/input/pyphen-0100/Pyphen-0.10.0-py3-none-any.whl\"","metadata":{"_uuid":"96060e4b-9b4b-40fe-ac8b-842091ae42fa","_cell_guid":"3b3e8af0-84aa-493b-a455-252faeb1f4a2","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T04:55:10.180191Z","iopub.execute_input":"2023-10-11T04:55:10.180449Z","iopub.status.idle":"2023-10-11T04:55:44.342642Z","shell.execute_reply.started":"2023-10-11T04:55:10.180403Z","shell.execute_reply":"2023-10-11T04:55:44.341669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pyphen\nfrom nltk.sentiment import SentimentIntensityAnalyzer\n\ndic = pyphen.Pyphen(lang='en')\nsid = SentimentIntensityAnalyzer()\n\nclass Preprocessor:\n    def __init__(self, \n                model_name: str,\n                ) -> None:\n        self.tokenizer = AutoTokenizer.from_pretrained(INIT_MODEL)\n        self.twd = TreebankWordDetokenizer()\n        self.STOP_WORDS = set(stopwords.words('english'))\n        \n        self.spacy_ner_model = spacy.load('en_core_web_sm',)\n        self.speller = Speller(lang='en')\n        self.spellchecker = SpellChecker() \n        \n    def calculate_text_similarity(self, row):\n        vectorizer = TfidfVectorizer()\n        tfidf_matrix = vectorizer.fit_transform([row['prompt_text'], row['text']])\n        return cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2]).flatten()[0]\n    \n    def sentiment_analysis(self, text):\n        analysis = TextBlob(text)\n        return analysis.sentiment.polarity, analysis.sentiment.subjectivity\n    \n    def word_overlap_count(self, row):\n        \"\"\" intersection(prompt_text, text) \"\"\"        \n        def check_is_stop_word(word):\n            return word in self.STOP_WORDS\n        \n        prompt_words = row['prompt_tokens']\n        summary_words = row['summary_tokens']\n        if self.STOP_WORDS:\n            prompt_words = list(filter(check_is_stop_word, prompt_words))\n            summary_words = list(filter(check_is_stop_word, summary_words))\n        return len(set(prompt_words).intersection(set(summary_words)))\n            \n    def ngrams(self, token, n):\n        # Use the zip function to help us generate n-grams\n        # Concatentate the tokens into ngrams and return\n        ngrams = zip(*[token[i:] for i in range(n)])\n        return [\" \".join(ngram) for ngram in ngrams]\n\n    def ngram_co_occurrence(self, row, n: int) -> int:\n        # Tokenize the original text and summary into words\n        original_tokens = row['prompt_tokens']\n        summary_tokens = row['summary_tokens']\n\n        # Generate n-grams for the original text and summary\n        original_ngrams = set(self.ngrams(original_tokens, n))\n        summary_ngrams = set(self.ngrams(summary_tokens, n))\n\n        # Calculate the number of common n-grams\n        common_ngrams = original_ngrams.intersection(summary_ngrams)\n        return len(common_ngrams)\n    \n    def ner_overlap_count(self, row, mode:str):\n        model = self.spacy_ner_model\n        def clean_ners(ner_list):\n            return set([(ner[0].lower(), ner[1]) for ner in ner_list])\n        prompt = model(row['prompt_text'])\n        summary = model(row['text'])\n\n        if \"spacy\" in str(model):\n            prompt_ner = set([(token.text, token.label_) for token in prompt.ents])\n            summary_ner = set([(token.text, token.label_) for token in summary.ents])\n        elif \"stanza\" in str(model):\n            prompt_ner = set([(token.text, token.type) for token in prompt.ents])\n            summary_ner = set([(token.text, token.type) for token in summary.ents])\n        else:\n            raise Exception(\"Model not supported\")\n\n        prompt_ner = clean_ners(prompt_ner)\n        summary_ner = clean_ners(summary_ner)\n\n        intersecting_ners = prompt_ner.intersection(summary_ner)\n        \n        ner_dict = dict(Counter([ner[1] for ner in intersecting_ners]))\n        \n        if mode == \"train\":\n            return ner_dict\n        elif mode == \"test\":\n            return {key: ner_dict.get(key) for key in self.ner_keys}\n\n    \n    def quotes_count(self, row):\n        summary = row['text']\n        text = row['prompt_text']\n        quotes_from_summary = re.findall(r'\"([^\"]*)\"', summary)\n        if len(quotes_from_summary)>0:\n            return [quote in text for quote in quotes_from_summary].count(True)\n        else:\n            return 0\n\n    def spelling(self, text):\n        \n        wordlist=text.split()\n        amount_miss = len(list(self.spellchecker.unknown(wordlist)))\n\n        return amount_miss\n    \n    def calculate_unique_words(self,text):\n        unique_words = set(text.split())\n        return len(unique_words)\n    \n    def add_spelling_dictionary(self, tokens: List[str]) -> List[str]:\n        \"\"\"dictionary update for pyspell checker and autocorrect\"\"\"\n        self.spellchecker.word_frequency.load_words(tokens)\n        self.speller.nlp_data.update({token:1000 for token in tokens})\n        \n    def calculate_pos_ratios(self , text):\n        pos_tags = pos_tag(nltk.word_tokenize(text))\n        pos_counts = Counter(tag for word, tag in pos_tags)\n        total_words = len(pos_tags)\n        ratios = {tag: count / total_words for tag, count in pos_counts.items()}\n        return ratios\n    \n    def calculate_punctuation_ratios(self,text):\n        total_chars = len(text)\n        punctuation_counts = Counter(char for char in text if char in '.,!?;:\"()[]{}')\n        ratios = {char: count / total_chars for char, count in punctuation_counts.items()}\n        return ratios\n    \n    def calculate_keyword_density(self,row):\n        keywords = set(row['prompt_text'].split())\n        text_words = row['text'].split()\n        keyword_count = sum(1 for word in text_words if word in keywords)\n        return keyword_count / len(text_words)\n    \n    def count_syllables(self,word):\n        hyphenated_word = dic.inserted(word)\n        return len(hyphenated_word.split('-'))\n\n    def flesch_reading_ease_manual(self,text):\n        total_sentences = len(TextBlob(text).sentences)\n        total_words = len(TextBlob(text).words)\n        total_syllables = sum(self.count_syllables(word) for word in TextBlob(text).words)\n\n        if total_sentences == 0 or total_words == 0:\n            return 0\n\n        flesch_score = 206.835 - 1.015 * (total_words / total_sentences) - 84.6 * (total_syllables / total_words)\n        return flesch_score\n    \n    def flesch_kincaid_grade_level(self, text):\n        total_sentences = len(TextBlob(text).sentences)\n        total_words = len(TextBlob(text).words)\n        total_syllables = sum(self.count_syllables(word) for word in TextBlob(text).words)\n\n        if total_sentences == 0 or total_words == 0:\n            return 0\n\n        fk_grade = 0.39 * (total_words / total_sentences) + 11.8 * (total_syllables / total_words) - 15.59\n        return fk_grade\n    \n    def gunning_fog(self, text):\n        total_sentences = len(TextBlob(text).sentences)\n        total_words = len(TextBlob(text).words)\n        complex_words = sum(1 for word in TextBlob(text).words if self.count_syllables(word) > 2)\n\n        if total_sentences == 0 or total_words == 0:\n            return 0\n\n        fog_index = 0.4 * ((total_words / total_sentences) + 100 * (complex_words / total_words))\n        return fog_index\n    \n    def calculate_sentiment_scores(self,text):\n        sentiment_scores = sid.polarity_scores(text)\n        return sentiment_scores\n    \n    def count_difficult_words(self, text, syllable_threshold=3):\n        words = TextBlob(text).words\n        difficult_words_count = sum(1 for word in words if self.count_syllables(word) >= syllable_threshold)\n        return difficult_words_count\n\n\n    \n    def run(self, \n            prompts: pd.DataFrame,\n            summaries:pd.DataFrame,\n            mode:str\n        ) -> pd.DataFrame:\n        \n        # before merge preprocess\n        prompts[\"prompt_length\"] = prompts[\"prompt_text\"].apply(\n            lambda x: len(word_tokenize(x))\n        )\n        prompts[\"prompt_tokens\"] = prompts[\"prompt_text\"].apply(\n            lambda x: word_tokenize(x)\n        )\n\n        summaries[\"summary_length\"] = summaries[\"text\"].apply(\n            lambda x: len(word_tokenize(x))\n        )\n        summaries[\"summary_tokens\"] = summaries[\"text\"].apply(\n            lambda x: word_tokenize(x)\n        )\n        \n        # Add prompt tokens into spelling checker dictionary\n        prompts[\"prompt_tokens\"].apply(\n            lambda x: self.add_spelling_dictionary(x)\n        )\n        \n        prompts['gunning_fog_prompt'] = prompts['prompt_text'].apply(self.gunning_fog)\n        prompts['flesch_kincaid_grade_level_prompt'] = prompts['prompt_text'].apply(self.flesch_kincaid_grade_level)\n        prompts['flesch_reading_ease_prompt'] = prompts['prompt_text'].apply(self.flesch_reading_ease_manual)\n\n        \n#         from IPython.core.debugger import Pdb; Pdb().set_trace()\n        # fix misspelling\n#         summaries[\"fixed_summary_text\"] = summaries[\"text\"].progress_apply(\n#             lambda x: self.speller(x)\n#         )\n\n        # fix misspelling\n        summaries[\"fixed_summary_text\"] = summaries[\"text\"].progress_apply(\n            lambda x: self.speller(x)\n        )\n        \n        \n        # count misspelling\n        summaries[\"splling_err_num\"] = summaries[\"text\"].progress_apply(self.spelling)\n        \n        # merge prompts and summaries\n        input_df = summaries.merge(prompts, how=\"left\", on=\"prompt_id\")\n        input_df['flesch_reading_ease'] = input_df['text'].apply(self.flesch_reading_ease_manual)\n        input_df['word_count'] = input_df['text'].apply(lambda x: len(x.split()))\n        input_df['sentence_length'] = input_df['text'].apply(lambda x: len(x.split('.')))\n        input_df['vocabulary_richness'] = input_df['text'].apply(lambda x: len(set(x.split())))\n\n        input_df['word_count2'] = [len(t.split(' ')) for t in input_df.text]\n        input_df['num_unq_words']=[len(list(set(x.lower().split(' ')))) for x in input_df.text]\n        input_df['num_chars']= [len(x) for x in input_df.text]\n\n        # Additional features\n        input_df['avg_word_length'] = input_df['text'].apply(lambda x: np.mean([len(word) for word in x.split()]))\n        input_df['comma_count'] = input_df['text'].apply(lambda x: x.count(','))\n        input_df['semicolon_count'] = input_df['text'].apply(lambda x: x.count(';'))\n\n        # after merge preprocess\n        input_df['length_ratio'] = input_df['summary_length'] / input_df['prompt_length']\n        \n        input_df['word_overlap_count'] = input_df.progress_apply(self.word_overlap_count, axis=1)\n        input_df['bigram_overlap_count'] = input_df.progress_apply(\n            self.ngram_co_occurrence,args=(2,), axis=1 \n        )\n        input_df['bigram_overlap_ratio'] = input_df['bigram_overlap_count'] / (input_df['summary_length'] - 1)\n        \n        input_df['trigram_overlap_count'] = input_df.progress_apply(\n            self.ngram_co_occurrence, args=(3,), axis=1\n        )\n        input_df['trigram_overlap_ratio'] = input_df['trigram_overlap_count'] / (input_df['summary_length'] - 2)\n        \n        input_df['quotes_count'] = input_df.progress_apply(self.quotes_count, axis=1)\n        \n        input_df['exclamation_count'] = input_df['text'].apply(lambda x: x.count('!'))\n        input_df['question_count'] = input_df['text'].apply(lambda x: x.count('?'))\n        input_df['pos_ratios'] = input_df['text'].apply(self.calculate_pos_ratios)\n\n        # Convert the dictionary of POS ratios into a single value (mean)\n        input_df['pos_mean'] = input_df['pos_ratios'].apply(lambda x: np.mean(list(x.values())))\n        input_df['punctuation_ratios'] = input_df['text'].apply(self.calculate_punctuation_ratios)\n\n        # Convert the dictionary of punctuation ratios into a single value (sum)\n        input_df['punctuation_sum'] = input_df['punctuation_ratios'].apply(lambda x: np.sum(list(x.values())))\n        input_df['keyword_density'] = input_df.apply(self.calculate_keyword_density, axis=1)\n        input_df['jaccard_similarity'] = input_df.apply(lambda row: len(set(word_tokenize(row['prompt_text'])) & set(word_tokenize(row['text']))) / len(set(word_tokenize(row['prompt_text'])) | set(word_tokenize(row['text']))), axis=1)\n        tqdm.pandas(desc=\"Performing Sentiment Analysis\")\n        input_df[['sentiment_polarity', 'sentiment_subjectivity']] = input_df['text'].progress_apply(\n            lambda x: pd.Series(self.sentiment_analysis(x))\n        )\n        tqdm.pandas(desc=\"Calculating Text Similarity\")\n        input_df['text_similarity'] = input_df.progress_apply(self.calculate_text_similarity, axis=1)\n        #Calculate sentiment scores for each row\n        input_df['sentiment_scores'] = input_df['text'].apply(self.calculate_sentiment_scores)\n        \n        input_df['gunning_fog'] = input_df['text'].apply(self.gunning_fog)\n        input_df['flesch_kincaid_grade_level'] = input_df['text'].apply(self.flesch_kincaid_grade_level)\n        input_df['count_difficult_words'] = input_df['text'].apply(self.count_difficult_words)\n\n        # Convert sentiment_scores into individual columns\n        sentiment_columns = pd.DataFrame(list(input_df['sentiment_scores']))\n        input_df = pd.concat([input_df, sentiment_columns], axis=1)\n        input_df['sentiment_scores_prompt'] = input_df['prompt_text'].apply(self.calculate_sentiment_scores)\n        # Convert sentiment_scores_prompt into individual columns\n        sentiment_columns_prompt = pd.DataFrame(list(input_df['sentiment_scores_prompt']))\n        sentiment_columns_prompt.columns = [col +'_prompt' for col in sentiment_columns_prompt.columns]\n        input_df = pd.concat([input_df, sentiment_columns_prompt], axis=1)\n        columns =  ['pos_ratios', 'sentiment_scores', 'punctuation_ratios', 'sentiment_scores_prompt']\n        cols_to_drop = [col for col in columns if col in input_df.columns]\n        if cols_to_drop:\n            input_df = input_df.drop(columns=cols_to_drop)\n        \n        print(cols_to_drop)\n        return input_df.drop(columns=[\"summary_tokens\", \"prompt_tokens\"])\n    \npreprocessor = Preprocessor(model_name=CFG.model_name)","metadata":{"_uuid":"890726ab-010c-41a1-b64a-6eb7a4a56298","_cell_guid":"f90081f0-82f5-4a12-8b08-e9a946625f11","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T04:55:44.344891Z","iopub.execute_input":"2023-10-11T04:55:44.345153Z","iopub.status.idle":"2023-10-11T04:55:45.765195Z","shell.execute_reply.started":"2023-10-11T04:55:44.345121Z","shell.execute_reply":"2023-10-11T04:55:45.764279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocess\n\n[Using features]\n\n- Text Length\n- Length Ratio\n- Word Overlap\n- N-grams Co-occurrence\n  - count\n  - ratio\n- Quotes Overlap\n- Grammar Check\n  - spelling: pyspellchecker","metadata":{"_uuid":"cf00f8ea-ed79-4792-95d6-268e2a6f7dc9","_cell_guid":"617ee232-9ed8-4722-9ead-caa4756838ff","trusted":true}},{"cell_type":"code","source":"# class Preprocessor:\n#     def __init__(self, \n#                 model_name: str,\n#                 ) -> None:\n#         self.tokenizer = AutoTokenizer.from_pretrained(INIT_MODEL)\n#         self.twd = TreebankWordDetokenizer()\n#         self.STOP_WORDS = set(stopwords.words('english'))\n        \n#         self.spacy_ner_model = spacy.load('en_core_web_sm',)\n#         self.speller = Speller(lang='en')\n#         self.spellchecker = SpellChecker() \n        \n#     def word_overlap_count(self, row):\n#         \"\"\" intersection(prompt_text, text) \"\"\"        \n#         def check_is_stop_word(word):\n#             return word in self.STOP_WORDS\n        \n#         prompt_words = row['prompt_tokens']\n#         summary_words = row['summary_tokens']\n#         if self.STOP_WORDS:\n#             prompt_words = list(filter(check_is_stop_word, prompt_words))\n#             summary_words = list(filter(check_is_stop_word, summary_words))\n#         return len(set(prompt_words).intersection(set(summary_words)))\n            \n#     def ngrams(self, token, n):\n#         # Use the zip function to help us generate n-grams\n#         # Concatentate the tokens into ngrams and return\n#         ngrams = zip(*[token[i:] for i in range(n)])\n#         return [\" \".join(ngram) for ngram in ngrams]\n\n#     def ngram_co_occurrence(self, row, n: int) -> int:\n#         # Tokenize the original text and summary into words\n#         original_tokens = row['prompt_tokens']\n#         summary_tokens = row['summary_tokens']\n\n#         # Generate n-grams for the original text and summary\n#         original_ngrams = set(self.ngrams(original_tokens, n))\n#         summary_ngrams = set(self.ngrams(summary_tokens, n))\n\n#         # Calculate the number of common n-grams\n#         common_ngrams = original_ngrams.intersection(summary_ngrams)\n#         return len(common_ngrams)\n    \n#     def ner_overlap_count(self, row, mode:str):\n#         model = self.spacy_ner_model\n#         def clean_ners(ner_list):\n#             return set([(ner[0].lower(), ner[1]) for ner in ner_list])\n#         prompt = model(row['prompt_text'])\n#         summary = model(row['text'])\n\n#         if \"spacy\" in str(model):\n#             prompt_ner = set([(token.text, token.label_) for token in prompt.ents])\n#             summary_ner = set([(token.text, token.label_) for token in summary.ents])\n#         elif \"stanza\" in str(model):\n#             prompt_ner = set([(token.text, token.type) for token in prompt.ents])\n#             summary_ner = set([(token.text, token.type) for token in summary.ents])\n#         else:\n#             raise Exception(\"Model not supported\")\n\n#         prompt_ner = clean_ners(prompt_ner)\n#         summary_ner = clean_ners(summary_ner)\n\n#         intersecting_ners = prompt_ner.intersection(summary_ner)\n        \n#         ner_dict = dict(Counter([ner[1] for ner in intersecting_ners]))\n        \n#         if mode == \"train\":\n#             return ner_dict\n#         elif mode == \"test\":\n#             return {key: ner_dict.get(key) for key in self.ner_keys}\n\n    \n#     def quotes_count(self, row):\n#         summary = row['text']\n#         text = row['prompt_text']\n#         quotes_from_summary = re.findall(r'\"([^\"]*)\"', summary)\n#         if len(quotes_from_summary)>0:\n#             return [quote in text for quote in quotes_from_summary].count(True)\n#         else:\n#             return 0\n\n#     def spelling(self, text):\n        \n#         wordlist=text.split()\n#         amount_miss = len(list(self.spellchecker.unknown(wordlist)))\n\n#         return amount_miss\n    \n#     def add_spelling_dictionary(self, tokens: List[str]) -> List[str]:\n#         \"\"\"dictionary update for pyspell checker and autocorrect\"\"\"\n#         self.spellchecker.word_frequency.load_words(tokens)\n#         self.speller.nlp_data.update({token:1000 for token in tokens})\n    \n#     def run(self, \n#             prompts: pd.DataFrame,\n#             summaries:pd.DataFrame,\n#             mode:str\n#         ) -> pd.DataFrame:\n        \n#         # before merge preprocess\n#         prompts[\"prompt_length\"] = prompts[\"prompt_text\"].apply(\n#             lambda x: len(word_tokenize(x))\n#         )\n#         prompts[\"prompt_tokens\"] = prompts[\"prompt_text\"].apply(\n#             lambda x: word_tokenize(x)\n#         )\n\n#         summaries[\"summary_length\"] = summaries[\"text\"].apply(\n#             lambda x: len(word_tokenize(x))\n#         )\n#         summaries[\"summary_tokens\"] = summaries[\"text\"].apply(\n#             lambda x: word_tokenize(x)\n#         )\n        \n#         # Add prompt tokens into spelling checker dictionary\n#         prompts[\"prompt_tokens\"].apply(\n#             lambda x: self.add_spelling_dictionary(x)\n#         )\n        \n# #         from IPython.core.debugger import Pdb; Pdb().set_trace()\n#         # fix misspelling\n#         summaries[\"fixed_summary_text\"] = summaries[\"text\"].progress_apply(\n#             lambda x: self.speller(x)\n#         )\n        \n#         # count misspelling\n#         summaries[\"splling_err_num\"] = summaries[\"text\"].progress_apply(self.spelling)\n        \n#         # merge prompts and summaries\n#         input_df = summaries.merge(prompts, how=\"left\", on=\"prompt_id\")\n\n#         # after merge preprocess\n#         input_df['length_ratio'] = input_df['summary_length'] / input_df['prompt_length']\n\n#         input_df['word_overlap_count'] = input_df.progress_apply(self.word_overlap_count, axis=1)\n#         input_df['bigram_overlap_count'] = input_df.progress_apply(\n#             self.ngram_co_occurrence,args=(2,), axis=1 \n#         )\n#         input_df['bigram_overlap_ratio'] = input_df['bigram_overlap_count'] / (input_df['summary_length'] - 1)\n        \n#         input_df['trigram_overlap_count'] = input_df.progress_apply(\n#             self.ngram_co_occurrence, args=(3,), axis=1\n#         )\n#         input_df['trigram_overlap_ratio'] = input_df['trigram_overlap_count'] / (input_df['summary_length'] - 2)\n        \n#         input_df['quotes_count'] = input_df.progress_apply(self.quotes_count, axis=1)\n        \n#         return input_df.drop(columns=[\"summary_tokens\", \"prompt_tokens\"])\n    \n# preprocessor = Preprocessor(model_name=CFG.model_name)","metadata":{"_uuid":"a23c4e5a-5d45-429d-9439-e239289f612a","_cell_guid":"2691b13b-af8d-49a0-ada2-3eda447d8ac8","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T04:55:45.767172Z","iopub.execute_input":"2023-10-11T04:55:45.767433Z","iopub.status.idle":"2023-10-11T04:55:45.776334Z","shell.execute_reply.started":"2023-10-11T04:55:45.767386Z","shell.execute_reply":"2023-10-11T04:55:45.775473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summaries_train.head()","metadata":{"_uuid":"78cefd26-0bd3-407a-a12a-4937cb36b3be","_cell_guid":"d8cf26eb-2c71-4f53-81b2-df3eb9d9de7e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T04:55:45.777541Z","iopub.execute_input":"2023-10-11T04:55:45.77828Z","iopub.status.idle":"2023-10-11T04:55:45.801684Z","shell.execute_reply.started":"2023-10-11T04:55:45.778249Z","shell.execute_reply":"2023-10-11T04:55:45.800705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\nwith open(MODEL_DIR+'/pickled.pkl', 'rb') as f:\n    train = pickle.load(f)\n    test = pickle.load(f)","metadata":{"_uuid":"0b97772a-3663-45b1-a24e-384de29f86d1","_cell_guid":"bb9208a6-42ec-4130-9c30-a84aa5acb50b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T04:55:45.803135Z","iopub.execute_input":"2023-10-11T04:55:45.804122Z","iopub.status.idle":"2023-10-11T04:55:45.882074Z","shell.execute_reply.started":"2023-10-11T04:55:45.804089Z","shell.execute_reply":"2023-10-11T04:55:45.881203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from textblob import TextBlob\nfrom nltk import ne_chunk, word_tokenize, pos_tag\nfrom sklearn.metrics.pairwise import cosine_similarity\ntrain = preprocessor.run(prompts_train, summaries_train, mode=\"train\")\ntest = preprocessor.run(prompts_test, summaries_test, mode=\"test\")\n\ntrain.head()","metadata":{"_uuid":"758e5dd6-8c3c-4970-9447-84535a07b3c3","_cell_guid":"13433945-1545-4ff7-9652-501d7561a511","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T04:55:45.883382Z","iopub.execute_input":"2023-10-11T04:55:45.883662Z","iopub.status.idle":"2023-10-11T05:06:17.43449Z","shell.execute_reply.started":"2023-10-11T04:55:45.883622Z","shell.execute_reply":"2023-10-11T05:06:17.433616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"a4fe46d0-aff0-4898-a9e7-76c540827df3","_cell_guid":"3933e6b0-4b1b-4647-97eb-bae599975976","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_keep = train.copy()\ntest_keep = test.copy()","metadata":{"_uuid":"29253e6c-815a-4709-b30b-59f4cd42f639","_cell_guid":"ef6e4601-c164-4139-bac4-fe1eaa2d20bd","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:06:17.435806Z","iopub.execute_input":"2023-10-11T05:06:17.436539Z","iopub.status.idle":"2023-10-11T05:06:17.447278Z","shell.execute_reply.started":"2023-10-11T05:06:17.436502Z","shell.execute_reply":"2023-10-11T05:06:17.446176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train = train[:256]","metadata":{"_uuid":"2f2f6a95-330a-4409-882f-55b9e3023482","_cell_guid":"ec114eea-e60d-4579-9409-b6b0fa18f94e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:06:17.449128Z","iopub.execute_input":"2023-10-11T05:06:17.449412Z","iopub.status.idle":"2023-10-11T05:06:17.456752Z","shell.execute_reply.started":"2023-10-11T05:06:17.449361Z","shell.execute_reply":"2023-10-11T05:06:17.455687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gkf = GroupKFold(n_splits=CFG.n_splits)\n\nfor i, (_, val_index) in enumerate(gkf.split(train, groups=train[\"prompt_id\"])):\n    train.loc[val_index, \"fold\"] = i\n\ntrain.head()","metadata":{"_uuid":"07b3b28f-ad03-43af-9fd4-e126e0529646","_cell_guid":"43d0c6b0-450d-4260-bd15-d2b10cf0550d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:06:17.458147Z","iopub.execute_input":"2023-10-11T05:06:17.458373Z","iopub.status.idle":"2023-10-11T05:06:17.49649Z","shell.execute_reply.started":"2023-10-11T05:06:17.458344Z","shell.execute_reply":"2023-10-11T05:06:17.495647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Function Definition","metadata":{"_uuid":"79815b91-c6e4-4990-8fd8-a99aeb920bbe","_cell_guid":"61dca0dc-16fb-4807-a357-e412956998fb","trusted":true}},{"cell_type":"code","source":"import shutil\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    rmse = mean_squared_error(labels, predictions, squared=False)\n    return {\"rmse\": rmse}\n\ndef compute_mcrmse(eval_pred):\n    \"\"\"\n    Calculates mean columnwise root mean squared error\n    https://www.kaggle.com/competitions/commonlit-evaluate-student-summaries/overview/evaluation\n    \"\"\"\n    preds, labels = eval_pred\n\n    col_rmse = np.sqrt(np.mean((preds - labels) ** 2, axis=0))\n    mcrmse = np.mean(col_rmse)\n\n    return {\n        \"content_rmse\": col_rmse[0],\n        \"wording_rmse\": col_rmse[1],\n        \"mcrmse\": mcrmse,\n    }\n\ndef compt_score(content_true, content_pred, wording_true, wording_pred):\n    content_score = mean_squared_error(content_true, content_pred)**(1/2)\n    wording_score = mean_squared_error(wording_true, wording_pred)**(1/2)\n    \n    return (content_score + wording_score)/2","metadata":{"_uuid":"7618b238-ce95-4324-846f-43fdc83cc86a","_cell_guid":"f0c168e0-9982-4e77-8c65-0055ad1fc54b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:06:17.497744Z","iopub.execute_input":"2023-10-11T05:06:17.497949Z","iopub.status.idle":"2023-10-11T05:06:17.504802Z","shell.execute_reply.started":"2023-10-11T05:06:17.497922Z","shell.execute_reply":"2023-10-11T05:06:17.503815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Deberta Regressor","metadata":{"_uuid":"e0c3240a-c13b-4f00-ac30-36851f97274d","_cell_guid":"49df7933-c336-46e1-8b10-15189d46698a","trusted":true}},{"cell_type":"code","source":"class ContentScoreRegressor:\n    def __init__(self, \n                model_name: str,\n                model_dir: str,\n                target: str,\n                hidden_dropout_prob: float,\n                attention_probs_dropout_prob: float,\n                max_length: int,\n                ):\n        self.inputs = [\"prompt_text\", \"prompt_title\", \"prompt_question\", \"fixed_summary_text\"]\n        self.input_col = \"input\"\n        \n        self.text_cols = [self.input_col] \n        self.target = target\n        self.target_cols = [target]\n\n        self.model_name = model_name\n        self.model_dir = model_dir\n        self.max_length = max_length\n        \n        self.tokenizer = AutoTokenizer.from_pretrained(INIT_MODEL)\n        self.model_config = AutoConfig.from_pretrained(INIT_MODEL)\n        \n        self.model_config.update({\n            \"hidden_dropout_prob\": hidden_dropout_prob,\n            \"attention_probs_dropout_prob\": attention_probs_dropout_prob,\n            \"num_labels\": 1,\n            \"problem_type\": \"regression\",\n        })\n        \n        seed_everything(seed=42)\n\n        self.data_collator = DataCollatorWithPadding(\n            tokenizer=self.tokenizer\n        )\n\n\n    def tokenize_function(self, examples: pd.DataFrame):\n        labels = [examples[self.target]]\n        tokenized = self.tokenizer(examples[self.input_col],\n                         padding=False,\n                         truncation=True,\n                         max_length=self.max_length)\n        return {\n            **tokenized,\n            \"labels\": labels,\n        }\n    \n    def tokenize_function_test(self, examples: pd.DataFrame):\n        tokenized = self.tokenizer(examples[self.input_col],\n                         padding=False,\n                         truncation=True,\n                         max_length=self.max_length)\n        return tokenized\n        \n    def train(self, \n            fold: int,\n            train_df: pd.DataFrame,\n            valid_df: pd.DataFrame,\n            batch_size: int,\n            learning_rate: float,\n            weight_decay: float,\n            num_train_epochs: float,\n            save_steps: int,\n        ) -> None:\n        \"\"\"fine-tuning\"\"\"\n        \n        sep = self.tokenizer.sep_token\n        train_df[self.input_col] = (\n                    train_df[\"prompt_title\"] + sep \n                    + train_df[\"prompt_question\"] + sep \n                    + train_df[\"fixed_summary_text\"]\n                  )\n\n        valid_df[self.input_col] = (\n                    valid_df[\"prompt_title\"] + sep \n                    + valid_df[\"prompt_question\"] + sep \n                    + valid_df[\"fixed_summary_text\"]\n                  )\n        \n        train_df = train_df[[self.input_col] + self.target_cols]\n        valid_df = valid_df[[self.input_col] + self.target_cols]\n        \n        model_content = AutoModelForSequenceClassification.from_pretrained(\n            f\"/kaggle/input/{self.model_name}\", \n            config=self.model_config,\n            ignore_mismatched_sizes=True\n        )\n\n        train_dataset = Dataset.from_pandas(train_df, preserve_index=False) \n        val_dataset = Dataset.from_pandas(valid_df, preserve_index=False) \n    \n        train_tokenized_datasets = train_dataset.map(self.tokenize_function, batched=False)\n        val_tokenized_datasets = val_dataset.map(self.tokenize_function, batched=False)\n\n        # eg. \"bert/fold_0/\"\n        model_fold_dir = os.path.join(self.model_dir, str(fold)) \n        \n        training_args = TrainingArguments(\n            output_dir=model_fold_dir,\n            load_best_model_at_end=True, # select best model\n            learning_rate=learning_rate,\n            per_device_train_batch_size=batch_size,\n            per_device_eval_batch_size=8,\n            num_train_epochs=num_train_epochs,\n            weight_decay=weight_decay,\n            report_to='none',\n            greater_is_better=False,\n            save_strategy=\"steps\",\n            evaluation_strategy=\"steps\",\n            eval_steps=save_steps,\n            save_steps=save_steps,\n            metric_for_best_model=\"rmse\",\n            save_total_limit=1\n        )\n\n        trainer = Trainer(\n            model=model_content,\n            args=training_args,\n            train_dataset=train_tokenized_datasets,\n            eval_dataset=val_tokenized_datasets,\n            tokenizer=self.tokenizer,\n            compute_metrics=compute_metrics,\n            data_collator=self.data_collator\n        )\n\n        trainer.train()\n        \n        shutil.rmtree(self.model_dir)\n        \n        model_content.save_pretrained(self.model_dir)\n        self.tokenizer.save_pretrained(self.model_dir)\n\n        \n    def predict(self, \n                test_df: pd.DataFrame,\n                fold: int,\n               ):\n        \"\"\"predict content score\"\"\"\n        \n        sep = self.tokenizer.sep_token\n        in_text = (\n                    test_df[\"prompt_title\"] + sep \n                    + test_df[\"prompt_question\"] + sep \n                    + test_df[\"fixed_summary_text\"]\n                  )\n        test_df[self.input_col] = in_text\n\n        test_ = test_df[[self.input_col]]\n    \n        test_dataset = Dataset.from_pandas(test_, preserve_index=False) \n        test_tokenized_dataset = test_dataset.map(self.tokenize_function_test, batched=False)\n\n        model_content = AutoModelForSequenceClassification.from_pretrained(f\"{self.model_dir}\")\n        model_content.eval()\n        \n        # e.g. \"bert/fold_0/\"\n        model_fold_dir = os.path.join(self.model_dir, str(fold)) \n        model_fold_dir = \"valid_log\" #f\"bert-{fold}\"\n#         print(\"model_fold_dir\", model_fold_dir)\n        \n        test_args = TrainingArguments(\n            output_dir=model_fold_dir,\n            do_train = False,\n            do_predict = True,\n            per_device_eval_batch_size = 4,   \n            dataloader_drop_last = False,\n        )\n\n        # init trainer\n        infer_content = Trainer(\n                      model = model_content, \n                      tokenizer=self.tokenizer,\n                      data_collator=self.data_collator,\n                      args = test_args)\n\n        preds = infer_content.predict(test_tokenized_dataset)[0]\n\n        return preds","metadata":{"_uuid":"b7b827f4-3165-48f3-9ffb-5aac9eca786e","_cell_guid":"fd767dd4-c0c9-48bc-90d9-9a9a7bda6e54","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:06:17.506396Z","iopub.execute_input":"2023-10-11T05:06:17.506657Z","iopub.status.idle":"2023-10-11T05:06:17.525556Z","shell.execute_reply.started":"2023-10-11T05:06:17.506627Z","shell.execute_reply":"2023-10-11T05:06:17.524661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_by_fold(\n        train_df: pd.DataFrame,\n        model_name: str,\n        target:str,\n        save_each_model: bool,\n        n_splits: int,\n        batch_size: int,\n        learning_rate: int,\n        hidden_dropout_prob: float,\n        attention_probs_dropout_prob: float,\n        weight_decay: float,\n        num_train_epochs: int,\n        save_steps: int,\n        max_length:int\n    ):\n\n    # delete old model files\n    if os.path.exists(model_name):\n        shutil.rmtree(model_name)\n    \n    os.mkdir(model_name)\n        \n    for fold in range(CFG.n_splits):\n        print(f\"fold {fold}:\")\n        \n        train_data = train_df[train_df[\"fold\"] != fold]\n        valid_data = train_df[train_df[\"fold\"] == fold]\n        \n        if save_each_model == True:\n            model_dir =  f\"{target}/{model_name}/fold_{fold}\"\n        else: \n            model_dir =  f\"{model_name}/fold_{fold}\"\n\n        csr = ContentScoreRegressor(\n            model_name=model_name,\n            target=target,\n            model_dir = model_dir, \n            hidden_dropout_prob=hidden_dropout_prob,\n            attention_probs_dropout_prob=attention_probs_dropout_prob,\n            max_length=max_length,\n           )\n        \n        csr.train(\n            fold=fold,\n            train_df=train_data,\n            valid_df=valid_data, \n            batch_size=batch_size,\n            learning_rate=learning_rate,\n            weight_decay=weight_decay,\n            num_train_epochs=num_train_epochs,\n            save_steps=save_steps,\n        )\n\ndef validate(\n    train_df: pd.DataFrame,\n    target:str,\n    save_each_model: bool,\n    model_name: str,\n    hidden_dropout_prob: float,\n    attention_probs_dropout_prob: float,\n    max_length : int\n    ) -> pd.DataFrame:\n    \"\"\"predict oof data\"\"\"\n    for fold in range(CFG.n_splits):\n        print(f\"fold {fold}:\")\n        \n        valid_data = train_df[train_df[\"fold\"] == fold]\n        \n        if save_each_model == True:\n            model_dir =  f\"{MODEL_DIR}/{target}/{model_name}/fold_{fold}\"\n        else: \n            model_dir =  f\"{MODEL_DIR}/{model_name}/fold_{fold}\"\n        \n#         print(model_dir, model_name)\n        csr = ContentScoreRegressor(\n            model_name=model_name,\n            target=target,\n            model_dir = model_dir,\n            hidden_dropout_prob=hidden_dropout_prob,\n            attention_probs_dropout_prob=attention_probs_dropout_prob,\n            max_length=max_length,\n           )\n        \n        pred = csr.predict(\n            test_df=valid_data, \n            fold=fold\n        )\n        \n        train_df.loc[valid_data.index, f\"{target}_pred\"] = pred\n\n    return train_df\n    \ndef predict(\n    test_df: pd.DataFrame,\n    target:str,\n    save_each_model: bool,\n    model_name: str,\n    hidden_dropout_prob: float,\n    attention_probs_dropout_prob: float,\n    max_length : int\n    ):\n    \"\"\"predict using mean folds\"\"\"\n\n    for fold in range(CFG.n_splits):\n        print(f\"fold {fold}:\")\n        \n        if save_each_model == True:\n            model_dir =  f\"{MODEL_DIR}/{target}/{model_name}/fold_{fold}\"\n        else: \n            model_dir =  f\"{MODEL_DIR}/{model_name}/fold_{fold}\"\n\n        csr = ContentScoreRegressor(\n            model_name=model_name,\n            target=target,\n            model_dir = model_dir, \n            hidden_dropout_prob=hidden_dropout_prob,\n            attention_probs_dropout_prob=attention_probs_dropout_prob,\n            max_length=max_length,\n           )\n        \n        pred = csr.predict(\n            test_df=test_df, \n            fold=fold\n        )\n        \n        test_df[f\"{target}_pred_{fold}\"] = pred\n    \n    test_df[f\"{target}\"] = test_df[[f\"{target}_pred_{fold}\" for fold in range(CFG.n_splits)]].mean(axis=1)\n\n    return test_df","metadata":{"_uuid":"6a5ebb69-21f0-4cba-9c18-539c8ab49a48","_cell_guid":"2c216e04-23e9-4538-a5a6-26971690c3d8","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:06:17.526942Z","iopub.execute_input":"2023-10-11T05:06:17.527337Z","iopub.status.idle":"2023-10-11T05:06:17.544097Z","shell.execute_reply.started":"2023-10-11T05:06:17.527308Z","shell.execute_reply":"2023-10-11T05:06:17.543216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for target in [\"content\", \"wording\"]:\n#     train_by_fold(\n#         train,\n#         model_name=CFG.model_name,\n#         save_each_model=True,\n#         target=target,\n#         learning_rate=CFG.learning_rate,\n#         hidden_dropout_prob=CFG.hidden_dropout_prob,\n#         attention_probs_dropout_prob=CFG.attention_probs_dropout_prob,\n#         weight_decay=CFG.weight_decay,\n#         num_train_epochs=CFG.num_train_epochs,\n#         n_splits=CFG.n_splits,\n#         batch_size=CFG.batch_size,\n#         save_steps=CFG.save_steps,\n#         max_length=CFG.max_length\n#     )\n    \n    print(\"[validate]\")\n    train = validate(\n        train,\n        target=target,\n        save_each_model=True,\n        model_name=CFG.model_name,\n        hidden_dropout_prob=CFG.hidden_dropout_prob,\n        attention_probs_dropout_prob=CFG.attention_probs_dropout_prob,\n        max_length=CFG.max_length\n    )\n\n    rmse = mean_squared_error(train[target], train[f\"{target}_pred\"], squared=False)\n    print(f\"cv {target} rmse: {rmse}\")\n\n    print(\"[test]\")\n    test = predict(\n        test,\n        target=target,\n        save_each_model=True,\n        model_name=CFG.model_name,\n        hidden_dropout_prob=CFG.hidden_dropout_prob,\n        attention_probs_dropout_prob=CFG.attention_probs_dropout_prob,\n        max_length=CFG.max_length\n    )","metadata":{"_uuid":"aaf52771-7996-4ca3-998c-b1e8952cdfb5","_cell_guid":"1a7f62fd-c842-4ec0-9c07-a228bf9caec3","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:06:17.545454Z","iopub.execute_input":"2023-10-11T05:06:17.545682Z","iopub.status.idle":"2023-10-11T05:18:25.668617Z","shell.execute_reply.started":"2023-10-11T05:06:17.545654Z","shell.execute_reply":"2023-10-11T05:18:25.667458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !rm -r wording content","metadata":{"_uuid":"a3a27df6-7710-4d90-adfd-df7124807e80","_cell_guid":"5a4ed139-3f9f-4682-b534-2dbe4a134c37","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:18:25.671693Z","iopub.execute_input":"2023-10-11T05:18:25.67219Z","iopub.status.idle":"2023-10-11T05:18:25.676486Z","shell.execute_reply.started":"2023-10-11T05:18:25.672153Z","shell.execute_reply":"2023-10-11T05:18:25.675563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"_uuid":"414a8b63-b6a7-42a1-b5a2-dc30136825aa","_cell_guid":"130c98ee-03c7-4415-b720-0afdc338df5f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:18:25.678231Z","iopub.execute_input":"2023-10-11T05:18:25.678505Z","iopub.status.idle":"2023-10-11T05:18:25.721794Z","shell.execute_reply.started":"2023-10-11T05:18:25.678471Z","shell.execute_reply":"2023-10-11T05:18:25.720749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_1st = train\ntest_1st = test","metadata":{"_uuid":"8a7cd8ba-aea3-44f4-aad0-3f9cb0459925","_cell_guid":"e7cd31e4-c36e-4af7-b8f9-855e9b8eab56","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:18:25.723338Z","iopub.execute_input":"2023-10-11T05:18:25.723596Z","iopub.status.idle":"2023-10-11T05:18:25.732134Z","shell.execute_reply.started":"2023-10-11T05:18:25.723564Z","shell.execute_reply":"2023-10-11T05:18:25.731279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LGBM model","metadata":{"_uuid":"5b7cf52d-0629-4e37-8a82-33d93c1a1149","_cell_guid":"49c003e2-df3c-45bf-a2b2-8c5cf408e310","trusted":true}},{"cell_type":"code","source":"targets = [\"content\", \"wording\"]\n\ndrop_columns = [\"fold\", \"student_id\", \"prompt_id\", \"text\", \"fixed_summary_text\",\n                \"prompt_question\", \"prompt_title\", \n                \"prompt_text\",\"title\", \"author\", \"description\", \"genre\", \"grade\"\n               ] + targets","metadata":{"_uuid":"7c1f46e1-aa54-4c97-9663-351df0f729ae","_cell_guid":"fb73d226-c91d-4e99-ab71-e639eab46370","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:18:25.733578Z","iopub.execute_input":"2023-10-11T05:18:25.734021Z","iopub.status.idle":"2023-10-11T05:18:25.744273Z","shell.execute_reply.started":"2023-10-11T05:18:25.733986Z","shell.execute_reply":"2023-10-11T05:18:25.743221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[train[\"fold\"] != 0].drop(columns=drop_columns)","metadata":{"_uuid":"ed6b4fa0-21cf-476f-8933-4da4891e3636","_cell_guid":"627f3649-9e2f-4ad1-bd6f-f1e0f3e0ed68","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:18:25.745796Z","iopub.execute_input":"2023-10-11T05:18:25.746018Z","iopub.status.idle":"2023-10-11T05:18:25.780087Z","shell.execute_reply.started":"2023-10-11T05:18:25.74599Z","shell.execute_reply":"2023-10-11T05:18:25.779275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model_dict = {}\n\n# for target in targets:\n#     models = []\n    \n#     for fold in range(CFG.n_splits):\n\n#         X_train_cv = train[train[\"fold\"] != fold].drop(columns=drop_columns)\n#         y_train_cv = train[train[\"fold\"] != fold][target]\n\n#         X_eval_cv = train[train[\"fold\"] == fold].drop(columns=drop_columns)\n#         y_eval_cv = train[train[\"fold\"] == fold][target]\n\n#         dtrain = lgb.Dataset(X_train_cv, label=y_train_cv)\n#         dval = lgb.Dataset(X_eval_cv, label=y_eval_cv)\n\n#         params = {\n#             'boosting_type': 'gbdt',\n#             'random_state': 42,\n#             'objective': 'regression',\n#             'metric': 'rmse',\n#             'learning_rate': 0.048,\n#             'max_depth': 4,\n#             'lambda_l1': 0.001,\n#             'lambda_l2': 0.011\n#         }\n\n#         evaluation_results = {}\n#         model = lgb.train(params,\n#                           num_boost_round=10000,\n#                             #categorical_feature = categorical_features,\n#                           valid_names=['train', 'valid'],\n#                           train_set=dtrain,\n#                           valid_sets=dval,\n#                           callbacks=[\n#                               lgb.early_stopping(stopping_rounds=30, verbose=True),\n#                               lgb.log_evaluation(100),\n#                               lgb.callback.record_evaluation(evaluation_results)\n#                             ],\n#                           )\n#         models.append(model)\n    \n#     model_dict[target] = models","metadata":{"_uuid":"da52f10b-fada-4ab9-8fa0-b3ae53f93633","_cell_guid":"e434a808-b98d-4265-852e-bfd1ff66270c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:18:25.781282Z","iopub.execute_input":"2023-10-11T05:18:25.782553Z","iopub.status.idle":"2023-10-11T05:18:25.787753Z","shell.execute_reply.started":"2023-10-11T05:18:25.782521Z","shell.execute_reply":"2023-10-11T05:18:25.786642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from catboost import CatBoostRegressor\n\n\nmodel_dict = {}\n\nfor target in targets:\n    models = []\n    \n    for fold in range(CFG.n_splits):\n\n        X_train_cv = train[train[\"fold\"] != fold].drop(columns=drop_columns)\n        y_train_cv = train[train[\"fold\"] != fold][target]\n\n        X_eval_cv = train[train[\"fold\"] == fold].drop(columns=drop_columns)\n        y_eval_cv = train[train[\"fold\"] == fold][target]\n\n        model = CatBoostRegressor(\n            learning_rate = 0.048,\n            depth = 4,\n            min_data_in_leaf = 34,\n            iterations = 10000,\n            early_stopping_rounds = 300,\n            task_type ='CPU',\n            loss_function ='RMSE'\n          )\n        model.fit(X_train_cv, \n                  y_train_cv, \n                  eval_set=[(X_eval_cv, y_eval_cv)],\n                  verbose=False)\n        models.append(model)\n\n    model_dict[target] = models","metadata":{"_uuid":"8a573319-332c-41c1-80a8-44fc3b822576","_cell_guid":"ce8a7888-fa87-4615-9872-901973ab0145","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:18:25.788967Z","iopub.execute_input":"2023-10-11T05:18:25.789554Z","iopub.status.idle":"2023-10-11T05:18:43.275069Z","shell.execute_reply.started":"2023-10-11T05:18:25.789522Z","shell.execute_reply":"2023-10-11T05:18:43.274083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CV Score","metadata":{"_uuid":"255039a3-ff70-4355-93b2-c1f904c31f09","_cell_guid":"a27e11dc-7888-4ba4-958b-3d3565df545c","trusted":true}},{"cell_type":"code","source":"# cv\nrmses = []\n\nfor target in targets:\n    models = model_dict[target]\n\n    preds = []\n    trues = []\n    \n    for fold, model in enumerate(models):\n        X_eval_cv = train[train[\"fold\"] == fold].drop(columns=drop_columns)\n        y_eval_cv = train[train[\"fold\"] == fold][target]\n\n        pred = model.predict(X_eval_cv)\n\n        trues.extend(y_eval_cv)\n        preds.extend(pred)\n        \n    rmse = np.sqrt(mean_squared_error(trues, preds))\n    print(f\"{target}_rmse : {rmse}\")\n    rmses = rmses + [rmse]\n\nprint(f\"mcrmse : {sum(rmses) / len(rmses)}\")","metadata":{"_uuid":"eb54895a-04b8-42eb-acd4-8f3a6a5a4e55","_cell_guid":"b653fbc8-4190-4d4d-a49f-c999bfde577f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:18:43.276438Z","iopub.execute_input":"2023-10-11T05:18:43.276907Z","iopub.status.idle":"2023-10-11T05:18:43.35321Z","shell.execute_reply.started":"2023-10-11T05:18:43.276876Z","shell.execute_reply":"2023-10-11T05:18:43.352333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predict","metadata":{"_uuid":"f966895a-4d6f-410b-8a15-8982bf7863c1","_cell_guid":"c71c4ab2-efcd-4ddb-9797-7e27443ada72","trusted":true}},{"cell_type":"code","source":"drop_columns = [\n                #\"fold\", \n                \"student_id\", \"prompt_id\", \"text\", \"fixed_summary_text\",\n                \"prompt_question\", \"prompt_title\", \n                \"prompt_text\",\n                \"input\",\"title\", \"author\", \"description\", \"genre\", \"grade\"\n               ] + [\n                f\"content_pred_{i}\" for i in range(CFG.n_splits)\n                ] + [\n                f\"wording_pred_{i}\" for i in range(CFG.n_splits)\n                ]","metadata":{"_uuid":"fdc84757-3248-4d80-bc30-63a5a545dc59","_cell_guid":"b4a6b222-1311-435f-ac2a-7d1397921cd3","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:18:43.354373Z","iopub.execute_input":"2023-10-11T05:18:43.354838Z","iopub.status.idle":"2023-10-11T05:18:43.361023Z","shell.execute_reply.started":"2023-10-11T05:18:43.354805Z","shell.execute_reply":"2023-10-11T05:18:43.360086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_dict = {}\nfor target in targets:\n    models = model_dict[target]\n    preds = []\n\n    for fold, model in enumerate(models):\n        X_eval_cv = test.drop(columns=drop_columns)\n        \n        if str(type(model))==\"<class 'xgboost.sklearn.XGBRegressor'>\":\n            print(\"pred xgb. rename cols\")\n            X_eval_cv = X_eval_cv.rename({'content':'content_pred','wording':'wording_pred'},axis=1)\n        elif str(type(model))==\"<class 'catboost.core.CatBoostRegressor'>\":\n            print(\"pred cat. rename cols\")\n            X_eval_cv = X_eval_cv.rename({'content':'content_pred','wording':'wording_pred'},axis=1)\n        else:\n            print(\"pred lgb\")\n\n        pred = model.predict(X_eval_cv)\n        preds.append(pred)\n    \n    pred_dict[target] = preds","metadata":{"_uuid":"8e2f47fb-fd03-4b69-a369-af9bc52f0da1","_cell_guid":"8b802259-551b-4092-a7a2-53183c346da1","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:18:43.362412Z","iopub.execute_input":"2023-10-11T05:18:43.363024Z","iopub.status.idle":"2023-10-11T05:18:43.404522Z","shell.execute_reply.started":"2023-10-11T05:18:43.362988Z","shell.execute_reply":"2023-10-11T05:18:43.403652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for target in targets:\n    preds = pred_dict[target]\n    for i, pred in enumerate(preds):\n        test[f\"{target}_pred_{i}\"] = pred\n\n    test[target] = test[[f\"{target}_pred_{fold}\" for fold in range(CFG.n_splits)]].mean(axis=1)","metadata":{"_uuid":"1a7596c2-b9bd-442f-91ae-81f490b81a22","_cell_guid":"44c85080-c8d2-499f-b00e-162ed03782bd","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:18:43.405642Z","iopub.execute_input":"2023-10-11T05:18:43.406333Z","iopub.status.idle":"2023-10-11T05:18:43.417825Z","shell.execute_reply.started":"2023-10-11T05:18:43.406299Z","shell.execute_reply":"2023-10-11T05:18:43.416499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create Submission file","metadata":{"_uuid":"46aa877c-f679-4b29-9a47-86f4e54e3466","_cell_guid":"f33ad233-36f2-4696-8c7b-c0a2f8f90120","trusted":true}},{"cell_type":"code","source":"test_1 = test[[\"student_id\", \"content\", \"wording\"]]\ntest_1.head()","metadata":{"_uuid":"38bc4f24-f63b-420e-ac94-577a742c5a0c","_cell_guid":"99a4cf90-18ad-41ba-85c6-068a9aef7929","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:18:43.419327Z","iopub.execute_input":"2023-10-11T05:18:43.419919Z","iopub.status.idle":"2023-10-11T05:18:43.439341Z","shell.execute_reply.started":"2023-10-11T05:18:43.419884Z","shell.execute_reply":"2023-10-11T05:18:43.438375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2nd MODEL","metadata":{"_uuid":"3a98f678-ea98-44da-aa1a-2523fe21530a","_cell_guid":"f80e2393-d94f-4bb6-95ba-7801371d88ef","trusted":true}},{"cell_type":"code","source":"from typing import List\nimport numpy as np\nimport pandas as pd\nimport warnings\nimport logging\nimport os\nimport shutil\nimport json\nimport transformers\nfrom transformers import AutoModel, AutoTokenizer, AutoConfig, AutoModelForSequenceClassification\nfrom transformers import DataCollatorWithPadding\nfrom datasets import Dataset,load_dataset, load_from_disk\nfrom transformers import TrainingArguments, Trainer\nfrom datasets import load_metric, disable_progress_bar\nfrom sklearn.metrics import mean_squared_error\nimport torch\nfrom sklearn.model_selection import KFold, GroupKFold\nfrom tqdm import tqdm\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize.treebank import TreebankWordDetokenizer\nfrom collections import Counter\nimport spacy\nimport re\nfrom autocorrect import Speller\nfrom spellchecker import SpellChecker\nimport lightgbm as lgb\n\nwarnings.simplefilter(\"ignore\")\nlogging.disable(logging.ERROR)\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \ndisable_progress_bar()\ntqdm.pandas()","metadata":{"_uuid":"45ce0bc8-b9df-43df-aa2d-ce74f2bd3125","_cell_guid":"1a7fdc91-f93e-4ad9-afd4-3cd49ab7a663","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:18:43.440623Z","iopub.execute_input":"2023-10-11T05:18:43.441117Z","iopub.status.idle":"2023-10-11T05:18:43.453253Z","shell.execute_reply.started":"2023-10-11T05:18:43.441082Z","shell.execute_reply":"2023-10-11T05:18:43.452264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed: int):\n    import random, os\n    import numpy as np\n    import torch\n    \n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    \nseed_everything(seed=42)","metadata":{"_uuid":"498aa611-20fd-43db-a2f3-eafb567da4f6","_cell_guid":"c4530ba0-f37b-4faf-a5e8-2c7d2aed39a8","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:18:43.454586Z","iopub.execute_input":"2023-10-11T05:18:43.455061Z","iopub.status.idle":"2023-10-11T05:18:43.475764Z","shell.execute_reply.started":"2023-10-11T05:18:43.455011Z","shell.execute_reply":"2023-10-11T05:18:43.473924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    model_name = \"another-bert\"\n    learning_rate=1.5e-5\n    weight_decay=0.02\n    hidden_dropout_prob=0.007\n    attention_probs_dropout_prob=0.007\n    num_train_epochs=5\n    n_splits=4\n    batch_size=12\n    \n    random_seed=42\n    save_steps=20\n    max_length=512","metadata":{"_uuid":"1a1aee8d-c854-4b47-9dfb-42d7cc8e2d7c","_cell_guid":"bad574f1-4f5c-46a4-b3ab-bc99fc9bc5ba","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:18:43.500026Z","iopub.execute_input":"2023-10-11T05:18:43.500213Z","iopub.status.idle":"2023-10-11T05:18:43.505107Z","shell.execute_reply.started":"2023-10-11T05:18:43.50019Z","shell.execute_reply":"2023-10-11T05:18:43.504242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MODEL_DIR = '/kaggle/input/commitlit-deberta-v3-large-nofix'\nINIT_MODEL = f\"{MODEL_DIR}/content/{CFG.model_name}/fold_0\"","metadata":{"_uuid":"d1d1a10b-440c-4d5b-8fdc-15ae5eaf0d7a","_cell_guid":"c7b627ec-2a6e-449f-b913-3a3d0c00f214","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:18:43.506637Z","iopub.execute_input":"2023-10-11T05:18:43.507199Z","iopub.status.idle":"2023-10-11T05:18:43.520524Z","shell.execute_reply.started":"2023-10-11T05:18:43.507137Z","shell.execute_reply":"2023-10-11T05:18:43.519315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataload","metadata":{"_uuid":"84805414-ab0f-47ff-85e0-c04a3e3993e4","_cell_guid":"d99ec4c4-c46e-42fa-8782-29a1712e59b5","trusted":true}},{"cell_type":"code","source":"DATA_DIR = \"/kaggle/input/commonlit-evaluate-student-summaries/\"\n\nprompts_train = pd.read_csv(DATA_DIR + \"prompts_train.csv\")\nprompts_test = pd.read_csv(DATA_DIR + \"prompts_test.csv\")\nsummaries_train = pd.read_csv(DATA_DIR + \"summaries_train.csv\")\nsummaries_test = pd.read_csv(DATA_DIR + \"summaries_test.csv\")\nsample_submission = pd.read_csv(DATA_DIR + \"sample_submission.csv\")","metadata":{"_uuid":"202b8e75-2287-4c9a-9c08-c673f91259a0","_cell_guid":"28212a4f-d874-428b-9928-55840a737238","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:18:43.524829Z","iopub.execute_input":"2023-10-11T05:18:43.526909Z","iopub.status.idle":"2023-10-11T05:18:43.678264Z","shell.execute_reply.started":"2023-10-11T05:18:43.526869Z","shell.execute_reply":"2023-10-11T05:18:43.677383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocess\n\n[Using features]\n\n- Text Length\n- Length Ratio\n- Word Overlap\n- N-grams Co-occurrence\n  - count\n  - ratio\n- Quotes Overlap\n- Grammar Check\n  - spelling: pyspellchecker","metadata":{"_uuid":"d8cb4a2b-cc7f-418e-bdc6-834401da990d","_cell_guid":"f5a67470-cf9e-4c68-94c1-503a96d9f43b","trusted":true}},{"cell_type":"code","source":"class Preprocessor:\n    def __init__(self, \n                model_name: str,\n                ) -> None:\n        self.tokenizer = AutoTokenizer.from_pretrained(INIT_MODEL)\n        self.twd = TreebankWordDetokenizer()\n        self.STOP_WORDS = set(stopwords.words('english'))\n        \n        self.spacy_ner_model = spacy.load('en_core_web_sm',)\n        self.speller = Speller(lang='en')\n        self.spellchecker = SpellChecker() \n        \n    def word_overlap_count(self, row):\n        \"\"\" intersection(prompt_text, text) \"\"\"        \n        def check_is_stop_word(word):\n            return word in self.STOP_WORDS\n        \n        prompt_words = row['prompt_tokens']\n        summary_words = row['summary_tokens']\n        if self.STOP_WORDS:\n            prompt_words = list(filter(check_is_stop_word, prompt_words))\n            summary_words = list(filter(check_is_stop_word, summary_words))\n        return len(set(prompt_words).intersection(set(summary_words)))\n            \n    def ngrams(self, token, n):\n        # Use the zip function to help us generate n-grams\n        # Concatentate the tokens into ngrams and return\n        ngrams = zip(*[token[i:] for i in range(n)])\n        return [\" \".join(ngram) for ngram in ngrams]\n\n    def ngram_co_occurrence(self, row, n: int) -> int:\n        # Tokenize the original text and summary into words\n        original_tokens = row['prompt_tokens']\n        summary_tokens = row['summary_tokens']\n\n        # Generate n-grams for the original text and summary\n        original_ngrams = set(self.ngrams(original_tokens, n))\n        summary_ngrams = set(self.ngrams(summary_tokens, n))\n\n        # Calculate the number of common n-grams\n        common_ngrams = original_ngrams.intersection(summary_ngrams)\n        return len(common_ngrams)\n    \n    def ner_overlap_count(self, row, mode:str):\n        model = self.spacy_ner_model\n        def clean_ners(ner_list):\n            return set([(ner[0].lower(), ner[1]) for ner in ner_list])\n        prompt = model(row['prompt_text'])\n        summary = model(row['text'])\n\n        if \"spacy\" in str(model):\n            prompt_ner = set([(token.text, token.label_) for token in prompt.ents])\n            summary_ner = set([(token.text, token.label_) for token in summary.ents])\n        elif \"stanza\" in str(model):\n            prompt_ner = set([(token.text, token.type) for token in prompt.ents])\n            summary_ner = set([(token.text, token.type) for token in summary.ents])\n        else:\n            raise Exception(\"Model not supported\")\n\n        prompt_ner = clean_ners(prompt_ner)\n        summary_ner = clean_ners(summary_ner)\n\n        intersecting_ners = prompt_ner.intersection(summary_ner)\n        \n        ner_dict = dict(Counter([ner[1] for ner in intersecting_ners]))\n        \n        if mode == \"train\":\n            return ner_dict\n        elif mode == \"test\":\n            return {key: ner_dict.get(key) for key in self.ner_keys}\n\n    \n    def quotes_count(self, row):\n        summary = row['text']\n        text = row['prompt_text']\n        quotes_from_summary = re.findall(r'\"([^\"]*)\"', summary)\n        if len(quotes_from_summary)>0:\n            return [quote in text for quote in quotes_from_summary].count(True)\n        else:\n            return 0\n\n    def spelling(self, text):\n        \n        wordlist=text.split()\n        amount_miss = len(list(self.spellchecker.unknown(wordlist)))\n\n        return amount_miss\n    \n    def add_spelling_dictionary(self, tokens: List[str]) -> List[str]:\n        \"\"\"dictionary update for pyspell checker and autocorrect\"\"\"\n        self.spellchecker.word_frequency.load_words(tokens)\n        self.speller.nlp_data.update({token:1000 for token in tokens})\n    \n    def run(self, \n            prompts: pd.DataFrame,\n            summaries:pd.DataFrame,\n            mode:str\n        ) -> pd.DataFrame:\n        \n        # before merge preprocess\n        prompts[\"prompt_length\"] = prompts[\"prompt_text\"].apply(\n            lambda x: len(word_tokenize(x))\n        )\n        prompts[\"prompt_tokens\"] = prompts[\"prompt_text\"].apply(\n            lambda x: word_tokenize(x)\n        )\n\n        summaries[\"summary_length\"] = summaries[\"text\"].apply(\n            lambda x: len(word_tokenize(x))\n        )\n        summaries[\"summary_tokens\"] = summaries[\"text\"].apply(\n            lambda x: word_tokenize(x)\n        )\n        \n        # Add prompt tokens into spelling checker dictionary\n        prompts[\"prompt_tokens\"].apply(\n            lambda x: self.add_spelling_dictionary(x)\n        )\n        \n#         from IPython.core.debugger import Pdb; Pdb().set_trace()\n        # fix misspelling\n        summaries[\"fixed_summary_text\"] = summaries[\"text\"].progress_apply(\n            lambda x: self.speller(x)\n        )\n        \n        # count misspelling\n        summaries[\"splling_err_num\"] = summaries[\"text\"].progress_apply(self.spelling)\n        \n        # merge prompts and summaries\n        input_df = summaries.merge(prompts, how=\"left\", on=\"prompt_id\")\n\n        # after merge preprocess\n        input_df['length_ratio'] = input_df['summary_length'] / input_df['prompt_length']\n\n        input_df['word_overlap_count'] = input_df.progress_apply(self.word_overlap_count, axis=1)\n        input_df['bigram_overlap_count'] = input_df.progress_apply(\n            self.ngram_co_occurrence,args=(2,), axis=1 \n        )\n        input_df['bigram_overlap_ratio'] = input_df['bigram_overlap_count'] / (input_df['summary_length'] - 1)\n        \n        input_df['trigram_overlap_count'] = input_df.progress_apply(\n            self.ngram_co_occurrence, args=(3,), axis=1\n        )\n        input_df['trigram_overlap_ratio'] = input_df['trigram_overlap_count'] / (input_df['summary_length'] - 2)\n        \n        input_df['quotes_count'] = input_df.progress_apply(self.quotes_count, axis=1)\n        \n        return input_df.drop(columns=[\"summary_tokens\", \"prompt_tokens\"])\n    \npreprocessor = Preprocessor(model_name=CFG.model_name)","metadata":{"_uuid":"c1f43314-d8e0-4fb1-8910-921ce8458579","_cell_guid":"44096ef7-372b-44a3-a0f2-fbc653898dd8","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:18:43.683269Z","iopub.execute_input":"2023-10-11T05:18:43.685975Z","iopub.status.idle":"2023-10-11T05:18:45.250578Z","shell.execute_reply.started":"2023-10-11T05:18:43.685942Z","shell.execute_reply":"2023-10-11T05:18:45.249687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summaries_train.head()","metadata":{"_uuid":"e67a8b6f-434b-4884-a07c-b07c30ff7b67","_cell_guid":"eb05d33b-4327-4317-8221-f4ed2e5ae06f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:18:45.252598Z","iopub.execute_input":"2023-10-11T05:18:45.252841Z","iopub.status.idle":"2023-10-11T05:18:45.263174Z","shell.execute_reply.started":"2023-10-11T05:18:45.252811Z","shell.execute_reply":"2023-10-11T05:18:45.262237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\nwith open(MODEL_DIR+'/pickled.pkl', 'rb') as f:\n    train = pickle.load(f)\n    test = pickle.load(f)\n\n# train = preprocessor.run(prompts_train, summaries_train, mode=\"train\")\n#test = preprocessor.run(prompts_test, summaries_test, mode=\"test\")\n\n# train.head()","metadata":{"_uuid":"7b1eaf10-36da-4b56-8c72-0595adc2c3ca","_cell_guid":"38099732-e1d1-4e74-acd9-f57ae43022a9","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:18:45.264751Z","iopub.execute_input":"2023-10-11T05:18:45.265282Z","iopub.status.idle":"2023-10-11T05:18:45.375753Z","shell.execute_reply.started":"2023-10-11T05:18:45.26525Z","shell.execute_reply":"2023-10-11T05:18:45.370834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train_keep.copy()\ntest = test_keep.copy()","metadata":{"_uuid":"9e716d46-3da1-4944-b6c7-724d017e42c8","_cell_guid":"3103e810-4e74-4e78-a47a-cf3de9ecaa53","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:18:45.380687Z","iopub.execute_input":"2023-10-11T05:18:45.381119Z","iopub.status.idle":"2023-10-11T05:18:45.394149Z","shell.execute_reply.started":"2023-10-11T05:18:45.381077Z","shell.execute_reply":"2023-10-11T05:18:45.393008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train = train[:256]","metadata":{"_uuid":"fc45e407-1a4d-4232-ab05-48decd7237a5","_cell_guid":"8f86e50e-8a74-4e7e-a37c-e6567cbadd47","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:18:45.395723Z","iopub.execute_input":"2023-10-11T05:18:45.395983Z","iopub.status.idle":"2023-10-11T05:18:45.401064Z","shell.execute_reply.started":"2023-10-11T05:18:45.395948Z","shell.execute_reply":"2023-10-11T05:18:45.400156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gkf = GroupKFold(n_splits=CFG.n_splits)\n\nfor i, (_, val_index) in enumerate(gkf.split(train, groups=train[\"prompt_id\"])):\n    train.loc[val_index, \"fold\"] = i\n\ntrain.head()","metadata":{"_uuid":"f0df1f12-913e-435e-a4ea-ef7740f416e3","_cell_guid":"b563a066-23d1-4582-abac-b8a97a8fe29b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:18:45.402471Z","iopub.execute_input":"2023-10-11T05:18:45.402697Z","iopub.status.idle":"2023-10-11T05:18:45.443704Z","shell.execute_reply.started":"2023-10-11T05:18:45.402672Z","shell.execute_reply":"2023-10-11T05:18:45.44276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Function Definition","metadata":{"_uuid":"8f9f2da8-c8b2-4e2d-838b-52b85339aba3","_cell_guid":"01f1e619-0e8e-4340-8e70-7872a01f2c54","trusted":true}},{"cell_type":"code","source":"import shutil\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    rmse = mean_squared_error(labels, predictions, squared=False)\n    return {\"rmse\": rmse}\n\ndef compute_mcrmse(eval_pred):\n    \"\"\"\n    Calculates mean columnwise root mean squared error\n    https://www.kaggle.com/competitions/commonlit-evaluate-student-summaries/overview/evaluation\n    \"\"\"\n    preds, labels = eval_pred\n\n    col_rmse = np.sqrt(np.mean((preds - labels) ** 2, axis=0))\n    mcrmse = np.mean(col_rmse)\n\n    return {\n        \"content_rmse\": col_rmse[0],\n        \"wording_rmse\": col_rmse[1],\n        \"mcrmse\": mcrmse,\n    }\n\ndef compt_score(content_true, content_pred, wording_true, wording_pred):\n    content_score = mean_squared_error(content_true, content_pred)**(1/2)\n    wording_score = mean_squared_error(wording_true, wording_pred)**(1/2)\n    \n    return (content_score + wording_score)/2","metadata":{"_uuid":"dd5e2d41-a5ca-417b-bffd-71a113ba92f3","_cell_guid":"c5706085-0975-4eb4-b0f1-82a116497073","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:18:45.445163Z","iopub.execute_input":"2023-10-11T05:18:45.445394Z","iopub.status.idle":"2023-10-11T05:18:45.451894Z","shell.execute_reply.started":"2023-10-11T05:18:45.445365Z","shell.execute_reply":"2023-10-11T05:18:45.450858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Deberta Regressor","metadata":{"_uuid":"d8a5488e-2094-429a-9c65-8c54ccbbfaf5","_cell_guid":"663f3138-a04a-4f3d-80f5-3d87da1f6c3a","trusted":true}},{"cell_type":"code","source":"class ContentScoreRegressor:\n    def __init__(self, \n                model_name: str,\n                model_dir: str,\n                target: str,\n                hidden_dropout_prob: float,\n                attention_probs_dropout_prob: float,\n                max_length: int,\n                ):\n        self.inputs = [\"prompt_text\", \"prompt_title\", \"prompt_question\", \"fixed_summary_text\"]\n        self.input_col = \"input\"\n        \n        self.text_cols = [self.input_col] \n        self.target = target\n        self.target_cols = [target]\n\n        self.model_name = model_name\n        self.model_dir = model_dir\n        self.max_length = max_length\n        \n        self.tokenizer = AutoTokenizer.from_pretrained(INIT_MODEL)\n        self.model_config = AutoConfig.from_pretrained(INIT_MODEL)\n        \n        self.model_config.update({\n            \"hidden_dropout_prob\": hidden_dropout_prob,\n            \"attention_probs_dropout_prob\": attention_probs_dropout_prob,\n            \"num_labels\": 1,\n            \"problem_type\": \"regression\",\n        })\n        \n        seed_everything(seed=42)\n\n        self.data_collator = DataCollatorWithPadding(\n            tokenizer=self.tokenizer\n        )\n\n\n    def tokenize_function(self, examples: pd.DataFrame):\n        labels = [examples[self.target]]\n        tokenized = self.tokenizer(examples[self.input_col],\n                         padding=False,\n                         truncation=True,\n                         max_length=self.max_length)\n        return {\n            **tokenized,\n            \"labels\": labels,\n        }\n    \n    def tokenize_function_test(self, examples: pd.DataFrame):\n        tokenized = self.tokenizer(examples[self.input_col],\n                         padding=False,\n                         truncation=True,\n                         max_length=self.max_length)\n        return tokenized\n        \n    def train(self, \n            fold: int,\n            train_df: pd.DataFrame,\n            valid_df: pd.DataFrame,\n            batch_size: int,\n            learning_rate: float,\n            weight_decay: float,\n            num_train_epochs: float,\n            save_steps: int,\n        ) -> None:\n        \"\"\"fine-tuning\"\"\"\n        \n        sep = self.tokenizer.sep_token\n        train_df[self.input_col] = (\n                    train_df[\"prompt_title\"] + sep \n                    + train_df[\"prompt_question\"] + sep \n                    + train_df[\"text\"]\n                  )\n\n        valid_df[self.input_col] = (\n                    valid_df[\"prompt_title\"] + sep \n                    + valid_df[\"prompt_question\"] + sep \n                    + valid_df[\"text\"]\n                  )\n        \n        train_df = train_df[[self.input_col] + self.target_cols]\n        valid_df = valid_df[[self.input_col] + self.target_cols]\n        \n        model_content = AutoModelForSequenceClassification.from_pretrained(\n            f\"/kaggle/input/{self.model_name}\", \n            config=self.model_config,\n            ignore_mismatched_sizes=True\n        )\n\n        train_dataset = Dataset.from_pandas(train_df, preserve_index=False) \n        val_dataset = Dataset.from_pandas(valid_df, preserve_index=False) \n    \n        train_tokenized_datasets = train_dataset.map(self.tokenize_function, batched=False)\n        val_tokenized_datasets = val_dataset.map(self.tokenize_function, batched=False)\n\n        # eg. \"bert/fold_0/\"\n        model_fold_dir = os.path.join(self.model_dir, str(fold)) \n        \n        training_args = TrainingArguments(\n            output_dir=model_fold_dir,\n            load_best_model_at_end=True, # select best model\n            learning_rate=learning_rate,\n            per_device_train_batch_size=batch_size,\n            per_device_eval_batch_size=8,\n            num_train_epochs=num_train_epochs,\n            weight_decay=weight_decay,\n            report_to='none',\n            greater_is_better=False,\n            save_strategy=\"steps\",\n            evaluation_strategy=\"steps\",\n            eval_steps=save_steps,\n            save_steps=save_steps,\n            metric_for_best_model=\"rmse\",\n            save_total_limit=1\n        )\n\n        trainer = Trainer(\n            model=model_content,\n            args=training_args,\n            train_dataset=train_tokenized_datasets,\n            eval_dataset=val_tokenized_datasets,\n            tokenizer=self.tokenizer,\n            compute_metrics=compute_metrics,\n            data_collator=self.data_collator\n        )\n\n        trainer.train()\n        \n        shutil.rmtree(self.model_dir)\n        \n        model_content.save_pretrained(self.model_dir)\n        self.tokenizer.save_pretrained(self.model_dir)\n\n        \n    def predict(self, \n                test_df: pd.DataFrame,\n                fold: int,\n               ):\n        \"\"\"predict content score\"\"\"\n        \n        sep = self.tokenizer.sep_token\n        in_text = (\n                    test_df[\"prompt_title\"] + sep \n                    + test_df[\"prompt_question\"] + sep \n                    + test_df[\"text\"]\n                  )\n        test_df[self.input_col] = in_text\n\n        test_ = test_df[[self.input_col]]\n    \n        test_dataset = Dataset.from_pandas(test_, preserve_index=False) \n        test_tokenized_dataset = test_dataset.map(self.tokenize_function_test, batched=False)\n\n        model_content = AutoModelForSequenceClassification.from_pretrained(f\"{self.model_dir}\")\n        model_content.eval()\n        \n        # e.g. \"bert/fold_0/\"\n        model_fold_dir = os.path.join(self.model_dir, str(fold)) \n        model_fold_dir = \"valid_log\" #f\"bert-{fold}\"\n#         print(\"model_fold_dir\", model_fold_dir)\n        \n        test_args = TrainingArguments(\n            output_dir=model_fold_dir,\n            do_train = False,\n            do_predict = True,\n            per_device_eval_batch_size = 4,   \n            dataloader_drop_last = False,\n        )\n\n        # init trainer\n        infer_content = Trainer(\n                      model = model_content, \n                      tokenizer=self.tokenizer,\n                      data_collator=self.data_collator,\n                      args = test_args)\n\n        preds = infer_content.predict(test_tokenized_dataset)[0]\n\n        return preds","metadata":{"_uuid":"46abfee5-df9a-4f06-85d8-97c89d7956a3","_cell_guid":"c73114af-8b12-416d-a3e4-7fd3320a4b47","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:18:45.453389Z","iopub.execute_input":"2023-10-11T05:18:45.453936Z","iopub.status.idle":"2023-10-11T05:18:45.477102Z","shell.execute_reply.started":"2023-10-11T05:18:45.453905Z","shell.execute_reply":"2023-10-11T05:18:45.476211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_by_fold(\n        train_df: pd.DataFrame,\n        model_name: str,\n        target:str,\n        save_each_model: bool,\n        n_splits: int,\n        batch_size: int,\n        learning_rate: int,\n        hidden_dropout_prob: float,\n        attention_probs_dropout_prob: float,\n        weight_decay: float,\n        num_train_epochs: int,\n        save_steps: int,\n        max_length:int\n    ):\n\n    # delete old model files\n    if os.path.exists(model_name):\n        shutil.rmtree(model_name)\n    \n    os.mkdir(model_name)\n        \n    for fold in range(CFG.n_splits):\n        print(f\"fold {fold}:\")\n        \n        train_data = train_df[train_df[\"fold\"] != fold]\n        valid_data = train_df[train_df[\"fold\"] == fold]\n        \n        if save_each_model == True:\n            model_dir =  f\"{target}/{model_name}/fold_{fold}\"\n        else: \n            model_dir =  f\"{model_name}/fold_{fold}\"\n\n        csr = ContentScoreRegressor(\n            model_name=model_name,\n            target=target,\n            model_dir = model_dir, \n            hidden_dropout_prob=hidden_dropout_prob,\n            attention_probs_dropout_prob=attention_probs_dropout_prob,\n            max_length=max_length,\n           )\n        \n        csr.train(\n            fold=fold,\n            train_df=train_data,\n            valid_df=valid_data, \n            batch_size=batch_size,\n            learning_rate=learning_rate,\n            weight_decay=weight_decay,\n            num_train_epochs=num_train_epochs,\n            save_steps=save_steps,\n        )\n\ndef validate(\n    train_df: pd.DataFrame,\n    target:str,\n    save_each_model: bool,\n    model_name: str,\n    hidden_dropout_prob: float,\n    attention_probs_dropout_prob: float,\n    max_length : int\n    ) -> pd.DataFrame:\n    \"\"\"predict oof data\"\"\"\n    for fold in range(CFG.n_splits):\n        print(f\"fold {fold}:\")\n        \n        valid_data = train_df[train_df[\"fold\"] == fold]\n        \n        if save_each_model == True:\n            model_dir =  f\"{MODEL_DIR}/{target}/{model_name}/fold_{fold}\"\n        else: \n            model_dir =  f\"{MODEL_DIR}/{model_name}/fold_{fold}\"\n        \n#         print(model_dir, model_name)\n        csr = ContentScoreRegressor(\n            model_name=model_name,\n            target=target,\n            model_dir = model_dir,\n            hidden_dropout_prob=hidden_dropout_prob,\n            attention_probs_dropout_prob=attention_probs_dropout_prob,\n            max_length=max_length,\n           )\n        \n        pred = csr.predict(\n            test_df=valid_data, \n            fold=fold\n        )\n        \n        train_df.loc[valid_data.index, f\"{target}_pred\"] = pred\n\n    return train_df\n    \ndef predict(\n    test_df: pd.DataFrame,\n    target:str,\n    save_each_model: bool,\n    model_name: str,\n    hidden_dropout_prob: float,\n    attention_probs_dropout_prob: float,\n    max_length : int\n    ):\n    \"\"\"predict using mean folds\"\"\"\n\n    for fold in range(CFG.n_splits):\n        print(f\"fold {fold}:\")\n        \n        if save_each_model == True:\n            model_dir =  f\"{MODEL_DIR}/{target}/{model_name}/fold_{fold}\"\n        else: \n            model_dir =  f\"{MODEL_DIR}/{model_name}/fold_{fold}\"\n\n        csr = ContentScoreRegressor(\n            model_name=model_name,\n            target=target,\n            model_dir = model_dir, \n            hidden_dropout_prob=hidden_dropout_prob,\n            attention_probs_dropout_prob=attention_probs_dropout_prob,\n            max_length=max_length,\n           )\n        \n        pred = csr.predict(\n            test_df=test_df, \n            fold=fold\n        )\n        \n        test_df[f\"{target}_pred_{fold}\"] = pred\n    \n    test_df[f\"{target}\"] = test_df[[f\"{target}_pred_{fold}\" for fold in range(CFG.n_splits)]].mean(axis=1)\n\n    return test_df","metadata":{"_uuid":"045b0c01-8a40-459c-b284-c77752f63151","_cell_guid":"3abf27d5-c1ff-43dd-8338-a23884f3c2b9","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:18:45.478619Z","iopub.execute_input":"2023-10-11T05:18:45.478891Z","iopub.status.idle":"2023-10-11T05:18:45.494577Z","shell.execute_reply.started":"2023-10-11T05:18:45.478843Z","shell.execute_reply":"2023-10-11T05:18:45.4937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for target in [\"content\", \"wording\"]:\n#     train_by_fold(\n#         train,\n#         model_name=CFG.model_name,\n#         save_each_model=True,\n#         target=target,\n#         learning_rate=CFG.learning_rate,\n#         hidden_dropout_prob=CFG.hidden_dropout_prob,\n#         attention_probs_dropout_prob=CFG.attention_probs_dropout_prob,\n#         weight_decay=CFG.weight_decay,\n#         num_train_epochs=CFG.num_train_epochs,\n#         n_splits=CFG.n_splits,\n#         batch_size=CFG.batch_size,\n#         save_steps=CFG.save_steps,\n#         max_length=CFG.max_length\n#     )\n    \n    print(\"[validate]\")\n    train = validate(\n        train,\n        target=target,\n        save_each_model=True,\n        model_name=CFG.model_name,\n        hidden_dropout_prob=CFG.hidden_dropout_prob,\n        attention_probs_dropout_prob=CFG.attention_probs_dropout_prob,\n        max_length=CFG.max_length\n    )\n\n    rmse = mean_squared_error(train[target], train[f\"{target}_pred\"], squared=False)\n    print(f\"cv {target} rmse: {rmse}\")\n\n    print(\"[test]\")\n    test = predict(\n        test,\n        target=target,\n        save_each_model=True,\n        model_name=CFG.model_name,\n        hidden_dropout_prob=CFG.hidden_dropout_prob,\n        attention_probs_dropout_prob=CFG.attention_probs_dropout_prob,\n        max_length=CFG.max_length\n    )","metadata":{"_uuid":"73f7bd77-42c6-4de9-baa7-effc315681e4","_cell_guid":"7806ca4f-8605-4b72-929e-78a3fec5d488","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:18:45.495797Z","iopub.execute_input":"2023-10-11T05:18:45.496597Z","iopub.status.idle":"2023-10-11T05:31:01.193699Z","shell.execute_reply.started":"2023-10-11T05:18:45.496566Z","shell.execute_reply":"2023-10-11T05:31:01.192757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !rm -r wording content","metadata":{"_uuid":"8b1b66dc-4c7c-49e8-8ee4-0a0222e3a9e5","_cell_guid":"9e2f2034-35db-4ef0-a9c3-faaf41d3f972","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:31:01.196309Z","iopub.execute_input":"2023-10-11T05:31:01.196912Z","iopub.status.idle":"2023-10-11T05:31:01.200799Z","shell.execute_reply.started":"2023-10-11T05:31:01.196877Z","shell.execute_reply":"2023-10-11T05:31:01.199991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"_uuid":"7597a3cf-f848-408e-a581-5f9949a7a995","_cell_guid":"db31159a-01a8-4c10-95b4-0a7610c22f10","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:31:01.202103Z","iopub.execute_input":"2023-10-11T05:31:01.20319Z","iopub.status.idle":"2023-10-11T05:31:01.238949Z","shell.execute_reply.started":"2023-10-11T05:31:01.203157Z","shell.execute_reply":"2023-10-11T05:31:01.237785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_2nd = train\ntest_2nd = test","metadata":{"_uuid":"82b39550-7914-44da-a8b0-6a7b47e5a11e","_cell_guid":"aaba1fdc-bdc4-4e3b-93cf-b9af40561b21","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:31:01.240215Z","iopub.execute_input":"2023-10-11T05:31:01.241053Z","iopub.status.idle":"2023-10-11T05:31:01.248201Z","shell.execute_reply.started":"2023-10-11T05:31:01.241017Z","shell.execute_reply":"2023-10-11T05:31:01.247261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LGBM model","metadata":{"_uuid":"cf04a07b-d0fb-4c41-a835-4ae32474acb2","_cell_guid":"5e2b68ef-b587-4987-a1f5-e6d1d64353d9","trusted":true}},{"cell_type":"code","source":"targets = [\"content\", \"wording\"]\n\ndrop_columns = [\"fold\", \"student_id\", \"prompt_id\", \"text\", \"fixed_summary_text\",\n                \"prompt_question\", \"prompt_title\", \n                \"prompt_text\",\"title\", \"author\", \"description\", \"genre\", \"grade\"\n               ] + targets","metadata":{"_uuid":"15a6a6aa-50af-4798-801d-383d3923927e","_cell_guid":"275bd7a3-7042-4c4b-8055-9b860146136f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:31:01.249494Z","iopub.execute_input":"2023-10-11T05:31:01.250525Z","iopub.status.idle":"2023-10-11T05:31:01.258056Z","shell.execute_reply.started":"2023-10-11T05:31:01.250491Z","shell.execute_reply":"2023-10-11T05:31:01.257231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[train[\"fold\"] != 0].drop(columns=drop_columns)","metadata":{"_uuid":"264ca89d-4a4e-4685-8eb4-c568a423f629","_cell_guid":"61c9cf59-d9fb-4ea9-b598-d6e44e80e74a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:31:01.259364Z","iopub.execute_input":"2023-10-11T05:31:01.260446Z","iopub.status.idle":"2023-10-11T05:31:01.294408Z","shell.execute_reply.started":"2023-10-11T05:31:01.260388Z","shell.execute_reply":"2023-10-11T05:31:01.293648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model_dict = {}\n\n# for target in targets:\n#     models = []\n    \n#     for fold in range(CFG.n_splits):\n\n#         X_train_cv = train[train[\"fold\"] != fold].drop(columns=drop_columns)\n#         y_train_cv = train[train[\"fold\"] != fold][target]\n\n#         X_eval_cv = train[train[\"fold\"] == fold].drop(columns=drop_columns)\n#         y_eval_cv = train[train[\"fold\"] == fold][target]\n\n#         dtrain = lgb.Dataset(X_train_cv, label=y_train_cv)\n#         dval = lgb.Dataset(X_eval_cv, label=y_eval_cv)\n\n#         params = {\n#             'boosting_type': 'gbdt',\n#             'random_state': 42,\n#             'objective': 'regression',\n#             'metric': 'rmse',\n#             'learning_rate': 0.048,\n#             'max_depth': 4,\n#             'lambda_l1': 0.001,\n#             'lambda_l2': 0.011\n#         }\n\n#         evaluation_results = {}\n#         model = lgb.train(params,\n#                           num_boost_round=10000,\n#                             #categorical_feature = categorical_features,\n#                           valid_names=['train', 'valid'],\n#                           train_set=dtrain,\n#                           valid_sets=dval,\n#                           callbacks=[\n#                               lgb.early_stopping(stopping_rounds=30, verbose=True),\n#                               lgb.log_evaluation(100),\n#                               lgb.callback.record_evaluation(evaluation_results)\n#                             ],\n#                           )\n#         models.append(model)\n    \n#     model_dict[target] = models","metadata":{"_uuid":"a1b0ab88-ab57-41d4-acaf-3314bac1d694","_cell_guid":"b52b05a9-a297-4bba-95d9-fbf7a5cc23c6","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:31:01.295844Z","iopub.execute_input":"2023-10-11T05:31:01.296054Z","iopub.status.idle":"2023-10-11T05:31:01.300963Z","shell.execute_reply.started":"2023-10-11T05:31:01.296026Z","shell.execute_reply":"2023-10-11T05:31:01.30009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from catboost import CatBoostRegressor\n\n\nmodel_dict = {}\n\nfor target in targets:\n    models = []\n    \n    for fold in range(CFG.n_splits):\n\n        X_train_cv = train[train[\"fold\"] != fold].drop(columns=drop_columns)\n        y_train_cv = train[train[\"fold\"] != fold][target]\n\n        X_eval_cv = train[train[\"fold\"] == fold].drop(columns=drop_columns)\n        y_eval_cv = train[train[\"fold\"] == fold][target]\n\n        model = CatBoostRegressor(\n            learning_rate = 0.048,\n            depth = 4,\n            min_data_in_leaf = 34,\n            iterations = 10000,\n            early_stopping_rounds = 300,\n            task_type ='CPU',\n            loss_function ='RMSE'\n          )\n        model.fit(X_train_cv, \n                  y_train_cv, \n                  eval_set=[(X_eval_cv, y_eval_cv)],\n                  verbose=False)\n        models.append(model)\n\n    model_dict[target] = models","metadata":{"_uuid":"e020b722-a74f-46d4-8df7-ace34bde4f78","_cell_guid":"da8abe04-dadf-4d1f-abf2-eff4ccf1d4c3","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:31:01.302118Z","iopub.execute_input":"2023-10-11T05:31:01.30289Z","iopub.status.idle":"2023-10-11T05:31:20.581756Z","shell.execute_reply.started":"2023-10-11T05:31:01.302861Z","shell.execute_reply":"2023-10-11T05:31:20.580851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CV Score","metadata":{"_uuid":"a882276b-88cf-4608-aa55-4be976a28860","_cell_guid":"e61dc986-2673-4eba-bbe6-f3b7238f9bbf","trusted":true}},{"cell_type":"code","source":"# cv\nrmses = []\n\nfor target in targets:\n    models = model_dict[target]\n\n    preds = []\n    trues = []\n    \n    for fold, model in enumerate(models):\n        X_eval_cv = train[train[\"fold\"] == fold].drop(columns=drop_columns)\n        y_eval_cv = train[train[\"fold\"] == fold][target]\n\n        pred = model.predict(X_eval_cv)\n\n        trues.extend(y_eval_cv)\n        preds.extend(pred)\n        \n    rmse = np.sqrt(mean_squared_error(trues, preds))\n    print(f\"{target}_rmse : {rmse}\")\n    rmses = rmses + [rmse]\n\nprint(f\"mcrmse : {sum(rmses) / len(rmses)}\")","metadata":{"_uuid":"facdb70b-3981-4ffd-81c1-a5eadbe2b3de","_cell_guid":"f6cb1121-0289-41db-a2a5-a66aad2f57e0","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:31:20.582974Z","iopub.execute_input":"2023-10-11T05:31:20.583189Z","iopub.status.idle":"2023-10-11T05:31:20.659872Z","shell.execute_reply.started":"2023-10-11T05:31:20.58316Z","shell.execute_reply":"2023-10-11T05:31:20.659009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predict","metadata":{"_uuid":"e8573bf8-ab18-416c-9fe6-ffd771250ca4","_cell_guid":"d6800c26-569f-400d-b8e7-6a23191df30b","trusted":true}},{"cell_type":"code","source":"drop_columns = [\n                #\"fold\", \n                \"student_id\", \"prompt_id\", \"text\", \"fixed_summary_text\",\n                \"prompt_question\", \"prompt_title\", \n                \"prompt_text\",\n                \"input\",\"title\", \"author\", \"description\", \"genre\", \"grade\"\n               ] + [\n                f\"content_pred_{i}\" for i in range(CFG.n_splits)\n                ] + [\n                f\"wording_pred_{i}\" for i in range(CFG.n_splits)\n                ]","metadata":{"_uuid":"6a6cb33a-2d56-4be6-8422-ab895b99b9d0","_cell_guid":"d579599a-c860-4bd8-883e-dfd2375cadc6","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:31:20.661199Z","iopub.execute_input":"2023-10-11T05:31:20.661925Z","iopub.status.idle":"2023-10-11T05:31:20.668327Z","shell.execute_reply.started":"2023-10-11T05:31:20.661892Z","shell.execute_reply":"2023-10-11T05:31:20.66741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_dict = {}\nfor target in targets:\n    models = model_dict[target]\n    preds = []\n\n    for fold, model in enumerate(models):\n        X_eval_cv = test.drop(columns=drop_columns)\n        \n        if str(type(model))==\"<class 'xgboost.sklearn.XGBRegressor'>\":\n            print(\"pred xgb. rename cols\")\n            X_eval_cv = X_eval_cv.rename({'content':'content_pred','wording':'wording_pred'},axis=1)\n        elif str(type(model))==\"<class 'catboost.core.CatBoostRegressor'>\":\n            print(\"pred cat. rename cols\")\n            X_eval_cv = X_eval_cv.rename({'content':'content_pred','wording':'wording_pred'},axis=1)\n        else:\n            print(\"pred lgb\")\n\n        pred = model.predict(X_eval_cv)\n        preds.append(pred)\n    \n    pred_dict[target] = preds","metadata":{"_uuid":"a7af70ee-a979-49cf-9474-96e9170fcb8c","_cell_guid":"843fd4a8-f7d8-484b-9a2a-7ef3302ca503","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:31:20.669976Z","iopub.execute_input":"2023-10-11T05:31:20.670432Z","iopub.status.idle":"2023-10-11T05:31:20.712364Z","shell.execute_reply.started":"2023-10-11T05:31:20.670387Z","shell.execute_reply":"2023-10-11T05:31:20.711383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for target in targets:\n    preds = pred_dict[target]\n    for i, pred in enumerate(preds):\n        test[f\"{target}_pred_{i}\"] = pred\n\n    test[target] = test[[f\"{target}_pred_{fold}\" for fold in range(CFG.n_splits)]].mean(axis=1)","metadata":{"_uuid":"977db337-c4df-47e0-99d9-8d0021e05d08","_cell_guid":"905039f7-a28d-4da8-85a7-94d72cd8c442","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:31:20.713931Z","iopub.execute_input":"2023-10-11T05:31:20.714469Z","iopub.status.idle":"2023-10-11T05:31:20.724749Z","shell.execute_reply.started":"2023-10-11T05:31:20.714435Z","shell.execute_reply":"2023-10-11T05:31:20.72382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create Submission file","metadata":{"_uuid":"24579e83-c8f8-4bf8-ab4f-8d5db3425c39","_cell_guid":"7eba779e-d6da-484c-bb4c-1499cf8b42a4","trusted":true}},{"cell_type":"code","source":"test_2 = test[[\"student_id\", \"content\", \"wording\"]]\ntest_2.head()","metadata":{"_uuid":"0eecd3ee-c510-41cc-8a82-f81e9a246ffc","_cell_guid":"7beb1e23-0f0d-483e-b5c0-5afe58aa6482","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:31:20.726179Z","iopub.execute_input":"2023-10-11T05:31:20.726889Z","iopub.status.idle":"2023-10-11T05:31:20.738573Z","shell.execute_reply.started":"2023-10-11T05:31:20.726856Z","shell.execute_reply":"2023-10-11T05:31:20.737671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3rd MODEL","metadata":{"_uuid":"d8921d2b-8349-48e7-ad50-bed87f55913a","_cell_guid":"02afbf2f-fccf-4fb6-876a-8fea214afedd","trusted":true}},{"cell_type":"code","source":"from typing import List\nimport numpy as np\nimport pandas as pd\nimport warnings\nimport logging\nimport os\nimport shutil\nimport json\nimport transformers\nfrom transformers import AutoModel, AutoTokenizer, AutoConfig, AutoModelForSequenceClassification\nfrom transformers import DataCollatorWithPadding\nfrom datasets import Dataset,load_dataset, load_from_disk\nfrom transformers import TrainingArguments, Trainer\nfrom datasets import load_metric, disable_progress_bar\nfrom sklearn.metrics import mean_squared_error\nimport torch\nfrom sklearn.model_selection import KFold, GroupKFold\nfrom tqdm import tqdm\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize.treebank import TreebankWordDetokenizer\nfrom collections import Counter\nimport spacy\nimport re\nfrom autocorrect import Speller\nfrom spellchecker import SpellChecker\nimport lightgbm as lgb\n\nwarnings.simplefilter(\"ignore\")\nlogging.disable(logging.ERROR)\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \ndisable_progress_bar()\ntqdm.pandas()","metadata":{"_uuid":"edafaca5-8dba-468f-af57-928149ada257","_cell_guid":"0304f33b-613c-4666-bfe4-143ce6118ac3","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:31:20.740246Z","iopub.execute_input":"2023-10-11T05:31:20.740772Z","iopub.status.idle":"2023-10-11T05:31:20.74954Z","shell.execute_reply.started":"2023-10-11T05:31:20.74074Z","shell.execute_reply":"2023-10-11T05:31:20.748617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed: int):\n    import random, os\n    import numpy as np\n    import torch\n    \n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    \nseed_everything(seed=42)","metadata":{"_uuid":"9073eedc-19f4-442f-ba52-9eee280d30fc","_cell_guid":"89a9c4dc-3b02-4d50-84f9-c0c786255e65","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:31:20.7511Z","iopub.execute_input":"2023-10-11T05:31:20.751325Z","iopub.status.idle":"2023-10-11T05:31:20.765012Z","shell.execute_reply.started":"2023-10-11T05:31:20.751296Z","shell.execute_reply":"2023-10-11T05:31:20.764164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    model_name = \"another-bert\"\n    learning_rate=1.5e-5\n    weight_decay=0.02\n    hidden_dropout_prob=0.007\n    attention_probs_dropout_prob=0.007\n    num_train_epochs=5\n    n_splits=4\n    batch_size=12\n    \n    random_seed=42\n    save_steps=20\n    max_length=512","metadata":{"_uuid":"a00e4a2b-73ba-4119-9ac5-707b77182b10","_cell_guid":"342ab2cc-92d3-4291-9891-e9ff009507dd","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:31:20.766202Z","iopub.execute_input":"2023-10-11T05:31:20.766944Z","iopub.status.idle":"2023-10-11T05:31:20.777032Z","shell.execute_reply.started":"2023-10-11T05:31:20.766912Z","shell.execute_reply":"2023-10-11T05:31:20.776153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MODEL_DIR = '/kaggle/input/commitlit-deberta-v3-large-misspellings'\nINIT_MODEL = f\"{MODEL_DIR}/content/{CFG.model_name}/fold_0\"","metadata":{"_uuid":"8b6d4208-327f-48c4-b590-d1e3fde11fe1","_cell_guid":"f5ffcef3-c1c3-4b44-8dc3-411bd12f0b35","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:31:20.778278Z","iopub.execute_input":"2023-10-11T05:31:20.778984Z","iopub.status.idle":"2023-10-11T05:31:20.789215Z","shell.execute_reply.started":"2023-10-11T05:31:20.778948Z","shell.execute_reply":"2023-10-11T05:31:20.788312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataload","metadata":{"_uuid":"a26a20ee-e5b5-457d-b302-e23f2cf76bd8","_cell_guid":"4a506a8c-5ed8-4e4b-9b67-971fd53a8df4","trusted":true}},{"cell_type":"code","source":"DATA_DIR = \"/kaggle/input/commonlit-evaluate-student-summaries/\"\n\nprompts_train = pd.read_csv(DATA_DIR + \"prompts_train.csv\")\nprompts_test = pd.read_csv(DATA_DIR + \"prompts_test.csv\")\nsummaries_train = pd.read_csv(DATA_DIR + \"summaries_train.csv\")\nsummaries_test = pd.read_csv(DATA_DIR + \"summaries_test.csv\")\nsample_submission = pd.read_csv(DATA_DIR + \"sample_submission.csv\")","metadata":{"_uuid":"c297340e-ca27-4a4e-b376-3068f936a24a","_cell_guid":"6b42e18a-5b7c-404b-904f-274cee546ad7","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:31:20.790348Z","iopub.execute_input":"2023-10-11T05:31:20.791186Z","iopub.status.idle":"2023-10-11T05:31:20.918309Z","shell.execute_reply.started":"2023-10-11T05:31:20.791155Z","shell.execute_reply":"2023-10-11T05:31:20.917477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocess\n\n[Using features]\n\n- Text Length\n- Length Ratio\n- Word Overlap\n- N-grams Co-occurrence\n  - count\n  - ratio\n- Quotes Overlap\n- Grammar Check\n  - spelling: pyspellchecker","metadata":{"_uuid":"b903f15d-d2e8-4858-a38c-a26c3f82c502","_cell_guid":"db92ba64-c5d5-4955-9626-61f774bb0dfb","trusted":true}},{"cell_type":"code","source":"class Preprocessor:\n    def __init__(self, \n                model_name: str,\n                ) -> None:\n        self.tokenizer = AutoTokenizer.from_pretrained(INIT_MODEL)\n        self.twd = TreebankWordDetokenizer()\n        self.STOP_WORDS = set(stopwords.words('english'))\n        \n        self.spacy_ner_model = spacy.load('en_core_web_sm',)\n        self.speller = Speller(lang='en')\n        self.spellchecker = SpellChecker() \n        \n    def word_overlap_count(self, row):\n        \"\"\" intersection(prompt_text, text) \"\"\"        \n        def check_is_stop_word(word):\n            return word in self.STOP_WORDS\n        \n        prompt_words = row['prompt_tokens']\n        summary_words = row['summary_tokens']\n        if self.STOP_WORDS:\n            prompt_words = list(filter(check_is_stop_word, prompt_words))\n            summary_words = list(filter(check_is_stop_word, summary_words))\n        return len(set(prompt_words).intersection(set(summary_words)))\n            \n    def ngrams(self, token, n):\n        # Use the zip function to help us generate n-grams\n        # Concatentate the tokens into ngrams and return\n        ngrams = zip(*[token[i:] for i in range(n)])\n        return [\" \".join(ngram) for ngram in ngrams]\n\n    def ngram_co_occurrence(self, row, n: int) -> int:\n        # Tokenize the original text and summary into words\n        original_tokens = row['prompt_tokens']\n        summary_tokens = row['summary_tokens']\n\n        # Generate n-grams for the original text and summary\n        original_ngrams = set(self.ngrams(original_tokens, n))\n        summary_ngrams = set(self.ngrams(summary_tokens, n))\n\n        # Calculate the number of common n-grams\n        common_ngrams = original_ngrams.intersection(summary_ngrams)\n        return len(common_ngrams)\n    \n    def ner_overlap_count(self, row, mode:str):\n        model = self.spacy_ner_model\n        def clean_ners(ner_list):\n            return set([(ner[0].lower(), ner[1]) for ner in ner_list])\n        prompt = model(row['prompt_text'])\n        summary = model(row['text'])\n\n        if \"spacy\" in str(model):\n            prompt_ner = set([(token.text, token.label_) for token in prompt.ents])\n            summary_ner = set([(token.text, token.label_) for token in summary.ents])\n        elif \"stanza\" in str(model):\n            prompt_ner = set([(token.text, token.type) for token in prompt.ents])\n            summary_ner = set([(token.text, token.type) for token in summary.ents])\n        else:\n            raise Exception(\"Model not supported\")\n\n        prompt_ner = clean_ners(prompt_ner)\n        summary_ner = clean_ners(summary_ner)\n\n        intersecting_ners = prompt_ner.intersection(summary_ner)\n        \n        ner_dict = dict(Counter([ner[1] for ner in intersecting_ners]))\n        \n        if mode == \"train\":\n            return ner_dict\n        elif mode == \"test\":\n            return {key: ner_dict.get(key) for key in self.ner_keys}\n\n    \n    def quotes_count(self, row):\n        summary = row['text']\n        text = row['prompt_text']\n        quotes_from_summary = re.findall(r'\"([^\"]*)\"', summary)\n        if len(quotes_from_summary)>0:\n            return [quote in text for quote in quotes_from_summary].count(True)\n        else:\n            return 0\n\n    def spelling(self, text):\n        \n        wordlist=text.split()\n        amount_miss = len(list(self.spellchecker.unknown(wordlist)))\n\n        return amount_miss\n    \n    def add_spelling_dictionary(self, tokens: List[str]) -> List[str]:\n        \"\"\"dictionary update for pyspell checker and autocorrect\"\"\"\n        self.spellchecker.word_frequency.load_words(tokens)\n        self.speller.nlp_data.update({token:1000 for token in tokens})\n    \n    def run(self, \n            prompts: pd.DataFrame,\n            summaries:pd.DataFrame,\n            mode:str\n        ) -> pd.DataFrame:\n        \n        # before merge preprocess\n        prompts[\"prompt_length\"] = prompts[\"prompt_text\"].apply(\n            lambda x: len(word_tokenize(x))\n        )\n        prompts[\"prompt_tokens\"] = prompts[\"prompt_text\"].apply(\n            lambda x: word_tokenize(x)\n        )\n\n        summaries[\"summary_length\"] = summaries[\"text\"].apply(\n            lambda x: len(word_tokenize(x))\n        )\n        summaries[\"summary_tokens\"] = summaries[\"text\"].apply(\n            lambda x: word_tokenize(x)\n        )\n        \n        # Add prompt tokens into spelling checker dictionary\n        prompts[\"prompt_tokens\"].apply(\n            lambda x: self.add_spelling_dictionary(x)\n        )\n        \n#         from IPython.core.debugger import Pdb; Pdb().set_trace()\n        # fix misspelling\n        summaries[\"fixed_summary_text\"] = summaries[\"text\"].progress_apply(\n            lambda x: self.speller(x)\n        )\n        \n        # count misspelling\n        summaries[\"splling_err_num\"] = summaries[\"text\"].progress_apply(self.spelling)\n        \n        # merge prompts and summaries\n        input_df = summaries.merge(prompts, how=\"left\", on=\"prompt_id\")\n\n        # after merge preprocess\n        input_df['length_ratio'] = input_df['summary_length'] / input_df['prompt_length']\n\n        input_df['word_overlap_count'] = input_df.progress_apply(self.word_overlap_count, axis=1)\n        input_df['bigram_overlap_count'] = input_df.progress_apply(\n            self.ngram_co_occurrence,args=(2,), axis=1 \n        )\n        input_df['bigram_overlap_ratio'] = input_df['bigram_overlap_count'] / (input_df['summary_length'] - 1)\n        \n        input_df['trigram_overlap_count'] = input_df.progress_apply(\n            self.ngram_co_occurrence, args=(3,), axis=1\n        )\n        input_df['trigram_overlap_ratio'] = input_df['trigram_overlap_count'] / (input_df['summary_length'] - 2)\n        \n        input_df['quotes_count'] = input_df.progress_apply(self.quotes_count, axis=1)\n        \n        return input_df.drop(columns=[\"summary_tokens\", \"prompt_tokens\"])\n    \npreprocessor = Preprocessor(model_name=CFG.model_name)","metadata":{"_uuid":"d038ea64-dfd1-467a-8c45-bd94a7473664","_cell_guid":"3876f553-a5c0-4d51-b088-a9ba8a1d9e09","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:31:20.919691Z","iopub.execute_input":"2023-10-11T05:31:20.919926Z","iopub.status.idle":"2023-10-11T05:31:22.03738Z","shell.execute_reply.started":"2023-10-11T05:31:20.919899Z","shell.execute_reply":"2023-10-11T05:31:22.036479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summaries_train.head()","metadata":{"_uuid":"78c02744-fac1-41bb-840c-64e921d94217","_cell_guid":"db736e1a-321d-433f-9897-cfcb65587288","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:31:22.039385Z","iopub.execute_input":"2023-10-11T05:31:22.039884Z","iopub.status.idle":"2023-10-11T05:31:22.051175Z","shell.execute_reply.started":"2023-10-11T05:31:22.039851Z","shell.execute_reply":"2023-10-11T05:31:22.050109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\nwith open(MODEL_DIR+'/pickled.pkl', 'rb') as f:\n    train = pickle.load(f)\n    test = pickle.load(f)\n\n# train = preprocessor.run(prompts_train, summaries_train, mode=\"train\")\n#test = preprocessor.run(prompts_test, summaries_test, mode=\"test\")\n\n# train.head()","metadata":{"_uuid":"28755208-9aed-4af1-900b-dbc61e7af831","_cell_guid":"292426d2-dc14-415e-a0c3-77b87c009643","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:31:22.052529Z","iopub.execute_input":"2023-10-11T05:31:22.053273Z","iopub.status.idle":"2023-10-11T05:31:22.179313Z","shell.execute_reply.started":"2023-10-11T05:31:22.053242Z","shell.execute_reply":"2023-10-11T05:31:22.178478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train_keep.copy()\ntest = test_keep.copy()","metadata":{"_uuid":"04eea647-86e4-40b6-a88c-935f069cddfe","_cell_guid":"3e07da20-5211-4194-a588-753764f36670","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:31:22.180867Z","iopub.execute_input":"2023-10-11T05:31:22.181285Z","iopub.status.idle":"2023-10-11T05:31:22.18857Z","shell.execute_reply.started":"2023-10-11T05:31:22.181254Z","shell.execute_reply":"2023-10-11T05:31:22.187741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train = train[:256]","metadata":{"_uuid":"9a3a2f93-ba25-4c78-94fd-c9135bb37271","_cell_guid":"05f50bcd-f063-49e7-899e-013869fe9487","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:31:22.189994Z","iopub.execute_input":"2023-10-11T05:31:22.190278Z","iopub.status.idle":"2023-10-11T05:31:22.197733Z","shell.execute_reply.started":"2023-10-11T05:31:22.190244Z","shell.execute_reply":"2023-10-11T05:31:22.19692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gkf = GroupKFold(n_splits=CFG.n_splits)\n\nfor i, (_, val_index) in enumerate(gkf.split(train, groups=train[\"prompt_id\"])):\n    train.loc[val_index, \"fold\"] = i\n\ntrain.head()","metadata":{"_uuid":"aeecac04-b980-40a9-9866-8a53de9b1ce0","_cell_guid":"2639a1bd-548f-482d-8245-40f08ae5d88d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:31:22.199041Z","iopub.execute_input":"2023-10-11T05:31:22.199519Z","iopub.status.idle":"2023-10-11T05:31:22.236476Z","shell.execute_reply.started":"2023-10-11T05:31:22.199466Z","shell.execute_reply":"2023-10-11T05:31:22.235668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Function Definition","metadata":{"_uuid":"37a7c9bf-1824-41ed-99f0-c302ce5f0ae5","_cell_guid":"c32b3469-d20e-4cb0-af7c-4e625a96de89","trusted":true}},{"cell_type":"code","source":"import shutil\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    rmse = mean_squared_error(labels, predictions, squared=False)\n    return {\"rmse\": rmse}\n\ndef compute_mcrmse(eval_pred):\n    \"\"\"\n    Calculates mean columnwise root mean squared error\n    https://www.kaggle.com/competitions/commonlit-evaluate-student-summaries/overview/evaluation\n    \"\"\"\n    preds, labels = eval_pred\n\n    col_rmse = np.sqrt(np.mean((preds - labels) ** 2, axis=0))\n    mcrmse = np.mean(col_rmse)\n\n    return {\n        \"content_rmse\": col_rmse[0],\n        \"wording_rmse\": col_rmse[1],\n        \"mcrmse\": mcrmse,\n    }\n\ndef compt_score(content_true, content_pred, wording_true, wording_pred):\n    content_score = mean_squared_error(content_true, content_pred)**(1/2)\n    wording_score = mean_squared_error(wording_true, wording_pred)**(1/2)\n    \n    return (content_score + wording_score)/2","metadata":{"_uuid":"781ba0b1-94ad-475a-9ad3-b0ce3533e7d7","_cell_guid":"b1eda540-0a78-4d91-9928-6d205a33c9e1","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:31:22.237649Z","iopub.execute_input":"2023-10-11T05:31:22.238067Z","iopub.status.idle":"2023-10-11T05:31:22.24533Z","shell.execute_reply.started":"2023-10-11T05:31:22.238037Z","shell.execute_reply":"2023-10-11T05:31:22.244475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Deberta Regressor","metadata":{"_uuid":"9537549c-2f16-41e6-b380-844779b53140","_cell_guid":"9537a793-7230-4ee3-9d57-b6abffb6d0e3","trusted":true}},{"cell_type":"code","source":"class ContentScoreRegressor:\n    def __init__(self, \n                model_name: str,\n                model_dir: str,\n                target: str,\n                hidden_dropout_prob: float,\n                attention_probs_dropout_prob: float,\n                max_length: int,\n                ):\n        self.inputs = [\"prompt_text\", \"prompt_title\", \"prompt_question\", \"fixed_summary_text\"]\n        self.input_col = \"input\"\n        \n        self.text_cols = [self.input_col] \n        self.target = target\n        self.target_cols = [target]\n\n        self.model_name = model_name\n        self.model_dir = model_dir\n        self.max_length = max_length\n        \n        self.tokenizer = AutoTokenizer.from_pretrained(INIT_MODEL)\n        self.model_config = AutoConfig.from_pretrained(INIT_MODEL)\n        \n        self.model_config.update({\n            \"hidden_dropout_prob\": hidden_dropout_prob,\n            \"attention_probs_dropout_prob\": attention_probs_dropout_prob,\n            \"num_labels\": 1,\n            \"problem_type\": \"regression\",\n        })\n        \n        seed_everything(seed=42)\n\n        self.data_collator = DataCollatorWithPadding(\n            tokenizer=self.tokenizer\n        )\n\n\n    def tokenize_function(self, examples: pd.DataFrame):\n        labels = [examples[self.target]]\n        tokenized = self.tokenizer(examples[self.input_col],\n                         padding=False,\n                         truncation=True,\n                         max_length=self.max_length)\n        return {\n            **tokenized,\n            \"labels\": labels,\n        }\n    \n    def tokenize_function_test(self, examples: pd.DataFrame):\n        tokenized = self.tokenizer(examples[self.input_col],\n                         padding=False,\n                         truncation=True,\n                         max_length=self.max_length)\n        return tokenized\n        \n    def train(self, \n            fold: int,\n            train_df: pd.DataFrame,\n            valid_df: pd.DataFrame,\n            batch_size: int,\n            learning_rate: float,\n            weight_decay: float,\n            num_train_epochs: float,\n            save_steps: int,\n        ) -> None:\n        \"\"\"fine-tuning\"\"\"\n        \n        sep = self.tokenizer.sep_token\n        train_df[self.input_col] = (\n                    train_df[\"prompt_title\"] + sep\n                    + train_df[\"prompt_question\"] + sep\n                    + train_df['splling_err_num'].astype(str) + \" misspellings\" + sep\n                    + train_df[\"fixed_summary_text\"]\n                  )\n\n        valid_df[self.input_col] = (\n                    valid_df[\"prompt_title\"] + sep\n                    + valid_df[\"prompt_question\"] + sep\n                    + valid_df['splling_err_num'].astype(str) + \" misspellings\" + sep\n                    + valid_df[\"fixed_summary_text\"]\n                  )\n        \n        train_df = train_df[[self.input_col] + self.target_cols]\n        valid_df = valid_df[[self.input_col] + self.target_cols]\n        \n        model_content = AutoModelForSequenceClassification.from_pretrained(\n            f\"/kaggle/input/{self.model_name}\", \n            config=self.model_config,\n            ignore_mismatched_sizes=True\n        )\n\n        train_dataset = Dataset.from_pandas(train_df, preserve_index=False) \n        val_dataset = Dataset.from_pandas(valid_df, preserve_index=False) \n    \n        train_tokenized_datasets = train_dataset.map(self.tokenize_function, batched=False)\n        val_tokenized_datasets = val_dataset.map(self.tokenize_function, batched=False)\n\n        # eg. \"bert/fold_0/\"\n        model_fold_dir = os.path.join(self.model_dir, str(fold)) \n        \n        training_args = TrainingArguments(\n            output_dir=model_fold_dir,\n            load_best_model_at_end=True, # select best model\n            learning_rate=learning_rate,\n            per_device_train_batch_size=batch_size,\n            per_device_eval_batch_size=8,\n            num_train_epochs=num_train_epochs,\n            weight_decay=weight_decay,\n            report_to='none',\n            greater_is_better=False,\n            save_strategy=\"steps\",\n            evaluation_strategy=\"steps\",\n            eval_steps=save_steps,\n            save_steps=save_steps,\n            metric_for_best_model=\"rmse\",\n            save_total_limit=1\n        )\n\n        trainer = Trainer(\n            model=model_content,\n            args=training_args,\n            train_dataset=train_tokenized_datasets,\n            eval_dataset=val_tokenized_datasets,\n            tokenizer=self.tokenizer,\n            compute_metrics=compute_metrics,\n            data_collator=self.data_collator\n        )\n\n        trainer.train()\n        \n        shutil.rmtree(self.model_dir)\n        \n        model_content.save_pretrained(self.model_dir)\n        self.tokenizer.save_pretrained(self.model_dir)\n\n        \n    def predict(self, \n                test_df: pd.DataFrame,\n                fold: int,\n               ):\n        \"\"\"predict content score\"\"\"\n        \n        sep = self.tokenizer.sep_token\n        in_text = (\n                    test_df[\"prompt_title\"] + sep\n                    + test_df[\"prompt_question\"] + sep\n                    + test_df['splling_err_num'].astype(str) + \" misspellings\" + sep\n                    + test_df[\"fixed_summary_text\"]\n                  )\n        test_df[self.input_col] = in_text\n\n        test_ = test_df[[self.input_col]]\n    \n        test_dataset = Dataset.from_pandas(test_, preserve_index=False) \n        test_tokenized_dataset = test_dataset.map(self.tokenize_function_test, batched=False)\n\n        model_content = AutoModelForSequenceClassification.from_pretrained(f\"{self.model_dir}\")\n        model_content.eval()\n        \n        # e.g. \"bert/fold_0/\"\n        model_fold_dir = os.path.join(self.model_dir, str(fold)) \n        model_fold_dir = \"valid_log\" #f\"bert-{fold}\"\n#         print(\"model_fold_dir\", model_fold_dir)\n        \n        test_args = TrainingArguments(\n            output_dir=model_fold_dir,\n            do_train = False,\n            do_predict = True,\n            per_device_eval_batch_size = 4,   \n            dataloader_drop_last = False,\n        )\n\n        # init trainer\n        infer_content = Trainer(\n                      model = model_content, \n                      tokenizer=self.tokenizer,\n                      data_collator=self.data_collator,\n                      args = test_args)\n\n        preds = infer_content.predict(test_tokenized_dataset)[0]\n\n        return preds","metadata":{"_uuid":"f71a39b9-0788-488d-b214-a97af4f6fb75","_cell_guid":"f41da48f-b961-4602-b963-5e79be3efa8b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:31:22.246769Z","iopub.execute_input":"2023-10-11T05:31:22.246988Z","iopub.status.idle":"2023-10-11T05:31:22.269866Z","shell.execute_reply.started":"2023-10-11T05:31:22.24696Z","shell.execute_reply":"2023-10-11T05:31:22.26888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_by_fold(\n        train_df: pd.DataFrame,\n        model_name: str,\n        target:str,\n        save_each_model: bool,\n        n_splits: int,\n        batch_size: int,\n        learning_rate: int,\n        hidden_dropout_prob: float,\n        attention_probs_dropout_prob: float,\n        weight_decay: float,\n        num_train_epochs: int,\n        save_steps: int,\n        max_length:int\n    ):\n\n    # delete old model files\n    if os.path.exists(model_name):\n        shutil.rmtree(model_name)\n    \n    os.mkdir(model_name)\n        \n    for fold in range(CFG.n_splits):\n        print(f\"fold {fold}:\")\n        \n        train_data = train_df[train_df[\"fold\"] != fold]\n        valid_data = train_df[train_df[\"fold\"] == fold]\n        \n        if save_each_model == True:\n            model_dir =  f\"{target}/{model_name}/fold_{fold}\"\n        else: \n            model_dir =  f\"{model_name}/fold_{fold}\"\n\n        csr = ContentScoreRegressor(\n            model_name=model_name,\n            target=target,\n            model_dir = model_dir, \n            hidden_dropout_prob=hidden_dropout_prob,\n            attention_probs_dropout_prob=attention_probs_dropout_prob,\n            max_length=max_length,\n           )\n        \n        csr.train(\n            fold=fold,\n            train_df=train_data,\n            valid_df=valid_data, \n            batch_size=batch_size,\n            learning_rate=learning_rate,\n            weight_decay=weight_decay,\n            num_train_epochs=num_train_epochs,\n            save_steps=save_steps,\n        )\n\ndef validate(\n    train_df: pd.DataFrame,\n    target:str,\n    save_each_model: bool,\n    model_name: str,\n    hidden_dropout_prob: float,\n    attention_probs_dropout_prob: float,\n    max_length : int\n    ) -> pd.DataFrame:\n    \"\"\"predict oof data\"\"\"\n    for fold in range(CFG.n_splits):\n        print(f\"fold {fold}:\")\n        \n        valid_data = train_df[train_df[\"fold\"] == fold]\n        \n        if save_each_model == True:\n            model_dir =  f\"{MODEL_DIR}/{target}/{model_name}/fold_{fold}\"\n        else: \n            model_dir =  f\"{MODEL_DIR}/{model_name}/fold_{fold}\"\n        \n#         print(model_dir, model_name)\n        csr = ContentScoreRegressor(\n            model_name=model_name,\n            target=target,\n            model_dir = model_dir,\n            hidden_dropout_prob=hidden_dropout_prob,\n            attention_probs_dropout_prob=attention_probs_dropout_prob,\n            max_length=max_length,\n           )\n        \n        pred = csr.predict(\n            test_df=valid_data, \n            fold=fold\n        )\n        \n        train_df.loc[valid_data.index, f\"{target}_pred\"] = pred\n\n    return train_df\n    \ndef predict(\n    test_df: pd.DataFrame,\n    target:str,\n    save_each_model: bool,\n    model_name: str,\n    hidden_dropout_prob: float,\n    attention_probs_dropout_prob: float,\n    max_length : int\n    ):\n    \"\"\"predict using mean folds\"\"\"\n\n    for fold in range(CFG.n_splits):\n        print(f\"fold {fold}:\")\n        \n        if save_each_model == True:\n            model_dir =  f\"{MODEL_DIR}/{target}/{model_name}/fold_{fold}\"\n        else: \n            model_dir =  f\"{MODEL_DIR}/{model_name}/fold_{fold}\"\n\n        csr = ContentScoreRegressor(\n            model_name=model_name,\n            target=target,\n            model_dir = model_dir, \n            hidden_dropout_prob=hidden_dropout_prob,\n            attention_probs_dropout_prob=attention_probs_dropout_prob,\n            max_length=max_length,\n           )\n        \n        pred = csr.predict(\n            test_df=test_df, \n            fold=fold\n        )\n        \n        test_df[f\"{target}_pred_{fold}\"] = pred\n    \n    test_df[f\"{target}\"] = test_df[[f\"{target}_pred_{fold}\" for fold in range(CFG.n_splits)]].mean(axis=1)\n\n    return test_df","metadata":{"_uuid":"69550d83-d2a6-45b6-97e9-358348bc1d68","_cell_guid":"5318c0fc-282b-4144-8d56-5c642d495faa","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:31:22.271475Z","iopub.execute_input":"2023-10-11T05:31:22.271915Z","iopub.status.idle":"2023-10-11T05:31:22.290565Z","shell.execute_reply.started":"2023-10-11T05:31:22.271883Z","shell.execute_reply":"2023-10-11T05:31:22.289568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for target in [\"content\", \"wording\"]:\n#     train_by_fold(\n#         train,\n#         model_name=CFG.model_name,\n#         save_each_model=True,\n#         target=target,\n#         learning_rate=CFG.learning_rate,\n#         hidden_dropout_prob=CFG.hidden_dropout_prob,\n#         attention_probs_dropout_prob=CFG.attention_probs_dropout_prob,\n#         weight_decay=CFG.weight_decay,\n#         num_train_epochs=CFG.num_train_epochs,\n#         n_splits=CFG.n_splits,\n#         batch_size=CFG.batch_size,\n#         save_steps=CFG.save_steps,\n#         max_length=CFG.max_length\n#     )\n    \n    print(\"[validate]\")\n    train = validate(\n        train,\n        target=target,\n        save_each_model=True,\n        model_name=CFG.model_name,\n        hidden_dropout_prob=CFG.hidden_dropout_prob,\n        attention_probs_dropout_prob=CFG.attention_probs_dropout_prob,\n        max_length=CFG.max_length\n    )\n\n    rmse = mean_squared_error(train[target], train[f\"{target}_pred\"], squared=False)\n    print(f\"cv {target} rmse: {rmse}\")\n\n    print(\"[test]\")\n    test = predict(\n        test,\n        target=target,\n        save_each_model=True,\n        model_name=CFG.model_name,\n        hidden_dropout_prob=CFG.hidden_dropout_prob,\n        attention_probs_dropout_prob=CFG.attention_probs_dropout_prob,\n        max_length=CFG.max_length\n    )","metadata":{"_uuid":"a564a80a-cf3d-4c0c-982d-57122574180b","_cell_guid":"dd04f8d1-0a3c-4b25-a546-47d9830a1a5b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:31:22.292021Z","iopub.execute_input":"2023-10-11T05:31:22.292513Z","iopub.status.idle":"2023-10-11T05:43:42.364389Z","shell.execute_reply.started":"2023-10-11T05:31:22.292476Z","shell.execute_reply":"2023-10-11T05:43:42.36348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !rm -r wording content","metadata":{"_uuid":"88aea61d-a371-4c6b-9744-bc0138576449","_cell_guid":"f26f149c-0fd1-4b3d-a92b-3b15f5b477bd","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:43:42.366999Z","iopub.execute_input":"2023-10-11T05:43:42.367834Z","iopub.status.idle":"2023-10-11T05:43:42.371858Z","shell.execute_reply.started":"2023-10-11T05:43:42.367799Z","shell.execute_reply":"2023-10-11T05:43:42.370972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"_uuid":"43a6fd37-9e21-44ca-9e13-206a54c0e063","_cell_guid":"4242cce7-b918-47b4-b578-cc655a5afad3","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:43:42.373099Z","iopub.execute_input":"2023-10-11T05:43:42.373967Z","iopub.status.idle":"2023-10-11T05:43:42.408389Z","shell.execute_reply.started":"2023-10-11T05:43:42.373822Z","shell.execute_reply":"2023-10-11T05:43:42.407514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LGBM model","metadata":{"_uuid":"a1839f47-6b40-4980-a68c-1d94178b18e5","_cell_guid":"a6c4e912-04f9-4f41-adc6-c753cfcfbfa2","trusted":true}},{"cell_type":"code","source":"targets = [\"content\", \"wording\"]\n\ndrop_columns = [\"fold\", \"student_id\", \"prompt_id\", \"text\", \"fixed_summary_text\",\n                \"prompt_question\", \"prompt_title\", \n                \"prompt_text\",\"title\", \"author\", \"description\", \"genre\", \"grade\"\n               ] + targets","metadata":{"_uuid":"bf2ef3b3-e915-44b4-9371-d4ead28f69e0","_cell_guid":"a8f4c4a1-1c88-4aa7-8631-381b61b03b45","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:43:42.41Z","iopub.execute_input":"2023-10-11T05:43:42.410907Z","iopub.status.idle":"2023-10-11T05:43:42.421352Z","shell.execute_reply.started":"2023-10-11T05:43:42.410729Z","shell.execute_reply":"2023-10-11T05:43:42.420428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[train[\"fold\"] != 0].drop(columns=drop_columns)","metadata":{"_uuid":"54aed3ad-9db9-4bfe-bf58-5ddfcc79d491","_cell_guid":"ce5dbb08-5857-4649-a691-8459cbc9d479","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:43:42.422809Z","iopub.execute_input":"2023-10-11T05:43:42.423032Z","iopub.status.idle":"2023-10-11T05:43:42.459108Z","shell.execute_reply.started":"2023-10-11T05:43:42.423003Z","shell.execute_reply":"2023-10-11T05:43:42.458222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model_dict = {}\n\n# for target in targets:\n#     models = []\n    \n#     for fold in range(CFG.n_splits):\n\n#         X_train_cv = train[train[\"fold\"] != fold].drop(columns=drop_columns)\n#         y_train_cv = train[train[\"fold\"] != fold][target]\n\n#         X_eval_cv = train[train[\"fold\"] == fold].drop(columns=drop_columns)\n#         y_eval_cv = train[train[\"fold\"] == fold][target]\n\n#         dtrain = lgb.Dataset(X_train_cv, label=y_train_cv)\n#         dval = lgb.Dataset(X_eval_cv, label=y_eval_cv)\n\n#         params = {\n#             'boosting_type': 'gbdt',\n#             'random_state': 42,\n#             'objective': 'regression',\n#             'metric': 'rmse',\n#             'learning_rate': 0.048,\n#             'max_depth': 4,\n#             'lambda_l1': 0.001,\n#             'lambda_l2': 0.011\n#         }\n\n#         evaluation_results = {}\n#         model = lgb.train(params,\n#                           num_boost_round=10000,\n#                             #categorical_feature = categorical_features,\n#                           valid_names=['train', 'valid'],\n#                           train_set=dtrain,\n#                           valid_sets=dval,\n#                           callbacks=[\n#                               lgb.early_stopping(stopping_rounds=30, verbose=True),\n#                               lgb.log_evaluation(100),\n#                               lgb.callback.record_evaluation(evaluation_results)\n#                             ],\n#                           )\n#         models.append(model)\n    \n#     model_dict[target] = models","metadata":{"_uuid":"9376addd-a8a7-4cd0-8585-dd86933bd477","_cell_guid":"a3da94c9-6220-4a9a-8ff9-4557aaa9840e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:43:42.460603Z","iopub.execute_input":"2023-10-11T05:43:42.461078Z","iopub.status.idle":"2023-10-11T05:43:42.46665Z","shell.execute_reply.started":"2023-10-11T05:43:42.461046Z","shell.execute_reply":"2023-10-11T05:43:42.465847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from catboost import CatBoostRegressor\n\n\nmodel_dict = {}\n\nfor target in targets:\n    models = []\n    \n    for fold in range(CFG.n_splits):\n\n        X_train_cv = train[train[\"fold\"] != fold].drop(columns=drop_columns)\n        y_train_cv = train[train[\"fold\"] != fold][target]\n\n        X_eval_cv = train[train[\"fold\"] == fold].drop(columns=drop_columns)\n        y_eval_cv = train[train[\"fold\"] == fold][target]\n\n        model = CatBoostRegressor(\n            learning_rate = 0.048,\n            depth = 4,\n            min_data_in_leaf = 34,\n            iterations = 10000,\n            early_stopping_rounds = 300,\n            task_type ='CPU',\n            loss_function ='RMSE'\n          )\n        model.fit(X_train_cv, \n                  y_train_cv, \n                  eval_set=[(X_eval_cv, y_eval_cv)],\n                  verbose=False)\n        models.append(model)\n\n    model_dict[target] = models","metadata":{"_uuid":"22fb0d8d-af28-4264-9cdb-1cdcc12827aa","_cell_guid":"5f76aa1b-8488-44a2-a7fe-7cbc3a89f771","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:43:42.468074Z","iopub.execute_input":"2023-10-11T05:43:42.468292Z","iopub.status.idle":"2023-10-11T05:44:02.284137Z","shell.execute_reply.started":"2023-10-11T05:43:42.468264Z","shell.execute_reply":"2023-10-11T05:44:02.283219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CV Score","metadata":{"_uuid":"d32014d1-294b-4a57-9103-8210d9f1b20b","_cell_guid":"5989b8b8-30fa-4646-b38a-85dbb1f7eb55","trusted":true}},{"cell_type":"code","source":"# cv\nrmses = []\n\nfor target in targets:\n    models = model_dict[target]\n\n    preds = []\n    trues = []\n    \n    for fold, model in enumerate(models):\n        X_eval_cv = train[train[\"fold\"] == fold].drop(columns=drop_columns)\n        y_eval_cv = train[train[\"fold\"] == fold][target]\n\n        pred = model.predict(X_eval_cv)\n\n        trues.extend(y_eval_cv)\n        preds.extend(pred)\n        \n    rmse = np.sqrt(mean_squared_error(trues, preds))\n    print(f\"{target}_rmse : {rmse}\")\n    rmses = rmses + [rmse]\n\nprint(f\"mcrmse : {sum(rmses) / len(rmses)}\")","metadata":{"_uuid":"59c8ec9c-085b-4a35-afcc-e3ad8ed5b212","_cell_guid":"39ff183f-d19f-4a4f-9f7d-c7b89b7ac602","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:44:02.285865Z","iopub.execute_input":"2023-10-11T05:44:02.286313Z","iopub.status.idle":"2023-10-11T05:44:02.803357Z","shell.execute_reply.started":"2023-10-11T05:44:02.28628Z","shell.execute_reply":"2023-10-11T05:44:02.802463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predict","metadata":{"_uuid":"19687e0a-9aca-4922-9bf6-f9f5044ce46f","_cell_guid":"eccced52-9826-489e-982a-addc60406544","trusted":true}},{"cell_type":"code","source":"drop_columns = [\n                #\"fold\", \n                \"student_id\", \"prompt_id\", \"text\", \"fixed_summary_text\",\n                \"prompt_question\", \"prompt_title\", \n                \"prompt_text\",\n                \"input\",\"title\", \"author\", \"description\", \"genre\", \"grade\"\n               ] + [\n                f\"content_pred_{i}\" for i in range(CFG.n_splits)\n                ] + [\n                f\"wording_pred_{i}\" for i in range(CFG.n_splits)\n                ]","metadata":{"_uuid":"2f333827-8e66-45b7-be1e-2855f8670c69","_cell_guid":"a6536af6-dfc6-49d5-9041-a23d00487d85","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:44:02.804672Z","iopub.execute_input":"2023-10-11T05:44:02.804923Z","iopub.status.idle":"2023-10-11T05:44:02.811369Z","shell.execute_reply.started":"2023-10-11T05:44:02.804892Z","shell.execute_reply":"2023-10-11T05:44:02.810391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_dict = {}\nfor target in targets:\n    models = model_dict[target]\n    preds = []\n\n    for fold, model in enumerate(models):\n        X_eval_cv = test.drop(columns=drop_columns)\n        \n        if str(type(model))==\"<class 'xgboost.sklearn.XGBRegressor'>\":\n            print(\"pred xgb. rename cols\")\n            X_eval_cv = X_eval_cv.rename({'content':'content_pred','wording':'wording_pred'},axis=1)\n        elif str(type(model))==\"<class 'catboost.core.CatBoostRegressor'>\":\n            print(\"pred cat. rename cols\")\n            X_eval_cv = X_eval_cv.rename({'content':'content_pred','wording':'wording_pred'},axis=1)\n        else:\n            print(\"pred lgb\")\n\n        pred = model.predict(X_eval_cv)\n        preds.append(pred)\n    \n    pred_dict[target] = preds","metadata":{"_uuid":"c91adcd3-067f-4119-b6eb-21d490d0d5b1","_cell_guid":"46632ed8-a00e-483c-8478-0d723704a84e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:44:02.812843Z","iopub.execute_input":"2023-10-11T05:44:02.813313Z","iopub.status.idle":"2023-10-11T05:44:02.856216Z","shell.execute_reply.started":"2023-10-11T05:44:02.813281Z","shell.execute_reply":"2023-10-11T05:44:02.855241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for target in targets:\n    preds = pred_dict[target]\n    for i, pred in enumerate(preds):\n        test[f\"{target}_pred_{i}\"] = pred\n\n    test[target] = test[[f\"{target}_pred_{fold}\" for fold in range(CFG.n_splits)]].mean(axis=1)","metadata":{"_uuid":"5f109eaa-3319-4f38-a14b-14ebd6d29219","_cell_guid":"bf33aa50-946f-41c4-bc5a-2b3bcbc4574f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:44:02.857588Z","iopub.execute_input":"2023-10-11T05:44:02.858018Z","iopub.status.idle":"2023-10-11T05:44:02.868058Z","shell.execute_reply.started":"2023-10-11T05:44:02.857987Z","shell.execute_reply":"2023-10-11T05:44:02.867213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create Submission file","metadata":{"_uuid":"bd277720-9cac-4bc3-9c65-e6d51d405c14","_cell_guid":"bd2b8bbe-bd1e-4603-ace8-6788b299dcdc","trusted":true}},{"cell_type":"code","source":"test_3 = test[[\"student_id\", \"content\", \"wording\"]]\ntest_3.head()","metadata":{"_uuid":"692e046e-2a80-451d-90fa-030852069f4c","_cell_guid":"1745275d-1f15-4dc2-9941-13615f5a92a3","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:44:02.869631Z","iopub.execute_input":"2023-10-11T05:44:02.869852Z","iopub.status.idle":"2023-10-11T05:44:02.88174Z","shell.execute_reply.started":"2023-10-11T05:44:02.869824Z","shell.execute_reply":"2023-10-11T05:44:02.880642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ADD 1st pred","metadata":{"_uuid":"33eab2f8-1340-42cb-a8f9-27d352f92b56","_cell_guid":"73531b13-50b1-4a1e-bcb5-f03d50f6b520","trusted":true}},{"cell_type":"code","source":"sel = ['student_id','prompt_id','content_pred', 'wording_pred']\ntmp = train_1st[sel].rename(columns={'content_pred': 'content_pred_1st', \n                          'wording_pred': 'wording_pred_1st'})\ntmp.head()","metadata":{"_uuid":"73b3ea8e-138d-444b-a2a8-37b69321d031","_cell_guid":"a3e6b385-84c6-47fd-9ee7-4a5bcfa9f8b4","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:44:02.883252Z","iopub.execute_input":"2023-10-11T05:44:02.884044Z","iopub.status.idle":"2023-10-11T05:44:02.897099Z","shell.execute_reply.started":"2023-10-11T05:44:02.884011Z","shell.execute_reply":"2023-10-11T05:44:02.895979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train.merge(tmp, on=['student_id','prompt_id'], how=\"left\")\ntrain.head()","metadata":{"_uuid":"66ee4012-f10e-4b4e-8d21-c6297aa1c730","_cell_guid":"a453ef6a-6cec-4edf-b1ec-9481dfdc7035","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:44:02.898476Z","iopub.execute_input":"2023-10-11T05:44:02.89922Z","iopub.status.idle":"2023-10-11T05:44:02.934024Z","shell.execute_reply.started":"2023-10-11T05:44:02.899187Z","shell.execute_reply":"2023-10-11T05:44:02.933137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['content_1st'] = test_1st['content']\ntest['wording_1st'] = test_1st['wording']\ntest.head()","metadata":{"_uuid":"bef55c32-ff6e-4ffe-8189-a80e21e1baeb","_cell_guid":"b4c765c7-7e03-470a-acec-b22e2f59658b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:44:02.935184Z","iopub.execute_input":"2023-10-11T05:44:02.935861Z","iopub.status.idle":"2023-10-11T05:44:02.958612Z","shell.execute_reply.started":"2023-10-11T05:44:02.935829Z","shell.execute_reply":"2023-10-11T05:44:02.957744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Add 2nd pred","metadata":{"_uuid":"5dc1a479-0eb9-42a0-bbe7-c147d6ca3cf8","_cell_guid":"1185ff57-de6b-4b69-a0b0-dea4690641ab","trusted":true}},{"cell_type":"code","source":"sel = ['student_id','prompt_id','content_pred', 'wording_pred']\ntmp = train_2nd[sel].rename(columns={'content_pred': 'content_pred_2nd', \n                          'wording_pred': 'wording_pred_2nd'})\ntmp.head()","metadata":{"_uuid":"1d32cc10-2e52-4986-a8b0-fe7e69828fb3","_cell_guid":"c3ed6056-9107-476c-8d55-95f61ccd78a4","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:44:02.959793Z","iopub.execute_input":"2023-10-11T05:44:02.960536Z","iopub.status.idle":"2023-10-11T05:44:02.974285Z","shell.execute_reply.started":"2023-10-11T05:44:02.960502Z","shell.execute_reply":"2023-10-11T05:44:02.973324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train.merge(tmp, on=['student_id','prompt_id'], how=\"left\")\ntrain.head()","metadata":{"_uuid":"4099a90a-86f3-451c-94b5-a08dd741c892","_cell_guid":"ccefb0d9-cfca-43b1-aefb-079bc6f2fff4","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:44:02.97575Z","iopub.execute_input":"2023-10-11T05:44:02.976032Z","iopub.status.idle":"2023-10-11T05:44:03.009219Z","shell.execute_reply.started":"2023-10-11T05:44:02.976002Z","shell.execute_reply":"2023-10-11T05:44:03.008264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['content_2nd'] = test_2nd['content']\ntest['wording_2nd'] = test_2nd['wording']\ntest.head()","metadata":{"_uuid":"62588165-7430-4151-93de-750c24f1f567","_cell_guid":"d25fb857-a806-498c-9763-d01092620236","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:44:03.010893Z","iopub.execute_input":"2023-10-11T05:44:03.011392Z","iopub.status.idle":"2023-10-11T05:44:03.035461Z","shell.execute_reply.started":"2023-10-11T05:44:03.011356Z","shell.execute_reply":"2023-10-11T05:44:03.034394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Add Features","metadata":{"_uuid":"ad173258-711f-4fd3-94c4-77efb0648b1d","_cell_guid":"bda745bc-aa13-47bf-802f-76bf5df6d2e6","trusted":true}},{"cell_type":"code","source":"wd = pd.read_csv('/kaggle/input/worddifficulty/WordDifficulty.csv')\ndic = dict(zip(wd['Word'], wd['I_Zscore']))","metadata":{"_uuid":"058bc1f7-5b7f-44ec-92dc-acc38838c01c","_cell_guid":"eb54f84b-2bc4-4100-8eed-a816d60592f4","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:44:03.037065Z","iopub.execute_input":"2023-10-11T05:44:03.038459Z","iopub.status.idle":"2023-10-11T05:44:03.13244Z","shell.execute_reply.started":"2023-10-11T05:44:03.038407Z","shell.execute_reply":"2023-10-11T05:44:03.131482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def difficulty(data) :\n    words = word_tokenize(data['text'])\n#     s = ['``','\\'\\'','.',',']\n#     stop_words = set(stopwords.words('english') + s)\n#     filtered_words = [word for word in words if word.lower() not in stop_words]\n    filtered_words = words\n    score = 0\n    num = 0\n    sep = 0\n    for e in filtered_words:\n        if e in dic:\n            score += dic[e]\n            num+=1\n        elif e == '.' or e == ',' :\n            sep+=1\n        else:\n            pass\n#             print(e,\"**\")\n\n    nn = max(1, len(filtered_words))\n    sep = max(1, sep)\n    num = max(1, num)\n    return score/num, score/nn, nn/sep, (nn-num)/nn\n\n\n\n#         prompts[\"prompt_tokens\"] = prompts[\"prompt_text\"].apply(\n#             lambda x: word_tokenize(x)\n#         )\nlabels = ['difficulty0', 'difficulty1', 'ave_text_len', 'unknown_words']\ntrain[labels]=train.apply(lambda x:difficulty(x),axis=1, result_type='expand')\ntest[labels]=test.apply(lambda x:difficulty(x),axis=1, result_type='expand')","metadata":{"_uuid":"c8934a01-ce06-47e4-8457-6e008c013caa","_cell_guid":"8552f21c-f137-4c12-bafa-46351df65fa6","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:44:03.133921Z","iopub.execute_input":"2023-10-11T05:44:03.134676Z","iopub.status.idle":"2023-10-11T05:44:07.143923Z","shell.execute_reply.started":"2023-10-11T05:44:03.134639Z","shell.execute_reply":"2023-10-11T05:44:07.142886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from readability import Readability","metadata":{"_uuid":"241a827b-dfc1-4d59-8991-de1a57319df8","_cell_guid":"ab7e776b-e769-4c12-bee8-c7784c73da7b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:44:07.145194Z","iopub.execute_input":"2023-10-11T05:44:07.145445Z","iopub.status.idle":"2023-10-11T05:44:07.150173Z","shell.execute_reply.started":"2023-10-11T05:44:07.145398Z","shell.execute_reply":"2023-10-11T05:44:07.149353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"no_score = -10000\ndef rscore(data) :\n    txt = data['text']\n    words = word_tokenize(txt)\n    n = len(words) + 1\n    if n == 0 : \n        return no_score, no_score, no_score, no_score, no_score, no_score, no_score, no_score\n    tot = n\n    new = txt\n    while tot < 200 :\n        new += \" \" + txt\n        tot += n\n    r = Readability(new)\n    try :\n        ret = (r.flesch_kincaid().score, r.flesch().score, r.gunning_fog().score,\n               r.coleman_liau().score,r.dale_chall().score, r.ari().score,\n               r.linsear_write().score, r.spache().score)\n    except:\n        return no_score, no_score, no_score, no_score, no_score, no_score, no_score, no_score\n    return ret\n\nlabels = ['flesch_kincaid', 'flesch', 'gunning_fog', 'coleman_liau',\n         'dale_chall','ari','linsear_write','spache']\n\ntrain[labels]=train.progress_apply(lambda x:rscore(x),axis=1, result_type='expand')\ntest[labels]=test.progress_apply(lambda x:rscore(x),axis=1, result_type='expand')","metadata":{"_uuid":"f1044213-923e-4b57-a3d0-9bdd8cc25dd4","_cell_guid":"e0c848d0-f810-4898-a977-9c70f69f4c5b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:44:07.151752Z","iopub.execute_input":"2023-10-11T05:44:07.1522Z","iopub.status.idle":"2023-10-11T05:45:19.70035Z","shell.execute_reply.started":"2023-10-11T05:44:07.152167Z","shell.execute_reply":"2023-10-11T05:45:19.699488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport nltk\n\ndic = dict()\nfor i in range(len(prompts_train)) :\n    pid = prompts_train['prompt_id'][i]\n    txt = prompts_train['prompt_text'][i]\n    original_tokens = nltk.word_tokenize(txt)\n    original_text = ' '.join(original_tokens)\n    dic[pid] = original_text\nfor i in range(len(prompts_test)) :\n    pid = prompts_test['prompt_id'][i]\n    txt = prompts_test['prompt_text'][i]\n    original_tokens = nltk.word_tokenize(txt)\n    original_text = ' '.join(original_tokens)\n    dic[pid] = original_text\n    \ndic.keys()\n\ndef cosine_sim(data):\n    original_text = dic[data['prompt_id']]\n    summary = data['fixed_summary_text']\n#     original_tokens = nltk.word_tokenize(original_text)\n    summary_tokens = nltk.word_tokenize(summary)\n\n    # トークンを結合して文に戻す\n#     original_text = ' '.join(original_tokens)\n    summary = ' '.join(summary_tokens)\n\n    # CountVectorizerを使用して文をベクトル化\n    vectorizer = CountVectorizer().fit_transform([original_text, summary])\n\n    # コサイン類似度を計算\n    cosine_scores = cosine_similarity(vectorizer)\n\n    # 要約と元の文章の類似度を表示\n    similarity_score = cosine_scores[0][1]\n    \n    return similarity_score","metadata":{"_uuid":"dee7da70-9bc2-4b25-bbb5-9b58a9d40def","_cell_guid":"130e923c-5833-419b-9517-1b7fe4c49c17","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:45:19.701735Z","iopub.execute_input":"2023-10-11T05:45:19.70255Z","iopub.status.idle":"2023-10-11T05:45:19.729267Z","shell.execute_reply.started":"2023-10-11T05:45:19.702516Z","shell.execute_reply":"2023-10-11T05:45:19.728351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cosine_sim(train.iloc[0])\ntrain['cos_sim']=train.progress_apply(lambda x:cosine_sim(x),axis=1, result_type='expand')\ntest['cos_sim']=test.progress_apply(lambda x:cosine_sim(x),axis=1, result_type='expand')","metadata":{"_uuid":"f0f1eae8-81b2-4c08-9815-3b5bd2c7a218","_cell_guid":"0ad54f3e-e531-4e9b-b54b-09139b43a166","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:45:19.730643Z","iopub.execute_input":"2023-10-11T05:45:19.731576Z","iopub.status.idle":"2023-10-11T05:45:38.165407Z","shell.execute_reply.started":"2023-10-11T05:45:19.731543Z","shell.execute_reply":"2023-10-11T05:45:38.164482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import textstat\ndef txts(data):\n    text = data['text']\n    return (\n        textstat.flesch_reading_ease(text),\n        textstat.flesch_kincaid_grade(text),\n        textstat.gunning_fog(text),\n        textstat.smog_index(text),\n        textstat.automated_readability_index(text),\n        textstat.coleman_liau_index(text),\n        textstat.linsear_write_formula(text),\n        textstat.dale_chall_readability_score(text),\n        textstat.text_standard(text, float_output=True),\n        textstat.reading_time(text, ms_per_char=14.69),\n        textstat.syllable_count(text),\n        textstat.lexicon_count(text, removepunct=True),\n        textstat.sentence_count(text),\n        textstat.char_count(text, ignore_spaces=True),\n        textstat.letter_count(text, ignore_spaces=True),\n        textstat.monosyllabcount(text)\n    )\n\nsample = txts(train.iloc[1])\nprint(sample)\nlabels = [f'f{i}' for i in range(len(sample))]\ntrain[labels]=train.progress_apply(lambda x:txts(x),axis=1, result_type='expand')\ntest[labels]=test.progress_apply(lambda x:txts(x),axis=1, result_type='expand')","metadata":{"_uuid":"979abc25-d598-4668-8112-eeb6ceae7c73","_cell_guid":"52ce360c-0137-4f07-bbdd-2123d9819b45","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:45:38.166665Z","iopub.execute_input":"2023-10-11T05:45:38.167516Z","iopub.status.idle":"2023-10-11T05:45:45.674574Z","shell.execute_reply.started":"2023-10-11T05:45:38.167482Z","shell.execute_reply":"2023-10-11T05:45:45.673721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train","metadata":{"_uuid":"c50da3aa-fa34-4929-91bd-9d862a322a5d","_cell_guid":"d3dbb0a2-ad68-45dc-9147-c4f11d91f86e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:45:45.676011Z","iopub.execute_input":"2023-10-11T05:45:45.676247Z","iopub.status.idle":"2023-10-11T05:45:45.709969Z","shell.execute_reply.started":"2023-10-11T05:45:45.676216Z","shell.execute_reply":"2023-10-11T05:45:45.709125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.to_csv(\"train.csv\", index=False)","metadata":{"_uuid":"1f3015d0-82ea-4573-a9c1-08bc85fdebb6","_cell_guid":"fc13dbc7-d283-4e49-b4b4-5170f75b6511","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:45:45.711638Z","iopub.execute_input":"2023-10-11T05:45:45.71223Z","iopub.status.idle":"2023-10-11T05:45:47.211188Z","shell.execute_reply.started":"2023-10-11T05:45:45.712196Z","shell.execute_reply":"2023-10-11T05:45:47.210221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LGBM model","metadata":{"_uuid":"6fa9a0e8-c851-4c59-801b-f6217f419171","_cell_guid":"4afd34ff-1ca4-492b-bc3e-dcba4717fb9e","trusted":true}},{"cell_type":"code","source":"targets = [\"content\", \"wording\"]\n\ndrop_columns = [\"fold\", \"student_id\", \"prompt_id\", \"text\", \"fixed_summary_text\",\n                \"prompt_question\", \"prompt_title\", \n                \"prompt_text\",\"title\", \"author\", \"description\", \"genre\", \"grade\"\n               ] + targets","metadata":{"_uuid":"c4aa7945-d668-482b-b7bd-fb1c16cec83c","_cell_guid":"56ba6922-e1e4-42b7-937b-fd05334bb38e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:45:47.212638Z","iopub.execute_input":"2023-10-11T05:45:47.213539Z","iopub.status.idle":"2023-10-11T05:45:47.21866Z","shell.execute_reply.started":"2023-10-11T05:45:47.213504Z","shell.execute_reply":"2023-10-11T05:45:47.217726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[train[\"fold\"] != 0].drop(columns=drop_columns)","metadata":{"_uuid":"b20a1fae-09f6-4b1d-8c7a-a776ae594d58","_cell_guid":"c1e89e5b-3597-4b98-bac0-eaa378c0c3cd","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:45:47.219817Z","iopub.execute_input":"2023-10-11T05:45:47.220685Z","iopub.status.idle":"2023-10-11T05:45:47.257657Z","shell.execute_reply.started":"2023-10-11T05:45:47.220654Z","shell.execute_reply":"2023-10-11T05:45:47.256749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import optuna\n# def objective(trial):\n    \n#     model_dict = {}\n\n#     for target in targets:\n#         models = []\n\n#         for fold in range(CFG.n_splits):\n\n#             X_train_cv = train[train[\"fold\"] != fold].drop(columns=drop_columns)\n#             y_train_cv = train[train[\"fold\"] != fold][target]\n\n#             X_eval_cv = train[train[\"fold\"] == fold].drop(columns=drop_columns)\n#             y_eval_cv = train[train[\"fold\"] == fold][target]\n\n#             dtrain = lgb.Dataset(X_train_cv, label=y_train_cv)\n#             dval = lgb.Dataset(X_eval_cv, label=y_eval_cv)\n\n#             params = {\n#                 'boosting_type': 'gbdt',\n#                 'random_state': 42,\n#                 'objective': 'regression',\n#                 'metric': 'rmse',\n#                 'learning_rate': trial.suggest_uniform('learning_rate', 0.01, 0.2),\n#                 'max_depth': trial.suggest_int('max_depth', 3, 10),\n#                 'lambda_l1': trial.suggest_uniform('lambda_l1', 0, 0.01),\n#                 'lambda_l2': trial.suggest_uniform('lambda_l2', 0, 0.10),\n#                 'num_leaves': trial.suggest_int('num_leaves', 16, 64),\n#                 'verbosity': -1\n#             }\n\n#             evaluation_results = {}\n#             model = lgb.train(params,\n#                               num_boost_round=10000,\n#                                 #categorical_feature = categorical_features,\n#                               valid_names=['train', 'valid'],\n#                               train_set=dtrain,\n#                               valid_sets=dval,\n#                               callbacks=[\n#                                   lgb.early_stopping(stopping_rounds=30, verbose=False),\n# #                                   lgb.log_evaluation(100),\n# #                                   lgb.callback.record_evaluation(evaluation_results)\n#                                 ],\n#                               )\n#             models.append(model)\n\n#         model_dict[target] = models\n\n#         rmses = []\n\n#     for target in targets:\n#         models = model_dict[target]\n\n#         preds = []\n#         trues = []\n\n#         for fold, model in enumerate(models):\n#             X_eval_cv = train[train[\"fold\"] == fold].drop(columns=drop_columns)\n#             y_eval_cv = train[train[\"fold\"] == fold][target]\n\n#             pred = model.predict(X_eval_cv)\n\n#             trues.extend(y_eval_cv)\n#             preds.extend(pred)\n\n#         rmse = np.sqrt(mean_squared_error(trues, preds))\n# #         print(f\"{target}_rmse : {rmse}\")\n#         rmses = rmses + [rmse]\n\n#     print(f\"mcrmse : {sum(rmses) / len(rmses)}\")\n#     return sum(rmses) / len(rmses)\n        \n\n# study = optuna.create_study(direction='minimize')\n# study.optimize(objective, n_trials=30)\n\n# print('Number of finished trials:', len(study.trials))\n# print('Best trial:', study.best_trial.params)","metadata":{"_uuid":"b2acdc00-a9db-4ecf-a95f-fcdd84616673","_cell_guid":"9f7ffa22-9704-43d3-94f8-317d93fe235b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:45:47.259316Z","iopub.execute_input":"2023-10-11T05:45:47.259562Z","iopub.status.idle":"2023-10-11T05:45:47.26576Z","shell.execute_reply.started":"2023-10-11T05:45:47.259531Z","shell.execute_reply":"2023-10-11T05:45:47.264815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_dict = {}\n\nfor target in targets:\n    models = []\n    \n    for fold in range(CFG.n_splits):\n\n        X_train_cv = train[train[\"fold\"] != fold].drop(columns=drop_columns)\n        y_train_cv = train[train[\"fold\"] != fold][target]\n\n        X_eval_cv = train[train[\"fold\"] == fold].drop(columns=drop_columns)\n        y_eval_cv = train[train[\"fold\"] == fold][target]\n\n        dtrain = lgb.Dataset(X_train_cv, label=y_train_cv)\n        dval = lgb.Dataset(X_eval_cv, label=y_eval_cv)\n\n        params = {\n            'boosting_type': 'gbdt',\n            'random_state': 42,\n            'objective': 'regression',\n            'metric': 'rmse',\n            'learning_rate': 0.048,\n            'max_depth': 4,\n            'lambda_l1': 0.001,\n            'lambda_l2': 0.011\n        }\n\n        evaluation_results = {}\n        model = lgb.train(params,\n                          num_boost_round=10000,\n                            #categorical_feature = categorical_features,\n                          valid_names=['train', 'valid'],\n                          train_set=dtrain,\n                          valid_sets=dval,\n                          callbacks=[\n                              lgb.early_stopping(stopping_rounds=30, verbose=True),\n                              lgb.log_evaluation(100),\n                              lgb.callback.record_evaluation(evaluation_results)\n                            ],\n                          )\n        models.append(model)\n    \n    model_dict[target] = models","metadata":{"_uuid":"a91ffe69-2562-4bb6-8cc3-83e01cdbaca7","_cell_guid":"4ce82d09-695c-483c-a31a-82d10b415dae","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:45:47.267021Z","iopub.execute_input":"2023-10-11T05:45:47.267724Z","iopub.status.idle":"2023-10-11T05:45:52.146848Z","shell.execute_reply.started":"2023-10-11T05:45:47.26769Z","shell.execute_reply":"2023-10-11T05:45:52.145957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Catboost","metadata":{"_uuid":"c12367de-a19c-4d1a-82a6-26d666f3aa4d","_cell_guid":"7462222d-3dd4-4a5b-948e-1321cdb1c70b","trusted":true}},{"cell_type":"code","source":"# from catboost import CatBoostRegressor\n\n\n# model_dict = {}\n\n# for target in targets:\n#     models = []\n    \n#     for fold in range(CFG.n_splits):\n\n#         X_train_cv = train[train[\"fold\"] != fold].drop(columns=drop_columns)\n#         y_train_cv = train[train[\"fold\"] != fold][target]\n\n#         X_eval_cv = train[train[\"fold\"] == fold].drop(columns=drop_columns)\n#         y_eval_cv = train[train[\"fold\"] == fold][target]\n\n#         model = CatBoostRegressor(\n#             learning_rate = 0.048,\n#             depth = 4,\n#             min_data_in_leaf = 34,\n#             iterations = 10000,\n#             early_stopping_rounds = 300,\n#             task_type ='CPU',\n#             loss_function ='RMSE'\n#           )\n#         model.fit(X_train_cv, \n#                   y_train_cv, \n#                   eval_set=[(X_eval_cv, y_eval_cv)],\n#                   verbose=False)\n#         models.append(model)\n\n#     model_dict[target] = models","metadata":{"_uuid":"8048f325-2efe-4fa8-87a5-f5c5f588ae13","_cell_guid":"3ed28d86-ca3b-41d9-b7d5-d6bb666c3966","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:45:52.148595Z","iopub.execute_input":"2023-10-11T05:45:52.149063Z","iopub.status.idle":"2023-10-11T05:45:52.154328Z","shell.execute_reply.started":"2023-10-11T05:45:52.149028Z","shell.execute_reply":"2023-10-11T05:45:52.153539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CV Score","metadata":{"_uuid":"98f6da51-eac5-4cf4-9697-83564bb1686a","_cell_guid":"cda1906b-2a0b-40a2-b30e-a60d6ddbefe3","trusted":true}},{"cell_type":"code","source":"# cv\nrmses = []\n\nfor target in targets:\n    models = model_dict[target]\n\n    preds = []\n    trues = []\n    \n    for fold, model in enumerate(models):\n        X_eval_cv = train[train[\"fold\"] == fold].drop(columns=drop_columns)\n        y_eval_cv = train[train[\"fold\"] == fold][target]\n\n        pred = model.predict(X_eval_cv)\n\n        trues.extend(y_eval_cv)\n        preds.extend(pred)\n        \n    rmse = np.sqrt(mean_squared_error(trues, preds))\n    print(f\"{target}_rmse : {rmse}\")\n    rmses = rmses + [rmse]\n\nprint(f\"mcrmse : {sum(rmses) / len(rmses)}\")","metadata":{"_uuid":"fe801e88-07de-40b8-a029-6177ef02857e","_cell_guid":"383ded19-171e-45ee-95d7-378f4a643c22","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:45:52.156297Z","iopub.execute_input":"2023-10-11T05:45:52.156579Z","iopub.status.idle":"2023-10-11T05:45:52.299873Z","shell.execute_reply.started":"2023-10-11T05:45:52.156545Z","shell.execute_reply":"2023-10-11T05:45:52.298888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predict","metadata":{"_uuid":"9ce662c9-a68a-49eb-a2cd-f66a8b72ab44","_cell_guid":"afffdfe0-2e6c-4d3b-b3f2-529f45582966","trusted":true}},{"cell_type":"code","source":"drop_columns = [\n                #\"fold\", \n                \"student_id\", \"prompt_id\", \"text\", \"fixed_summary_text\",\n                \"prompt_question\", \"prompt_title\", \n                \"prompt_text\",\n                \"input\",\"title\", \"author\", \"description\", \"genre\", \"grade\"\n               ] + [\n                f\"content_pred_{i}\" for i in range(CFG.n_splits)\n                ] + [\n                f\"wording_pred_{i}\" for i in range(CFG.n_splits)\n                ]","metadata":{"_uuid":"4b954b0b-792d-4467-bd3f-dae294f4015e","_cell_guid":"db7c2e79-7a0b-4150-afa7-d8a23c3bb659","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:45:52.301332Z","iopub.execute_input":"2023-10-11T05:45:52.302208Z","iopub.status.idle":"2023-10-11T05:45:52.308206Z","shell.execute_reply.started":"2023-10-11T05:45:52.302174Z","shell.execute_reply":"2023-10-11T05:45:52.30744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_dict = {}\nfor target in targets:\n    models = model_dict[target]\n    preds = []\n\n    for fold, model in enumerate(models):\n        X_eval_cv = test.drop(columns=drop_columns)\n        \n#         if str(type(model))==\"<class 'xgboost.sklearn.XGBRegressor'>\":\n#             print(\"pred xgb. rename cols\")\n#             X_eval_cv = X_eval_cv.rename({'content':'content_pred','wording':'wording_pred'},axis=1)\n#         elif str(type(model))==\"<class 'catboost.core.CatBoostRegressor'>\":\n#             print(\"pred cat. rename cols\")\n#             X_eval_cv = X_eval_cv.rename({'content':'content_pred','wording':'wording_pred'},axis=1)\n#         else:\n#             print(\"pred lgb\")\n\n        pred = model.predict(X_eval_cv)\n        preds.append(pred)\n    \n    pred_dict[target] = preds","metadata":{"_uuid":"ddf531f6-b9bd-4a47-a0c4-f0ed9a18b781","_cell_guid":"1556213d-593d-4739-b7a7-1031136db0ad","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:45:52.309503Z","iopub.execute_input":"2023-10-11T05:45:52.310522Z","iopub.status.idle":"2023-10-11T05:45:52.337871Z","shell.execute_reply.started":"2023-10-11T05:45:52.310484Z","shell.execute_reply":"2023-10-11T05:45:52.337066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for target in targets:\n    preds = pred_dict[target]\n    for i, pred in enumerate(preds):\n        test[f\"{target}_pred_{i}\"] = pred\n        \n    test[f\"{target}_lgbm\"] = test[[f\"{target}_pred_{fold}\" for fold in range(CFG.n_splits)]].mean(axis=1)","metadata":{"_uuid":"198ca966-abf5-4564-baef-e7b4391ed16b","_cell_guid":"0d33cf2e-55f2-422a-b2f1-70af688db0c4","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:45:52.339301Z","iopub.execute_input":"2023-10-11T05:45:52.339879Z","iopub.status.idle":"2023-10-11T05:45:52.350316Z","shell.execute_reply.started":"2023-10-11T05:45:52.339846Z","shell.execute_reply":"2023-10-11T05:45:52.349318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test","metadata":{"_uuid":"54cd8f6d-4de3-4784-bfb2-d2b4255d0bb9","_cell_guid":"b58368b0-3c53-4126-9e37-7ab2690ab600","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:45:52.351658Z","iopub.execute_input":"2023-10-11T05:45:52.352756Z","iopub.status.idle":"2023-10-11T05:45:52.375095Z","shell.execute_reply.started":"2023-10-11T05:45:52.352657Z","shell.execute_reply":"2023-10-11T05:45:52.374147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create Submission file","metadata":{"_uuid":"eb1d48e1-8fc5-49df-a133-6762689cfac5","_cell_guid":"0017569b-fe66-42b2-b08d-8fa16bed475a","trusted":true}},{"cell_type":"code","source":"test[[\"student_id\", \"content\", \"wording\"]]","metadata":{"execution":{"iopub.status.busy":"2023-10-11T05:45:52.376402Z","iopub.execute_input":"2023-10-11T05:45:52.376647Z","iopub.status.idle":"2023-10-11T05:45:52.386549Z","shell.execute_reply.started":"2023-10-11T05:45:52.376608Z","shell.execute_reply":"2023-10-11T05:45:52.385598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = [-0.05653549,  0.28765139,  0.51956897,  0.29386971, 0]\ntest['content'] =\\\n    test['content']*x[0]+\\\n    test_1st['content']*x[1]+\\\n    test_2nd['content']*x[2]+\\\n    test_zero['content']*x[3]+\\\n    test['content_lgbm']*x[4]","metadata":{"execution":{"iopub.status.busy":"2023-10-11T05:45:52.38789Z","iopub.execute_input":"2023-10-11T05:45:52.388602Z","iopub.status.idle":"2023-10-11T05:45:52.398744Z","shell.execute_reply.started":"2023-10-11T05:45:52.388571Z","shell.execute_reply":"2023-10-11T05:45:52.397876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = [0.25870747, 0.0424712 , 0.30386073, 0.39686579, 0]\ntest['wording'] =\\\n    test['wording']*x[0]+\\\n    test_1st['content']*x[1]+\\\n    test_2nd['content']*x[2]+\\\n    test_zero['content']*x[3]+\\\n    test['wording_lgbm']*x[4]","metadata":{"execution":{"iopub.status.busy":"2023-10-11T05:45:52.400258Z","iopub.execute_input":"2023-10-11T05:45:52.400648Z","iopub.status.idle":"2023-10-11T05:45:52.411631Z","shell.execute_reply.started":"2023-10-11T05:45:52.400566Z","shell.execute_reply":"2023-10-11T05:45:52.410815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission","metadata":{"_uuid":"688ca2ed-26e6-40aa-b8f8-65a757a1a5c8","_cell_guid":"300daba5-c627-41b6-a9a3-749036ede8b2","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:45:52.412882Z","iopub.execute_input":"2023-10-11T05:45:52.413803Z","iopub.status.idle":"2023-10-11T05:45:52.426825Z","shell.execute_reply.started":"2023-10-11T05:45:52.413772Z","shell.execute_reply":"2023-10-11T05:45:52.425858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test[[\"student_id\", \"content\", \"wording\"]]","metadata":{"_uuid":"8ab18089-48aa-482e-8bf7-e70011c0eb15","_cell_guid":"94493f90-e8fa-422d-8464-403c114a99dc","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:45:52.428288Z","iopub.execute_input":"2023-10-11T05:45:52.4292Z","iopub.status.idle":"2023-10-11T05:45:52.44567Z","shell.execute_reply.started":"2023-10-11T05:45:52.429165Z","shell.execute_reply":"2023-10-11T05:45:52.444611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test['content'] = (((test_1['content'] + test_2['content'] + test_3['content'])/3)+test['content'])/2\n# test['wording'] = (((test_1['wording'] + test_2['wording'] + test_3['wording'])/3)+test['wording'])/2","metadata":{"_uuid":"a5fb49e4-6adb-4a8b-a0e9-887b9e75c030","_cell_guid":"fdcd6a3d-2239-4efc-ac00-506fd63c2566","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:45:52.447291Z","iopub.execute_input":"2023-10-11T05:45:52.447835Z","iopub.status.idle":"2023-10-11T05:45:52.454089Z","shell.execute_reply.started":"2023-10-11T05:45:52.447801Z","shell.execute_reply":"2023-10-11T05:45:52.453225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test[[\"student_id\", \"content\", \"wording\"]]","metadata":{"execution":{"iopub.status.busy":"2023-10-11T05:45:52.455658Z","iopub.execute_input":"2023-10-11T05:45:52.456327Z","iopub.status.idle":"2023-10-11T05:45:52.465155Z","shell.execute_reply.started":"2023-10-11T05:45:52.456293Z","shell.execute_reply":"2023-10-11T05:45:52.464259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test['content'] = test['content']*0.6 + test_zero['content']*0.4\n# test['wording'] = test['wording']*0.6 + test_zero['wording']*0.4","metadata":{"execution":{"iopub.status.busy":"2023-10-11T05:45:52.466317Z","iopub.execute_input":"2023-10-11T05:45:52.467255Z","iopub.status.idle":"2023-10-11T05:45:52.475975Z","shell.execute_reply.started":"2023-10-11T05:45:52.467225Z","shell.execute_reply":"2023-10-11T05:45:52.475115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test[[\"student_id\", \"content\", \"wording\"]]","metadata":{"execution":{"iopub.status.busy":"2023-10-11T05:45:52.47735Z","iopub.execute_input":"2023-10-11T05:45:52.477877Z","iopub.status.idle":"2023-10-11T05:45:52.491039Z","shell.execute_reply.started":"2023-10-11T05:45:52.477847Z","shell.execute_reply":"2023-10-11T05:45:52.490075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test[[\"student_id\", \"content\", \"wording\"]].to_csv(\"submission.csv\", index=False)","metadata":{"_uuid":"16f1ec7e-3cd8-4029-a15b-ba06f42a15bc","_cell_guid":"b036749f-28a4-4763-9aa6-b1336176776e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-11T05:45:52.492331Z","iopub.execute_input":"2023-10-11T05:45:52.492841Z","iopub.status.idle":"2023-10-11T05:45:52.505011Z","shell.execute_reply.started":"2023-10-11T05:45:52.49281Z","shell.execute_reply":"2023-10-11T05:45:52.504053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Summary\n\nCV result is like this.\n\n| | content rmse |wording rmse | mcrmse | LB| |\n| -- | -- | -- | -- | -- | -- |\n|baseline| 0.494 | 0.630 | 0.562 | 0.509 | [link](https://www.kaggle.com/code/tsunotsuno/debertav3-baseline-content-and-wording-models)|\n| use title and question field | 0.476| 0.619 | 0.548 | 0.508 | [link](https://www.kaggle.com/code/tsunotsuno/debertav3-w-prompt-title-question-fields) |\n| Debertav3 + LGBM | 0.451 | 0.591 | 0.521 | 0.461 | [link](https://www.kaggle.com/code/tsunotsuno/debertav3-lgbm-with-feature-engineering) |\n| Debertav3 + LGBM with spell autocorrect | 0.448 | 0.581 | 0.514 | 0.459 |nogawanogawa's original code\n| Debertav3 + LGBM with spell autocorrect and tuning | 0.442 | 0.566 | 0.504 | 0.453 | this notebook |\n\nThe CV values improved slightly, and the LB value is improved.","metadata":{"_uuid":"2f669db0-4acd-406b-9a6b-fd9df34db3ba","_cell_guid":"dd03bc7d-d956-43d6-8bbb-c49da80d0126","trusted":true}},{"cell_type":"code","source":"","metadata":{"_uuid":"dba4f705-de35-494a-94e1-0b5f9b1732b9","_cell_guid":"df7255d1-d60b-4194-8153-a185a490c00c","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"09517576-b29e-4984-9f04-4f748c3ed4bb","_cell_guid":"7e7062aa-db24-40d1-9543-297a7bc3d7a9","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"2463fc6f-4e7b-41c0-8f4a-a73501d5cd07","_cell_guid":"2a34f9c4-d96a-480a-90bb-ab3af1cca8d1","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}