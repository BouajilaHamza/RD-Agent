[
    {
        "idea": "Ensemble Learning",
        "method": "Aggregate predictions from multiple models (LGBM and N-Beats) to improve accuracy.",
        "context": "The notebook combines bottom-level forecasts using a LightGBM model and top-level forecasts using N-Beats, and averages these predictions for the final result.",
        "hypothesis": {
            "problem": "Forecasting sales data over multiple hierarchical levels.",
            "data": "Hierarchical, time-series sales data with different levels of aggregation.",
            "method": "Combining predictions from models that operate on different levels of the hierarchy.",
            "reason": "The data in the scenario is hierarchical, and different models capture patterns at different aggregation levels, improving overall forecast accuracy."
        }
    },
    {
        "idea": "Feature Engineering",
        "method": "Use datetime and price features to predict the probability of an item being bought.",
        "context": "The notebook creates features based on datetime and price to train a LightGBM model for bottom-level forecasts.",
        "hypothesis": {
            "problem": "Predicting daily sales at the item level.",
            "data": "Time-dependent sales data with price variations.",
            "method": "Datetime and price features are pivotal for capturing seasonal and promotional effects.",
            "reason": "The scenario involves time-series data where sales patterns are influenced by time and price factors, making these features crucial."
        }
    },
    {
        "idea": "Custom Loss Function",
        "method": "Apply a multiplier in the custom loss function to adjust for trends.",
        "context": "A multiplier is used in the custom loss of the bottom-level LGBM models to help adjust for trends or other effects.",
        "hypothesis": {
            "problem": "Adjusting forecasts to better capture trends in sales data.",
            "data": "Sales data that exhibit trends over time.",
            "method": "Custom loss functions allow for fine-tuning models to better capture temporal trends.",
            "reason": "The scenario involves data with inherent trends, and the custom loss function helps in adjusting predictions accordingly."
        }
    },
    {
        "idea": "Hierarchical Time-Series Forecasting",
        "method": "Aggregate bottom-level forecasts to higher levels for alignment with top-level forecasts.",
        "context": "The notebook aggregates bottom-level 'probability draws' up to levels 1-5 for alignment with top-level forecasts.",
        "hypothesis": {
            "problem": "Forecasting across multiple hierarchical levels.",
            "data": "Hierarchical data with natural groupings (e.g., state, store).",
            "method": "Aggregating forecasts helps ensure consistency across hierarchical levels.",
            "reason": "Hierarchical structures require aggregation to maintain consistency across different levels, which is critical for accurate forecasting."
        }
    },
    {
        "idea": "Model Diversity in Ensemble",
        "method": "Use N-Beats with different settings to create model diversity in ensemble predictions.",
        "context": "N-Beats models are trained with different epochs settings to increase diversity in ensembles.",
        "hypothesis": {
            "problem": "Reducing overfitting and improving generalization in ensemble models.",
            "data": "Time-series data with potential overfitting risks due to noise or complexity.",
            "method": "Different model settings can introduce diversity, improving robustness.",
            "reason": "Diverse models reduce the risk of overfitting to noise or specific patterns, which is beneficial in complex time-series forecasting."
        }
    },
    {
        "idea": "Validation Data Handling",
        "method": "Fill validation rows with known ground truth values to prevent scoring issues.",
        "context": "The notebook sets submission validation values to ground truth as a precautionary measure against leaderboard scoring issues.",
        "hypothesis": {
            "problem": "Ensuring accurate validation scoring when ground truth is known.",
            "data": "Validation data with available ground truth values.",
            "method": "Using known ground truth ensures accurate validation results.",
            "reason": "When validation ground truth is available, using it prevents scoring discrepancies that could arise from incorrect predictions."
        }
    }
]